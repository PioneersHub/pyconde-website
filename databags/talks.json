{"talks": [{"title": "Building a Self-Hosted MLOps Platform with Kubernetes", "abstract": "Many managed MLOps platforms, while convenient, often fall short in providing flexibility, requiring complex integrations, and causing vendor lock-in. In this talk, we\u2019ll share our experience transitioning from managed MLOps tools to a self-hosted solution built on Kubernetes. We\u2019ll focus on how we leveraged open-source tools like Feast, MLflow, and Ray to build a more flexible, scalable, and customizable platform that is now in use at Rewe Digital. By migrating to this self-hosted architecture, we gained greater control over our ML pipelines, reduced our dependency on third-party services, and created a more adaptable infrastructure for our ML workloads.", "full_description": "Many managed MLOps platforms, while convenient, often fall short in providing flexibility, requiring complex integrations, and causing vendor lock-in. In this talk, we\u2019ll share our experience transitioning from managed MLOps tools to a self-hosted solution built on Kubernetes. We\u2019ll focus on how we leveraged open-source tools like Feast, MLflow, and Ray to build a more flexible, scalable, and customizable platform that is now in use at Rewe Digital. By migrating to this self-hosted architecture, we gained greater control over our ML pipelines, reduced our dependency on third-party services, and created a more adaptable infrastructure for our ML workloads.\r\n\r\nTalk Outline: \r\n\r\n1. Introduction (5 minutes):\r\n- The challenges of using managed MLOps platforms: vendor lock-in, integration complexity, and lack of flexibility.\r\n- Why transitioning to a self-hosted solution on Kubernetes can be beneficial.\r\n\r\n2. Proposed Solution (10 minutes):\r\n- Why Kubernetes for MLOps?\r\n- How open-source tools like Feast, MLflow, and Ray come together to form the core of a robust self-hosted MLOps stack.\r\n- Benefits of building a flexible, scalable platform that fits your needs.\r\n\r\n3. Building the Platform (10 minutes):\r\n- Practical steps for setting up and configuring Feast, MLflow, and Ray on Kubernetes.\r\n- Integration strategies and how to manage pipelines, model tracking, and feature storage.\r\n\r\n4. Lessons Learned and Q&A (5 minutes):\r\n- Challenges and takeaways during the migration process\r\n- Q&A", "code": "3CYZUH", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/3CYZUH.png", "speaker_names": "Josef Nagelschmidt", "speakers": "\n### Josef Nagelschmidt\n\nI'm Josef, an econometrician turned ML engineer. With a strong background in statistics and causal inference, I have developed my skills through rigorous work at institutions such as the University of Bonn and UC Berkeley, but also through the design and implementation of ML solutions at the Rewe Group. My passion lies in reducing model and ecosystem complexity, enhancing interpretability, and bridging the gap between academia and production settings in the context of machine learning. I believe that if we do not establish reliable machine learning systems, we risk failing to harness the immense potential they offer for humanity.\n", "track": "MLOps & DevOps"}, {"title": "Hands-On LLM Security: Attacks and Countermeasures You Need to Know!", "abstract": "Dive into the vulnerabilities of LLMs and learn how to prevent them\r\nFrom prompt injection to data poisoning, we\u2019ll demonstrate real-world attack scenarios and reveal essential countermeasures to safeguard your applications.", "full_description": "The rapid increase in usage of large language models (LLMs) in the last years makes it necessary to address the specific security risks of LLMs.\r\nIn this presentation, we will examine typical vulnerabilities in LLMs from a practical perspective. Starting with a systematic overview, we will use a specific demo app to illustrate the various attack scenarios. Vulnerabilities like prompt injection, data poisoning and system prompt leakage will be explained and demonstrated as well as attacks on RAG and agent implementations.\r\nIn addition to a basic introduction and a presentation of specific vulnerabilities, the talk also presents suitable countermeasures and general best practices for the use of LLMs in productive applications.\r\n\r\nWhat to expect? \r\nAttending this talk, you learn which vulnerabilities need to be considered when using and integrating LLMs. You will see how specific attacks work and what risks are associated with them. You will also learn which countermeasures are suitable and how these can be implemented technically.", "code": "3DSU8V", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/3DSU8V.png", "speaker_names": "Clemens H\u00fcbner, Florian Teutsch", "speakers": "\n### Clemens H\u00fcbner\n\nFor more than ten years, Clemens H\u00fcbner has been working at the interface between software and security. After roles as a software developer and in penetration testing, he joined inovex in 2018 as a software security engineer. Today, he supports development projects at the conception and implementation level and is a trainer both in-house and for clients. He advises on secure development processes and DevSecOps. As speaker, he is invited to national and international conferences.\n\n### Florian Teutsch\n\nFlorian Teutsch possesses extensive knowledge in the field of generative AI and works as a Machine Learning Engineer at inovex. After successfully completing his studies in Information Systems at the University of Cologne in 2020, he worked for two years as a Data Scientist on an innovative AI-based image search. Since joining inovex, he has been able to continuously expand his practical experience in the field of generative AI.\n", "track": "Security"}, {"title": "Machine Learning Models in a Dynamic Environment", "abstract": "\"We've only tested the happy path - now users are finding all sorts of creative ways to break the app.\"\r\n\r\nWhat is already a cause for headaches in traditional software engineering turns into a large challenge when the application is based on machine learning models: Data distribution may change from training phase to deployment. Even worse, humans interacting with the model may adjust their behaviour to the model making the gap between original training environment and deployment even larger. When deployed in a public environment the model may be exposed to users trying to game the system. When re-trained it may be exposed to users trying to poison the pool of training data.\r\n\r\nWe will take a tour of historic cases of models being gamed: What are the lessons we learnt a long time ago building e-mail spam filters? What happened when high search engine rankings started to be linked to monetary income? How can personalization and targeted advertising be exploited to influence public discourse? \r\n\r\n\u201c\u2026 it should be clear that improvements in communication tend to divide mankind \u2026\u201d by Harold Innis in Changing Concepts of Time\r\n\r\nThis keynote will turn interactive engaging the audience in sharing their stories on users playing interesting games with deployed models - including counter moves rolled out. \r\n\r\nIf we are to learn from IT security experience, one important ingredient to address these issues is a combination of collaboration and transparency - across organisations.", "full_description": "\"Collect data, choose an algorithm, train a model to match your target metric and deploy to production.\" ... sounds easy enough.\r\n\r\nBut what if user behaviour changes after the model was deployed? What if the deployment of the model itself causes a change in user behaviour? \r\n\r\nThis talk will look at examples for models changing user behaviour. In the interactive part the talk will collect stories from the audience.", "code": "3FUYVH", "state": "confirmed", "created": "2025-02-02", "social_card_image": "/static/media/social/talks/3FUYVH.png", "speaker_names": "Isabel Drost-Fromm", "speakers": "\n### Isabel Drost-Fromm\n\n\n", "track": "Plenary"}, {"title": "Reasonable AI", "abstract": "The relationship between humans and machines, especially in the context of Artificial Intelligence (AI), is shaped by hopes, concerns, and moral questions. On the one hand, advances in AI offer great promise: it can help us solve complex problems, improve healthcare, streamline workflows, and much more. Yet, at the same time, there are legitimate concerns about the control over this technology, its potential impact on jobs and society, and ethical issues related to discrimination and the loss of human autonomy. In the talk I shall will explore and illustrate the complex tension between innovation and moral responsibility in AI research.", "full_description": "The relationship between humans and machines, especially in the context of Artificial Intelligence (AI), is shaped by hopes, concerns, and moral questions. On the one hand, advances in AI offer great promise: it can help us solve complex problems, improve healthcare, streamline workflows, and much more. Yet, at the same time, there are legitimate concerns about the control over this technology, its potential impact on jobs and society, and ethical issues related to discrimination and the loss of human autonomy. In the talk I shall will explore and illustrate the complex tension between innovation and moral responsibility in AI research.", "code": "3MNGN8", "state": "confirmed", "created": "2025-02-02", "social_card_image": "/static/media/social/talks/3MNGN8.png", "speaker_names": "Kristian Kersting", "speakers": "\n### Kristian Kersting\n\nKristian Kersting is co-director of the Hessian Center for AI (hessian.AI), head of research at the German Research Center for AI / Darmstadt, and professor of AI and machine learning at TU Darmstadt. After his PhD at the Univrsity of  Freiburg in 2006, he was with the MIT, Fraunhofer IAIS, the University of Bonn and the TU Dortmund. He is an AAAI, EurAI and ELLIS Fellow, coauthor of the popoular science book \u201cWie Maschinen Lernen\u201d, winner of the \u201cGerman AI Prize\u201d, member of the Mainz Academy of Sciences and Literature and seed investor at Aleph Alpha, one of Europe's AI hopes. in collaboration with Aleph Alpha Research, he also runs the collaboration lab 1141 at TU Darsmtadt on safe and transparent generative AI. He had a regular AI column in the Welt (am Sonntag).\n", "track": "Plenary"}, {"title": "You don\u2019t think about your Streamlit app optimization until you try to deploy it to the cloud", "abstract": "Building Streamlit apps is easy for Data Scientists - but when it\u2019s time to deploy them to the cloud, challenges like slow model loading, scalability, and security can become major hurdles. This talk bridges two perspectives: the Data Scientist who builds the app and the MLOps engineer who deploys it. We'll dive into optimizing model loading from Hugging Face Hub, implementing features like autoscaling and authentication, and securing your app against potential threats. By the end of this talk, you\u2019ll be ready to design Streamlit apps that are functional and deployment-ready for the cloud.", "full_description": "#### Talk Outline:\r\n\r\n1. Introduction\r\n   - The disconnect: challenges when transitioning a Streamlit app from development to deployment.  \r\n   - Why deployment considerations should influence app design.  \r\n\r\n2. Optimizing model loading from HuggingFace hub \r\n   - Challenges:  \r\n     - Large model sizes slowing down app performance.  \r\n     - Inefficient loading processes increasing costs and user wait times.  \r\n   - Solutions:  \r\n     - Using Streamlit caching to reuse loaded models across sessions.  \r\n     - Preloading models during image build.\r\n     - Deploying models and calling them as APIs\r\n   - MLOps Perspective: How optimized model loading reduces deployment complexity and cloud costs.  \r\n\r\n3. AWS deployment considerations: autoscaling, authentication, and security  \r\n   - Autoscaling:  \r\n     - Challenges: Handling variable user traffic without incurring unnecessary costs.  \r\n     - Solutions:  \r\n       - Using Fargate with ECS for containerized apps with auto-scaling policies.  \r\n       - Setting thresholds to scale instances based on traffic and resource utilization.  \r\n       - Optimizing cost-performance balance with reserved vs. spot instances.  \r\n\r\n   - Authentication:  \r\n     - Challenges: Providing a secure and user-friendly authentication mechanism.  \r\n     - Solutions:  \r\n       - Integrating AWS Cognito for user management.  \r\n       - Adding role-based access control to limit app functionality based on user roles.  \r\n\r\n   - Security:  \r\n     - Challenges: Protecting the app from attacks and unauthorized access.  \r\n     - Solutions:  \r\n       - Using AWS Web Application Firewall (WAF) to block malicious traffic.  \r\n       - Configuring CloudFront to protect against DDoS attacks and improve performance.  \r\n       - Setting up HTTPS with Route 53 and TLS certificates for secure connections.  \r\n   - MLOps Perspective: Balancing simplicity and scalability in app deployment.  \r\n\r\n4. Secrets Storage  \r\n   - Challenges: Hardcoding sensitive credentials into the app.  \r\n   - Solutions:  \r\n     - Using AWS Secrets Manager or Parameter Store for secure secrets management.  \r\n     - Employing environment variables for flexible app configuration.  \r\n   - MLOps Perspective: How to ensure security without complicating deployment workflows.  \r\n\r\n5. Key Takeaways \r\n   - Data Scientist\u2019s Perspective:  \r\n     - Why it\u2019s critical to consider performance, scalability, authentication, and security during app development.  \r\n   - MLOps Perspective:  \r\n     - How to simplify deployment while ensuring performance and security.  \r\n   - Encouraging collaboration between Data Scientists and MLOps engineers for smoother deployment processes.  \r\n\r\n#### What you will learn:  \r\n- How to efficiently load Hugging Face models in Streamlit apps to reduce costs and improve performance.  \r\n- How to design apps with AWS autoscaling to handle variable traffic seamlessly.  \r\n- Best practices for implementing user authentication with AWS Cognito.  \r\n- How to secure your Streamlit app using cloud services.\r\n- Best practices for secure secrets management in Streamlit apps.\r\n- How to approach Streamlit app development with deployment in mind.", "code": "3VYSMS", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/3VYSMS.png", "speaker_names": "Darya Petrashka", "speakers": "\n### Darya Petrashka\n\nDarya Petrashka is a Data Scientist at SLB with 5 years of experience, focusing on supply chain projects in data analysis, NLP, and generative AI. She is passionate about using data for problem-solving, with a strong interest in classical machine learning, NLP, and AWS services. An AWS Community Builder and Authorized Instructor, Darya actively shares her expertise through public speaking at various industry events, including AWS Community Days, AWS Cloud Day, and PyCon. A dedicated learner, Darya continually hones her skills by participating in workshops, courses, and tech schools.\n", "track": "MLOps & DevOps"}, {"title": "Navigating the Security Maze: An Interactive Adventure", "abstract": "How to integrate security into a software development project? Without jeopardizing timeline or budget?  You decide! \r\nThis interactive session covers crucial decisions for software security, and the audience decides how the story ends...", "full_description": "Although DevSecOps has been a trend topic for years, it is still far from being a solved problem.\r\n\r\nThis interactive session brings the challenges of security in the development process to life: Participants are confronted with several scenarios from everyday project work and their decisions help shape the further course of the presentation. They have to reconcile security requirements with budget, development speed and user-friendliness and bring the project safely from the idea to live operation.\r\n\r\nThe session covers the entire development process, but each run is different as the audience decides the course of the story via online-voting: How to proceed with the development project and think about security at the same time?", "code": "3WLDMQ", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/3WLDMQ.png", "speaker_names": "Clemens H\u00fcbner", "speakers": "\n### Clemens H\u00fcbner\n\nFor more than ten years, Clemens H\u00fcbner has been working at the interface between software and security. After roles as a software developer and in penetration testing, he joined inovex in 2018 as a software security engineer. Today, he supports development projects at the conception and implementation level and is a trainer both in-house and for clients. He advises on secure development processes and DevSecOps. As speaker, he is invited to national and international conferences.\n", "track": "Security"}, {"title": "Bridging the gap: unlocking SAP data for data lakes with Python and PySpark via SAP Datasphere", "abstract": "SAP's data often remains locked away, hindering the creation of a complete data picture. This talk presents a hands-on proof of concept leveraging SAP Datasphere, Python and PySpark to bridge an Azure-based, data mesh-inspired open data lake with a centralized SAP BI environment. \r\n\r\nThis presentation will delve into the architecture of SAP Datasphere and its integration interfaces with Python. It will explore network integration, authentication, authorization and resource management options, as well as data integration patterns. The presentation will summarize the evaluated features and limitations discovered during the PoC.", "full_description": "In many enterprises relying on SAP ERP systems, a wealth of valuable master data remains trapped within a closed ecosystem. This creates significant obstacles when striving for a comprehensive, 360\u00b0 view, especially when integrating with modern, open data lakes built on platforms like Azure and designed around data mesh principles. This talk presents a practical PoC that tackles this challenge head-on, utilizing SAP Datasphere as the key integration point.\r\n\r\nOutline:\r\n\r\n1. The challenge: navigating sap's data silos and the pursuit of a unified view\r\n* The section outlines the enterprise data landscape of RATIONAL where valuable master data resides within SAP\u2019s traditionally closed ecosystem, hindering data democratization and the creation of a comprehensive, 360\u00b0 operational view. This scenario is quite common - at least for German manufacturing companies. This situation is frequently encountered, particularly among German manufacturers.\r\n* The inherent conflict between the open, distributed nature of data lakes (especially those built on data mesh principles) and the centralized, closed nature of traditional SAP BI environments is discussed.\r\n\r\n2. Solution overview: leveraging sap datasphere as the integration layer\r\n* An introduction to sap datasphere and its capabilities is provided, with a focus on its ability to connect with non-SAP systems.\r\n* This part explains how datasphere was chosen as the central integration layer for the proof of concept and its role in enabling bi-directional data flow between SAP and the open data lake.\r\n\r\n3. Architecture of SAP Datasphere\r\n* Introduction in architecture of SAP Datasphere and role of underlying SAP HANA database\r\n* Explanation of openSQL schema as key integration option\r\n\r\n4. Security first: exploring network integration, authentication and authorization options\r\n* This section details the evaluation of network connectivity options between the Azure services like Azure Databricks, PostgresQL, ADLS and SAP Datasphere\r\n* The methods used to authenticate Python and Pypark to SAP datasphere are explained\r\n* The implementation and evaluation of data authorization mechanisms within SAP Datasphere are described\r\n\r\n5. Python and PySpark integration\r\n* Available interfaces for python integration (ODBC/JDBC, OData), their features and limitations\r\n* Explanation of practical data integration patterns implemented within the poc for extracting data from sap and loading it into the data lake for full and delta load scenarios\r\n\r\n5. Reflecting PoC: summary and key learnings\r\n* This section summarizes the core findings and lessons learned from the PoC, particularly regarding security and software quality best practices\r\n* A hint for the SAP open data alliance launched in 2023\r\n\r\nMain takeaways:\r\n* An understanding of SAP Datasphere's architecture and its potential for integrating non-SAP, open-source technologies like Python and PySpark\r\n* Knowledge of current features and limitations of SAP Datasphere in the area of data integration with the open source world", "code": "7CL3KS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/7CL3KS.png", "speaker_names": "Rostislaw Krassow", "speakers": "\n### Rostislaw Krassow\n\nRostislaw, a data architect at RATIONAL AG, specializes in distributed databases, the Apache Hadoop ecosystem and Azure cloud. He leverages his expertise to oversee the company's Data & Analytics platform, where his daily work involves reconciling diverse stakeholder perspectives to deliver optimal solutions.\n", "track": "Data Handling & Engineering"}, {"title": "Reinventing Streamlit", "abstract": "Dreaming of creating sleek, interactive web apps with just Python? Streamlit is great for dashboards, but what if your needs go beyond that? Discover how Reflex.dev, a cutting-edge full-stack Python framework, lets you level up from dashboards to full-fledged web apps!", "full_description": "Have you ever wished you could build sleek, interactive web apps using just Python? Maybe you\u2019ve tried Streamlit and loved its simplicity. But maybe you also had the feeling that your dashboard is no longer a dashboard and your needs have outgrown Streamlit's data model.\r\n\r\nIn this talk, I\u2019ll introduce Reflex.dev, a powerful Python framework that makes web development effortless. Reflex combines the ease of Python with the flexibility of React, enabling you to create full-stack, interactive apps quickly.\r\n\r\nWe\u2019ll cover the basics: what Reflex.dev is and how it stacks up against familiar frameworks. Then, we\u2019ll dive into building a Streamlit-inspired app from scratch in Reflex.dev by creating an API compatibility wrapper. Along the way, I\u2019ll show you how Reflex can:\r\n\r\n- Help you build dynamic, shareable web apps with only Python.\r\n- Smoothly transition your Streamlit app into a stateful Reflex app.\r\n- Make the whole react ecosystem accessible.\r\n- Help testing your application.\r\n\r\nNo web development experience? No problem. This talk is for anyone who wants to create web apps without diving into JavaScript. We\u2019ll stick to Python and start from the ground up.\r\n\r\nBy the end, you\u2019ll leave with a working Streamlit clone and a powerful new tool in your Python arsenal. Let\u2019s make web development fun again!", "code": "7CXSPN", "state": "confirmed", "created": "2024-12-16", "social_card_image": "/static/media/social/talks/7CXSPN.png", "speaker_names": "Malte Klemm", "speakers": "\n### Malte Klemm\n\nMalte is a seasoned Data Engineer with over 10 years at Blue Yonder, where he recently worked on  company's forecasting service. Holding a PhD from the University of Hertfordshire's Adaptive Systems Research Group, he explored Information Theory in the context of Multi-Agent Systems. Beyond tech, Malte is passionate about great UX, typography, and baking the perfect loaf of bread.\n", "track": "Django & Web"}, {"title": "Deploy RAG Applications Using Docker: A Step-by-Step Guide", "abstract": "Docker is a game-changer for deploying AI applications, offering portable and consistent environments across platforms. In this tutorial, we\u2019ll explore how to build and deploy a Retrieval-Augmented Generation (RAG) application using Docker. RAG applications combine data retrieval with generative AI to produce contextually relevant and accurate responses, making them powerful tools for interactive Q&A systems.\r\n\r\nWe will build a document-based Q&A application that allows users to upload files, retrieve context from them, and answer questions interactively. We will use LlamaIndex to build the RAG pipeline and use both open-source LLM from Groq and closed-source LLM, such as OpenAI models, for LLM access. This tutorial will guide you through the entire lifecycle\u2014from building the app to deploying it using Docker on Hugging Face Spaces. Whether you\u2019re new to Docker or experienced with RAG, this guide offers a hands-on approach to deploying scalable and efficient AI solutions.", "full_description": "Retrieval-Augmented Generation (RAG) applications are reshaping AI by combining real-time data retrieval with large language models to generate accurate, dynamic, and context-aware responses. This tutorial provides a comprehensive, step-by-step guide to building and deploying a RAG application using Docker. Attendees will learn how to create portable and consistent environments that simplify deployment, ensure compatibility, and streamline operations for RAG workflows.\r\n\r\nThe tutorial will begin with Setting Up the Environment, including installing Docker, creating a project directory, and managing API keys securely with .env files. Next, attendees will learn how to Build the RAG Application by writing Python scripts to integrate file parsing, embedding generation, and querying LLMs. We will also implement key functions for handling uploaded files, generating context-aware responses, and creating a Gradio-based user interface for seamless interaction.\r\nIn the Creating the Dockerfile section, attendees will package the application into a Docker image by writing a Dockerfile to automate setup and expose the application interface. This is followed by Building and Running the Docker Image, where participants will learn to test the application locally by running it as a Docker container, using Docker commands to manage and troubleshoot the setup.\r\n\r\nThe tutorial will then cover Deploying to Hugging Face Spaces, a beginner-friendly cloud platform for hosting Docker-based applications. We will demonstrate how to set up a Hugging Face Space, configure API key secrets, and automate the deployment process for the RAG application. Finally, attendees will learn best practices for Monitoring and Optimizing the Application. The session will also include tips for optimizing Docker configurations to reduce image size and improve performance.\r\n\r\nThis tutorial is tailored for developers, data scientists, and ML engineers interested in deploying scalable and efficient RAG applications. Whether you're new to Docker or have experience with LLM-based workflows, this guide offers actionable insights and practical skills to streamline deployment processes. By the end of the session, participants will have a fully functional RAG application running in the cloud and the confidence to deploy similar solutions across various environments.", "code": "7FV8B3", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/7FV8B3.png", "speaker_names": "Brain Aboze", "speakers": "\n### Brain Aboze\n\nAboze Brain John is a Data Scientist with extensive experience in Data Science and Analytics, Product Research, and Technical Writing, Brain has successfully executed end-to-end data analytics projects. His expertise spans data collection, exploration, transformation/wrangling, modeling, and deriving actionable business insights, providing knowledge leadership in these areas.\r\n\r\nBrain has authored multiple articles on Artificial Intelligence and Software Engineering. His master\u2019s dissertation at the university of Sunderland involved extensive research on Large Language Models (LLMs) and LLM agents, reflecting his deep engagement with cutting-edge AI technologies.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Writing reliable software while depending on hazardous APIs", "abstract": "As we develop business critical software, we often need to rely on external APIs to get the job done. And all services are not born equal: although the ideal world would provide well operated APIs with over-met service levels, the real world is usually way worse than that. Timeouts, HTTP errors, cascading failures, unclear or changing contracts, approximate protocol implementations ... And even the oh-so-human bad faith while trying to pinpoint the root cause... Most of us have written hacks to handle commonly seen failures, from the quick and dirty implementation to well thought resilience patterns implementation, but this is usually hard to do correctly, and rarely a business priority to invest the correct amount of time and money on the topic. We'll present the options, both including direct dependencies (not framework dependant, although some families can emerge (async/sync ...)) and including a service/proxy based approach.", "full_description": "The two most common causes for software failure are, in order, human errors then external services. Working extensively with external APIs, we often encounter tricky issues in maintaining the responsiveness of our end-user services (both in terms of speed, but also plain availability). Many teams are addressing those issues on a case-by-case basis, most often using a homemade patchwork of external libraries and failing cases, and we used to do the same. Over time, we have come to rethink our approach to this problem.\r\n\r\nWe will present the usual suspects (and their consequences) we're usually facing: timeouts, HTTP errors, cascading failures, unclear or changing contracts, and the difficulty of forensic analysis after an incident occurs when the root cause stems from external data or calls.\r\n\r\nThen, we'll show various approaches we use or have seen be used by teams of different sizes.\r\n\r\nWe'll finish by presenting an innovative approach delegating the issues to a forward proxy so that the development team can both avoid having to spend time on reinventing the resilience and reliability patterns, while providing them the tools to act quickly when things go wrong.", "code": "7PDARV", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/7PDARV.png", "speaker_names": "Romain Dorgueil", "speakers": "\n### Romain Dorgueil\n\nMy first pieces of code ran on Atari ST machines. I had the chance to see the Internet baby say its first words while I was starting to get interested in building software. I'm a proud Software Craftsman and Open-Source Software advocate. I spend a few of my other lifes playing afro-cuban & jazz music, or playing some go games.\n", "track": "MLOps & DevOps"}, {"title": "From Trees to Transformers: GetYourGuide\u2019s Journey Towards Deep Learning for Ranking", "abstract": "GetYourGuide, a global marketplace for travel experiences, reached diminishing returns with its XGBoost-based ranking system. We switched to a Deep Learning pipeline in just nine months, maintaining high throughput and low latency. We iterated on over 50 offline models and conducted more than 10 live A/B tests, ultimately deploying a PyTorch transformer that yielded significant gains. In this talk, we will share our phased approach\u2014from a simple baseline to a high-impact launch\u2014and discuss the key operational and modeling challenges we faced. Learn how to transition from tree-based methods to neural networks and unlock new possibilities for real-time ranking.", "full_description": "GetYourGuide is a global online marketplace that helps travelers discover and book the best experiences. One of our core challenges is ensuring users always see the most relevant activities first\u2014a task historically powered by an XGBoost-based ranking system. However, as we continued refining our tree-based models, returns on incremental improvements began to plateau. To spark our next step change in performance, we decided to adopt Deep Learning.\r\n\r\nIn this talk, we will share how, in just nine months, we migrated our ranking pipeline to a Deep Learning architecture while maintaining tight latency and high-throughput requirements. We will walk through our phased approach, starting with a minimal viable model to confirm our production setup and gradually increasing its complexity. Along the way, we tested over 50 iterations offline and ran more than 10 live A/B tests to validate the impact on our customers. Ultimately, we rolled out a PyTorch transformer-based model with significant business impact. We will also discuss the main challenges we faced on the operational and modeling sides, how we overcame them, and the lessons we learned.\r\n\r\nYou will leave with practical strategies for transitioning from traditional tree-based models to neural networks in production. Join us to learn how to advance your machine-learning capabilities and unlock new dimensions of relevance and personalization for real-time ranking.", "code": "83QH37", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/83QH37.png", "speaker_names": "Theodore Meynard, Mihail Douhaniaris", "speakers": "\n### Theodore Meynard\n\nTheodore Meynard is a data science manager at GetYourGuide.He leads the evolution of their ranking algorithm, helping customers to find the best activities to book and locations to explore. Beyond work, he is one of the co-organizers of the Pydata Berlin meetup and the conference. \r\nWhen he is not programming, he loves riding his bike, looking for the best bakery-patisserie in town.\n\n### Mihail Douhaniaris\n\nMihail Douhaniaris is a Senior Data Scientist at GetYourGuide, a leading online travel marketplace for discovering and booking tours, attractions and activities worldwide. At GetYourGuide, Mihail works on the marketplace ranking algorithms, enhancing search relevance and enabling travelers discover experiences best tailored to their preferences. Beyond his role, Mihail is also passionate about responsible AI, ML observability, and deploying machine learning at scale.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Towards Intelligent Monitoring: Detecting Degraded Flame Torch Nozzles", "abstract": "Flame cutting is a method where metals are efficiently cut using precise control of the oxygen jet and consistent mixing of fuel gas. The condition of the nozzle is changing over time: deposits formed during the cutting process can degrade the flame quality, reducing the precision of the cut. Traditionally, nozzles suspected of wear are sent back for manual inspection, where experts evaluated the flame visually and audibly to determine whether repair or replacement is needed. This project leverages machine learning to optimize this process by analyzing acoustic emission data.", "full_description": "Flame cutting is a method where metals are efficiently cut using precise control of the oxygen jet and consistent mixing of fuel gas. The condition of the nozzle is changing over time: deposits formed during the cutting process degrade the flame quality, reducing the precision of the cut. Currently, nozzle testing relies on manual inspections, where experts visually and audibly evaluate the flame. This method is risky, as worn nozzles may remain in operation, posing safety hazards through ejection of high-temperature material. Additionally, the process is costly, especially when industrial equipment is damaged.\r\n\r\nLaboratory Evaluation: This section outlines the preliminary experiments aimed at assessing whether this sensor is suitable for distinguishing different machine states. The experiments focus on identifying the optimal sensor placement and analyzing how various machine states impact sensor readings. The design process for the laboratory experiments and the subsequent systematic data collection is shown. The results suggest that while detecting every machine state may not be feasible, the sensor shows promise in identifying degraded nozzles.\r\n\r\nData Preprocessing & Annotation: For a proof of concept, the raw acoustic emission data required manual labeling, as prior assessments depended on expert evaluations. Here, we utilized Label Studio, an annotation tool that streamlines the labeling process. \r\nModelling  & Feature Engineering: We extract features using statistical methods and transform the acoustic emission signals into the frequency domain through scipy, focusing on features in  the frequency domain.\r\n\r\nEvaluation: We discuss the approach for splitting the data, considering that multiple observations from the same nozzle are present. In a computational study, we evaluate the feature sets developed in the previous step using two different classification models: Support Vector Classifier and Multilayer Perceptron. This section explains how the experiments are computed and parallelized including the time required for execution.\r\n\r\nLastly, we discuss the dataset's limitations and the challenges faced during development. We also highlight steps taken to improve generalization and provide an outlook on future objectives, mostly aimed at a broader applicability of the models.", "code": "89BX8V", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/89BX8V.png", "speaker_names": "Dominik Falkner", "speakers": "\n### Dominik Falkner\n\nDominik Falkner completed his bachelor's degree in Software Engineering in 2018 and his master's degree in Data Science and Engineering with a specialization in Data Analysis in Production and Marketing at Hagenberg University of Applied Sciences in 2020. \r\n\r\nDuring his studies, he already worked on various software systems, including some for collecting and storing data. Since 2019, he has been employed by the RISC Software GmbH as a Data Scientist, working in customer and research projects. His interests and focus lie in the following disciplines:\r\n\r\n    Employing machine learning techniques.\r\n    The fusion of expert knowledge and machine learning methods\r\n    Predictive and prescriptive analytics\r\n    Design and architecture of software systems\r\n\r\nIn the course of his work as a Data Scientist, he mainly deals with time series analyses and classification settings from various industries. In 2022 he will start his PhD studies at the Institute for Formal Models and Verification at Johannes Kepler University in Linz.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Demystifying Design Patterns: A Practical Guide for Developers", "abstract": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation will empower you with a clear, structured understanding of these reusable solutions to address common programming challenges.\r\n\r\nWe'll delve into design patterns\u2019 key categories: Behavioral, Structural, and Creational, as well as explore their functionality and how they can be applied in your daily development workflow. For each category, we'll also explore a practical design pattern in detail and showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation.\r\n\r\nYou'll gain valuable insight into how these patterns can translate into real-world development scenarios, such as facilitating communication between objects (Behavioral), separating interfaces from implementation for flexibility (Structural), and enabling dynamic algorithm selection at runtime (Creational).", "full_description": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation will empower you with a clear, structured understanding of these reusable solutions to address common programming challenges.\r\n\r\nWe'll delve into design patterns\u2019 key categories: Behavioral, Structural, and Creational, as well as explore their functionality and how they can be applied in your daily development workflow. For each category, we'll also explore a practical design pattern in detail and showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation.\r\n\r\nYou'll gain valuable insight into how these patterns can translate into real-world development scenarios, such as facilitating communication between objects (Behavioral), separating interfaces from implementation for flexibility (Structural), and enabling dynamic algorithm selection at runtime (Creational).", "code": "8PFFPS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/8PFFPS.png", "speaker_names": "Tanu", "speakers": "\n### Tanu\n\nTanu is a Software Engineer at Bloomberg on the BQL (Bloomberg Query Language) team. BQL provides intelligent query suggestions to empower users for efficient data exploration.  A passion for crafting clean, maintainable, and efficient software solutions fuels her work in this role and throughout her career. She has a Master's degree in Distributed Systems and 6 years of industry experience building scalable systems. She is a tech writer for Medium, has organized hands-on workshops and delivered technical presentations internally for 100+ people  . She is passionate about staying on top of tech and sharing knowledge at conferences. In her free time, Tanu enjoys traveling and playing music.\n", "track": "Programming & Software Engineering"}, {"title": "The aesthetics of AI: from cyberpunk to fascism", "abstract": "Let\u2019s explore the visual grammars, references and cultural norms at play in the field of AI; from Kismet to Spot\u00ae, from Clippy to Claude. As a sector we can be hyper-focused on technical process and function, to the extent that it blinkers our understanding of the cultural and political impacts of our work. Aesthetics infuse every aspect of technology. Aesthetic interpretations are manifold and mutable, constructed in-congress with the observer and not fully defined by the original designer. AI technologies add additional layers of subtext: character, consciousness, agency, intent.\r\n\r\nDespite this murkiness, or perhaps because of it, this talk makes an passionate argument for engaging with historical aesthetic movements, for building our shared professional knowledge of fads and fashions\u23afnot just from the past 40 years of internet culture\u23afbut also the past 140 years of ideology, technology, and thought.", "full_description": "**Talk Outline** \r\n- Define aesthetics\r\n- Differentiate aesthetics from visual design\r\n- Aesthetics of contemporary AI\r\n- History of aesthetics in technology\r\n- Link current technologies to historical aesthetic movements\r\n\r\n**Detailed description**\r\nThe field of artificial intelligence has long been dominated by discussions of technical capabilities, algorithmic improvements, and functional benchmarks. Beneath this technical layer lies a rich but often unexamined tapestry of visual and cultural decisions that profoundly shape how we perceive, interact with, and ultimately integrate AI systems into our society. \r\n\r\nThis talk moves beyond simple visual design to explore how aesthetics \u2013 study of the principles of beauty and artistic taste \u2013 shapes both the creation and interpretation of AI technologies. The aesthetic choices woven into today\u2019s AI interfaces, both for end-users and industry practitioners, reveal our deep-seated assumptions about the world. \r\n\r\nPhilosophers of art ask us to introspect: What is goodness? What is beauty? Which endeavours are most worthy of our attention? We can use these same questions to explore the ideas framing AI.\r\n\r\n\u201cAll watched over by machines of loving grace\u201d, a poem by Richard Brautigan, imagines a utopian future where the natural and technological worlds achieve balance and harmony, and where humans are free to pursue creative, embodied pursuits, freed of menial labour. Is this the utopia imagined by OpenAI or DeepMind when they describe the imminent arrival of AGI? Does the world as described by the big brands of AI actually align with our own imaginings of progress, of utopia? \r\n\r\nA brief historical overview will trace how technological aesthetics have evolved, examining how different eras have visualized and presented technological innovations. This context sets the stage for drawing direct connections between current AI aesthetics and historical movements \u2013 revealing how contemporary design choices often unconsciously echo past ideological and artistic approaches.\r\n\r\nThrough these connections, I\u2019ll demonstrate why developing a broader aesthetic literacy is crucial for AI practitioners. Understanding these historical and cultural reference points can lead to more thoughtful and effective uses of AI. As our field continues to shape the future of human-machine interaction, this aesthetic awareness becomes not just an academic exercise, but a practical necessity: providing both the groundwork for nuanced critique, and the capacity to clearly define how we expect technologies to fit into and improve our lives.", "code": "933YXH", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/933YXH.png", "speaker_names": "Laura Summers", "speakers": "\n### Laura Summers\n\nLaura is a very technical designer\u2122\ufe0f, working at  Pydantic as Lead Design Engineer. Her side projects include Sweet Summer Child Score (summerchild.dev) and Ethics Litmus Tests (ethical-litmus.site). Laura is passionate about feminism, digital rights and designing for privacy. She speaks, writes and runs workshops at the intersection of design and technology.\n", "track": "Others"}, {"title": "What we talk about when we talk about AI skills.", "abstract": "Defining what constitutes AI skills has always been ambiguous. As AI adoption accelerates across industries and the European AI Act mandates companies to ensure AI literacy among their staff, organizations face growing even more challenges in defining and developing AI competencies. In this talk, we'll present a comprehensive framework developed by the appliedAI Institute's experts that categorizes AI skills across technical, regulatory, strategic, and innovation domains. We'll also share initial data on current AI skills levels and upskilling needs and provide practical strategies for organizations to assess, develop, and acquire the AI capabilities required for their specific needs.", "full_description": "What it means to \"work in/with AI\" and the corresponding roles, tasks, and required skills have been ambiguous since the emergence of AI professionals in industry. And while the demand for AI-skilled professionals continues to grow, both organizations and individuals seeking to work in the AI field often struggle with two challenges: 1) clearly defining the competencies and responsibilities that positions and projects require, and 2) identifying appropriate upskilling opportunities to match these needs.\r\nThe urgency to upskill professionals in AI topics has not only become more nuanced since the emergence of generative AI but is also growing rapidly. This trend is further amplified by the upcoming European AI Act, which will soon require companies to \"ensure, to their best extent, a sufficient level of AI literacy among their staff.\" This regulation has created an urgent need to define and understand what constitutes AI literacy and AI skills in practical terms.\r\n\r\nTo help organizations and professionals navigate this landscape, we have developed a comprehensive framework categorizing AI skills into distinct domains spanning technical competencies, regulatory knowledge, AI strategy, and ecosystem understanding. Our framework, developed by the multidisciplinary team of AI experts in the appliedAI Institute for Europe, provides a structured approach to defining skill requirements, guiding career development, identifying training gaps, and helping educational providers align their offerings with market demands.\r\n\r\nIn this presentation, we will introduce our framework and share initial data reflecting the current state of AI skills levels and upskilling needs across a sample of companies. We will also discuss practical strategies for implementing this AI Skills framework within organizations, enabling them to better assess, develop, and acquire the AI capabilities they need to fulfill their specific needs.", "code": "98FQDY", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/98FQDY.png", "speaker_names": "Paula Gonzalez Avalos", "speakers": "\n### Paula Gonzalez Avalos\n\nData Nerd & Python Pydata community lover. AI education specialist with five years of experience shaping data science and AI educational offers. Currently leading the AI Academy at the appliedAI Institute for Europe.\n", "track": "Education, Career & Life"}, {"title": "Building a HybridRAG Document Question-Answering System", "abstract": "Retrieval Augmented Generation (RAG) is a powerful technique for searching across unstructured documents, but it often falls short when the task demands an understanding of intricate relationships between entities. GraphRAG addresses this by leveraging knowledge graphs to capture these relationships, but it struggles with scalability and handling diverse unstructured formats. In this talk, we\u2019ll explore how HybridRAG combines the strengths of both approaches - RAG for scalable unstructured data retrieval and GraphRAG for semantic richness- to deliver accurate and contextually relevant answers. We\u2019ll dive into its application, challenges, and the significant improvements it offers for question-answering systems across various domains.", "full_description": "#### Outline:\r\n\r\n1. Introduction  \r\n   - The challenge of extracting information from unstructured and domain-specific text (e.g., legal documents).  \r\n   - Overview of traditional RAG techniques and their limitations:  \r\n     - Scalability and unstructured data handling.  \r\n     - Lack of semantic depth to capture intricate relationships.  \r\n   - Why HybridRAG is a game-changer.  \r\n\r\n2. What is RAG? \r\n   - Explanation of vector-based retrieval using embeddings and databases.  \r\n   - Advantages of RAG:  \r\n     - Scalable search across diverse unstructured formats.  \r\n     - Domain-agnostic retrieval capabilities.  \r\n   - Limitations:  \r\n     - Inability to capture relationships between entities.  \r\n     - Difficulty handling domain-specific or complex queries.  \r\n\r\n3. What is GraphRAG?  \r\n   - Explanation of GraphRAG: How knowledge graphs enhance retrieval by mapping relationships between entities.  \r\n   - Benefits of GraphRAG:  \r\n     - Semantic richness and contextual understanding.  \r\n     - Effective for domains requiring deep relational reasoning (e.g., finance, healthcare).  \r\n   - Challenges of GraphRAG:  \r\n     - Building high-quality knowledge graphs from unstructured data.  \r\n     - Scalability and integration with generative models.  \r\n\r\n4. Introducing HybridRAG: Combining RAG and GraphRAG \r\n   - The HybridRAG architecture:  \r\n     - RAG for scalable retrieval of unstructured data.  \r\n     - GraphRAG for refining answers with relational and semantic context.  \r\n   - Benefits of HybridRAG:  \r\n     - Combining scalability with semantic depth.  \r\n     - Improved retrieval accuracy and contextual relevance.  \r\n   - Use case: Legal documents processing (e.g., extracting Q&A insights).  \r\n     - How RAG retrieves general context.  \r\n     - How GraphRAG captures relationships (e.g., between companies, documents, events).  \r\n\r\n5. Challenges in Building HybridRAG Systems\r\n   - Creating high-quality knowledge graphs from diverse and unstructured data.  \r\n   - Balancing computational overhead from combining RAG and GraphRAG.  \r\n   - Addressing domain-specific terminology and ensuring generalizability to other domains.  \r\n\r\n6. Key Takeaways\r\n   - HybridRAG effectively combines the strengths of RAG and GraphRAG.  \r\n   - It\u2019s particularly powerful for domains requiring both scalability and semantic depth.  \r\n   - Practical advice for building HybridRAG systems in your projects.  \r\n\r\n#### What You\u2019ll Learn:\r\n- The strengths and limitations of RAG and GraphRAG techniques for question-answering systems.  \r\n- How HybridRAG bridges the gap by combining scalable retrieval with semantic richness.  \r\n- Practical challenges and solutions for building HybridRAG systems, including knowledge graph creation and integration.  \r\n- Insights into real-world applications where HybridRAG delivers superior results.", "code": "9CRNU3", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/9CRNU3.png", "speaker_names": "Darya Petrashka", "speakers": "\n### Darya Petrashka\n\nDarya Petrashka is a Data Scientist at SLB with 5 years of experience, focusing on supply chain projects in data analysis, NLP, and generative AI. She is passionate about using data for problem-solving, with a strong interest in classical machine learning, NLP, and AWS services. An AWS Community Builder and Authorized Instructor, Darya actively shares her expertise through public speaking at various industry events, including AWS Community Days, AWS Cloud Day, and PyCon. A dedicated learner, Darya continually hones her skills by participating in workshops, courses, and tech schools.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Transformers for Game Log Data", "abstract": "The Transformer architecture, originally designed for machine translation, has revolutionized deep learning with applications in natural language processing, computer vision, and time series forecasting. Recently, its capabilities have extended to sequence-to-sequence tasks involving log data, such as telemetric event data from computer games.\r\n\r\nThis talk demonstrates how to apply a Transformer-based model to game log data, showcasing its potential for sequence prediction and representation learning. Attendees will gain insights into implementing a simple Transformer in Python, optimizing it through hyperparameter tuning, architectural adjustments, and defining an appropriate vocabulary for game logs.\r\n\r\nReal-world applications, including clustering and user level predictions, will be explored using a dataset of over 175 million events from an MMORPG. The talk will conclude with a discussion of the model's performance, computational requirements, and future opportunities for this approach.", "full_description": "The paper[1] introducing the Transformer architecture has been cited almost 150k times. By now, this deep learning architecture has been used for a large number of use cases. Obviously, language generation and large language models are among the most prominent use cases. However, the architecture has also been successfully employed to solve problems in computer vision and to forecast time series data to name only a few other examples.\r\n\r\nAt its core, the Transformer architecture is a deep neural network designed for sequence-to-sequence prediction tasks. E.g., mapping a sequence of words in one language to a sequence of words in another language as it is done in machine translation tasks. This architecture has recently gained attention for another application well-suited to sequence-to-sequence mapping: the analysis of telemetric log data from games[2]. While log data from games is one specific area that has been explored lately, this approach generally works for log data in other domains arising from websites or mobile apps.\r\n\r\nIn this talk, I will walk the audience through a simple Transformer architecture in Python that can be used to train a model on game log data. I will discuss the challenges of constructing a vocabulary and tokenizer based on log data. Unlike language data, game logs often contain structured events with properties, making vocabulary design non-trivial. I will highlight design choices in the model construction to balance the predictive power of the model and computational efficiency. This includes hyper-parameter selection for the model (e.g., embedding size, number of layers, etc.) and the training procedure (e.g., batch size, learning rate, etc.). I will also explain how to adapt the Transformer architecture to handle long sequences of log data efficiently, including architectural changes to the basic network.\r\n\r\nI will demonstrate how representations derived from the model can be applied to various use cases, such as clustering and prediction tasks arising in game data science. Typical prediction tasks in game data science are survival time prediction for regression or purchase prediction for classification. Insights from clustering or player level predictions can help to improve retention or optimize monetization models. To evaluate the effectiveness of this approach, I trained multiple models on a publicly available 100GB game log dataset containing over 175 million events from NCSOFT\u2019s MMORPG Blade and Soul. In addition to presenting qualitative results, I will compare the computational resources and hardware requirements of this method to those of a simple baseline algorithm. \r\n\r\nBy the end of the talk, attendees will gain actionable insights into building and training Transformers for log data, equipping them to tackle similar challenges in their own domains.\r\n\r\nTentative agenda of the talk:\r\n5  - Intro\r\n5  - Review of the Transformer architecture and its usage in GPT\r\n10 - Adjusting the architecture to game log data\r\n10 - Training of different models\r\n10 - Obtaining player representations from the models for clustering and prediction tasks\r\n5  - Outlook & Conclusion\r\n\r\n[1] \u201cAttention is all you need\u201d, Vaswani et al., 2017\r\n[2] \u201cplayer2vec: A Language Modeling Approach to Understand Player Behavior in Games\u201d, Wang et al., 2024\r\n[3] \u201cGame Data Mining Competition on Churn Prediction and Survival Analysis using Commercial Game Log Data\u201d, Lee et al., 2018", "code": "9NFHAS", "state": "confirmed", "created": "2025-01-01", "social_card_image": "/static/media/social/talks/9NFHAS.png", "speaker_names": "Fabian Hadiji", "speakers": "\n### Fabian Hadiji\n\nFabian combines his passion for data, machine learning, and computer games with his professional activities. In addition to his role as Head of Business Intelligence at Lotum, a mobile game publisher, he also lectures at TH K\u00f6ln, where he leads a project group focused on game data science. Additionally, Fabian co-organizes the Cologne AI and Machine Learning Meetup (CAIML), hosting bi-monthly events that bring together the local AI and ML community.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Tactile Sensing for Robotics and the Role Python can Play", "abstract": "What if machines could not only act but also feel? Tactile sensing gives robots the ability to perceive touch, enabling them to handle delicate objects, recognize textures, and adapt their grip, just like humans. This talk introduces tactile sensing, its evolution, and advancements like vision-based systems, which combine visual and tactile information to enhance adaptability. It will also highlight how Python developers can contribute to this exciting field by leveraging tools like PyTorch for building machine learning models and OpenCV for processing tactile data. By the end, attendees will understand the importance of touch in robotics and the role Python plays in advancing this innovation.", "full_description": "Robotics has excelled at navigation and manipulation, but tactile sensing takes it further by giving robots the ability to handle delicate objects, recognize textures, and adapt based on touch. This session explores how tactile sensing mimics human touch, making robots more adaptable and effective in real-world applications.\r\n\r\nWhat We Will Explore:\r\n1. The basics of tactile sensing and its importance in robotics.\r\n2. How tactile sensing mimics human touch to enhance robotic adaptability.\r\n3. Vision-based tactile systems: combining touch and sight for precision.\r\n4. The role of Python in tactile sensing:\r\n   - Using PyTorch to build machine learning models for tactile data.\r\n   - Leveraging OpenCV for tactile data preprocessing and visualization.\r\n5. Real-world applications of tactile sensing in industries like healthcare, manufacturing, and automation.\r\n6. Challenges and opportunities in the field of tactile sensing.\r\n\r\nThrough practical insights and examples, attendees will gain:\r\n1. A clear understanding of tactile sensing and its importance in robotics.\r\n2. Insights into the latest advancements in vision-based tactile sensing.\r\n3. Knowledge of how Python can be used to process, analyze, and model tactile data.", "code": "9RBNMV", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/9RBNMV.png", "speaker_names": "Zaynab Awofeso", "speakers": "\n### Zaynab Awofeso\n\n\n", "track": "Embedded Systems & Robotics"}, {"title": "The future of AI training is federated", "abstract": "Since it\u2019s introduction in 2016, Federated Learning (FL) has become a key paradigm to AI models in scenarios when training data cannot leave its source. This applies in many industrial settings where centralizing data is challenging due to a combination of reasons, including but not limited to privacy, legal, and logistics.\r\n\r\nThe main focus of this tutorial is to introduce an alternative approach to training AI models that is straightforward and accessible. We\u2019ll walk you through the basics of an FL system, how to iterate on your workflow and code in a research setting, and finally deploy your code to a production environment. You will learn all of these approaches using a real-world application based on open-sourced datasets, and the open-source federated AI framework, [Flower](https://github.com/adap/flower), which is written in Python and designed for Python users. Throughout the tutorial, you\u2019ll have access to hands-on open-sourced code examples to follow along.", "full_description": "Federated Learning has quickly become the preferred form of training of AI models when the training data cannot leave their point of origin due to privacy regulations (e.g. GDPR), legal constraints (e.g. in different jurisdictions), and logistical challenges (e.g. large volumes of data, sparse connectivity), among other reasons. Furthermore, contracts and regulations establish boundaries for data sharing, particularly in industries like healthcare and finance, where misuse prevention is crucial. One could also argue that we are [running out of publicly and ethically sourced datasets](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training), for instance to [scale large foundational models](https://arxiv.org/abs/2410.08892), and federated learning offers one way to train models on protected data.\r\n\r\nThe key point of this tutorial is to introduce an alternative approach to training AI models that is straightforward and accessible.\r\n\r\nThis tutorial is sequenced in 3 parts. We\u2019ll first introduce federated learning and its prototypical architecture. In part 2, we\u2019ll dive into a series of live Python code demos that showcase how to convert a classical centralized machine learning workflow into a federated workflow involving multiple federated clients. We\u2019ll demonstrate the similarities and differences of how the iteration of a federated research project is conducted. Finally, in part 3, we\u2019ll demonstrate how you can take your research code and deploy it in a production setting using a mixture of physical edge devices and VMs.\r\n\r\nThroughout the tutorial, we\u2019ll use [Flower](https://github.com/adap/flower), the fully open-sourced federated AI framework, which is written in Python and designed for Python users. With simplicity as one of it\u2019s main goals, Flower provides multiple features and libraries to accelerate research, such as [Flower Baselines](https://flower.ai/docs/baselines/) (for reproducing federated learning benchmarks) and [Flower Datasets](https://flower.ai/docs/datasets/) (a standalone Python library for easily creating federated datasets). We\u2019ll showcase how to use the Flower CLI in both research and production setting.\r\n\r\nThis tutorial addresses people with fluency in Python, CLI, and basic knowledge of a machine learning project. It would help if you\u2019ve also used Docker before. Any data practitioner is encouraged to attend the tutorial to learn and discuss how to federate and distribute the training of an ML model. \r\n\r\nYou will learn:\r\n\r\n- What\u2019s Federated Learning?\r\n    - Basics and real-world examples\r\n- How to federate your existing ML training code, and more FL-specific steps such as how to:\r\n    - Configure the behaviours of each federated client\r\n    - Persist the state of each client across global rounds\r\n    - Evaluate both aggregated and local models\r\n    - Standardize your FL experiments\r\n    - Track your experiments\r\n- How to deploy your research code in a production setting, such as how to:\r\n    - Deploy Flower federated learning clients using Docker\r\n    - Set-up secure connection and node authentication\r\n    - Run, monitory, and manage the federated learning runs.\r\n\r\nBring your own laptop if you\u2019d like to follow along. Some code examples will be executed in Google Colab, others can be locally executed on your favourite IDE. A GitHub repo containing the code examples will be shared before the event.\r\n\r\nThe tutorial session is structured in the following way:\r\n\r\n- 0:00 Introduction, and getting to know the audience.\r\n- 0:05 What\u2019s Federated Learning? Basics and real-world-examples.\r\n- 0:25 Overview of the Flower framework for federated learning\r\n- 0:30 Quickstart examples with PyTorch. Moving from a centralized training to federated.\r\n- 1:00 Deploying your research to production\r\n- 1:20 Feedback and Q&A", "code": "9Y9DM8", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/9Y9DM8.png", "speaker_names": "Chong Shen Ng", "speakers": "\n### Chong Shen Ng\n\nDr. Chong Shen Ng is a Research Engineer at Flower Labs with over a decade of experience in both research and industry, specializing in federated learning, data science, and parallel computing. As a key developer, he focuses on scaling Flower to deploy privacy-enhanced distributed AI solutions for real-world applications. Chong Shen is passionate about contributing to the open-source community, developing trustworthy AI systems through federated learning, and advancing edge AI technologies. A dedicated advocate for open-source software, he has co-chaired PyData Global events and volunteered at SciPy and PyData London conferences.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "From Tensors to Clouds \u2014 A Practical Guide to Zarr V3 and Zarr-Python 3", "abstract": "A key feature of the Python data ecosystem is the reliance on simple but efficient primitives that follow well-defined interfaces to make tools work seamlessly together (Cf. http://data-apis.org/). NumPy provides an in-memory representation for tensors. Dask provides parallelisation of tensor access. Xarray provides metadata linking tensor dimensions. **Zarr** provides a missing feature, namely the scalable, persistent storage for annotated hierarchies of tensors. Defined through a community process, the Zarr specification enables the storage of large out-of-memory datasets locally and in the cloud. Implementations exist in C++, C, Java, Javascript, Julia, and Python, enabling.\r\n\r\nThis talk presents a systematic approach to understanding and implementing the newer version of [Zarr-Python](https://github.com/zarr-developers/zarr-python), i.e. Zarr-Python 3 by explaining the new API, deprecations, new storage backend, improved codec pipeline, etc.\r\n\r\nI will also show the performance improvements in ZP-3 while creating, reading and writing async Zarr arrays across local and remote storage like AWS S3.", "full_description": "Zarr is a data format for storing chunked, compressed N-dimensional arrays and is sponsored by [NumFOCUS]((https://numfocus.org/project/zarr)) under their umbrella.\r\n\r\nIt is based on open-source technical specification and has implementations in several languages, with [Zarr-Python](https://github.com/zarr-developers/zarr-python) being the most used.\r\n\r\nAfter the successful adoption of Specification V3, our team has worked tirelessly over the last year to ensure the Python library's compliance with the latest spec.\r\n\r\n## Outline\r\n\r\nFirst, I\u2019d be talking about:\r\n\r\n### Understanding Zarr basics (5 mins.)\r\n\r\n- What is Zarr, and how it works?\r\n    - The inner workings of Zarr using illustrated graphics\r\n- What is the Zarr Specification?\r\n    - What's new in Zarr Spec V3?\r\n\r\nThen, I'll be talking about the new Zarr-Python 3 and its significant features:\r\n\r\n### What's new in Zarr-Python 3? (15 mins.)\r\n\r\n- Major design updates\r\n    - New storage backend\r\n    - Creating Zarr arrays and groups asynchronously\r\n    - New and improved codec pipeline\r\n    - Native GPU support for creating and writing arrays\r\n- Changes and deprecations\r\n    - Overview of the new API\r\n    - Optimising performance for large arrays\r\n    - Deprecation of several stores like LMDBStore, SQLStore, MongoDBStore, etc.\r\n- 3.0 Migration guide\r\n    - Steps to migrate from Zarr-Python 2 to Zarr-Python 3\r\n- Extensions\r\n    - How can Zarr-Python 3 be extended to add new custom data types, stores, chunking strategies, etc.?\r\n\r\nThen, I\u2019d be doing a hands-on session, which would cover the following:\r\n\r\n### Hands-on (5 mins.)\r\n\r\n- Creating Zarr arrays and groups using Zarr-Python 3\r\n    - Plus walkthrough of the new features (mentioned above)\r\n- Writing and reading from Cloud object storage\r\n    - Using S3/GCS/Azure to create Zarr arrays and write data to it\r\n- Looking under the hood\r\n    - Use store and info functions to explain how your Zarr data is stored and display important information\r\n\r\n### Conclusion (5 mins.)\r\n \r\n- Key takeaways\r\n- How can you get involved?\r\n- QnA\r\n\r\nThis talk aims to address an audience that works with large amounts of data and is looking for a transparent, open-source, reliable, cloud-optimised, and environmentally friendly format.\r\n\r\nThe tone of the talk is set to be informative, story-telling and fun.\r\n\r\nIntermediate knowledge of Python and NumPy arrays is required for the attendees to attend this talk.\r\n\r\n### After this talk, you\u2019d:\r\n\r\n- understand the basics of Zarr and what's new in V3,\r\n- leverage the new functionalities of Zarr-Python 3 with improved performance,\r\n- make an informed decision on what data format to use for your data", "code": "ABWHSD", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/ABWHSD.png", "speaker_names": "Sanket Verma", "speakers": "\n### Sanket Verma\n\nSanket is a data scientist based out of New Delhi, India. He likes to build data science tools and products and has worked with startups, governments, and organisations. He loves building community and bringing everyone together and is Chair of PyData Delhi and PyData Global.\r\n\r\nCurrently, he's taking care of the community and OSS at Zarr as their Community Manager.\r\n\r\nWhen he\u2019s not working, he likes to play the violin and computer games and sometimes thinks of saving the world!\n", "track": "Data Handling & Engineering"}, {"title": "Where have all the post offices gone? Discovering neighborhood facilities with Python and OSM", "abstract": "When it comes to open geographic data, OpenStreetMap is an awesome resource. Getting started and figuring out how to make the most out of the data available can be challenging.\r\n\r\nUsing a personal example: frustration at the apparent lack of post offices in my neighborhood, we'll walk through examples of how to parse, filter, process, and visualize geospatial data with Python.\r\n\r\nAt the end of this talk, you will know how to process geographic data from OpenStreetMap using Python and find out some surprising info that I learned while answering the question: Where have all the post offices gone?", "full_description": "**Problem statement and brief intro**\r\n\r\nNeeding an international postcard stamp, I headed to my nearest post office only to find out that it was permanently closed, the latest closure among others in recent memory. Was this just in my neighborhood or was this happening all over the state? To answer these questions, I turned to open data and Python.\r\n- What is OpenStreetMap?\r\n\r\n**How can we identify types of places, like post offices and districts, in OSM data?**\r\n\r\n- Types of data in OSM\r\n- Tags\r\n- Tools for diving into the data to get an idea of how it is structured and how to construct queries: Overpass API, overpass turbo\r\n\r\n**How can we access the raw OSM data and work with it in Python?**\r\n\r\n- How many post offices are there in each neighborhood? What about by area or population?\r\n- Working with PBF files: parsing and filtering with the PyOsmium\r\n- Using GeoPandas to store the data in a GeoDataFrame and apply transformations\r\n\r\n**What are some tools for visualizing the data?**\r\n\r\n- How can we make an interactive plot of post offices in each neighborhood? What about other facilities and resources?\r\n- Plot directly from a GeoDataFrame\r\n- Interactive plotting\r\n\r\nWhile this talk is aimed at those beginning with geographic data, it would be helpful to have some background knowledge about Python and data handling.", "code": "ADSXCA", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/ADSXCA.png", "speaker_names": "Katie Richardson", "speakers": "\n### Katie Richardson\n\nKatie Richardson is a Data Scientist, who is passionate about the intersection of data science and ML Ops. With experience in recommendation systems and search ranking, she is currently focused on demand forecasting.\n", "track": "Data Handling & Engineering"}, {"title": "PosePIE: Replace Your Keyboard and Mouse With AI-Driven Gesture Control", "abstract": "In this talk, we show how to leverage publicly available tools to control any game or program using hand or body movements. To achieve this, we introduce PosePIE, an open-source programmable input emulator that generates input events on virtual gamepads, keyboards and mice based on gestures recognized by using AI-driven pose estimation. PosePIE is fully configurable by the user through Python scripts, making it easily adaptable to new applications.", "full_description": "Recent advancements in machine learning and AI hardware acceleration have enabled the use of complex models for solving computer vision problems in real-time applications. Pose estimation is one such problem, involving the detection of keypoints of the human body within an image.\r\n\r\nIn this talk, we show how PosePIE uses pose estimation to control any game or program using hand or body movements. By using state-of-the-art models, PosePIE does not require expensive specialized sensors but works entirely on the monocular image from an off-the-shelf webcam. By leveraging readily available Graphics Processing Unit (GPU) hardware, it is able to do all processing at a high frame rate to support interactive applications.\r\n\r\nAs PosePIE is fully configurable by the user through Python scripts, it can be easily adapted to new applications. This lowers the barrier to use pose estimation and gesture recognition in creative ways and for novel applications.\r\n\r\nThe source code of PosePIE is available on GitHub under the GNU GPLv3+ license.", "code": "AEUZGX", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/AEUZGX.png", "speaker_names": "Daniel Stolpmann", "speakers": "\n### Daniel Stolpmann\n\nDaniel Stolpmann received the B.Sc. and M.Sc. degrees in computer science and engineering from Hamburg University of Technology (TUHH), Germany, in 2017 and 2019. During his master studies, he started working at the Institute of Communication Networks (ComNets) as a student assistant and became a research fellow after his graduation. At ComNets, he conducted research on machine learning for communication networks, network coding and network emulation. In 2024, he joined Tegtmeier Inkubator as a senior software developer and started working on AI-enabled smart home systems.\n", "track": "Computer Vision (incl. Generative AI CV)"}, {"title": "Serverless Orchestration: Exploring the Future of Workflow Automation", "abstract": "Orchestration is a typical challenge in the data engineering world. Scheduling your data transformation jobs via CRON-jobs is cumbersome and error-prone. Furthermore, with an increasing number of jobs to manage it gets in-oversee able. Tools like Apache Airflow, Dagster, Luigi, and Prefect are known for addressing these challenges but often require additional resources or investment. With the advent of serverless orchestration tools, many of these disadvantages are mitigated, offering a more streamlined and cost-effective solution.\r\n\r\nThis session provides a comprehensive overview of combining serverless architecture with orchestration. We will start by defining the core concepts of orchestration and serverless technologies and discuss the benefits of integrating them. The talk will then analyze solutions available in the cloud vendor space. Attendees will leave with a well-rounded understanding of the tools and strategies available in serverless orchestration.", "full_description": "Orchestration is a typical challenge in the data engineering world. Scheduling your data transformation jobs via CRON-jobs is cumbersome and error-prone. Furthermore, with an increasing number of jobs to manage it gets in-oversee able. Tools like Apache Airflow, Dagster, Luigi, and Prefect are known for addressing these challenges but often require additional resources or investment. With the advent of serverless orchestration tools, many of these disadvantages are mitigated, offering a more streamlined and cost-effective solution.\r\nBeyond data engineering, serverless orchestration holds substantial potential for classical software engineering, especially as organizations explore serverless approaches for optimizing efficiency and reducing overhead.\r\n\r\nIn this talk you will explore:\r\n* Basic Introduction to Serverless Orchestration: \r\n      - What is orchestration about?\r\n      - What is serverless about?\r\n      - Why combining the two of them?\r\n* Offerings from Major Cloud Vendors:\r\n      - Analyzing solutions from leading cloud providers in the realm of serverless orchestration\r\n* Patterns and Solutions for Serverless Orchestration in Software Engineering:\r\n      - Exploring how serverless orchestration can be applied within classical software engineering contexts\r\n\r\nParticipants will leave this session equipped with a comprehensive understanding of the serverless orchestration landscape and its applications across different engineering disciplines.", "code": "AGY8CT", "state": "confirmed", "created": "2024-12-06", "social_card_image": "/static/media/social/talks/AGY8CT.png", "speaker_names": "Tim Bossenmaier", "speakers": "\n### Tim Bossenmaier\n\nTim is a Data Engineer at Cloudflight, based in Innsbruck. There he architects and builds modern data infrastructures in customer projects, ranging from streaming ETL pipelines to data catalogs and entire data platforms. His focus areas include software engineering, cloud technologies, data platform engineering, and DataOps. Tim is also a passionate Open Source contributor, actively working on projects like Apache StreamPipes and DataHub, among others.\n", "track": "Programming & Software Engineering"}, {"title": "Python Performance Unleashed: Essential Optimization Techniques Beyond Libraries", "abstract": "Every Python developer faces performance challenges, from slow data processing to memory-intensive operations. While external libraries like Numba or Cython offer solutions, understanding core Python optimization techniques is crucial for writing efficient code. This talk explores practical optimization strategies using Python's built-in capabilities, demonstrating how to achieve significant performance improvements without external dependencies. Through real-world examples from machine learning pipelines and data processing applications, we'll examine common bottlenecks and their solutions. Whether you're building data pipelines, web applications, or ML systems, these techniques will help you write faster, more efficient Python code.", "full_description": "Performance optimization remains a critical challenge in Python development. While Python's simplicity and extensive ecosystem make it the language of choice for many applications, its interpreted nature can lead to significant performance bottlenecks. This is particularly evident in data-intensive applications, machine learning pipelines, and large-scale production systems where every millisecond counts.\r\n\r\nMany developers immediately reach for external libraries or complex solutions when facing performance issues. However, Python's standard library and built-in features offer powerful optimization opportunities that are often overlooked. Understanding these fundamental optimization techniques not only improves code performance but also helps developers write more efficient code from the start.\r\n\r\nThis talk addresses the core performance challenges faced by Python developers daily. From memory management to algorithmic efficiency, we'll explore how seemingly simple code changes can lead to substantial performance improvements. Through practical examples drawn from real-world applications, we'll demonstrate how to identify, measure, and optimize performance bottlenecks effectively.", "code": "AJDYRL", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/AJDYRL.png", "speaker_names": "Thomas Berger", "speakers": "\n### Thomas Berger\n\nHi, I\u2019m Thomas Berger! I work as a Machine Learning Engineer at a FinTech company and also teach part-time as a lecturer. I\u2019ve been working with Python for over six years, starting during my studies, where I focused on machine learning. For the last three years, I\u2019ve been applying these skills professionally in my full-time role as a Machine Learning Engineer. I\u2019ve been diving deep into Python for things like **machine learning**, **reinforcement learning**, and **high-performance computing**. I love finding ways to make Python run faster and more efficiently, especially when tackling big data or complex models.\r\n\r\nAt PyCon, I\u2019ll be talking about **high-performance Python** and sharing tips and tricks to help you optimize your code for demanding tasks. I\u2019m excited to share what I\u2019ve learned and connect with others in the Python community!\n", "track": "Python Language & Ecosystem"}, {"title": "The earth is no longer flat - introducing support for spherical geometries in Spherely and GeoPandas", "abstract": "The geometries in GeoPandas, using the Shapely library, are assumed to be in projected coordinates on a flat plane. While this approximation is often just fine, for global data this runs into its limitations. This presentation introduces spherely, a Python library for working with vector geometries on the sphere, and its integration into GeoPandas.", "full_description": "Not all geospatial data are best represented using a projected coordinate system. Unfortunately, the Python geospatial ecosystem is almost fully based on planar geometries using Shapely, and is still lacking a general purpose library for efficient manipulation of geometric objects on the sphere. We introduce Spherely: a new Python library that fills this gap, aiming to provide a similar API as Shapely, but then gor geometries on the sphere.\r\n\r\nSpherely provides Python/Numpy vectorized bindings to S2Geometry, a mature and performant C++ library for spherical geometry that is widely used for indexing and processing geographic data, notably in popular database systems. This is done via S2Geography, a C++ library that has emerged from the R-spatial ecosystem and that provides a GEOS-like compatibility layer on top of S2Geometry. Unlike S2Geometry\u2019s SWIG wrappers or S2Sphere (pure-Python implementation), Spherely exposes its functionality via \u201cuniversal\u201d functions operating on n-dimensional Numpy arrays, therefore greatly reducing the overhead of the Python interpreter.\r\n\r\nComplementary to Shapely 2.0, Spherely may be used as a backend geometry engine for Python geospatial libraries like GeoPandas, hence extending their functionality to more robust and accurate manipulation of geographic data (i.e., using longitude and latitude coordinates).\r\n\r\nThis presentation introduces spherely and its capabilities to work with vector geometries on the sphere, and its integration into GeoPandas.\r\n\r\nCode repository: https://github.com/benbovy/spherely", "code": "AWPYGE", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/AWPYGE.png", "speaker_names": "Joris Van den Bossche", "speakers": "\n### Joris Van den Bossche\n\nI am a core contributor to Pandas and Apache Arrow, and one of the maintainers of GeoPandas and Shapely. I did a PhD at Ghent University and VITO in air quality research, worked at the Paris-Saclay Center for Data Science and at Voltron Data contributing to Apache Arrow. I am a freelance open source software developer and teacher.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Machine Reasoning and System 2 Thinking", "abstract": "Raw large language models struggle with complex reasoning. New techniques have\r\nemerged that allow these models to spend more time thinking before giving an answer.\r\nDirect token sampling can be seen as system-1 thinking and explicit step-by-step\r\nreasoning as system-2. How can this reasoning ability be improved and what is the future?", "full_description": "Basic large language models struggle with complex reasoning. New techniques, broadly referred to as \"test time compute\" have emerged that allow these models to spend more time processing before giving an answer. Direct token sampling can be seen as analogous to system-1 thinking and explicit step-by-step reasoning as system-2. Many top AI researchers and companes are now working on building system-2 into AI systems to improve general reasoning.\r\n\r\nWe will review the newest open research on test time computation including promising techniques that have appeared in top entries for Fran\u00e7ois Chollet's ARC-AGI challenge. While OpenAI has shamefully kept the research behind their o1, o3 and o-N models secret, other researchers have worked in public, demonstrating how to use test time compute to greatly boost model performance with the right fine-tuning and test time procedures.\r\n\r\nThis talk will explore the latest developments in the rapidly developing area of system-2 AI reasoning, the engine behind the only significant gains in LLM performance recently. Giving LLMs system-2 like capabilities improves problem solving, code generation quality and reduces hallucinations, get up to speed on research behind these techniques.", "code": "AYN837", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/AYN837.png", "speaker_names": "Andy Kitchen", "speakers": "\n### Andy Kitchen\n\nAndy Kitchen is a AI/neuroscience researcher, startup founder, and all-around hacker. He co-founded Cortical Labs where the team taught live brain cells to play pong. He's still trying to figure out how to catch the ghost in the machine.\n", "track": "Generative AI"}, {"title": "Navigating the LLM hype: What science can tell us about where LLMs win and where they fail.", "abstract": "Large Language Models (LLMs) have gained tremendous popularity over the past two years, with many tech companies rushing to integrate them into their products. However, these applications have often failed or led to embarrassment due to unrealistic expectations and a limited understanding of LLMs\u2019 constraints. This presentation will explore the true capabilities of LLMs through empirical experimentation and scientific rigor. We will investigate whether LLMs can reason, plan, or simply recite information, and uncover the actual level of \u201cintelligence\u201d they possess. By identifying these limitations, we will provide actionable guidelines for effectively applying LLMs, helping you determine when they are suitable for your product and avoid common pitfalls. This session is accessible to anyone considering the integration of LLMs into their work, regardless of technical background.", "full_description": "Over the past two years, Large Language Models (LLMs) have taken the tech world by storm, leading many companies to rapidly adopt them in hopes of transforming their products. Yet, despite their potential, many LLM-based applications have fallen short, sometimes even causing embarrassment or public backlash. These failures often arise from overly optimistic expectations and a lack of understanding of the models' limitations.\r\nIn this session, we will systematically explore the true capabilities of LLMs. Through empirical experimentation, we\u2019ll examine key questions: Can LLMs reason, or are they just sophisticated parrots? Can they engage in abstract thinking or demonstrate planning? After several years of research, science has a lot to say about the current and projected capabilities of LLMs.\r\nWith this knowledge, we will establish practical, actionable guidelines for applying LLMs effectively in products. You\u2019ll gain insights into which use cases are best suited for LLMs, how to avoid common pitfalls, and how to align expectations with the current capabilities of these models. This session does not require a deep technical understanding of LLMs, making it ideal for anyone, from product managers to decision-makers, who is considering using these models in their applications.", "code": "B7GU98", "state": "confirmed", "created": "2024-12-03", "social_card_image": "/static/media/social/talks/B7GU98.png", "speaker_names": "Oren Matar", "speakers": "\n### Oren Matar\n\nI am a freelance consultant specializing in AI-driven solutions, with expertise in 3D modeling and its intersections with AI, as well as advanced analysis of NLP systems. My research focuses on addressing the limitations of generative AI and developing practical frameworks to unlock its potential in both NLP and image processing.\r\n\r\nPreviously, I worked as a principal data scientist and algorithms developer, with a specialization in NLP and time series forecasting for supply chain management. Notably, I contributed to optimizing Facebook\u2019s Prophet library by enhancing runtime efficiency through Bayesian methods. Today, I continue to bridge theory and practice, leveraging innovative approaches to help clients and collaborators solve complex challenges in AI and beyond.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Optimizing Energy Tariffing System with Formal Concept Analysis and Dash", "abstract": "As a data scientist, I value the power of insightful visualizations to unlock unique interpretations of complex data. In my talk, I will introduce an elegant mathematical framework called Formal Concept Analysis (FCA), developed in the 1980s in Darmstadt.\r\n\r\nFCA transforms binary data into concepts that can be visualized as a hierarchical graph, offering a fresh perspective on multidimensional data analysis. Leveraging this theory and its open-source Python libraries, I am developing an interactive Dash-based tool featuring interactive tables and graphs to explore data insights.\r\n\r\nTo illustrate its potential, I will showcase an optimization of the entire tariffing system of an energy provider company, highlighting how FCA can bring structure and clarity to even such tangled datasets.", "full_description": "My goal is to introduce Formal Concept Analysis (FCA) as a fascinating mathematical framework. I aim to inspire Python enthusiasts to explore its potential and uncover insights in their data analysis tasks. The talk is divided into three sections:\r\n\r\n1 FCA Basics\r\n\r\n- What is a \"concept\"? *First, I am going to introduce the main terms used in FCA and define the central object of the theory - the formal concept.*\r\n\r\n- Illustrative example. *To show the power of FCA in action, I will provide a relatable example to explain the hierarchical structure of the graph visualization.*\r\n\r\n2 Python Implementation\r\n\r\n- ``fcapy`` Python library. *Core functionality overview of the library and the data formats it can use.*\r\n\r\n- Introducing interactivity with Python Dash: *Enhancing exploration and user experience with interactive tables (AG Grid) and dynamic graph visualizations (Cytoscape).*\r\n\r\n3 Applications and Practical Relevance \r\n\r\n- Use Case: Energy Tariffing System Optimization. *In this section, I am going to showcase the real data in its original complexity and the optimization process of identifying redundancies, overlaps, or inefficiencies.*\r\n- Examples of other  applications and key takeaways", "code": "B8TUR9", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/B8TUR9.png", "speaker_names": "Dr. Irina Smirnova-Pinchukova", "speakers": "\n### Dr. Irina Smirnova-Pinchukova\n\nAfter my PhD in Astronomy @ Max Planck Institute for Astronomy in Heidelberg, I have switched from academia to industry. Working as a Data Scientist @ DSC GmbH I am developing in python for various projects including those involving language models. I am attending PyData meetings in Heidelberg and even presented a lightning talk on my \"Croshapes\" hobby project.\n", "track": "Visualisation & Jupyter"}, {"title": "Decoding Topics: A Comparative Analysis of Python\u2019s Leading Topic Modeling Libraries Using Climate C", "abstract": "Topic modelling has come a long way, evolving from traditional statistical methods to leveraging advanced embeddings and neural networks. Python\u2019s diverse library ecosystem includes tools like Latent Dirichlet Allocation (LDA) using gensim, Top2Vec, BERTopic, and Contextualized Topic Models (CTM). This talk evaluates these popular approaches using a dataset of UK climate change policies, considering use cases relevant to organisations like DEFRA (Department for Environment, Food & Rural Affairs). The analysis explores real-time integration, dynamic topic modelling over time, adding new documents, and retrieving similar ones. Attendees will learn the strengths, limitations, and practical applications of each library to make informed decisions for their projects.", "full_description": "Objectives:\r\nThe session aims to:\r\n\r\n1.  Compare Python-based topic modelling libraries, highlighting their relevance to real-world scenarios like policy analysis.\r\n2.    Explore practical use cases, including real-time document integration, tracking topic evolution, and finding similar documents.\r\n3.    Evaluate the tools based on performance, interpretability, scalability, and flexibility, with a focus on climate change policy data presented by [1] focusing on adaptation and mitigation.\r\n4.    Provide actionable guidance on selecting the right library for different project needs and datasets.\r\n\r\nOutline:\r\n\r\n1. Introduction to Topic Modeling: Overview of traditional and modern approaches, including their practical significance.\r\n\r\n2. Algorithms & Libraries Overview: LDA (gensim) [2], CTM [3], Top2Vec [4], BERTopic [5]\r\n\r\n3. Dataset and Use Cases:\r\n      - Overview of the UK climate change policy dataset.\r\n      - Use cases inspired by DEFRA and similar organisations, such as:\r\n            - Real-time integration for continuously adding new documents.\r\n            - Tracking topic development over time (dynamic topic modeling).\r\n            - Retrieving similar documents for faster insights.\r\n           (- Classification)\r\n\r\n4. Evaluation Criteria: Analysis of libraries based on:\r\n        - Ease of Use: How easy it is for no coding experts\r\n        - Quality: Coherence and diversity of extracted topics.\r\n        - Efficiency: Runtime performance and scalability.\r\n        - Flexibility: Features like contextual embeddings and integration capabilities.\r\n        - Interpretability: Ease of understanding topics and output.\r\n\r\n5. Results: Detailed findings, including specific advantages and limitations of each library in supporting the outlined use cases.\r\n\r\n6. Practical Recommendations: Guidance on choosing a library based on project goals, dataset characteristics, and organisational needs.\r\n\r\n7. Conclusion and Future Directions: Summary of key insights and the evolving role of embedding-based methods in topic modelling.\r\n\r\nOutcomes:\r\nBy attending this session, participants will:\r\n\r\n- Gain an in-depth understanding of Python\u2019s top topic modeling libraries.\r\n- Learn how to apply these tools to real-world challenges in policy analysis and other fields.\r\n- Understand how to handle use cases like real-time document integration and topic evolution over time.\r\n- Develop the skills to evaluate and choose the best tool for specific datasets and objectives.\r\n\r\nTarget Audience\r\n\r\nThis talk is for:\r\n- Data scientists and NLP practitioners seeking to apply topic modelling to unstructured text data.\r\n- Policy analysts and researchers working with large textual datasets, such as government or environmental policies.\r\n- Professionals in organisations like DEFRA, where tracking changes, adding new documents, or finding similar records are critical tasks.\r\n- Python enthusiasts interested in cutting-edge NLP techniques for extracting meaningful insights.\r\n\r\n[1] R. Biesbroek, S. Badloe, and I. Athanasiadis. Machine learning for research on cli-\r\nmate change adaptation policy integration: an exploratory uk case study. Regional\r\nEnvironmental Change, 20, 07 2020.\r\n\r\n[2] https://pypi.org/project/gensim/\r\n[3] https://github.com/MilaNLProc/contextualized-topic-models\r\n[4] https://github.com/ddangelov/Top2Vec\r\n[5] https://maartengr.github.io/BERTopic/index.html\r\n5 https://github.com/MilaNLProc/contextualized-topic-models", "code": "BJKSGK", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/media/social/talks/BJKSGK.png", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "speakers": "\n### Dr. Lisa Andreevna Chalaguine\n\nLisa is an accomplished educator, researcher, and freelancer specializing in data science, natural language processing (NLP), and artificial intelligence. With a PhD in Intelligent Systems from UCL and a master's from Imperial College London, Lisa has extensive experience in academia and industry, having taught at UCL, and contributed to impactful projects like those with Cancer Research UK.\r\n\r\nA digital nomad at heart, Lisa teaches corporate clients and supervises university students worldwide, focusing on Python, machine learning, and NLP. Known for their engaging teaching style and passion for problem-solving, they are currently developing innovative courses and creating a YouTube channel featuring masterclasses on data analysis and machine learning.\r\n\r\nDriven by a love for teaching, research, and helping others succeed, Lisa is exploring opportunities to return to academia, with aspirations to lecture in Eastern Europe and Central Asia. Multilingual and versatile, they are shaping the future of data science education while continuing to inspire learners globally.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Streamlining Python deployment with Pixi:  A Perspective from production", "abstract": "In our quest to improve Python deployments, we explored Pixi, a tool designed to enhance dependency management within the Conda ecosystem. This talk recounts our experience integrating Pixi into a setup used in production. We leveraged Pixi to create lockfiles, ensuring consistent builds, and to automate deployments via CI/CD pipelines. This integration led to greater reliability and efficiency, minimizing deployment errors and allowing us to concentrate more on development. Join us as we share how Pixi transformed our deployment process and offer insights into optimizing your own workflows.", "full_description": "In modern software development, managing dependencies effectively is crucial for ensuring that applications run smoothly across various environments. This talk explores our journey to optimize Python deployments by integrating Pixi into our workflow. As a tool that enhances the Conda ecosystem, Pixi offers a reliable and efficient solution to the common challenges in dependency management. While concepts such as consistent builds, reproducibility, and automated deployments are well-established, Pixi simplifies their implementation within a Conda-based environment, making these practices more accessible and manageable.\r\n\r\nThe talk will cover\r\n- DevOps Concept\r\n  Introducing concepts like lockfile, reproducible environments and CI/CD pipeline to set out a good\r\n  baseline for deploying python code productively\r\n- Conda vs Pypi comparison\r\n  Considering the tradeoffs between isolation and development comfort\r\n- Pixi introduction\r\n  An introduction to the philosphy of pixi and how it compares to other conda tooling.\r\n  This also covers how Pixi streamlines the implementation of DevOps concepts\r\n- Implementing DevOps concepts using pixi\r\n  \r\nThis talk is designed for professional software developers who prioritize a robust setup for deploying Python code as services into production. While familiarity with the Conda ecosystem is beneficial, it is not a prerequisite for this session.", "code": "BLKYGU", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/BLKYGU.png", "speaker_names": "Dennis Weyland", "speakers": "\n### Dennis Weyland\n\nHey there! I'm Dennis Weyland, and I've been part of the Blue Yonder team for the last five years. I kicked off my career as a Data Scientist but soon found my groove in Data Engineering. Python has been my go-to language for the past seven years, and I love diving into project setups to make everything run smoothly. Before diving into my professional career, I studied Physics at KIT, where I completed my master's thesis and discovered my passion for Python software development and machine learning.\r\n\r\nOutside of work I'm passionate about running, diving, and climbing.\n", "track": "MLOps & DevOps"}, {"title": "From Algorithm to Action: Building a DIY Distributed Trading Platform with Open Source", "abstract": "In this talk, we'll explore how you can implement your own distributed system for algorithmic trading leveraging the power of open source without being dependent on trading bot providers.\r\n\r\nWe will discuss different challenges occurring in HFT inter alia processing massive amounts of data with low latency and reliable risk control and how to solve them. Furthermore we will touch on the topic of regulatory requirements in trading.\r\n\r\nThese challenges will be addressed through a distributed system implemented in Python, utilizing Kafka for real-time data streaming, PostgreSQL for persistent storage and DuckDB for high-performance analysis. We will examine approaches to decouple the components to re-use and scale them across different markets.\r\n\r\nCryptocurrency markets are used as a proving ground for the PoC due to easy availability for everyone.", "full_description": "## Who is this talk for\r\n\r\nThis talk is ideal for all software engineers interested in financial technology, quantitative developers looking to understand modern trading infrastructure, and technical architects exploring distributed systems in high-stakes environments.  \r\nThis talk will NOT discuss specific trading strategies or give any financial advice.\r\n\r\n## Outline\r\n\r\n* Motivation  \r\n* Fundamental trading concepts and market mechanics  \r\n* Market data ingestion and processing  \r\n* Order management and execution  \r\n* Implementation of trading strategies  \r\n* Post-trade analysis  \r\n* Outlook\r\n\r\n## Motivation\r\n\r\nThe landscape of financial trading has undergone a dramatic transformation over the past decades. What was once the exclusive domain of institutional players on physical trading floors has evolved into a digitized, accessible marketplace where individual traders can participate from anywhere in the world. The emergence of commission-free trading apps and cryptocurrency exchanges has brought market participation to millions of new retail traders.  \r\nThis enables everyone to participate with their own trading system in global markets.\r\n\r\nIn this talk, we'll explore how you can implement your own distributed system for exchange trading leveraging the power of open source without being dependent on trading bot providers. While we won't be able to cover every aspect in depth, we'll address the most essential elements.\r\n\r\nCryptocurrency markets are used as a proving ground for the PoC due to easy availability for everyone.\r\n\r\n## Fundamental Trading Concepts and Market Mechanics\r\n\r\nWe'll begin by exploring essential trading concepts:\r\n\r\n* Order book dynamics  \r\n* Orders, Trades and Positions  \r\n* Different types of orders and their implications for system implementation  \r\n* Regulatory requirements  \r\n* Performance of strategies\r\n\r\nThese lead to different considerations in system design and architecture:\r\n\r\n* De-coupling of exchange interfaces and trading strategies to use same strategy for different markets by using adapter pattern  \r\n* Horizontal scaling to handle data load  \r\n* Need of low latency components and their communication to properly react to market  \r\n* Need of streaming data for real-time risk management  \r\n* Need of persistent storage for regulatory data and post-trading-analysis  \r\n* Need of order action recording and post-trading analysis for performance evaluation\r\n\r\n## Market Data Ingestion and Processing\r\n\r\nThe foundation of any trading system is its ability to efficiently process market data. This includes a Python component responsible for real-time normalization and standardization of multi-venue data:\r\n\r\n* Efficient market data representation and storage structures  \r\n* Techniques for handling high-throughput data without compromising latency  \r\n* Market data recording for post-trading analysis using Kafka\r\n\r\n## Order Management and Execution\r\n\r\nCritical components for managing the trading lifecycle. This includes a Python component responsible for normalization and standardization of multi-venue order interfaces:\r\n\r\n* Order action handling (placing orders, modifying orders) and keeping track of orders  \r\n* Global real-time position tracking and risk calculation using Kafka  \r\n* State recovery and system restart procedures  \r\n* Audit trail implementation and transaction logging using Postgres\r\n\r\n## Implementation of Trading Strategies\r\n\r\nWe'll explore the practical aspects of implementing trading strategies in Python using the previously discussed system components:\r\n\r\n* Usage of provided market data  \r\n* Placing orders and keeping track of positions  \r\n* Fast communication with market data and order components using gRPC  \r\n* Recording of strategy internals for post-trade analysis\r\n\r\n## Post-trade Analysis and Performance Monitoring\r\n\r\nWe will take a closer look to evaluate trading strategies performance with:\r\n\r\n* High-performance post-trade analytics using DuckDB  \r\n* Live performance monitoring using Kafka\r\n\r\n## Outlook\r\n\r\nAt the end we will have a brief outlook what other challenges might occur e.g.:\r\n\r\n* Other market types (Finance/Equity/ETF and Energy)  \r\n* Latency considerations  \r\n* Taxes", "code": "BR3D83", "state": "confirmed", "created": "2024-12-14", "social_card_image": "/static/media/social/talks/BR3D83.png", "speaker_names": "Eugen Geist", "speakers": "\n### Eugen Geist\n\nSeasoned Software & Data Engineering Professional with extensive experience in high-frequency trading systems, data warehousing, and cloud solutions. Expert in optimizing mission-critical systems and implementing engineering best practices. Specialized in Python, SQL, and cloud technologies.\r\n\r\nCurrently working as a Freelance Developer focusing on software and data engineerin.\r\n\r\nSkilled in developing distributed systems, data pipelines, and performance optimization, consistently delivering solutions that maximize business value.\n", "track": "Programming & Software Engineering"}, {"title": "Unlocking the Predictive Power of Relational Data with Automated Feature Engineering", "abstract": "Relational data can be a goldmine for classical Machine Learning applications \u2014 yet extracting useful features from multiple tables, time windows, and primary-foreign key relationships is notoriously difficult. In this code tutorial, we\u2019ll use the H&M Fashion dataset to demonstrate how\u00a0getML\u00a0FastProp automates feature engineering for both classification (churn prediction) and regression (sales prediction) with minimal manual effort, outperforming both\u00a0Relational Deep Learning\u00a0and a skilled\u00a0human data scientist according to the RelBench leaderboard.\r\n\r\nThis code tutorial is perfect for data scientists looking to leverage their relational and time-series data data effectively for any kind of predictive analytics applications.", "full_description": "This tutorial tackles a common pain point in data science \u2013 extracting useful features from relational data spread across multiple interconnected tables. Manually crafting these features is often tedious, error-prone, and heavily reliant on domain expertise.\r\n\r\nWhy is this important? Relational data powers industries from e-commerce and healthcare to finance. Yet, building predictive models on such datasets often involves laborious feature engineering. getML FastProp \u2013 the fastest open-source algorithm for automated feature engineering \u2013 streamlines this process, helping data scientists move faster and build better models.\r\n\r\nIn this hands-on tutorial, we\u2019ll work through two tasks from Stanford\u2019s Relational Learning Benchmark (RelBench) using the H&M Fashion dataset: 1) Predict customer churn with a classification model, 2) Forecast item sales using regression model.\r\n\r\nWe\u2019ll walk through the code and concepts needed to solve these tasks with getML FastProp, achieving state-of-the-art performance and outperforming both Relational Deep Learning models and an experienced human data scientist.\r\n\r\nBy the end of this tutorial, you'll learn how to:\r\n- Understand relational learning \u2013 Grasp the core challenges and concepts of working with multi-table datasets.\r\n- Reproduce results \u2013 Run the provided notebooks and code to reproduce the results at your own pace.\r\n- Automate feature engineering \u2013 Use getML\u2019s FastProp to extract features directly from relational data.\r\n- Build and optimize getML pipelines \u2013 Develop pipelines for both classification and regression tasks.\r\n- Integrate into MLOps workflows \u2013 Leverage getML alongside LightGBM and Optuna.\r\n\r\nThis tutorial provides a practical, reproducible framework for working with relational and time-series data, applicable across industries and domains.", "code": "C3RVM3", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/C3RVM3.png", "speaker_names": "Alexander Uhlig", "speakers": "\n### Alexander Uhlig\n\nAlexander Uhlig is the CEO of Code17, the company behind getML. With a background in Physics, he leads the development of getML and has worked hands-on with data teams to build prediction models across various domains, including healthcare, trading, and e-commerce.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Graph Machine Learning in all its glory!", "abstract": "Our world is complex: one approach to understanding and learning more about relationships within the data is to represent it as a network or a graph - with entities as nodes and relations between them as edges. Network applications are abundant - Facebook, knowledge databases like Wikipedia, traffic routes, molecular pathways, and the fun starts when we start thinking of the physical world as graphs. Where there\u2019s data, there\u2019s uncertainty and the need to predict the future to make it a *little less* uncertain, which is why we need machine learning.\r\n\r\n**This talk will be about modeling one such network using a Graph Neural Network (GNN) and predicting on it using Deep Graph Library, an efficient and scalable open source framework to train and serve GNNs. While GNNs can be used for any ML task, we\u2019ll focus on building a recommendation model.**\r\n\r\nBut the transition from traditional data structures to graph-based models is not straightforward. Unlike tabular data, graphs require adapting ML principles to accommodate their topological and relational complexities. \r\n\r\n**This talk will help participants gain practical insights into modeling their data as graphs and leveraging GNNs to build a recommendation system - from scratch. We\u2019ll go over each ML model building step and discuss how to adapt our learnings from tabular data to graph data.**", "full_description": "This talk is to get the audience excited about graph ML and hopefully inspire them to think of their data as a potential graph which can open doors for this new paradigm of ML. Graphs are complex, but then again so is most of the data now.\r\n\r\nThe talk will be supported by code snippets and will be using an open source recommendation dataset. \r\n\r\n### Outline: \r\n\r\n**1. Intro to Graph Machine Learning** (6 minutes)\r\n- Brief overview of traditional data vs. graph-based data and graph components\r\n- Networks in real-life and relevance\r\n- ML on graphs: recent advancements\r\n\r\n**2. Graph Neural Networks (GNNs) and DGL** (6 minutes)\r\n- What are GNNs and how do they work? \r\n- Overview of the Deep Graph Library (DGL) \r\n- Architecture for building recommendation models using GNNs\r\n- GNN Model Architecture\r\n\r\n**3. Training a GNN model in DGL** (15 minutes)\r\n\r\nThis section is meant to address the following key questions on training and serving models using DGL:\r\n- How do we transform tabular data into meaningful nodes and edges? \r\n- How do we prepare data for graph modeling?\r\n- Can we split train / test data randomly as is convention? \r\n- How do we represent categorical/numerical features? \r\n- How do we train and make predictions on this model? \r\n- (optional): How do we decide on a model architecture?\r\n- How do we evaluate this model?\r\n- How do we re-train this model? \r\n- What are the data / scalability limitations when training GNNs?\r\n\r\nWe will also discuss certain common pitfalls and how to avoid them. \r\n\r\n**4. Q&A and Closing Remarks** (3 minutes)\r\n\r\n\r\nAfter the talk, the audience should have a good understanding about:\r\n- How to structure their data as graph components and prepare it for modeling\r\n- What graph machine learning is all about, what a GNN is and how do we train and serve GNNs in DGL?\r\n- End-to-end ML tasks on graph data: best practices", "code": "CHAEXA", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/CHAEXA.png", "speaker_names": "Shreya Khurana", "speakers": "\n### Shreya Khurana\n\nI'm an AI scientist at Intuit in USA and I have experience working with recommendation systems, anomaly detection and deep learning. I was previously building NLP models at GoDaddy, but I enjoy working with data in general. I'm a Python enthusiast and enjoy sharing my learnings with the community - I've previously presented at the PyData, Grace Hopper Conference, PyCon US, EuroPython, and GeoPython. When not opposite a screen, I can be found frolicking in nature and exploring new trails.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Deploying Synchronous and Asynchronous Django Applications for Hobby Projects", "abstract": "Simplify deploying hybrid Django applications with synchronous views and asynchronous apps. This session covers ASGI support, Docker containerization, and Kamal for seamless, zero-downtime deployments on single-server setups, ideal for hobbyists and small-scale projects.", "full_description": "Hobby projects often start small but can quickly grow in complexity, especially when incorporating Django\u2019s support for asynchronous applications alongside traditional synchronous views. Deploying such hybrid projects on a single server\u2014whether in the cloud or on-premise\u2014can be daunting without the right tools and workflows.  \r\n\r\nThis talk focuses on simplifying the deployment process for hobbyists and developers who want to create and manage robust Django applications without requiring extensive infrastructure or expertise. We\u2019ll cover:  \r\n- Deploying Django projects that combine synchronous views and asynchronous apps using Django\u2019s ASGI support.  \r\n- Containerizing the application with Docker for consistent and manageable environments.  \r\n- Utilizing Kamal, an open-source deployment tool, to enable zero-downtime deployments, rolling updates, and seamless app management.  \r\n- Demonstrating the workflow on a single cloud server, with insights on adapting it to on-premise servers.  \r\n\r\nWhether you're building a passion project or experimenting with modern Django features, this session will provide you with practical tools and approaches to deploy hybrid Django applications effortlessly, keeping the process accessible and scalable for hobby-level development.", "code": "CMTKZS", "state": "confirmed", "created": "2024-11-30", "social_card_image": "/static/media/social/talks/CMTKZS.png", "speaker_names": "melhin", "speakers": "\n### melhin\n\nSoftware tinkerer, stumbling through software creation, with a deep enthusiasm for history and understanding human migration.\n", "track": "Django & Web"}, {"title": "Power up your Polars code with Polars extention", "abstract": "While Polars is written in Rust and has the advantages of speed and multi-threaded functionalities., everything will slow down if a Python function needs to be applied to the DataFrame. To avoid that, a Polar extension can be used to solve the problem. In this workshop, we will look at how to do it.", "full_description": "We love Polars because it is written in Rust so we can use Rust's security and speed. However, it is not the most efficient if we still have to call in a Python function to perform specific aggregation. In this workshop, we will use the Polars plugin. You will be writing simple functions in Rust, and then you will use it together with Polars in your Python data pipeline.\r\n\r\n## Target Audience\r\n\r\nEngineers and data scientists who use Polars and are confident to write a bit of Rust code. We expect you to have knowledge of Python and Polars and have a bit of Rust experience (or be able to pick it up relatively quickly). Not all concepts in Rust will be explained but we will link to material where you can find explanations.\r\n\r\n## Goal\r\n\r\nTo empower Polars users who want to do more and do better with Polars. For folks who don't mind learning a new programming language, it is also a good opportunity to learn and practice writing in Rust.\r\n\r\n## Outline\r\n\r\n- Introduction (15 mins): \r\n    1. What is Polars plugin \r\n    2. How does it work (using Maturin to develop packages)\r\n    3. How to use it with Polars (exercises)\r\n- Simple numerical functions (35 mins): \r\n    1. Creating numerical functions with 1 input (exercise)\r\n    2. Creating numerical functions with multiple inputs in the same row (exercise)\r\n    3. Creating numerical functions that support multiple types (exercise)\r\n- Advance usage with Polars plugin (40 mins):\r\n    1. Creating functions with multiple inputs across different rows (exercise)\r\n    2. Functions with user-set parameters (exercise)\r\n    3. Working with strings and lists (exercise)", "code": "CP3TKB", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/CP3TKB.png", "speaker_names": "Cheuk Ting Ho", "speakers": "\n### Cheuk Ting Ho\n\nAfter having a career as a Data Scientist and Developer Advocate, Cheuk dedicated her work to the open-source community. Currently, she is working as AI developer advocate for JetBrains. She has co-founded Humble Data, a beginner Python workshop that has been happening around the world. She has served the EuroPython Society board for two years and is now a fellow and director of the Python Software Foundation.\n", "track": "Data Handling & Engineering"}, {"title": "How Narwhals is silently bringing pandas, Polars, DuckDB, PyArrow, and more together", "abstract": "If you were writing a data science tool in 2015, you'd have ensured it supported pandas and then called it a day.\r\n\r\nBut it's not 2015 anymore, we've fast-forwarded to 2025. If you write a tool which only supports pandas, users will demand support for Polars, PyArrow, DuckDB, and so many other libraries that you'll feel like giving up.\r\n\r\nLearn about how Narwhals allows you to write dataframe-agnostic tools which can support all of the above, with zero dependencies, low overhead, static typing, and strong backwards-compatibility promises!", "full_description": "Suppose you want to write a data science tool to do feature engineering. Your experience may go like this:\r\n- Expectation: you can focus on state-of-the art techniques for feature engineering.\r\n- Reality: you keep having to make you codebase more complex because a new dataframe library has come out and users are demanding support for it.\r\n\r\nOr rather, it might have gone like that in the pre-Narwhals era. Because now, you can focus on solving the problems which your tool set out to do, and let Narwhals handle the subtle differences between different kinds of dataframe inputs!\r\n\r\nNarwhals is a lightweight and extensible compatibility layer between dataframe libraries. It is already used by several open source libraries including Altair, Marimo, Plotly, Scikit-lego, Vegafusion, and more. You will learn how to use Narwhals to build dataframe-agnostic tools.\r\n\r\nThis is a technical talk aimed at tool-builders. You'll be expected to be familiar with Python and dataframes. We will cover:\r\n- 2-3 minutes: motivation. Why are there so many dataframe libraries?\r\n- 2-3: minutes: life before vs after Narwhals - real-world examples of how the data landscape is changing\r\n- 7-8 minutes: basics of Narwhals, wrapping native objects, expressions vs Series, lazy vs eager\r\n- 7-8 minutes: advanced Narwhals concepts: row order, non-elementary group-by aggregations, multi-indices, null values, backwards-compatibility promises\r\n- 2-3 minutes: what comes next?\r\n- 5 minutes: engaging Q&A / awkward silence\r\n\r\nTool builders will benefit from the talk by learning how to build tools for modern dataframe libraries without sacrificing support for foundational classic libraries such as pandas.", "code": "CPCNRZ", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/CPCNRZ.png", "speaker_names": "Marco Gorelli", "speakers": "\n### Marco Gorelli\n\n\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Supercharge Your Testing with inline-snapshot", "abstract": "Snapshot tests are invaluable when you are working with large, complex, or frequently changing expected values in your tests.\r\nIntroducing inline-snapshot, a Python library designed for snapshot testing that integrates seamlessly with pytest, allowing you to embed snapshot values directly within your source code.\r\nThis approach not only simplifies test management but also boosts productivity by improving the maintenance of the tests.\r\nIt is particularly useful for integration testing and can be used to write your own abstractions to test complex Apis.", "full_description": "This Talk gives you an introduction into inline-snapshot and how it can transform your testing strategy:\r\n* Foundations of Snapshot Testing: Start with an introduction to what snapshot testing is and why it's a game-changer for Python developers.\r\n* Basic Usage: Learn the core functionality of the `snapshot()` function. Understand how it captures and manages snapshots inline with your tests.\r\n\r\nAdvanced Techniques:\r\n* Dirty Equals: Explore how you can leverage dirty-equals within your snapshots for more flexible assertions, allowing for partial matching which is particularly useful for complex data structures.\r\n* Parametrized Tests: See how inline-snapshot can be applied to parametrized tests, ensuring each parameter set has its own snapshot.\r\n* Customizable: Learn to create your own test functions to test your specific problems.", "code": "CRNJWQ", "state": "confirmed", "created": "2025-01-02", "social_card_image": "/static/media/social/talks/CRNJWQ.png", "speaker_names": "Frank Hoffmann", "speakers": "\n### Frank Hoffmann\n\nI'm a software developer. My goal with my opensource work is to help other people to develop better software faster.\n", "track": "Testing"}, {"title": "Streaming at 30,000 Feet: A Real-Time Journey from APIs to Stream Processing", "abstract": "Traditional API architectures face significant challenges in environments where repetitive and frequent requests are required to retrieve data updates. These request-response mechanisms introduce latency, as clients must continually query the server to check for changes, often receiving redundant or outdated information. This approach leads to increased network overhead, inefficient use of server resources and diminished scalability as the number of clients or requests grows. Additionally, frequent requests expand the attack surface, requiring security measures to mitigate risks such as (un-)authorised access, rate limiting and query sanitisation. Managing all of these inherent problem results in increasingly complex systems to maintain and improve while putting considerable implementation effort onto the customer.\r\nJoin to find out how transitioning to a streaming architecture can address these issues by providing proactive, event-based data delivery, reducing latency, minimising redundant processing, enhancing scalability and simplifying security management.", "full_description": "In this talk we will go over which benefits, drawbacks and lessons Airbus has encountered/learned in the switch from an API to a Python based Stream Architecture for continuous flight traffic prediction. The Goal is to highlight Stream based architectures as a architectural alternative and allow the attendees to decide if it could be a alternative to their current API based Setup worthwhile to look into.\r\n\r\nThe talk addresses the inefficiencies and limitations of traditional API-based architectures for real-time data delivery. Specifically, it explores challenges such as high latency, network overhead, customer effort, and scalability issues when APIs rely on polling mechanisms. These issues became apparent at Airbus during a project as customer needs evolved, highlighting the shortcomings of APIs in handling real-time updates effectively. This story of the project will aid as an example on how to identify a limiting architectural decision and what pain points can potentially be avoided by taking a new route guiding us along the talk.\r\n\r\nFor developers and architects building modern, data-driven applications, choosing the right architecture is critical. Many face similar challenges when scaling APIs for real-time use cases, such as IoT, financial data, or notifications. This problem is relevant because adopting an unsuitable architecture can lead to poor performance, higher costs, and frustrated users.\r\n\r\nThe proposed solution is to transition from an API-based architecture to a streaming architecture for real-time data delivery. This involves leveraging stream processing systems that push updates proactively, handle high-throughput data efficiently, and offer features like backpressure, partitioning, and stateful processing. Recent developments in Python based tools such as **Bytewax**, **Faust** and **Quix** are highlighted for their scalability and fault-tolerance capabilities.\r\n\r\n### Key Takeaways\r\n1. **Challenges of APIs**: Polling APIs is inefficient for real-time updates, leading to delays, resource wastage, and customer dissatisfaction.\r\n2. **Advantages of Streaming**: Streaming architectures offer real-time data delivery, lower latency, reduced customer effort, better scalability, and improved fault tolerance.\r\n3. **Key Streaming Concepts**: Understanding backpressure, partitioning, and stateful processing is essential for understanding streaming specific limitations and solutions.\r\n4. **Architectural Considerations**: Streaming is ideal for use cases where data changes frequently and needs to be delivered in real time, while APIs may still be suitable for low-frequency, static, or manual queries.\r\n5. **Strategic Transition**: Adopting a streaming approach requires a paradigm shift in thinking about how data is delivered and processed, with significant changes to the system architecture which needs to be cautiously managed.", "code": "CTUEJX", "state": "confirmed", "created": "2024-11-27", "social_card_image": "/static/media/social/talks/CTUEJX.png", "speaker_names": "Felix Leon Buck", "speakers": "\n### Felix Leon Buck\n\nSenior Data Scientist and Engineer at Airbus, leading the development of flight plan optimisation and continuous air traffic prediction services based on time series forecasting with ML and physics models.\n", "track": "Programming & Software Engineering"}, {"title": "Practical Python/Rust: Building and Maintaining Dual-Language Libraries", "abstract": "Building performant Python often means reaching for C extensions. This talk explores an alternative: leveraging Rust to create blazing-fast Python modules that also benefit the Rust ecosystem. I will share practical strategies from building `semantic-text-splitter`, a library for fast and accurate text segmentation used in both Python and Rust, demonstrating how to bridge the gap between these two languages and unlock new possibilities for performance and cross-language collaboration.", "full_description": "Building performant Python often means reaching for C extensions. But what if you could achieve similar performance with Rust, while also creating a library usable directly within the Rust ecosystem? This talk explores how Rust can be a powerful ally, creating blazing-fast Python modules that benefit both communities. I will share the strategies I use while building and maintaining my package, `semantic-text-splitter`, used for fast and accurate text segmentation, which sees significant usage in both Python and Rust ecosystems.\r\n\r\nSome key challenges arise when integrating these two languages, such as bridging the gap between Rust's generics and Python's dynamic typing, managing data representation and memory across the Python/Rust boundary, and maintaining type hints and documentation across both languages.\r\n\r\nBut with practical maintenance strategies, these challenges can be overcome. Moreover, you contribute to a growing ecosystem of high-performance Python tools powered by Rust. Join me to learn how to build and maintain dual-language Python/Rust libraries, and discover how this approach can unlock new possibilities for performance and cross-language collaboration.", "code": "CUJMCD", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/CUJMCD.png", "speaker_names": "Ben Brandt", "speakers": "\n### Ben Brandt\n\nBen has been identifying as a Rustacean since 2018. With a background in UI/UX, he's excited to use Rust to make products that are faster, more resilient, and delightful for his users. He's currently a Staff Engineer at Aleph Alpha, where he uses Rust to make AI applications easier to build and operate.\n", "track": "Rust"}, {"title": "Guardians of the Code: Safeguarding Machine Learning Models in a Climate Tech World", "abstract": "LLMs, Machine learning and AI are everywhere, yet their security is often overlooked, leaving your systems vulnerable to serious attacks. What happens when someone tampers with your model\u2019s input, poisons your training data, or steals your model?\r\n\r\nIn this talk, I\u2019ll explore these risks through the lens of the OWASP Machine Learning Security Top 10 using relatable, real-world examples from the climate tech world. I\u2019ll explain how these attacks happen, their impact, and why they matter to you as a Python developer, data scientist, or data engineer.\r\n\r\nYou\u2019ll learn practical ways to defend your models and pipelines, ensuring they\u2019re robust against adversarial forces. Bridging theory and practice, you'll leave equipped with insights and strategies to secure your machine learning systems, whether you\u2019re training models or deploying them in production. By the end, you\u2019ll have a solid understanding of the risks, a toolkit of best practices, and maybe even a new perspective on how important security is everywhere.", "full_description": "Machine learning is applied to a variety of challenges in climate tech, from optimising renewable energy to forecasting energy demands or predicting solar production. We rely more on these models, but we often forget a critical piece: their security. What happens if someone tampers with your model\u2019s inputs, poisons your training data, or sneaks malicious code into an open-source package you\u2019re using? These attacks can throw off predictions and disrupt energy systems or even the grid itself.\r\n\r\nIn this talk, I\u2019ll walk you through the OWASP Machine Learning Security Top 10, using real-world examples from climate tech to show how these attacks can happen. I'll show you cases like manipulating energy consumption forecasts, poisoning datasets, or sneaking malware into open-source libraries used for climate modelling. It\u2019s not just a hypothetical threat, these risks are real and the consequences can be serious.\r\n\r\nI\u2019ll also share practical solutions you can use as a Python developer, data scientist, or data engineer to protect your models and systems. I\u2019ll talk about securing your ML supply chain, validating data, and monitoring your pipelines for suspicious activity. You'll leave with strategies to defend your work so you can build systems that are not only smart but also safe and reliable.\r\n\r\nWhy does this matter? Because in climate tech, the stakes are incredibly high. The predictions we make and the systems we build influence the grid, energy policies, resource allocation, and consumers trust.\r\n\r\nDuring the talk, we'll cover:\r\n\r\n- How attacks on machine learning models can disrupt climate tech applications.\r\n- Examples of adversarial attacks, poisoned datasets, and supply chain vulnerabilities in renewable energy systems.\r\n- Practical steps to protect your machine learning pipelines.\r\n- Why security should be at the core of any ML project, especially in mission-critical fields like climate tech.\r\n\r\nOutline of the Talk:\r\n\r\n1. Why Security in Climate Tech Machine Learning Matters\r\n    - How machine learning is powering renewable energy and climate solutions.\r\n    - What can go wrong when systems are vulnerable.\r\n2. Breaking Down the OWASP ML Security Top 10\r\n    - Input manipulation: How attackers trick models with tampered data.\r\n    - Data poisoning: Real-life example of skewing optimization models with bad data.\r\n    - Supply chain attacks: How a hacked library could disrupt energy demand predictions.\r\n3. Real-World Impact of Attacks\r\n    - Manipulated energy consumption forecasts causing grid instability.\r\n    - Corrupted solar panel efficiency datasets leading to poor resource allocation.\r\n4. How to Protect Your Models\r\n    - How to spot tampered inputs.\r\n    - Data validation, cleaning and checking datasets.\r\n    - Best practices for safe use of open-source libraries.\r\n    - Monitoring and auditing: Setting up checks for unusual activity in your pipelines.\r\n\r\nKey Takeaways\r\n\r\n- Recap of risks and defences.\r\n- Practical steps you can take today to secure your ML systems.\r\n- A call to prioritize security as a core part of building trustworthy ML.\r\n\r\nClimate tech is one of the most exciting and meaningful areas to work in. The systems we\u2019re building have the potential to shape a more sustainable future. But if we don\u2019t make security a priority, we risk undermining the customer's trust. This talk will give you the tools and confidence to keep your machine learning models safe and ensure they\u2019re as reliable and impactful as they need to be.\r\n#ThereIsNoPlanetB", "code": "CVMPVG", "state": "confirmed", "created": "2024-12-03", "social_card_image": "/static/media/social/talks/CVMPVG.png", "speaker_names": "Doreen Sacker", "speakers": "\n### Doreen Sacker\n\nI'm an MLOps Engineer from Berlin working at the start-up 1KOMMA5\u00b0, and I'm part of the women's tech podcast Unmute IT. I aim to empower underrepresented groups to have a say in shaping the algorithms that impact our world today. Also, I\u2019m always on the lookout for the best coffee shop in town \u2615\ufe0f\n", "track": "MLOps & DevOps"}, {"title": "Filling in the Gaps: When Terraform Falls Short, Python and Typer Step In", "abstract": "Not all resources in today\u2019s cloud environments have native Terraform providers. That\u2019s where Python\u2019s Typer library can step in, offering a flexible, production-ready command-line interface (CLI) framework to help fill in the gaps. In this session, we\u2019ll explore how to integrate Typer with Terraform to manage resources that fall outside Terraform\u2019s direct purview. We\u2019ll share a real-life example of how Typer was used alongside Terraform to automate and streamline the management of an otherwise unsupported API. You\u2019ll learn how Terraform can invoke Python scripts\u2014passing arguments and parameters to control complex operations\u2014while still benefiting from Terraform\u2019s declarative model and lifecycle management. We\u2019ll also discuss best practices for defining resource lifecycles to ensure easy maintainability and consistency across deployments. By the end, participants will see how combining Terraform\u2019s robust infrastructure-as-code approach with Python\u2019s versatility and Typer\u2019s user-friendly CLI can create a powerful, cohesive strategy for managing even the trickiest resources in production environments.", "full_description": "In this session, we\u2019ll address a common challenge in managing resources and APIs that lack native Terraform providers but still need to integrate seamlessly into your CI/CD pipeline. I\u2019ll demonstrate how Python\u2019s Typer library can help bridge this gap by offering a straightforward yet powerful command-line interface (CLI). I\u2019ll explain how to create and configure Typer applications, pass parameters, and integrate these scripts with Terraform. \r\n\r\n1. Problem Statement (Managing APIs or resources with incomplete Terraform provider support) - 5 mins\r\n2. Typer (Key components, advantages, and how to use in production enviroment) - 10 mins\r\n3. Terraform resources that can execute CLI and how to work with them - 10 mins\r\n4. Conclusion - 2 mins", "code": "CZXBEP", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/CZXBEP.png", "speaker_names": "Yuliia Barabash", "speakers": "\n### Yuliia Barabash\n\nOver the last five years living in Germany, during which I have gained a diverse range of experiences in the tech industry. My expertise spans from developing web applications in Python to constructing AWS cloud solutions. I have a good understanding of design patterns, Object-Oriented Programming (OOP), event-driven architecture, and microservices architectures, REST API design and database technologies. I have hands-on experience with creating a web application as part of Cloud Foundation framework to manage and secure AWS accounts and creating a lightweight web application to quickly generate and provide results to users.\n", "track": "Infrastructure - Hardware & Cloud"}, {"title": "Distributed file-systems made easy with Python's fsspec", "abstract": "The cloud native revolution has impacted all aspects of engineering, and data engineering is not exempt. One of the ongoing challenges in the data engineering world remains the local and distributed cloud native storage. In this talk we\u2019ll explore working with distributed file systems in Python, through an intro to fsspec: a popular python library that is well-positioned to address the growing challenge of interacting with storage systems of different kinds in a consistent way.\r\n\r\nIn this talk we\u2019ll show hands-on examples of working with fsspec with some of the most popular data tools in the Python community: Pandas, Tensorflow and PyArrow. We\u2019ll demonstrate a real world implementation of fsspec and how it provides easy extensibility through open source tooling.\r\n\r\nYou\u2019ll come away from this session with a better understanding for how to implement and extend fsspec to work with different cloud native storage systems.", "full_description": "### **1. Setting the Stage: Local vs. Distributed Storage (5 minutes)**  \r\n- **What\u2019s the Big Deal with Storage?**  \r\n  - First, let\u2019s talk about the shift from local storage (where we keep files on our own machines) to cloud-native storage (where data is spread across servers in the cloud).  \r\n  - This shift is awsome but comes with new challenges: distributed systems can be tricky to work with, especially when you need to access them in a consistant way.  \r\n\r\n### **2. Enter fsspec: A Game Changer for File Systems (10 minutes)**  \r\n- **What is fsspec?**  \r\n  - fsspec is a Python library that makes working with any kind of file system\u2014whether it's local, in the cloud, or on a distributed system\u2014much easier.  \r\n  - It does this by giving us a unified way to interact with storage, no matter where the files actaully live.  \r\n\r\n- **Why is fsspec Awesome?**  \r\n  - It simplifies file operations (like opening and reading files) across different storage systems, saving us time and mental enery.  \r\n  - Plus, it\u2019s open-source, which means you can extend it and make it work for your own unique storage setup.  \r\n\r\n### **3. fsspec in Action: How It Works with Popular Python Tools (15 minutes)**  \r\n\r\n#### **A. Using fsspec with Pandas**  \r\n- **Pandas & fsspec:**  \r\n  - If you work with Pandas, you\u2019re probably familiar with loading and saving data. fsspec helps make this process smoother by letting you pull data from cloud storage (like AWS S3) with no fuss.  \r\n  - We\u2019ll see how this works in practise, making it easy to work with large datasets in the cloud. \r\n\r\n#### **B. Using fsspec with TensorFlow**  \r\n- **TensorFlow & fsspec:**  \r\n  - If you\u2019re building machine learning models, TensorFlow needs to access training data and models, sometimes stored in the cloud.  \r\n  - With fsspec, TensorFlow can seamlessly interact with cloud storage, making your ML pipelines more streamlined and less frustraiting.  \r\n\r\n#### **C. Using fsspec with PyArrow**  \r\n- **PyArrow & fsspec:**  \r\n  - PyArrow is great for high-performance data processing. When working with big data files like Parquet, fsspec makes it easy to load and save them from cloud storage without missing a beat.  \r\n\r\n### **4. Extending fsspec: Building Your Own Solutions (5 minutes)**  \r\n- **What if I Need Something Custom?**  \r\n  - Sometimes, you need to work with storage systems that aren\u2019t \u201cout of the box.\u201d The cool part about fsspec is that it\u2019s highly extensible.  \r\n  - I\u2019ll walk through how you can easily extend fsspec to work with your own custom storage systems, using a real-world example of how we did this.  \r\n\r\n### **5. Wrap-Up & Key Takeaways (5 minutes)**  \r\n- **The Big Picture:**  \r\n  - fsspec is a simple yet powerful tool for making cloud-native storage work seamlessly with Python data tools like Pandas, TensorFlow, and PyArrow.  \r\n  - It\u2019s the tool you didn\u2019t know you needed to simplify your cloud storage tasks.  \r\n\r\n- **Final Thought:**  \r\n  - With fsspec, working with distributed storage doesn\u2019t have to be hard. It makes everything feel like you\u2019re working with local files, even when they\u2019re scattered across the cloud.  \r\n\r\n### **6. Q&A Session (5 minutes)**", "code": "DEHZHK", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/DEHZHK.png", "speaker_names": "Einat Orr", "speakers": "\n### Einat Orr\n\nDr. Einat Orr has 20+ years of experience building R&D organizations and leading the technology vision at multiple companies, the latest being Similarweb, that IPO in NYSE last May.  Currently she serves as  Co-founder and CEO of Treeverse, the company behind lakeFS, an open source platform that delivers a git-like experience to object-storage based data lakes. She received her PhD. in Mathematics from Tel Aviv University, in the field of optimization in graph theory.\n", "track": "Data Handling & Engineering"}, {"title": "Electify - Retrieval-Augmented Generation for Voter Information in the 2024 European Election", "abstract": "In general elections, voters often face the challenge of navigating complex political landscapes and extensive party manifestos. To address this, we developed Electify, an interactive application that utilizes Retrieval-Augmented Generation (RAG) to provide concise summaries of political party positions based on individual user queries. During its first roll-out for the European Election 2024, Electify attracted more than 6,000 active users. This talk will explore its development and deployment. It will focus on its technical architecture, the integration of data from party manifestos and parliamentary speeches, and the challenges of ensuring political neutrality and providing accurate replies. Additionally, we will discuss user feedback and ethical considerations, focusing on how generative AI can enhance voter information systems.", "full_description": "In general elections, voters often face the  challenge of navigating complex political landscapes. These challenges include understanding the differences between nuanced policy positions, comparing extensive party manifestos, and reconciling conflicting information from various sources. The sheer volume of information and the high frequency of elections can lead to voter fatigue and disengagement [1]. Existing tools like Wahlomat are helpful for voters but don\u2019t adapt well to individual preferences or specific questions. To address these issues, we developed Electify\u2014an interactive application designed to empower voters by addressing these pain points. \r\n\r\nUsing Retrieval-Augmented Generation (RAG), Electify simplifies the decision-making process by enabling users to access concise and relevant summaries of political party positions tailored to their individual queries. Our user interface provides the possibility to fact-check the generated responses by directly showing the original sources. Additionally, we included a blinding feature to combat confirmation bias: users can hide party names and read summaries of their positions before unblinding. This talk will explore the technical development and deployment of Electify, covering its  architecture, integration of data from party manifestos and parliamentary speeches, and strategies to maintain political neutrality and accuracy in responses. In particular, we will discuss our efforts to use reranking to improve context relevancy and LLM-as-a-judge evaluation for parameter optimization. We identify a trade-off between factual accuracy and the frequency of denied responses, which we think is highly relevant for generative AI systems that operate within sensitive areas like voter information [2].\r\n\r\nDuring its first roll-out for the European Election 2024, Electify received significant attention, attracting 6,000 active users who leveraged the platform to make more informed and confident voting decisions. We will address the lessons learned from user feedback and discuss the ethical considerations involved, emphasizing the potential of generative AI to enhance voter information systems and promote political engagement.\r\n\r\nContributors: Christian Liedl, Anna Neifer, Joshua Nowak\r\n[Github Repository](https://github.com/electify-eu/europarl-ai)\r\n\r\n[1] Kostelka et al. \"Election frequency and voter turnout.\" Comparative Political Studies 56.14 (2023)\r\n[2] Cao, Lang. \"Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism.\" arXiv:2311.01041 (2023).", "code": "DNVCEY", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/DNVCEY.png", "speaker_names": "Christian Liedl", "speakers": "\n### Christian Liedl\n\nChristian hat einige Jahre als Physiker auf dem Gebiet der experimentellen Quantenoptik geforscht und sich seit 2024 auf Data Science und K\u00fcnstliche Intelligenz spezialisiert.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "GitMLOps \u2013 How we are managing 100+ ML pipelines in AWS SageMaker", "abstract": "Scaling machine learning pipelines is no small feat - especially when you\u2019re managing over 100 of them on AWS SageMaker. In this talk, I\u2019ll take you behind the scenes of how our team at idealo built a Git-based MLOps framework that powers millions of real-time recommendations every minute.\r\n\r\nI\u2019ll share the challenges we faced, the solutions we implemented, and the lessons we learned while streamlining model versioning, deployment, and monitoring. This session is packed with actionable takeaways for ML engineers, data scientists, and DevOps professionals looking to simplify their MLOps workflows and operate efficiently at scale.\r\n\r\nWhether you\u2019re running a handful of pipelines or preparing to scale up, this talk will equip you with the tools and strategies to tackle MLOps with confidence.", "full_description": "In 2022, idealo\u2019s Machine Learning Engineering (MLE) team took on a bold mission: to transform and scale the recommendation systems powering the idealo website. Fast forward to today, we\u2019re delivering over 1 million recommendations per minute across 20 key user touchpoints - driving seamless, personalized experiences at scale.\r\n\r\nBut how do you manage over 100 machine learning pipelines without breaking a sweat? In this talk, I\u2019ll reveal the three core principles that helped us build a sustainable and efficient MLOps workflow in AWS SageMaker:\r\n\r\n* Decoupling pipeline releases from deployments for ultimate flexibility\r\n* Testing pipelines to ensure seamless performance\r\n* Centrally managing infrastructure as code for full control and scalability\r\nIf you\u2019re ready to supercharge your MLOps game, this session will leave you with practical strategies and battle-tested solutions for running ML pipelines like a pro.", "code": "DPAPUA", "state": "confirmed", "created": "2024-12-16", "social_card_image": "/static/media/social/talks/DPAPUA.png", "speaker_names": "Bogdan Girman", "speakers": "\n### Bogdan Girman\n\nBogdan Girman is an expert in Machine Learning and DevOps, with extensive experience in implementing scalable, reproducible ML systems. He is passionate about bridging the gap between development and operations in AI.\n", "track": "MLOps & DevOps"}, {"title": "Beyond Basic Prompting: Supercharging Open Source LLMs with LMQL's Structured Generation", "abstract": "This intermediate-level talk demonstrates how to leverage Language Model Query Language (LMQL) for structured generation and tool usage with open-source models like Llama. You will learn how to build a RAG system that enforces output constraints, handles tool calls, and maintains response structure - all while using open-source components. The presentation includes hands-on examples where audience members can experiment with LMQL prompts, showcasing real-world applications of constrained generation in production environments.", "full_description": "1. Introduction to structured generation with LMQL and open-source LLMs\r\n   - Key differences between constrained and free-form generation\r\n   - Why structure matters for production applications\r\n   - Setting up LMQL with Llama\r\n\r\n2. Building a RAG system with structured outputs\r\n   - Implementing context retrieval with constraints\r\n   - Enforcing response formats through LMQL decorators\r\n   - Handling edge cases and error states\r\n\r\n3. Tool usage and function calling\r\n   - Implementing tool calls through LMQL\r\n   - Managing tool execution flow\r\n   - Error handling and fallbacks\r\n\r\n4. Interactive segment\r\n   - Audience members will write and test their own LMQL prompts through a live demo environment\r\n\r\n5. Production considerations\r\n   - Scaling structured generation\r\n   - Monitoring and logging strategies\r\n\r\nAttendees will leave with practical knowledge of how to implement structured generation in their own projects using LMQL, understanding both the technical implementation and best practices for production deployment.", "code": "DQTMJB", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/DQTMJB.png", "speaker_names": "Christiaan Swart", "speakers": "\n### Christiaan Swart\n\nOn a mission to structure unstructured text with NLP\r\n\r\nEx-cofounder with 8 years experience in NLP\r\n\r\nI come from a mixed Hungarian-Dutch background and live in Nuremberg at the moment\r\n\r\nIn my free time I enjoy improv theatre and swimming\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "3 Ways to Speed up Your Regression Modeling in Python", "abstract": "Linear Regression is the workhorse of statistics and data science. Some data scientists even go as far and argue that \"linear regression is all you need\". \r\n\r\nIn this talk, we will introduce three ways to run regression models faster by using smarter algorithms, implemented in the scikit-learn & fastreg (sparse solvers), pyfixest (Frisch-Waugh-Lovell), and duckreg (regression compression via duckdb) libraries.", "full_description": "We introduce three different ways to make regressions run faster. \r\n\r\nWe first introduce sparse solvers and show how to run regressions on sparse matrices via scikit-learn and the fastreg libraries. \r\n\r\nWe then lay out the Frisch-Waugh-Lovell theorem and the alternating projections algorithm and show how to speed it up on the CPU (via numba) and on the GPU (via JAX) as implemented in the pyfixest library. \r\n\r\nFinally, we demonstrate how to drastically speed up regression estimation by first preprocessing the data in duckdb and then fitting a regression via weighted least squares in memory. \r\n\r\nReferences: \r\n- fastreg: https://github.com/iamlemec/fastreg\r\n- scikit-learn: https://github.com/scikit-learn/scikit-learn\r\n- pyfixest: https://github.com/py-econometrics/pyfixest\r\n- duckreg: https://github.com/py-econometrics/duckreg", "code": "ECNDQM", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/ECNDQM.png", "speaker_names": "Alexander Fischer", "speakers": "\n### Alexander Fischer\n\nEconomist and Data Scientist. I spend most of my week working on online auctions at Trivago. In the evenings and weekend, I work on open source packages for regression modeling and inference in R and Python.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Learnings from migrating a Flask app to FastAPI", "abstract": "FastAPI has been constantly growing in popularity during the last years. A lot of this growth is driven by its relative simplicity and ease-of-use. In this talk, we'll discuss some practical insights into building a FastAPI application, based on my experience of migrating an existing Flask prototype to FastAPI. \r\n\r\nWe'll explore how FastAPI's core features like Pydantic integration and dependency injection can improve API development, while also talking about the drawbacks of FastAPI.", "full_description": "Building HTTP APIs has become a normal part of the work as a software or data engineer within the last 10 to 15 years. In the Python ecosystem Flask was the only option to build an HTTP API for many years. After its initial release in 2018 FastAPI quickly became a serious alternative to build such APIs with Python.\r\n\r\nIn this talk I will share my experiences from migrating an existing HTTP API built with flask to a FastAPI-based API. \r\n\r\nWe will discuss the following topics: \r\n\r\n- Why did we migrate at all?\r\n- Modeling data for input and output \r\n- Async is overrated\r\n- The different ecosystems and why it doesn't matter\r\n- Problems you **will** encounter \r\n\r\nThe talk will show you the practical differences between developing APIs with FastAPI or Flask.", "code": "EDJ8N7", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/EDJ8N7.png", "speaker_names": "Orell Garten", "speakers": "\n### Orell Garten\n\nFreelance Software and Data Engineer. I love building data systems of any kind while my main focus is on backend  engineering.\n", "track": "Django & Web"}, {"title": "Are LLMs the answer to all our problems?", "abstract": "Generative AI models have shaken up the German market. Since the release of ChatGPT, AI is available and usable for everyone. The number of ChatGPT-based agents is growing rapidly, but concerns about privacy, copyright and ethics remain. Regulation and ethical AI go hand in hand, but are often seen as barriers. The presentation will cover the different aspects of ethics and how they are addressed by regulation. It will give an overview of how to use large language models in a safe and practical way. This won't only address the various ethical issues, but also convince your next customer to invest in your AI-based product.", "full_description": "The talk will delve into the complexities of large language models (LLMs), exploring their capabilities and challenges. We'll look at bias in face recognition and word2vec, highlighting cases such as the COMPAS system and Amazon's recruitment tool, which have raised concerns about fairness and accuracy. The intersection of LLM and copyright will also be discussed, including the use of copyrighted material in training data and potential infringement issues.  When talking about data, regulations such as the EU AI Act, the CLOUD Act and data privacy will be examined, raising important questions about data sovereignty and cross-border data transfers. \r\n\r\nThe environmental impact of LLMs will be addressed, focusing on their significant carbon footprint and the need for sustainable solutions. An overview of the LLM landscape will be provided, including English models and European alternatives. By exploring these topics, participants will gain a deeper understanding of the opportunities and challenges presented by LLMs, as well as the regulatory frameworks and best practices that can help mitigate their risks.", "code": "EN3QPQ", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/EN3QPQ.png", "speaker_names": "Dr. Maria B\u00f6rner", "speakers": "\n### Dr. Maria B\u00f6rner\n\nDr Maria B\u00f6rner is a legal tech expert in the use of AI and heads the AI Competence Centre at Westernacher Solutions. In her role, she is responsible for the development of AI tools in the government, legal and church sectors. She has been working in AI for more than 8 years and bridges the gap between AI development and customers. She volunteers to support the Women in AI network by organising partnerships and visibility.\n", "track": "Ethics & Privacy"}, {"title": "Powering Up DDoS Defense with Python: Building Resilient Systems", "abstract": "This talk will explore how Python can be leveraged to build robust DDoS defense mechanisms, focusing on real-time threat detection, mitigation strategies, and system resilience. We will dive into key Python libraries, best practices, and techniques to protect your applications from large-scale DDoS attacks while ensuring high availability.", "full_description": "In the world of modern web applications, Distributed Denial of Service (DDoS) attacks are a growing concern that can cripple services, disrupt business operations, and damage brand reputation. In this session, we\u2019ll dive deep into how Python, when combined with Jenkins, ELK Stack, and ElastAlert, can be used to create an effective, scalable, and resilient DDoS defense system.\r\n\r\nKey Takeaways:\r\n\r\n1. Using Python for Real-Time Detection:\r\n   - Learn how Python\u2019s versatility and performance make it an ideal tool for real-time detection of abnormal traffic patterns that are indicative of a DDoS attack.\r\n   - We will explore how Python scripts can monitor traffic, analyze request headers, identify rate-limiting violations, and trigger alerts when thresholds are exceeded.\r\n\r\n2. Automating Defense with Jenkins:\r\n   - Jenkins, a widely used automation server, can play a crucial role in the DDoS defense workflow by automating the response processes.\r\n   - We'll walk through how Jenkins pipelines can trigger specific defense mechanisms, such as blocking malicious IPs, scaling up server resources, or engaging additional security measures based on attack patterns identified by Python.\r\n\r\n3. Leveraging ELK Stack for Enhanced Monitoring and Visualization:\r\n   - Learn how to integrate Python scripts with the ELK (Elasticsearch, Logstash, Kibana) Stack for better visibility into traffic patterns and security events.\r\n   - Elasticsearch will store and index traffic data, while Logstash processes and analyzes logs in real-time. Kibana\u2019s powerful visualization capabilities allow security teams to quickly identify anomalies and respond to potential DDoS threats.\r\n\r\n4. Proactive Alerts with ElastAlert:\r\n   - ElastAlert, a tool built on top of Elasticsearch, will be used to set up automated alerts and notifications whenever suspicious activity is detected by Python scripts.\r\n   - We\u2019ll discuss how to create custom alerting rules in ElastAlert to notify system administrators via email, Slack, or other communication channels when a DDoS attack is imminent or active.\r\n\r\n5. End-to-End Automation:\r\n   - Combining Python, Jenkins, ELK, and ElastAlert creates an integrated DDoS defense system where Python handles real-time detection and analysis, Jenkins automates defensive actions, ELK provides monitoring and visualization, and ElastAlert ensures timely alerts.\r\n   - This end-to-end solution improves the response time, reduces human error, and ensures that your system remains protected even under sustained attack.\r\n\r\nReal-World Use Case:\r\n\r\n- A gaming platform like PokerBaazi, with millions of active users, could benefit from this setup to defend against DDoS attacks that often target high-traffic periods or gaming events. Using Python, the platform can detect abnormal spikes in traffic patterns and trigger Jenkins to scale resources automatically or block malicious traffic through firewalls, all while sending out alerts to administrators through ElastAlert.\r\n\r\nWhy Attend?\r\n\r\nThis session will equip you with the knowledge to build an automated, scalable, and proactive DDoS defense system using Python and powerful open-source tools. Whether you're a DevOps engineer, a security analyst, or a developer, you\u2019ll walk away with actionable insights to protect your applications from the growing threat of DDoS attacks.", "code": "EPAVRR", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/EPAVRR.png", "speaker_names": "Siddharth Vijay", "speakers": "\n### Siddharth Vijay\n\nSiddharth Vijay is an Expert Panel Speaker at various DevOps, Cloud & CyberSecurity conferences held in India & Abroad - details on Linkedin profile.\r\n\r\nHighly experienced and results-driven AVP Engineering with a proven track record of delivering innovative solutions and driving technology excellence. With a strong background in technology, Mr. Vijay brings 12 years of experience in leading high-performing engineering teams and delivering successful projects.\r\n\r\nLeading the DevOps, Cloud, and DevSecOps team at Baazi Games for the past four years which caters to 1 million+ annual users. Previously led DevOps teams at companies such as Genpact and Fidelity. Implementing top-tier DevOps practices and fostering a culture of excellence throughout my career, he has had the privilege of mentoring hundreds of peers, leveraging my skills and expertise.\r\n\r\nThroughout his career, Mr. Vijay have demonstrated a deep understanding of engineering principles and a passion for leveraging cutting-edge technologies to solve complex problems. His expertise spans across Cloud, DevOps, Database, Platform Engineering (Backend), QA & Automation, Project Management and Delivery enabling him to drive strategic initiatives, optimize processes, and deliver scalable and reliable solutions with his niche skills and forte being DevOps.\r\n\r\nAs an AVP Engineering, he has a track record of building and managing high-performing DevOps teams, fostering a culture of innovation, and nurturing talent.\r\n\r\nRecently gave talks at KubeCon India and DevOpsCon Munich in Dec 2024.\n", "track": "Security"}, {"title": "Bias Meets Bayes: A Bayesian Perspective on Improving Model Fairness", "abstract": "Bias in machine learning models remains a pressing issue, often disproportionately affecting the most vulnerable groups in society. This talk introduces a Bayesian perspective to effectively tackle these challenges, focusing on improving fairness by modeling and addressing bias directly.\r\nYou will learn about the interplay between uncertainty, equity, and predictive accuracy, while gaining actionable insights to improve fairness in diverse applications. Using a practical example of a risk-scoring model trained on data with underrepresented minority groups, I will showcase how Bayesian methods compare to traditional techniques, demonstrating their unique potential to mitigate bias while maintaining performance.", "full_description": "Machine learning models often perpetuate biases that exacerbate societal inequities, particularly for vulnerable groups. As machine learning increasingly shapes critical decisions, addressing these biases is more important than ever. In this talk, I will explain how Bayesian methods offer a principled and effective approach to improving fairness by directly addressing bias and incorporating uncertainty into machine learning models.\u2028\r\n\r\nThe talk will cover:\r\n\r\n1.\tTheoretical Foundations: I will start by exploring the connection between Bayesian statistics, fairness, and accuracy, with a focus on why uncertainty is a crucial factor in fairness interventions.\r\n2.\tPractical Example: Using a risk-scoring model trained on a dataset with underrepresented minority groups, I will demonstrate how Bayesian methods compare to traditional fairness techniques. This example will illustrate their ability to not only mitigate bias but also adapt to complex, real-world data distributions while maintaining predictive accuracy.\r\n3.\tKey Insights and Applications: Finally, I will provide actionable takeaways on incorporating Bayesian thinking into existing workflows, enabling more equitable and robust outcomes across diverse applications.\u2028\r\n\r\nThis talk is designed to be accessible to a broad audience. While minimal familiarity with machine learning concepts and fairness principles is recommended, no advanced knowledge of statistics is required. Attendees will leave with practical tools, code examples, and insights to address bias effectively in real-world scenarios, empowering them to promote fairness in their own projects and organizations.", "code": "ER3V7W", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/ER3V7W.png", "speaker_names": "Vince Nelidov", "speakers": "\n### Vince Nelidov\n\nVince Nelidov is a Staff Data Science at Blue Yonder with diverse consulting experience in the data domain in a variety of industries from energy sector and banking to skincare and agriculture. Throughout his years in the data world, Vince has been combining advanced data science with business insights to make data work with an impact. He aspires to see far beyond what is on the surface and get to the essence of the problems, discovering robust and scalable long-term solutions rather than temporary fixes.\r\n\r\nVince is passionate about sharing his knowledge and insights, believing that Data literacy should not be a privilege of a few. And his goal is to be there to make this a reality. Making the intricacies of data science intelligible and uncovering the regularities hiding in the data is a major source of inspiration for Vince. With this goal in mind, he combines his years of experience in consulting with his background in statistics, research and teaching to make this knowledge accessible to businesses and individuals in need.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Building Reliable AI Agents for Publishing: A DSPy-Based Quality Assurance Framework", "abstract": "As publishers increasingly adopt AI agents for content generation and analysis, ensuring output quality and reliability becomes critical. This talk introduces a novel quality assurance framework built with DSPy that addresses the unique challenges of evaluating AI agents in publishing workflows. Using real-world examples from newsroom implementations, I will demonstrate how to design and implement systematic testing pipelines that verify factual accuracy, content consistency, and compliance with editorial standards. Attendees will learn practical techniques for building reliable agent evaluation systems that go beyond simple metrics to ensure AI-generated content meets professional publishing standards.", "full_description": "This presentation addresses one of the most pressing challenges in professional publishing today: ensuring quality and reliability when deploying AI agents in editorial environments. We'll take a deep dive into how DSPy's programmatic approach to language model development can be leveraged to create robust testing and validation pipelines that meet the demanding standards of modern newsrooms.\r\nThe discussion begins by exploring the current landscape of AI evaluation in publishing workflows, examining why traditional testing approaches fall short when dealing with language models, and identifying the specific quality requirements unique to journalistic and editorial content. We'll then move into a detailed technical exploration of solutions built with DSPy, demonstrating how to design modular evaluation pipelines, implement publishing-specific metrics, and create automated systems for fact-checking and consistency validation. Special attention will be given to the integration of knowledge graphs for reference-based evaluation and the incorporation of these systems into broader MLOps workflows.\r\nTo ground these concepts in reality, we'll examine a detailed case study of implementing this framework in an actual newsroom environment. This will include practical discussions of handling various content types, along with strategies for managing test data and evaluation criteria. We'll share real-world performance monitoring approaches and concrete improvement strategies that have proven successful in production environments.\r\nThe presentation concludes with hard-won insights and best practices, including practical strategies for finding the right balance between automated testing and human review, effective approaches to handling edge cases, and methods for scaling quality assurance processes across diverse content teams. Throughout the talk, we'll share code examples and practical implementations that attendees can adapt for their own projects.\r\nThis session is specifically designed for technical leads and machine learning engineers, though the principles and approaches discussed will be valuable for anyone involved in AI quality assurance. Attendees will leave with a comprehensive understanding of how to design and implement QA processes for AI agents, practical knowledge of DSPy implementation for automated testing, and concrete strategies for maintaining high quality standards in AI-assisted workflows.", "code": "F7RDPT", "state": "confirmed", "created": "2025-01-04", "social_card_image": "/static/media/social/talks/F7RDPT.png", "speaker_names": "Simonas \u010cerniauskas", "speakers": "\n### Simonas \u010cerniauskas\n\nDr.-Ing. Simonas \u010cerniauskas is the founder and CTO of tisix.io, specializing in developing practical LLM solutions for media and publishers. With a doctorate from RWTH Aachen and experience as a principal researcher at Research Center J\u00fclich, he combines deep technical expertise with hands-on implementation experience. His work focuses on multi-modal content generation and media processing. Drawing from his background in mechanical engineering, quality assurance and machine learning engineering, Simonas develops scalable AI solutions while maintaining a strong focus on quality assurance and risk management. He regularly shares insights through speaking engagements and technical publications, helping organizations navigate the complexities of AI implementation with practical, business-focused approaches.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Modern NLP for Proactive Harmful Content Moderation", "abstract": "Despite an array of regulations implemented by governments and social media platforms worldwide (i.e. famous DSA), the problem of digital abusive speech persists. At the same time, rapid advances in NLP and large language models (LLMs) are opening up new possibilities\u2014and responsibilities\u2014for using this technology to make a positive social impact. Can LLMs streamline content moderation efforts? Are they effective at spotting and countering hate speech, and can they help produce more proactive solutions like text detoxification and counter-speech generation?\r\n\r\nIn this talk, we will dive into the cutting-edge research and best practices of automatic textual content moderation today. From clarifying core definitions to detailing actionable methods for leveraging multilingual NLP models, we will provide a practical roadmap for researchers, developers, and policymakers aiming to tackle the challenges of harmful online content. Join us to discover how modern NLP can foster safer, more inclusive digital communities.", "full_description": "The rise of large language models (LLMs) has revolutionized natural language processing (NLP), creating opportunities to address complex societal challenges, including the pervasive issue of harmful online content. Despite global regulations and platform-specific policies, abusive speech and toxic content continue to plague digital spaces, highlighting the need for smarter, scalable, and multilingual solutions.\r\n\r\nThis talk explores how modern NLP technologies can play a transformative role in content moderation, moving beyond traditional detection methods to proactive measures that promote healthier online interactions. We will cover key topics, including:\r\n\r\n* Understanding the Landscape: Definitions and nuances of harmful content categories, including hate speech, misinformation, and harassment. We will bring practices not only from CS field, but from communication with social scientists and NGOs.\r\n* Hate Speech Detection: Can LLMs detect hate speech? How the models can be adapted to new languages?\r\n* Text Detoxification: Diving into nuances of toxicity of 9 languages (from our recent shared task) and sharing best practice on LLMs prompting for texts detoxification.\r\n* Counter-Speech Generation: Our recent research results on how make LLMs generate not a very general \"Please, it is not ok to talk like this report\" but indeed address the targeted group.\r\n* Ethical Considerations: Who, in the end, responsible for the content moderation? How the community can help to bring best practices? How the measure the \"effectiveness\" of LLMs for content moderation?", "code": "F9EFXA", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/F9EFXA.png", "speaker_names": "Daryna Dementieva", "speakers": "\n### Daryna Dementieva\n\nHello, I\u2019m Dr. Daryna Dementieva. Driven by both personal experiences and a deep passion, I am a dedicated advocate and researcher focused on leveraging AI and NLP for Positive Social Impact. Currently (as a technical person) I am exploring collaborations with NGOs and social scientists to bridge the gap between cutting-edge AI technology and societal needs. My goal is to share insights on responsible AI and Data Science, inspiring and enabling projects in these fields to transition from concept to impactful reality.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Conformal Prediction: uncertainty quantification to humanise models", "abstract": "Quantifying model uncertainties is critical to improve model reliability and make sound decisions. Conformal Prediction is a framework for uncertainty quantification that provides mathematical guarantees of true outcome coverage, allowing more informed decisions to be made by stakeholders", "full_description": "Quantifying uncertainties of Machine Learning models is crucial to improve their reliability, accurately assess risks and make more robust decisions. By quantifying and understanding uncertainty, we can build more reliable and trustworthy systems.\r\n\r\nImagine we have a model that predicts whether or not a CT scan contains a tumour: traditional approaches tend to provide binary predictions, while not providing information on the model\u2019s confidence in each prediction.\r\n\r\nConformal Prediction (CP) is a framework for uncertainty quantification that offers an estimate of the confidence in the model\u2019s predictions: instead of providing just a point estimate, it provides a set of possible outcomes (prediction set), together with a measure of confidence in each outcome. These prediction sets come with a (mathematical!) guarantee of coverage of the true outcome, ensuring that they will detect at least a pre-fixed percentage of true values. CP is a model-agnostic paradigm, requiring no retraining of the model and making no major assumptions about the distribution of the data.\r\n\r\nWe Humans, when faced with uncertainty, tend to express indecision and offer alternatives. We will see that CP can be a key tool to include a human in the decision-making loop, once the \u2018humanised\u2019 machine is able to express its uncertainty.\r\n\r\nCP therefore offers a robust framework that allows stakeholders to make more informed decisions, even more so in high-risk sectors such as healthcare, finance and autonomous systems.", "code": "FGEUJJ", "state": "confirmed", "created": "2024-12-13", "social_card_image": "/static/media/social/talks/FGEUJJ.png", "speaker_names": "Vincenzo Ventriglia", "speakers": "\n### Vincenzo Ventriglia\n\nA results-driven data professional \u2013 focused on hype-free solutions tailored to business needs.\r\n\r\nI am currently creating value at the National Institute of Geophysics and Volcanology (INGV), where I develop machine learning models in the Space Weather domain. My job is complemented by finding the hidden stories in data and make them accessible to stakeholders. I studied Physics in Italy (Napoli) and Germany (Frankfurt am Main), previously worked on Analytics in the strategic division of the world's largest professional services network, and in the Data Science department of the leading Italian publisher.\r\n\r\nWhen not at work, I enjoy theatre, talking about finance or learning a new language.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "\ud83e\udd80 R\u00fcstzeit: Asynchronous Concurrency in Python & Rust", "abstract": "Many Python developers are enhancing their Rust knowledge and want to take the next step in translating their understanding of advanced concepts like asynchronous programming. \r\n\r\nIn this talk, I'll help you take that step by juxtaposing Python's asyncio with Rust's async ecosystems, tokio and async-std. Through real-world examples and insights from conversations with graingert, co-author of Python's Anyio, we'll explore how each language approaches asynchronous execution, highlighting similarities and differences in syntax, performance, and ecosystem support. \r\n\r\nThis talk aims to persuade you that by leveraging Rust's powerful type system and compiler guarantees, we can build fast, reliable async code that's less prone to race conditions and concurrency bugs. Whether you're a Pythonista venturing into Rust or a Rustacean curious about Python's concurrency model, this session will provide practical insights to help you navigate async programming across both languages.\r\n\r\nWelcome to R\u00fcstzeit: Prepare to navigate async programming across both ecosystems.", "full_description": "Talk Timings (30 minutes):\r\n\r\nIntroduction and Hybrid Programming in Python and Rust [5 mins]\r\nAsynchronous Programming in Python [5 mins]\r\nAsynchronous Programming in Rust [5 mins]\r\nPerformance Comparison: Python vs. Rust [1 min]\r\nLeveraging Rust's Type System and Compiler Guarantees [5 mins]\r\nCase Study: \"A Million Large Language Monkeys at a Million Typewriters\" \u2013 Building Scalable Microservices with Tokio [7 mins]\r\n(Optional) Tom's Library: AnyIO and Unified Async in Python [3 mins]\r\nConclusion and Takeaways [3 mins]\r\n\r\n---\r\n\r\nMany Python developers are enhancing their Rust knowledge and want to take the next step in translating their understanding of advanced concepts like asynchronous programming. \r\n\r\nIn this talk, I'll help you take that step by juxtaposing Python's asyncio with Rust's async ecosystems, tokio and async-std. Through real-world examples and insights from conversations with graingert, co-author of Python's Anyio, we'll explore how each language approaches asynchronous execution, highlighting similarities and differences in syntax, performance, and ecosystem support. \r\n\r\nThis talk aims to persuade you that by leveraging Rust's powerful type system and compiler guarantees, we can build fast, reliable async code that's less prone to race conditions and concurrency bugs. Whether you're a Pythonista venturing into Rust or a Rustacean curious about Python's concurrency model, this session will provide practical insights to help you navigate async programming across both languages.\r\n\r\nWelcome to R\u00fcstzeit: It's time to prepare for async programming in Python and Rust.\r\n\r\nFurther Resources:\r\n\r\nhttps://rust-lang.github.io/async-book/\r\nhttps://anyio.readthedocs.io/en/stable/\r\nhttps://github.com/graingert", "code": "FGFFEE", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/FGFFEE.png", "speaker_names": "Jamie Coombes", "speakers": "\n### Jamie Coombes\n\nI am a Machine Learning Engineer with 4 years of Python and PyTorch development experience. I've provided ML expertise to startups and the UK government, and I'm particularly interested in beneficial AI applications. My background is in Physics and Atmospheric Physics, where I interpreted large tropical cyclone datasets at Imperial College London.\r\n\r\nMy previous talks are: \r\nEuroPython Prague 2022 - \ud83d\udc0d Large Language Model Zen\r\nPyCon/PyData DE Berlin 2023 - Mojo \ud83d\udd25 - Is it Python's faster cousin or just hype? \r\n\r\nCompleting my language trilogy: I recently began exploring Rust \ud83e\udd80.\n", "track": "Rust"}, {"title": "From Queries to Confidence: Ensuring SQL Reliability with Python", "abstract": "SQL remains a foundational component of data-driven applications, but ensuring the accuracy and reliability of SQL logic is often challenging. SQL testing can be cumbersome, time-consuming, and error-prone. However, these challenges can be addressed by leveraging the simplicity of Python's testing framework such as pytest, enabling clean, robust, and automated SQL testing.", "full_description": "SQL is an essential part of data-driven applications, powering everything from simple queries to complex data transformations. However, ensuring the accuracy and reliability of SQL code is often challenging, particularly when dealing with intricate logic or large-scale datasets. Also, deploying changes in SQL code to production is another complex task, as it requires careful validation to avoid breaking the query logic.\r\n\r\nFortunately, integrating Python\u2019s testing framework such as pytest into SQL workflows provides a streamlined solution for these challenges. Such approach enables creating clean, efficient, and automated testing processes for SQL code and database logic. Therefore, we can validate query results, enforce schema consistency, and simulate complex data scenarios, all while reducing manual effort and improving test coverage.\r\n\r\nThis talk will address:\r\n- configuring lightweight database fixtures\r\n- verifying SQL query result and testing scripts seamlessly\r\n- data mocking\r\n- schema validation\r\n- testing non-deterministic queries\r\n- handling large datasets\r\n\r\nAttendees will gain insights into improving SQL code quality, identifying issues early in the development process, and ensuring the reliability of data-driven products. This presentation is particularly beneficial for Data Scientists, Engineers, and Analysts seeking to enhance the efficiency and precision of their testing practices.", "code": "FSK3PE", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/FSK3PE.png", "speaker_names": "Anna Varzina", "speakers": "\n### Anna Varzina\n\nAnna Varzina is a Data Science Engineer at Lighthouse, where she has been developing data-driven solutions for the hospitality industry since 2021. She specialises in working with large datasets and performing complex data transformations using Python and SQL to extract meaningful insights.\r\nThis is Anna's first time speaking at PyCon/PyData, and she is excited to share her experiences in overcoming the challenges of building reliable and scalable data workflows.\n", "track": "Testing"}, {"title": "Conquering PDFs: document understanding beyond plain text", "abstract": "NLP and data science could be so easy if all of our data came as clean and plain text. But in practice, a lot of it is hidden away in PDFs, Word documents, scans and other formats that have been a nightmare to work with. In this talk, I'll present a new and modular approach for building robust document understanding systems, using state-of-the-art models and the awesome Python ecosystem. I'll show you how you can go from PDFs to structured data and even build fully custom information extraction pipelines for your specific use case.", "full_description": "For the practical examples, I'll be using spaCy, and the new Docling library and layout analysis models. I'll also cover Optical Character Recognition (OCR) for image-based text, how to convert tabular data to pandas DataFrames, and strategies for creating training and evaluation data for information extraction tasks like text classification and entity recognition using PDFs and other documents as inputs.", "code": "FUX3FR", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/media/social/talks/FUX3FR.png", "speaker_names": "Ines Montani", "speakers": "\n### Ines Montani\n\nInes Montani is a developer specializing in tools for AI and NLP technology. She\u2019s the co-founder and CEO of Explosion and a core developer of spaCy, a popular open-source library for Natural Language Processing in Python, and Prodigy, a modern annotation tool for creating training data for machine learning models.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "LLM Inference Arithmetics: the Theory behind Model Serving", "abstract": "Have you ever asked yourself how parameters for an LLM are counted, or wondered why Gemma 2B is actually closer to a 3B model? You have no clue about what a KV-Cache is? (And, before you ask: no, it's not a Redis fork.) Do you want to find out how much GPU VRAM you need to run your model smoothly? \r\n\r\nIf your answer to any of these questions was \"yes\", or you have another doubt about inference with LLMs - such as batching, or time-to-first-token - this talk is for you. Well, except for the Redis part.", "full_description": "The talk will cover the theory necessary to understand how to serve LLMs. The talk covers the math behind transformers inference in an accessible and light way. By the end of the talk, attendants will learn:\r\n\r\n1. How to count the parameters in an LLM, especially the ones in the attention layers.\r\n2. The difference between compute and memory in the context of LLM inference.\r\n3. That LLM inference is made up of two parts: prefill and decoding.\r\n4. What is an LLM server, and what features they implement to optimise GPU memory usage and reduce latency\r\n4. How batching affects your inference metrics, like time-to-first-token.\r\n\r\nThe talk will cover:\r\n\r\n**Did you pay attention?** (4 min). A short review of the attention mechanism and how to count parameters in a transformer-based model.\r\n\r\n**Get to know your params** (8 min). The math-y section of the talk, explaining how to translate parameter counts into memory and compute requirements.\r\n\r\n**Prefill and Decoding** (8 min) Explains that inference happens in two steps (prefill and decoding) and how KV-cache exploits this to make decoding faster. Common metrics to measure inference performance, like time-to-first-token and token-per-second.\r\n\r\n**Context and batch size** (5 min) Adds to the picture the sequence length, as well as the number of requests to process in parallel. Explains how LLM servers, like vLLM, use techniques like Paged Attention to optimise GPU usage\r\n\r\n**Conclusion** (5 min) Wrap up, Q&A.", "code": "G3AT7E", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/G3AT7E.png", "speaker_names": "Luca Baggi", "speakers": "\n### Luca Baggi\n\nAI Engineer at xtream by day, and open source maintainer by night. I strive to be an active part of the Python and PyData communities - e.g. as an organiser of PyData Milan. Feel free to reach out!\n", "track": "Generative AI"}, {"title": "Size matters: Inspecting Docker images for Efficiency and Security", "abstract": "Inspecting Docker images is crucial for building secure and efficient containers. In this session, we will analyze the structure of a Python-based Docker image using various tools, focusing on best practices for minimizing image size and reducing layers with multi-stage builds. We\u2019ll also address common security pitfalls, including proper handling of build and runtime secrets.\r\n\r\nWhile this talk offers valuable insights for anyone working with Docker, it is especially beneficial for Python developers seeking to master clean and secure containerization techniques.", "full_description": "1. **Introduction**\r\n    - We start with an example Dockerfile for a Python-based image.\r\n    - We will explore the role of OverlayFS, Docker\u2019s file system for combining layers, to understand how layers stack and how data (or even secrets) can be retrieved from individual layers.\r\n    \r\n2.  **Layer Analysis**\r\n    - To gain better understanding of layering, we use simple command-line tools like `docker history` and `docker inspect` to examine image layers.\r\n    - We introduce `dive`, a tool for exploring the contents of each layer.\r\n    - We apply these insights to optimize the image by implementing multi-stage builds to create a smaller image with fewer layers, improving storage efficiency, build speed, and security.\r\n    - We discuss the benefits of Docker\u2019s caching mechanism in reducing build times.\r\n\r\n3. **Security Enhancements**\r\n    - Given our example image, we will use `trivy`, a comprehensive security scanner, to scan the example image for vulnerabilities and demonstrate how to address common issues.\r\n    - Finally, we introduce `syft` for generating SBOMs (Software Bill of Materials) to catalog packages, libraries, and dependencies within the image.\r\n\r\nTo get the most out of this session, participants are encouraged to clone the session's repository and build the example Docker image beforehand. Doing so will enable you to follow the demo interactively and experiment with the tools and methods presented during the session.", "code": "GJ9MVT", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/GJ9MVT.png", "speaker_names": "Irena Grgic", "speakers": "\n### Irena Grgic\n\nAs a clean code enthusiast, Women in Tech advocate, DevOps engineer, and mathematician, I have worked in multiple tech fields. My journey has taken me from roles as a data scientist and machine learning engineer to MLOps, culminating in my current position as the lead DevOps engineer of a computer vision platform with hundreds of active users. I possess a broad range of experience in multiple programming languages, creating fast and structured CI/CD pipelines, deploying entire platforms to Kubernetes, and working with various cloud providers. I am passionate about efficient, well-readable, and easily maintainable code and strongly believe that machine learning products should be developed with the same standards as good software.\n", "track": "MLOps & DevOps"}, {"title": "Securing RAG Pipelines with Fine Grained Authorization", "abstract": "Using LLMs and AI in your Enterprise? Make sure you build Fine Grained Authorization to ensure your LLMs access only the data they are authorized to. \r\n\r\nThis talk will show how you can build Relationship Based Access Control (ReBAC) for fine-grained authorization for your RAG pipelines. The talk also includes a demo using Pinecone, Langchain, OpenAI, and SpiceDB.", "full_description": "Building enterprise-ready AI requires ensuring users can only augment prompts with data they're authorized to access. Relationship-based access control (ReBAC) is particularly well-suited for fine-grained authorization in Retrieval-Augmented Generation (RAG) because it makes decisions based on relationships between objects, offering more precise control compared to traditional models like RBAC and ABAC.\r\n\r\nThis talk covers how ReBAC systems can safeguard sensitive data in RAG pipelines. We'll start with why Authorization is critical for RAG pipelines, and how Google Zanzibar achieves this with ReBAC. We'll then illustrate how pre-filtering vector database queries with a list of authorized object IDs can improve efficiency & security. \r\n\r\nThe talk will also include a demo implementing fine-grained authorization for RAG using Pinecone, Langchain, OpenAI, and SpiceDB.", "code": "GRWYQB", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/GRWYQB.png", "speaker_names": "Sohan Maheshwar", "speakers": "\n### Sohan Maheshwar\n\nSohan is a Lead Developer Advocate at AuthZed, based in the Netherlands. He started his career as a developer building mobile apps and has worked in the developer relations space since 2013, in companies such as Amazon, Fermyon and Gupshup. He has always been interested in emerging technologies and how it shapes the world around us.\r\n\r\nHis interests outside work include visual arts, trivia, and playing frisbee.\n", "track": "Generative AI"}, {"title": "Offline Disaster Relief Coordination with OpenStreetMap and FastAPI", "abstract": "In natural disaster scenarios, reliable communication is crucial. This talk presents a solution for disaster relief coordination using OpenStreetMap vector maps hosted on a local device in the emergency vehicle with FastAPI, ensuring functionality without an internet connection. By integrating a database of post codes and street names, and leveraging a LORAWAN gateway to receive positional data and water levels, this system ensures access to critical information even in blackout situations.", "full_description": "In natural disaster scenarios, effective coordination of relief efforts is essential, especially when traditional communication networks are compromised or overloaded. This involves organizing various aspects of disaster response without internet connectivity, such as dike defense, sandbag logistics, power supply, distribution of relief goods, clearing and repairing roadways, searching for missing persons, securing structurally unstable buildings and salvaging property. Typically, emergency responders are mobilized from different regions and may not be familiar with the affected area. Since it is unlikely that responders will have pre-downloaded offline maps of the target region on their devices, a vehicle-hosted Wi-Fi hotspot providing nationwide maps would be invaluable. This presentation introduces a practical solution for offline disaster relief coordination using OpenStreetMap vector maps hosted on a local device in the emergency vehicle with FastAPI, allowing responders to use their existing devices to access critical geographical information and enhancing the efficiency and effectiveness of disaster relief operations.\r\n\r\nThe solution includes:\r\n\r\nFastAPI Server Setup: Configuring and deploying a FastAPI server to host OpenStreetMap vector maps offline on a Raspberry Pi, which also offers a WiFi hotspot for existing end devices, ensuring accessibility without an internet connection.\r\nDatabase Integration: Integrating a comprehensive database of post codes and street names to facilitate quick and accurate location searches.\r\nLORAWAN Gateway Integration: Implementing a LORAWAN gateway to receive real-time positional data and water level measurements, providing up-to-date situational awareness for disaster relief workers.", "code": "GUEAHT", "state": "confirmed", "created": "2024-12-10", "social_card_image": "/static/media/social/talks/GUEAHT.png", "speaker_names": "Jannis L\u00fcbbe", "speakers": "\n### Jannis L\u00fcbbe\n\n2008 \r\nM.Sc. Physics and Computer Science at Osnabr\u00fcck University\r\n\r\n2012 \r\nPhD in Physics at Osnabr\u00fcck University\r\n\r\n2013 - now \r\nSensor Developer at ROSEN Group\r\n\r\n2020 - now \r\nVolunteer operative in the Federal Agency for Technical Relief (THW, Germany)\n", "track": "Infrastructure - Hardware & Cloud"}, {"title": "Benchmarking Time Series Foundation Model with sktime", "abstract": "Recent time series foundation models such as ... promise to change time series forecasting. One central claim of foundation models is their ability to perform zero-shot forecasting, that is, to perform well with no training data. However, performance claims of foundation models are difficult to verify, as public benchmark datasets may have been a part of the training data, and only the already trained weights are available to the user.\r\n\r\nTherefore, performance in specific use cases must be verified on the use case data itself, to ensure a reliable assessment of forecasting performance. sktime allows users to easily produce a performance benchmark of any collection of forecasting models, foundation models, simple baselines, or custom methods, on their internal use case data.", "full_description": "In the past years, time series foundation models emerged. They have the potential to change time series forecasting. For example, multiple time series models such as LagLlama, Chronos, Moirai, TinyTimesMixer, etc., promise zero-shot forecasting for arbitrary time series. Furthermore, also sktime started to unify the interfaces of the various foundation models to make the usage of those models easy.\u00a0\r\nHowever, whether these time series foundation models provide added value to various forecasting applications is still unclear. Thus, benchmarking is necessary. In sktime, we have implemented a benchmarking module enabling easy comparison of those time series foundation models on custom datasets and with arbitrary metrics.\r\n\r\nOur talk will outline how sktime\u2019s benchmarking module works and how users can use it to evaluate time series foundation models.\u00a0\r\nWe will show how to combine the benchmarking module with the time series foundation models.\r\nWe will show the results of a small benchmarking study using time series foundation models and statistical time series models.\u00a0\r\nWe will outline our roadmap for time series foundation models.\u00a0\r\n\r\nsktime is developed by an open community with the aim of ecosystem integration in a commercially neutral, charitable space. We welcome contributions or donations and seek to provide opportunities for anyone worldwide.", "code": "GUKTNX", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/GUKTNX.png", "speaker_names": "Benedikt Heidrich", "speakers": "\n### Benedikt Heidrich\n\nI completed my PhD in deep learning based time series forecasting in 2023 with the Karlsruhe Institute of Technology. In sktime, I am focusing on forecasting methods (mainly deep learning based ones) and implementing pipelines.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Is Prompt Engineering Dead? How Auto-Optimization is Changing the Game", "abstract": "The rise of LLMs has elevated prompt engineering as a critical skill in the AI industry, but manual prompt tuning is often inefficient and model-specific. This talk explores various automatic prompt optimization approaches, ranging from simple ones like bootstrapped few-shot to more complex techniques such as MIPRO and TextGrad, and showcases their practical applications through frameworks like DSPy and AdalFlow. By exploring the benefits, challenges, and trade-offs of these approaches, the attendees will be able to answer the question: is prompt engineering dead, or has it just evolved?", "full_description": "With the rise of LLMs, prompt engineering has become a highly impactful skill in the AI industry. However, manual prompt tuning is challenging, time-consuming, and not always generalizable across different models. This raises a reasonable question: can prompts be automatically learned from data? The answer is yes, and in this talk, we will explore how.\r\n\r\nFirst, we will provide a high-level overview of various prompt optimization approaches, starting with a simple technique like bootstrapped few-shot, which automatically generates and selects an optimal set of demonstrations for each step in the LLM chain. Then, we will discuss more complex approaches, such as MIPRO and TextGrad, which directly optimize the instructions.\r\n\r\nAfterwards, we will move on to a more practical part by showcasing how these techniques can be used via popular frameworks such as DSPy and AdalFlow.\r\n\r\nFinally, we will discuss the benefits and trade-offs of these approaches and frameworks in terms of costs, complexity and performance, so the audience can decide whether prompt engineering is truly dead.\r\n\r\n**Outline:**\r\n* Introduction (2 min)\r\n* Discussion of problems with manual prompt engineering (2 min)\r\n* Overview of existing prompt optimization approaches (10 min):\r\n    * Bootstrapped few-shot (3 min)\r\n    * MIPRO (3 min)\r\n    * TextGrad (4 min)\r\n* Showcasing the prompt optimization frameworks (8 min):\r\n    * DSPy (4 min)\r\n    * AdalFlow (4 min)\r\n* Comparison of methods and concluding remarks (3 min)\r\n* Q&A (5 min)", "code": "GURXPK", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/GURXPK.png", "speaker_names": "Iryna Kondrashchenko, Oleh Kostromin", "speakers": "\n### Iryna Kondrashchenko\n\nIryna is a data scientist and co-founder of DataForce Solutions, a company specialized in delivering end-to-end data science and AI services. She contributes to several open-source libraries, and strongly believes that open-source products foster a more inclusive tech industry, equipping individuals and organizations with the necessary tools to innovate and compete.\n\n### Oleh Kostromin\n\nI am a Data Scientist primarily focused on Deep Learning and MLOps. In my spare time I contribute to several open-source python libraries.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Langfuse, OpenLIT, and Phoenix: Observability for the GenAI Era", "abstract": "Large Language Models (LLMs) are transforming digital products, but their non-deterministic behaviour challenges predictability and testing, making observability essential for quality and scalability.\r\n\r\nThis talk presents **observability for LLM-based applications**, spotlighting three tools: Langfuse, OpenLIT, and Phoenix. We'll share best practices about what and how to monitor LLM features and explore each tool's strengths and limitations. \r\n\r\nLangfuse excels in tracing and quality monitoring but lacks OpenTelemetry support and customization. OpenLIT, while less mature, integrates well with existing observability stacks using **OpenTelemetry**. Phoenix stands out in debugging and experimentation but struggles with real-time tracing.\r\n\r\nThe comparison will be enhanced by **live coding examples**.\r\n\r\nAttendees will walk away with an improved understanding of observability for **GenAI applications** and will understand which tool to use for their use case.", "full_description": "Large Language Models (LLMs) are becoming core components of modern digital products. However, their **non-deterministic nature** means that their behaviour cannot be fully predicted or tested before deployment. This makes **observability** an essential practice for building and maintaining applications with generative AI features.\r\n\r\nThis session focuses on observability in LLM-based systems.\r\n\r\nWe start by motivating why monitoring and understanding your application is key to ensuring quality, reliability, and scalability. We\u2019ll analyze three leading tools for observability in this domain: **Langfuse**, **OpenLIT**, and **Phoenix**. Each has unique strengths and challenges that make them suitable for different use cases.\r\n\r\nThrough examples and real-world scenarios, we\u2019ll explore:\r\n\r\n- How **Langfuse** provides detailed tracing and quality monitoring through developer-friendly APIs. While it supports multi-step workflows effectively, it lacks support for the OpenTelemetry protocol and can be difficult to customize for non-standard use cases.\r\n- Why **OpenLIT**, built on OpenTelemetry, offers strong observability for distributed systems. Although it is the least mature of the three tools, it integrates well with established observability stacks and has promising potential for future growth.\r\n- Where **Phoenix** fits into the process by combining experimentation and debugging capabilities with evaluation pipelines. Its strength lies in development-focused observability, but it has limitations in handling real-time tracing once systems are in production.\r\n\r\nThis talk will provide a clear, straightforward comparison of these tools, helping you understand which option best fits your LLM applications.\r\n\r\nYou\u2019ll leave with practical insights into how observability can enhance the reliability and performance of your generative AI systems.", "code": "HKYQDB", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/HKYQDB.png", "speaker_names": "Emanuele Fabbiani", "speakers": "\n### Emanuele Fabbiani\n\nEmanuele is an engineer, researcher, and entrepreneur with a passion for artificial intelligence.\r\n\r\nHe earned his PhD by exploring time series forecasting in the energy sector and spent time as a guest researcher at EPFL in Lausanne. Today, he is co-founder and Head of AI at xtream, a boutique company that applies cutting-edge technology to solve complex business challenges.\r\n\r\nEmanuele is also a contract professor in AI at the Catholic University of Milan. He has published eight papers in international journals and contributed to over 30 international conferences worldwide. His engagements include AMLD Lausanne, ODSC London, WeAreDevelopers Berlin, PyData Berlin, PyData Paris, PyCon Florence, the Swiss Python Summit in Zurich, and Codemotion Milan.\r\n\r\nEmanuele has been a guest lecturer at Italian, Swiss, and Polish universities.\n", "track": "Python Language & Ecosystem"}, {"title": "Outgrowing your node? Zero stress scaling with cuPyNumeric.", "abstract": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library, which  gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library on one of the user's examples covering some detail on its implementation.", "full_description": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library.  cuPyNumeric gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\n\r\nA compelling example when scaling is necessary is when scientists at the Stanford Linear Accelerator Center(SLAC) need to process a large amount of data within a fixed time window, called beam time.  The full dataset generated during experiments is too large to be processed on a single CPU. Additionally, the code often must be modified during the beam time to adapt to changing experimental needs. Being able to use NumPy syntax rather than lower level distributed computing libraries makes these changes quick and easy, allowing researchers to focus on conducting more experiments rather than debugging or optimizing code.\r\n\r\ncuPyNumeric is designed to be a drop-in replacement to NumPy. Built on top of task-based distributed runtime from Stanford University, it automatically parallelizes NumPy APIs across all available resources, taking care of data distribution, communication, asynchronous and accelerated execution of compute kernels on both GPUs or multi-core CPUs.  In addition, cuPyNumeric can be integrated with other popular Python libraries like SciPy, matplotlib, Jax.  With cuPyNumeric, SLAC scientists successfully ran their data processing code distributed across multiple nodes and GPUs, processing the full dataset with a 6x speed-up compared to the original single-node implementation.\r\n\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library covering some detail on its implementation.", "code": "HPGEKH", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/HPGEKH.png", "speaker_names": "Irina Demeshko, Quynh L. Nguyen", "speakers": "\n### Irina Demeshko\n\nIrina Demeshko is a senior software engineer at NVIDIA working on cuNumeric and Legate projects. Before NVIDIA, Irina was a research scientist and team leader of the Co-Design team at the Los Alamos National Laboratory. Her work and research interests are in the area of new HPC technologies and programming models. Irina received her Ph.D. in mathematical and computer science from the Tokyo Institute of Technology in 2013.\n\n### Quynh L. Nguyen\n\nQuynh L. Nguyen is an Associate Scientist at SLAC National Accelerator Laboratory. She completed her PhD in Physics at JILA-University of Colorado Boulder (2020) and postdoctoral work at SLAC National Accelerator Laboratory and Stanford University as a Q-FARM Bloch Fellow. Her research interests include nonequilibrium dynamics of materials and high performance computing with applications towards quantum information science.\n", "track": "Programming & Software Engineering"}, {"title": "Topological data analysis: How to quantify \"holes\" in your data and why?", "abstract": "Do you need to compare sets of points in a plane? Identify a potential cyclic event in high-dimensional time series data? Find the second or the third highest peak of a noisily sampled function? Topological data analysis (TDA) is not a universal hammer, but it might just be the 16 mm wrench for your 16 mm hex head bolt. There is no shortage of Python libraries implementing TDA methods for various settings, but navigating the options can be challanging without prior familiarity with the topic. In my talk I will demonstrate the utility of the tool with several simple examples, list various libraries used by the TDA community, and dive a bit deeper into the methods to explain what the libraries implement and how to interpret and work with the outputs.", "full_description": "For specific tasks, topological data analysis can be a more rigid, straightforward and interpretable alternative to complicated machine learning pipelines. However, it is not so widely known and can be intimidating to get into when starting from zero. The goal of this talk is to introduce persistent homology, the main tool of topological data analysis, show concrete examples of how to apply it using available Python libraries, and reveal more details about what is going on \"under the hood\", which is important to correctly utilize the methods. I will start with several examples showcasing the possible uses of persistent homology and how to establish an analysis pipeline in Python. Then I will describe more about different variants within such a pipeline, like a choice of a filtered complex or vectorization, and their advantages and disadvantages.", "code": "HQWAYP", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/HQWAYP.png", "speaker_names": "Ondrej Draganov", "speakers": "\n### Ondrej Draganov\n\nI am a researcher in Topological Data Analysis working both on theoretical mathematical aspects of it and applications. I have completed my PhD at ISTA in Austria and then moved to INRIA in France to apply the TDA methods to brain cancer data.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Scraping LEGO for Fun: A Hacky Dive into Dynamic Data Extraction", "abstract": "Unlock the full potential of modern web scraping by combining Python, Scrapy, and Playwright to extract data from dynamic, JavaScript-heavy sites\u2014exemplified by LEGO product pages. This talk introduces Model Context Protocol (MCP) servers for orchestrating advanced data fetching, refining CSS selectors, and integrating Large Language Models for automated code suggestions. Learn how to scale ethically, handle concurrency, and respect site policies, while maintaining flexible, maintainable pipelines for diverse use cases from research to robotics.", "full_description": "# Advanced Web Scraping: From LEGO to Production\r\n\r\nToday's web landscape is teeming with JavaScript-heavy content, complex layouts, and sometimes opaque data structures. But what if you could reliably scrape rich product information\u2014images, specs, descriptions\u2014from modern e-commerce sites without hitting constant roadblocks? This session tackles advanced scraping with Python, Scrapy, and Playwright, exemplified by data extraction from LEGO product pages. We'll explore a \"grey hat\" perspective\u2014applying a slightly \"hacky\" mindset\u2014while stressing practical ethics, performance considerations, and compliance with site policies.\r\n\r\n## Outline\r\n\r\n### 1. Introduction: The Hacky Spirit vs. Ethical Constraints\r\n- Why scrape LEGO?\r\n- Setting boundaries: terms of service, rate limiting, and disclaimers\r\n- When \"scraping for fun\" crosses into potential legal pitfalls\r\n\r\n### 2. Scraping Tech Stack Overview\r\n- Scrapy for structured crawling and item pipelines\r\n- Playwright for rendering JavaScript and handling dynamic elements\r\n- Comparison to traditional HTML-only approaches\r\n- Project structure, environment setup, and practical tips\r\n\r\n### 3. Spiders in Action\r\n- Product Spider: Extracting core product data (ID, name, specifications, multiple images)\r\n- Gallery Spider: Navigating hidden galleries, handling tricky JS-based carousels, and filtering unwanted images\r\n- Ensuring consistent output (JSON or database ingestion)\r\n\r\n### 4. Model Context Protocol (MCP) Integration\r\n- Definition: Leveraging specialized helper servers for orchestrating data fetching, refining selectors, and automating debugging\r\n- Chaining Large Language Models: Code suggestions, auto-generation of selectors, and reactive error handling\r\n- Example workflow: \"Broken selector? Ask the MCP server for an LLM-aided fix\"\r\n\r\n### 5. Performance & Scale\r\n- Polite but robust concurrency: balancing speed and TOS compliance\r\n- Handling large link lists, incremental updates, and site changes\r\n- Monitoring and logging for reliability, debugging, and optimization\r\n\r\n### 6. Ethics & Privacy\r\n- Respecting site ownership, disclaimers, and usage limits\r\n- Storing scraped data securely and avoiding personal information\r\n- A discussion of \"grey hat\" territory: testing site vulnerabilities without exploiting them\r\n\r\n### 7. Use Cases & Extensions\r\n- Research software engineering: building reproducible data sets\r\n- Robotics and embedded: offline or partial data ingestion for classification or motion planning\r\n- Future directions: advanced concurrency, containerization, and HPC\r\n\r\n### 8. Demo & Q&A\r\n- Live snippet showing an MCP-powered spider reacting to a changed DOM structure\r\n- Q&A session on bridging the gap between hackery and best practices\r\n\r\n## Key Takeaways\r\n- Techniques for scraping dynamic, JS-heavy sites using Python, Scrapy, and Playwright\r\n- Practical \"hacky\" methods balanced by responsible, 'ethical approaches'\r\n- Introduction to Model Context Protocol servers for automated code refinement\r\n- Scalable patterns for data handling, from small tests to large-scale deployments\r\n\r\nWhether you're a data engineer, hobbyist, or researcher, this talk provides a robust (and slightly subversive) recipe for capturing essential data from the wild world of modern websites\u2014without crossing into unethical or unlawful territory.", "code": "HSFR7A", "state": "confirmed", "created": "2024-12-24", "social_card_image": "/static/media/social/talks/HSFR7A.png", "speaker_names": "Peter Lodri", "speakers": "\n### Peter Lodri\n\nHacker-maker, specialising in system infiltration and enhancement. Expert in reverse engineering, distributed systems architecture, and AI integration. Proven track record in high-stakes technical operations and system security.\n", "track": "Data Handling & Engineering"}, {"title": "Multivariate Datastrophe: Methods to Detect Obscure Drift in Your Production Data", "abstract": "Getting your model into production isn\u2019t a trivial task, but it\u2019s only half the battle. Ensuring that your model continues to deliver great performance over time is even more critical. In this talk I would like to present a selection of what can kill your model\u2019s performance, zoom in on multivariate data drift, and present two methods to detect this type of drift in your production data.", "full_description": "Objectives: The goal of this talk is to explain what multivariate data drift is and to present methods for detecting it. You will learn about the challenges associated with multivariate drift and explore practical solutions to identify it\r\n\r\n1. Model\u2019s Aren\u2019t Forever: \r\n91% of models in production degrade over time. I will explain the importance of continuous model monitoring and discuss common approaches and pitfalls. \r\n\r\n2. All Ways Your Perfectly Fine ML Model Can Fail: \r\nMachine learning models can fail for numerous reasons. I will describe the root causes of ML model failure, including data quality, data drift and the final boss of every model -  concept drift.\r\nI will explore simple methods to detect univariate data drift and delve into the more complex challenge of concept drift, which can significantly impact your model\u2019s performance in production.\r\n\r\n3. Drift Happens - Multivariate Data Drift: \r\nMultivariate data drift occurs when the relationships between multiple variables in your data change over time. This type of drift can be challenging to detect with standard methods. I will provide an explanation of what multivariate data drift is and why traditional techniques may fall short in identifying it.\r\n\r\n4. Two Clever Ways to Detect Multivariate Data Drift:\r\nDomain Classifier: This method involves training a classifier to distinguish between data from different time periods. If the classifier can accurately separate the data, it indicates that the distribution has changed, signaling potential drift.\r\nPCA Reconstruction Error: Principal Component Analysis (PCA) can be used to reduce the dimensionality of your data. By comparing the reconstruction error over time, you can detect changes in the underlying data distribution that may indicate drift.", "code": "HVTSDP", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/HVTSDP.png", "speaker_names": "Magdalena Kowalczuk", "speakers": "\n### Magdalena Kowalczuk\n\ndata & ml fan with a soft spot for OSS - driven by curiosity, eager self-learner, hackathons enjoyer, PyData and PyLadiesCon speaker and volunteer, exploring and creating content about post-deployment data science at NannyML, in my free time contributing to Narwhals and hosting open source sprints\n", "track": "MLOps & DevOps"}, {"title": "Autonomous Browsing using Large Action Models", "abstract": "The browser serves as our gateway to the internet\u2014the largest repository of knowledge in human history. Proficiency in its use is a core skill across nearly all professions and is becoming increasingly important for Artificial Intelligence. But can Large Action Models (LAMs) autonomously operate a browser? What exactly are LAMs that promise to translate human intentions into actions? We report on a project that fully automates the job application process using AI: from navigating unfamiliar website structures and filling out forms to handling document uploads and cookie banners.", "full_description": "Large Action Models (LAMs) were first introduced by Rabbit with the launch of their R1 device, aiming to create end-to-end trained models that automatically translate human instructions into actions. Since then, the definition of LAMs has evolved to encompass Large Language Models (LLMs) utilized in multi-agent settings. Notable examples include Anthropic's \"Computer Use\" feature in their Claude model and Google's Project Mariner. These projects allow LLMs to operate a web browser or computer in a human-like manner by viewing the screen, moving the cursor, clicking buttons, and typing text, thereby fulfilling the original promise of LAMs by effectively translating human instructions into automated actions.\r\n\r\nWe present an innovative application of LAMs that automates the job application process using AI. Our system autonomously navigates unfamiliar website structures, fills out forms, handles document uploads, and manages cookie banners without human intervention. This level of automation streamlines the application process for job seekers while ensuring accurate and timely submissions.\r\n\r\nTo achieve this, we leveraged the LaVague framework, which employs a modular, agent-based approach:\r\n\r\n1.\t**Coordinator Agent:** A central agent powered by a multimodal model coordinates the entire process. It has access to website visuals, user data (e.g., personal details, CV information), previous instructions, and the overall objective. Based on this information, it delegates tasks to specialized agents.\r\n2.\t**Navigation Control Agent:** For simple website navigation, this agent utilizes a browser driver such as Selenium to directly interact (e.g., scroll) with the webpage.\r\n3.\t**Knowledge Agent:** When additional information is required, this agent performs knowledge-intensive tasks using an LLM. Examples include researching specific details or restructuring CV data.\r\n4.\t**Navigation Engine Agent:** For complex website interactions like inputting values or uploading files, this agent generates custom code for the browser driver. Using an LLM with access to the HTML code, it creates the necessary commands.\r\n\r\nThese agents work iteratively, performing tasks step by step until either the objective is achieved, or a maximum number of steps is reached.\r\n\r\nBy building a custom solution around the LaVague framework tailored specifically for the job application process, we successfully automated the entire workflow. In our presentation, we discuss our overall architecture, the challenges encountered during development and share valuable lessons learned for practical adoption.\r\n\r\nLarge Action Models like these highlight the transformative potential of AI in automating intricate tasks, bridging the gap between understanding human intentions and executing them in dynamic, real-world scenarios.", "code": "HYE8EX", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/HYE8EX.png", "speaker_names": "Nico Kreiling, Arne Grobr\u00fcgge", "speakers": "\n### Nico Kreiling\n\n\n\n### Arne Grobr\u00fcgge\n\nArne Grobr\u00fcgge, M. Sc. Wirtschaftsinformatiker mit Schwerpunkt Maschinelles Lernen und Informationssicherheit, arbeitet als Data Scientist bei der scieneers GmbH. Im Rahmen von diversen Kundenprojekten entwickelt und \u00fcberwacht er den Einsatz von Sprachmodellen und Mulit-Agenten Systemen in Unternehmen, um innovative und wertsch\u00f6pfende L\u00f6sungen zu schaffen.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Conquering the Queue: Lessons from processing one billion Celery tasks", "abstract": "At Userlike, Celery is the backbone of our application, orchestrating over a 100 million tasks per month. In this talk, I\u2019ll share real-world insights into scaling Celery, optimizing performance, avoiding common pitfalls, handling failures, and building a resilient architecture.", "full_description": "At Userlike, Celery plays a critical role as the backbone of our Django-based SaaS application, orchestrating over 100 million tasks per month with speed, reliability, and precision. In this talk, I\u2019ll share the lessons we\u2019ve learned while scaling Celery to handle massive workloads and support the needs of a growing user base. From optimizing performance and avoiding common pitfalls to handling failures gracefully and ensuring a resilient architecture, this session will provide actionable insights for developers and architects working with distributed task queues.\r\n\r\nWhether you\u2019re just starting with Celery or looking to scale an established system, you\u2019ll walk away with practical tips, battle-tested strategies, and a deeper understanding of how to harness Celery\u2019s full potential in real-world scenarios.\r\n\r\nOutline:\r\n\r\n\u2022\tIntroduction: Why Userlike needs a task queue, and why you need one too\r\n\u2022\tFundamental concepts: latency, throughput, failure modes\r\n\u2022\tOptimizing Performance: Strategies for faster and more efficient task execution\r\n\u2022\tAvoiding Pitfalls: Common mistakes and how to mitigate them\r\n\u2022\tHandling Failures: Building fault-tolerant workflows and monitoring systems\r\n\u2022\tResilient Architecture: Designing for reliability and scalability\r\n\u2022\tKey Takeaways: Practical tips for implementing and scaling Celery in your own projects\r\n\r\nThis talk is designed to be technical, engaging, and packed with real-world experiences to help you conquer the queue in your own applications.", "code": "J8FLDN", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/J8FLDN.png", "speaker_names": "Daniel Hepper", "speakers": "\n### Daniel Hepper\n\nDaniel is the CTO at Userlike, a leading SaaS company providing innovative customer communication solutions. He has a degree in Computer Science from the University of Karlsruhe and has been writing software professionally for over 20 years. He enjoys sharing his experiences and helping fellow developers level up their software development skills, and presented before at various Django and Python conferences throughout Europe. When not in front of a keyboard, he can be found training for his next marathon or building intricate Lego contraptions with his son.\n", "track": "Django & Web"}, {"title": "Why AI Projects Fail \u2013 Chronicles of Failure and How to Overcome Them", "abstract": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. Organizations often stumble over unrealistic expectations, siloed data, and cultural roadblocks. This talk dives into the human and organizational dynamics that cause AI initiatives to derail.\r\n\r\nUsing real-world examples, I\u2019ll showcase how missing tram stop location data was found on a hobbyist\u2019s blog when official systems failed. Or how critical historical data for resource planning sat locked in Excel files with forgotten passwords, forcing a desperate password recovery operation. These examples highlight the surprising ways poor data governance and lack of interdisciplinary collaboration sabotage even well-intentioned AI projects.\r\n\r\nBut all is not lost! We\u2019ll explore actionable strategies to navigate these challenges: shifting from shiny tools to R&D-driven approaches, fostering better communication between IT and business teams, and building strong foundations for clean, accessible data. Attendees will leave with insights into what AI success truly requires: cultural readiness, realistic expectations, and a commitment to collaboration.\r\n\r\nIf you\u2019re tired of AI hype and looking for practical solutions, this talk offers a clear roadmap to ensure AI projects deliver measurable impact.", "full_description": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. The real barriers are organizational, cultural, and human. In this talk, we\u2019ll uncover the reasons AI initiatives stumble and explore how to overcome them\u2014through a mix of anecdotes, real-world failures, and actionable best practices.\r\n\r\nWe begin with a big lamenti\u2014an unfiltered dive into the messy, chaotic world of AI project implementation. From tangled data silos to misaligned expectations, we\u2019ll uncover hard truths through real-world examples that many will recognize:\r\n\t\u2022\tThe missing data problem: Sometimes it\u2019s the smallest datasets that hold the biggest answers.\r\n\t\u2022\tPasswords might be in retirement.\r\n\t\u2022\tDecisions in the dark: When untrained people must make calls on things they don\u2019t fully understand.\r\n\t\u2022\tThe shiny facade: Expectations, the pressure for quick wins, and the relentless demand for good news.\r\n\r\nTo go beyond personal experience, I\u2019ll also bring fresh insights from a survey of my expert network closer to the conference. This will offer a broader, more representative perspective on why AI projects stumble\u2014and how to fix them.\r\n\r\nTimed Outline (45 minutes):\r\n1. Introduction (5 min)\r\n - What qualifies me for this talk.\r\n2. The Big Lamenti \u2013 Real-World Failures and Anecdotes (20 min)\r\n - Anecdotes that illustrate the challenges in a candid yet engaging way.  \r\n - Learnings from the survey\r\n3.  Solutions and Best Practices (15 min)\r\n - Mindset: How AI is perceived and approached within organizations.\r\n - Data: The role of accessibility, governance, and quality as foundational pillars.\r\n - Collaboration: Bridging silos between teams, leadership, and departments.\r\n - Scale: The progression from small, focused initiatives to sustainable implementation.\r\n - Expectations: Aligning vision, goals, and outcomes for realistic project success.\r\n5.\tTakeaways & Closing Thoughts (5 min)\r\n - Summary of insights and practical steps to avoid common pitfalls\r\n- Key mindset shifts for sustainable AI success\r\n5.\tQ&A (5 min)", "code": "JA9NFW", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/JA9NFW.png", "speaker_names": "Alexander CS Hendorf", "speakers": "\n### Alexander CS Hendorf\n\nAlexander Hendorf has over 20 years of experience in digitalization, data, and artificial intelligence. As an independent consultant, he specializes in the practical implementation, adoption, and communication of data- and AI-driven strategies and decision-making processes.\r\n\r\nA recognized expert in data intelligence, Alexander has served as a speaker and chair at leading international conferences, including PyCon DE & PyData Berlin, Data2Day, and EuroPython. His dedication to the tech community has earned him the titles of Python Software Foundation Fellow and EuroPython Fellow.\r\n\r\nHe currently serves as a board member of the Python Software Verband (Germany). Since 2024, Alexander has been driving Pioneers Hub, a non-profit organization focused on fostering and supporting tech communities.\r\n\r\n\ud83c\udf1f AI Strategy & Open Source Excellence | \ud83e\udd1d Inspiring Communities, Empowering Innovators\n", "track": "Others"}, {"title": "Enhancing RAG with Fast GraphRAG and InstructLab: A Scalable, Interpretable, and Efficient Framework", "abstract": "Retrieval Augmented Generation (RAG) has become a cornerstone in enriching GenAI outputs with external data, yet traditional frameworks struggle with challenges like data noise, domain specialization, and scalability. In this talk, Tuhin will dive into open-source frameworks Fast GraphRAG and InstructLab, which addresses these limitations by combining knowledge graphs with the classical PageRank algorithm and Fine-tuning, delivering a precision-focused, scalable, and interpretable solution. By leveraging the structured context of knowledge graphs, Fast GraphRAG enhances data adaptability, handles dynamic datasets efficiently, and provides traceable, explainable outputs while InstructLab adds domain depth to the LLM through Fine-tuning. Designed for real-world applications, it bridges the gap between raw data and actionable insights, redefining intelligent retrieval for developers, researchers, and enterprises. This talk will showcase Fast GraphRAG\u2019s transformative features coupled with domain specific Fine-tuning leveraging InstructLab and demonstrate its potential to elevate RAG\u2019s capabilities in handling the evolving demands of large language models (LLMs) for developers, researchers, and businesses.", "full_description": "Retrieval Augmented Generation (RAG) has changed the way AI systems incorporate external knowledge, but it often falls short when faced with real-world challenges like adapting to new data, managing complexity, or delivering reliable answers. Fast GraphRAG steps in to address these gaps with a refreshing approach that blends the structure of knowledge graphs with the proven efficiency of algorithms like PageRank. By focusing on interpretability, scalability, and adaptability, Fast GraphRAG creates a pathway for building AI systems that don\u2019t just retrieve data but leverage it in a meaningful way. \r\n\r\nThe agenda for the talk is as follows\r\n\r\nChallenges in Traditional RAG\r\n    - Lack of interpretability leads to untrustworthy outputs.\r\n    - High computational costs limit scalability.\r\n    - Inflexibility makes adapting to evolving data cumbersome.\r\nFast GraphRAG\u2019s Core Innovations\r\n    - Interpretability: Knowledge graphs provide clear, traceable reasoning.\r\n    - Scalability: Efficient query resolution with minimal overhead.\r\n    - Adaptability: Dynamic updates ensure relevance in changing domains.\r\n    - Precision: PageRank sharpens focus on high-value information.\r\n    - Robust Workflows: Typed and asynchronous handling for complex scenarios.\r\nHow Fast GraphRAG Works\r\n    - Architecture and algorithmic innovations.\r\n    - Knowledge graphs for intelligent reasoning.\r\n    - PageRank for multi-hop exploration and precise retrieval.\r\n    - Entity extraction, incremental updates, and graph exploration.\r\n    - Role of InstructLab and Fine-tuning.\r\nDemo and Practical Takeaways\r\n    - Building a knowledge graph and resolving queries.\r\n    - Open-source tools for scaling Fast GraphRAG.\r\n    - Real-World applications\r\n\r\nFast GraphRAG isn\u2019t just another tool. It's a game-changer for anyone frustrated by the limitations of traditional RAG systems. By combining the structured clarity of knowledge graphs with the power of algorithms like PageRank and fine-tuning by InstructLab, it makes retrieval smarter, faster, and the LLM more adaptable. This session will leave you with a clear understanding of how to build/train AI systems that deliver meaningful results while being transparent and trustworthy. Whether you\u2019re a developer, researcher, or just someone passionate about AI, Fast GraphRAG is a framework that sparks possibilities and redefines what intelligent retrieval can achieve.", "code": "JABVHK", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/JABVHK.png", "speaker_names": "Tuhin Sharma", "speakers": "\n### Tuhin Sharma\n\nTuhin Sharma is Senior Principal Data Scientist at Redhat in the Data Development Insights & Strategy AI team. Prior to that, he worked at Hypersonix as an AI architect. He also co-founded and has been CEO of Binaize (backed by Techstars), a website conversion intelligence product for e-commerce SMBs. Previously, he was part of  IBM Watson where he worked on NLP and ML projects featured on Star Sports and CNN-IBN. He received a master's degree from IIT Roorkee and a bachelor's degree from IIEST Shibpur in Computer Science. He loves to code and collaborate on open-source projects. He is one of the top 20 contributors of pandas. He has 4 research papers and 5 patents in the fields of AI and NLP. He is a reviewer of the IEEE MASS conference, Springer nature and Packt publication in the AI track. He writes deep learning articles for O'Reilly in collaboration with the AWS MXNET team. He is a regular speaker at prominent AI conferences like O'Reilly Strata & AI, ODSC, GIDS, Devconf, Datahack Summit etc.\n", "track": "Generative AI"}, {"title": "Pipeline-level differentiable programming for the real world", "abstract": "Automatic Differentiation (AD) is not only the backbone of modern deep learning but also a transformative tool across various domains such as control systems, materials science, weather prediction, 3D rendering, data-driven scientific discovery, and so on. Thanks to a mature ML framework ecosystem, powered by libraries like PyTorch and JAX, AD performs remarkably well at a component level; however, integrating these components into differentiable pipelines still remains a significant challenge. In this talk, we will provide an accessible introduction to (pipeline-level) AD, demonstrate some cool applications you can build with it, and see how to build differentiable pipelines that hold up in the real world.", "full_description": "The tools enabling automatic differentiation (AD), like JAX and PyTorch, are increasingly being adopted beyond machine learning to tackle optimization problems in various scientific and engineering contexts. These tools have catalyzed the development of differentiable simulators, solvers, 3D renderers, and other powerful components, under the umbrella of differentiable programming (DP).\r\n\r\nHowever, building pipelines that propagate gradients effortlessly across components introduces unique challenges. Real-world pipelines often span diverse technologies, frameworks (e.g., JAX, TensorFlow, PyTorch, Julia), computing environments (local vs. distributed clusters; CPU vs. GPU), and teams with varying expertise. Additionally, legacy systems and non-differentiable components often need to coexist with modern AD-enabled frameworks.\r\n\r\nThis talk will provide an overview of differentiable pipelines: why they matter and the types of optimization problems they address. We will revisit foundational concepts of automatic differentiation to set the stage for understanding the intricacies of orchestrating differentiable pipelines in Python.\r\n\r\nThen, using our open-source project, Tesseract, as a case study, we will share lessons learned and best practices for designing AD-friendly APIs with tools like Pydantic and FastAPI, achieving seamless integration with JAX, packaging scientific software, and enabling end-to-end systems-level optimization. \r\n\r\nAttendees will leave with practical insights on why they should care about differentiable programming, and how to overcome the challenges of building real-world differentiable pipelines.", "code": "JH97CL", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/JH97CL.png", "speaker_names": "Alessandro Angioi", "speakers": "\n### Alessandro Angioi\n\nI work at the boundary between physical simulations and machine learning. I have 5+ years experience in machine learning and data science, and my background is in theoretical physics. Born in Sardinia, but I've been living in the Rhein-Neckar region for the past 10 years. Cat person.\n", "track": "Research Software Engineering"}, {"title": "Why E.ON Loves Python", "abstract": "Join me as I share my 20-year journey with Python and its pivotal role at E.ON. Discover how we transitioned fully to Python, streamlined our development framework, and embraced MLOps principles. Learn about some of our AI projects, including image analysis and real-time inference, and our steps towards open-sourcing code to foster innovation in the energy sector. Explore why Python is our go-to language for data science and collaboration.", "full_description": "In this talk, I will share my journey with Python, spanning over 20 years, and how it has become an integral part of our work at E.ON. My experience with open source began over 30 years ago during my research as a Theoretical Particle Physicist, where sharing insights and code was a daily practice. Transitioning to a software developer role at a start-up, I initially used Perl for various tasks but soon realized the challenges of code readability and collaboration. Python, with its enforced indentation and readability, quickly became my language of choice.\r\n\r\nAt E.ON, Python is our go-to language for Data Science tasks. In our team we recently migrated another programming language codebase to Python to streamline our development framework and attract top talent. Python's straightforward modularization into packages and modules simplifies maintenance and lineage, especially in cloud-based pipelines, and helps prevent vendor lock-in. The robust toolchain for code quality checks, testing, and building packages makes Python a no-brainer for development and supports our MLOps principles.\r\n\r\nI will discuss how Python facilitates collaboration globally at E.ON and share examples of our MLOps principles in action. Highlights include image analysis projects like object detection with batch inferencing and instance segmentation with real-time inference endpoints. Additionally, I will detail E.ON's steps towards open-sourcing some of our codebases, enabling other energy companies to build on our projects.\r\n\r\nJoin me to explore why Python is not just a tool but a catalyst for innovation and collaboration at E.ON.", "code": "JM3G8S", "state": "confirmed", "created": "2024-12-13", "social_card_image": "/static/media/social/talks/JM3G8S.png", "speaker_names": "Christer Friberg", "speakers": "\n### Christer Friberg\n\n\n", "track": "MLOps & DevOps"}, {"title": "Challenges and Lessons Learned While Building a Real-Time Lakehouse using Apache Iceberg and Kafka", "abstract": "How do you build a large-scale data lakehouse architecture that makes data available for business analytics in real time, while being more cost-effective, more flexible and faster than the previous proprietary solution? With Python, Kafka and Iceberg, of course!\r\n\r\nWe built a large-scale data lakehouse based on Apache Iceberg for the Schwarz Group, Europe's largest retailer. The system collects business data from thousands of stores, warehouses and offices across Europe.\r\n\r\nIn this talk, we will present our architecture, the challenges we faced, and how Apache Iceberg is shaping up to be the data lakehouse format of the future.", "full_description": "The Schwarz Group is present in thirty-two countries around the world with over ten thousand stores, hundreds of warehouses, an assortment of over two thousand different products and a single ERP system to manage them all.\r\n\r\nEvery country maintains its own databases for operational purposes but all the data is also gathered in one central analytics platform for all countries. Not only does this platform need to be stable and reliable, the data also needs to be made available for the consumers in near real-time \u2013within mere minutes. The existing analytics platform was based on proprietary solutions which were expensive and required niche knowledge, severely limiting the number of available developers.\r\n\r\nTherefore, we set out on a journey to completely redesign the analytics platform; a new solution, based as much as possible on Open Source technologies like Python, Kafka and Iceberg. Leveraging Python for its great ecosystem and ease of use. Kafka for fast and reliable message processing of over one thousand tables per country into one central hub. And Iceberg at the core, as our data lakehouse format for its fully transparent schema evolution and high performance through its rich metadata layer.\r\nThrough our presentation we will showcase the different challenges we faced during the design of our new architecture, how our selected tech stack allowed us to tackle each of them and the lessons we learnt. We will focus on our challenges in four areas: scalability, performance, continuity of service and data quality.\r\n\r\nScalability: Ingesting changes on over one thousand tables coming from servers across thirty-two countries supporting operations of Europe\u2019s largest retailer is no easy task. Our architecture needs to support receiving tens of thousands of events per second. We will present how we set up Kafka to support our current load and potential for future growth, how we use Tabular\u2019s Iceberg sink connector to ingest all our tables, and how we leverage avro serialization and snappy compression of messages to reduce network traffic.\r\n\r\nPerformance: The large amounts of data we handle, paired with the influx of small files that can result from real-time data ingestion, made ensuring performance an extremely challenging aspect of our application. We will show how we designed our data lake house; using Iceberg's hidden partitioning to ensure performance while remaining flexible to evolve them over time and how we designed and implemented an effective maintenance job to reduce small files in our iceberg tables.\r\n\r\nContinuity of Service: The existing analytics platform contains the core of the business data which is used for many analytics and forecasting use cases across the organization. One of main requirements was to ensure a smooth transition to the new architecture with as little downtime as possible. This meant facilitating the access to existing users by allowing them to retrieve the data in the same way they were doing it in the past, with minimal changes. We will show how our architecture ensures flexibility by allowing access to the data from diverse query engines and show an example on how we integrated our architecture with Snowflake. \r\n\r\nData quality: We faced some challenges when it came to the consolidation of all the data we receive from all 32 countries. For some tables, the schemas diverged across countries; having different sets of columns, different data types being used and even different primary keys. We will talk about how we handled data quality issues coming from the operational databases by using iceberg\u2019s schema evolution capabilities, a schema registry and Kafka Connect single message transforms (SMTs).", "code": "JUAF3S", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/JUAF3S.png", "speaker_names": "Jonas B\u00f6er, Elena Ouro Paz", "speakers": "\n### Jonas B\u00f6er\n\nSoftware Engineer since 2018, Data Engineer at inovex since 2022 \u2013\u00a0happy to get the job done, but prefers beautiful solutions.\n\n### Elena Ouro Paz\n\n\n", "track": "Data Handling & Engineering"}, {"title": "High-performance dataframe-agnostic GLMs with glum", "abstract": "Generalized linear models (GLMs) are interpretable, relatively quick to train, and specifying them helps the modeler understand the main effects in the data. This makes them a popular choice today to complement other machine-learning approaches. `glum` was conceived with the aim of offering the community an efficient, feature-rich, and Python-first GLM library with a scikit-learn-style API. More recently, we are striving to keep up with PyData community's ongoing push for dataframe-agnosticism.\r\nWhile `glum` was originally heavily based on `pandas`, with the help of `narwhals`, we are close to being able to fit models on any dataset that the latter supports. This talk presents our experiences with achieving this goal.", "full_description": "Arguably, `glum`'s standout feature is its ability to efficiently handle datasets consisting of a mix of dense, sparse and categorical features. To facilitate this, it relies on our (similarly open-source) `tabmat` library, which provides classes and useful methods for mixed-sparsity data. `glum` fits models by first converting input data to `tabmat` matrices, and then using those matrices to do the necessary computations.\r\n\r\nTherefore, dataframe-agnostism in our case mostly boils down to handling the conversion of different dataframes to `tabmat` matrices (which themselves store data in `numpy` arrays and sparse `scipy` matrices) in an efficient manner. Most of it is rather smooth and straightforward due to `narwhals` providing a convenient compatibility layer for a wide range of dataframe functionality. However, we have encountered a couple of pain points that might be of interest to other package maintainers and the PyData community. In particular,\r\n\r\n- We heavily rely on manipulating the category order and encoding of categorical variables, for which there is somewhat limited support. This is due to various dataframe libraries handling cateegorical columns somewhat differently.\r\n- Most dataframe libraries do not support sparse columns, while for us, it is important to be able to accept sparse inputs.\r\n\r\nIn this talk I demonstrate how we used `narwhals` to easily accept multiple types of dataframes. I will go into details about categorical and sparse columns, and present the challenges we encountered with those. I will also examine the benefits and challenges of supporting sparse columns in dataframe libraries and the Arrow stardard. These points are meant to facilitate discussion among the participants and in the PyData community.\r\n\r\nAt the end of the talk I will also briefly mention potential future plans for `glum` and `tabmat`, including the possibility to do computations directly on Arrow objects without converting them to `numpy` and `scipy` arrays. \r\n\r\n### Outline\r\n\r\n1. A short intro to `glum`, it's backend library `tabmat`, and the main ideas that make them performant.\r\n2. Making `glum` dataframe-agnostic.\r\n    - Showcase how `narwhals` simplifies handling a wide variety of dataframes.\r\n    - Discuss handling categorical (and enum/dictionary) columns.\r\n    - Talk about representing sparse columns in dataframes.\r\n3. Concluding remarks and potential future plans.\r\n\r\n### Target audience\r\n- Basic understanding of the scientific Python ecosystem (with a focus on dataframe libraries) is recommended.\r\n- While some familiarity with linear models might be useful to get the most out of this talk, it is by no means required.\r\n\r\n### Main takeaways\r\n\r\n- How `glum` efficiently handles mixed-sparsity data\r\n- How `narwhals` helps to achieve dataframe-agnosticism with little effort\r\n- Differences between categorical types in various packages and the Apache Arrow specification.\r\n- How support for sparse column could be incorporated into dataframe libraries and the Arrow Columnar Format", "code": "JUQ9JJ", "state": "confirmed", "created": "2025-01-04", "social_card_image": "/static/media/social/talks/JUQ9JJ.png", "speaker_names": "Martin Stancsics", "speakers": "\n### Martin Stancsics\n\nMartin is a data-scientist/engineer at QuantCo. He is mainly working on developing the software packages that Quantco uses for insurance risk modeling and pricing. This includes QuantCo's open-source generalized linear modeling package, glum.\r\n\r\nHe has a background in economics, and has previously worked at the Central Bank of Hungary as an applied researcher. He also taught a number of 'Programming for Economists' courses for college and PhD students.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Safeguard your precious API endpoints built on FastAPI using OAuth 2.0", "abstract": "Is implementing authorization on your API endpoints an afterthought? Who should have access to your API endpoints? Is it secure? This talk covers using OAuth 2.0 to secure API endpoints built on FastAPI following industry-recognized best practices. Come on a journey with me from taking your API endpoints to being functional AND secure. When you follow secure identity standards, you\u2019ll be equipped with a deeper understanding of the critical need for authorization.", "full_description": "Audience Level: Beginners, Pythonistas who build on FastAPI who are not necessarily security experts but still need to deploy secure APIs.\r\n\r\nHistory of OAuth 2.0? (3 mins)\r\n- Background/history on OAuth \r\n- Why do we need OAuth 2.0?\r\n\r\nAuthorization Challenge (2 mins)\r\n- Why implement secure authorization now rather than later?\r\n- Data sensitivity\r\n\r\nOAuth 2.0 Overview (3 mins)\r\n- Core concepts\r\n- Key features: What are JWTs?\r\n- Benefits of using OAuth 2.0\r\n\r\nTechnical Implementation (4 mins)\r\n- Components of OAuth 2.0 \r\n- Different types of authorization flows and use cases\r\n- API setup on FastAPI\r\n\r\nDemo with FastAPI (12 mins) \r\n- Create an endpoint in FastAPI framework and secure it with OAuth 2.0\r\n- What are the different identity providers that can provide authorization?\r\n- Troubleshooting common issues \r\n\r\nBest Practices (4 mins)\r\n- Industry-standard protocol\r\n- Token-based security \r\n- Should you build your authorization server?\r\n\r\nNext Steps (2 mins)\r\n- Ability to integrate/provide SSO with various IdPs\r\n- Share resources to learn more including blogs, GitHub repo, etc.\r\n- Got questions? Connect with me!", "code": "K9ACTV", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/K9ACTV.png", "speaker_names": "Semona Igama", "speakers": "\n### Semona Igama\n\nSemona is a Developer Advocate at Okta. She enjoys chatting about OpenID Connect, OAuth 2.0, and web security, but most of all, learning how developers learn best. Outside work, Semona is a Pythonista, loves kombucha, and plays board/role-playing games and Ultimate!\n", "track": "Security"}, {"title": "Multi-tenant Conversational Analytics", "abstract": "Ever wondered how to use GenAI to enable self-service analytics through prompting? In this talk, I will share my experience of building a multi-tenant conversational analytics set-up that is built into a Software-as-a-Service (SaaS) platform. This talk is intended for AI engineers, data scientists, software engineers and anyone interested in using GenAI to power conversational analytics using open-source tools. \r\n\r\nI will discuss the challenges faced in designing and implementing, as well as the lessons learned along the way. We'll answer questions such as, why offer analytics through prompting? Why multi-tenancy and makes it so difficult? How to build it into an existing product? What makes open-source the preferred choice over proprietary solutions? What could the implications be for the analytics field?", "full_description": "This talk will start by answering the question: What is conversational analytics and how does it work? After which we'll dive into why this was built and how the implementation was done. \r\n\r\n* How analytics in SaaS can be fundamentally improved by conversational analytics (5 mins). \r\n* How the Text-to-SQL fundament was shaped using RAG with Embeddings in PGVector (5 mins). \r\n* Dealing with multi-tenancy in PostgreSQL and BigQuery to ensure data segregation & security (5 mins). \r\n* How to handle tenant specific pre-training and training examples (5 mins). \r\n* Building this into an existing application and supporting integrations (5 minutes). \r\n* Conclusion and thoughts on the implications for the field of analytics (5 mins). \r\n\r\nIn the end you should have a good idea on why conversational analytics can be a game changer, what the pitfalls are and how to build it with open source technologies.", "code": "KCSSJ7", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/KCSSJ7.png", "speaker_names": "Rodel van Rooijen", "speakers": "\n### Rodel van Rooijen\n\nI speak & write about my experiences in the world of data & AI. This comes from the perspective of having worked across data science, data engineering and ML engineering in start-ups, scale-ups and enterprises.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Beyond Alembic and Django Migrations", "abstract": "ORMs like Django and SQLAlchemy have become indispensable in Python development, simplifying the interaction between applications and databases. Yet, their built-in schema migration tools often fall short in projects that require advanced database features or robust CI/CD integration.\r\n\r\nIn this talk, we\u2019ll explore how you can go beyond the limitations of your ORM\u2019s migration tool. Using Atlas\u2014a language-agnostic schema management tool\u2014as a case study, we\u2019ll demonstrate how Python developers can automate migration planning, leverage advanced database features, and seamlessly integrate database changes into modern CI/CD pipelines.", "full_description": "Talk Structure: \"Beyond Your ORM's Migration Tool\"\r\n\r\n1. Introduction \u2013 Why ORMs Build Migration Tools\r\n   - ORMs like SQLAlchemy and Django ORM simplify database interactions and include migration tools (e.g., Alembic, Django Migrations) for schema changes.\r\n   - These tools are robust for ORM-defined schemas but lack advanced features and native CI/CD integrations.\r\n\r\n2. Where Built-in Tools Fall Short\r\n   - ORM migration tools focus on basic schema changes but don\u2019t support advanced database objects like triggers, materialized views, or stored procedures.\r\n   - Lack native integration with modern CI/CD tools, leaving teams to implement custom, often suboptimal solutions.\r\n\r\n3. Presenting Atlas \u2013 Bridging the Gap\r\n   - Atlas complements ORM tools by reading their schemas (e.g., Django models, SQLAlchemy models) and enabling advanced extensions.\r\n   - Key features:\r\n     - Support for triggers, materialized views, and other advanced objects.\r\n     - Native CI/CD integration for automating and validating schema changes.\r\n\r\n4. How Atlas Integrates with ORMs\r\n   - Atlas reads ORM-defined schemas and enhances them with advanced features.\r\n   - Combines ORM workflows with Atlas\u2019s robust schema management capabilities, enabling automation and database-specific optimizations.\r\n\r\n5. Demo \u2013 Atlas in Action\r\n   - Example: A Django project adds a materialized view and a trigger using Atlas.\r\n   - Steps:\r\n     - Use Atlas to read the ORM schema and extend it with advanced features.\r\n     - Automate migration validation and deployment through CI/CD pipelines.\r\n   - Outcome: Simplified and automated schema management with modern tooling.\r\n\r\n6. Conclusion and Q&A\r\n   - Key Takeaways:\r\n     - ORM migration tools like Alembic and Django Migrations are great for standard use cases but fall short for advanced workflows and CI/CD integration.\r\n     - Atlas bridges this gap, enabling automation and advanced database features.\r\n   - Call to Action: Try Atlas to enhance schema workflows.\r\n   - Q&A: Open floor for questions.", "code": "KCV9RS", "state": "confirmed", "created": "2024-12-26", "social_card_image": "/static/media/social/talks/KCV9RS.png", "speaker_names": "Rotem Tamir", "speakers": "\n### Rotem Tamir\n\nRotem Tamir (39), father of two. Co-founder and CTO of Ariga, creator of Atlas, an open-source database schema as code tool.\n", "track": "Django & Web"}, {"title": "Responsible AI with fmeval - an open source library to evaluate LLMs", "abstract": "The term \"Responsible AI\" has seen a threefold increase in search interest compared to 2020 across the globe. As developers, the questions like \"How can we build large language model-enabled applications that are responsible and accountable to its users?\" encountered in the conversation more often than before. And the discussion is further compounded by concerns surrounding uncertainty, bias, explainability, and other ethical considerations.\r\n\r\nIn this session, the speaker will guide you through fmeval, an open-source library designed to evaluate Large Language Models (LLMs) across a range of tasks. The library provides notebooks that you can integrate into your daily development process, enabling you to identify, measure, and mitigate potential responsible AI issues throughout your system development lifecycle.", "full_description": "The term \"Responsible AI\" has seen a threefold increase in search interest compared to 2020 across the globe. As developers, the questions like \"How can we build large language model-enabled applications that are responsible and accountable to its users?\" encountered in the conversation more often than before. And the discussion is further compounded by concerns surrounding uncertainty, bias, explainability, and other ethical considerations.\r\n\r\nIn this session, the speaker will guide you through fmeval, an open-source library designed to evaluate Large Language Models (LLMs) across a range of tasks. The library provides notebooks that you can integrate into your daily development process, enabling you to identify, measure, and mitigate potential responsible AI issues throughout your system development lifecycle.\r\n\r\nTarget Audience: Machine Learning Engineers/Data Scientists, AI/ML Researchers, Software Developers, AI/ML Project Managers, Solutions Architectures.", "code": "KTJY9V", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/KTJY9V.png", "speaker_names": "Mia Chang", "speakers": "\n### Mia Chang\n\nMia Chang is a GenAI/ML Specialist Solutions Architect for Amazon Web Services. She shares best practices for running GenAI/ML workloads through customer engagements, public speaking, blog posts, and authoring books. She works in a multi-culture environment with customers in EMEA, which brings her to see technology with different culture lens. In her free time, Mia spends time mentoring aspired data scientists, and she enjoys traveling, hiking, board games, and meditation.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Duplicate Code Dilemma: Unlocking Automation with Open Source!", "abstract": "\"Don't Repeat Yourself\" \u2013 a phrase that we have all heard many times. In this talk, we will have an overview how to deal with code duplication and how open-source template libraries such as Copier and Cookiecutter can assist us in managing similarly structured repositories. Furthermore, we will explore how code updates can be automated with the help of the open-source library Renovate Bot. By the end of this session, you will gain insights into these solutions while also questioning whether they truly eliminate repetition or merely contribute to another cycle of automation.", "full_description": "\u201cDon\u2019t Repeat Yourself\u201d (DRY) is one of the first principles that every programmer encounters in the early stages of their coding journey. Some of us even had to learn it the hard way. We promised ourselves to avoid repetitive code to never again deal with the extensive refactoring required for every small change.\r\n\r\nThis simple principle has found a fundamental place in every programmer's heart. It may also be the reason why, from time to time, every programmer doubts their code and begins to refactor it in the early stages of coding.\r\n\r\nThis talk provides an overview of different solutions for preventing code repetition. We will start with the most common solutions, such as using functions and classes, and then explore more intermediate approaches for managing similarly structured repositories with the help of open-source template libraries such as Copier and Cookiecutter. Finally, we will address a more complex problem and examine how to automate updates using open-source tools like Renovate Bot.\r\n\r\nAs a takeaway, participants will gain insights into various solutions and a glimpse into the usability of each open-source library. Participants are also encouraged to reconsider the entire process: Are these solutions truly preventing repetitive code, or are we merely caught in an endless cycle of automation?", "code": "KZKT9W", "state": "confirmed", "created": "2024-12-16", "social_card_image": "/static/media/social/talks/KZKT9W.png", "speaker_names": "Raana Saheb-Nassagh", "speakers": "\n### Raana Saheb-Nassagh\n\nHey, I am Raana!\r\n\r\nSome days, I am a Data Scientist with a passion for patterns, and other days, I am a Data Engineer with a love for code refactoring. I am also interested in methods for team building.\r\n\r\nBesides the nerdy stuff, I enjoy board games, bouldering, and singing in a choir!\n", "track": "Programming & Software Engineering"}, {"title": "What's inside the box? Building a deep learning framework from scratch.", "abstract": "Explore the inner workings of deep learning frameworks like TensorFlow and PyTorch by building your own in this workshop. We will start with the fundamental automatic differentiation mechanics and proceed to implementing more complex components like layers, modules and optimizers. This workshop is mainly designed for experienced data scientists, who want to expand their intuition about lower level framework internals.", "full_description": "Data scientists typically concentrate on the mathematical foundations when designing and training neural networks, often treating the process by which deep learning frameworks link high-level code with lower-level mathematical operations as a black box. As a result, the internal workings of these frameworks are frequently overlooked.\r\n\r\nThis workshop is aimed to open the black box by letting the participants construct a small deep learning framework from scratch. We will begin with creating a simple automatic differentiation engine, followed by more advanced elements such as modules, and optimizers.\r\n\r\nAs a result, the participants will be able to construct and train a neural networks architecture using the framework they have built in just 1.5 hours.\r\n\r\nThe detailed text guide and solutions for all of the exercises are going to be provided as a public GitHub repository.\r\n\r\nAfter constructing the framework from scratch, the participants will gain a comprehensive understanding of:\r\n- the inner workings of deep learning frameworks;\r\n- the mapping of high-level framework components to lower-level operations;\r\n- the operational principles of autograd engine and dynamic computational graphs;\r\n- higher-level abstractions such as modules, and their mechanisms of automatic parameters tracking;\r\n\r\n**Target audience**\r\n\r\nThis workshop is primarily intended for those with some experience in building deep learning models using popular frameworks like PyTorch, TensorFlow, or JAX. However, prior experience is not absolutely mandatory, as essential fundamentals will be briefly covered.\r\n\r\n**Outline**\r\n\r\nIntroduction, motivation and essential theory [15 min]\r\nImplementation [60 min]\r\nTensors + autograd engine [25 min]\r\nModules and layers [25 min]\r\nOptimizers [10 min]\r\nUsing the framework to build and train the model [10 min]\r\nConcluding remarks + sharing bonus exercises [5 min]", "code": "LBKU3T", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/LBKU3T.png", "speaker_names": "Oleh Kostromin", "speakers": "\n### Oleh Kostromin\n\nI am a Data Scientist primarily focused on Deep Learning and MLOps. In my spare time I contribute to several open-source python libraries.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Inclusive Data for 1.3 Billion: Designing Accessible Visualizations", "abstract": "According to the World Health Organization (WHO), an estimated 1.3 billion people (1 in 6 individuals) experience a disability, and nearly 2.2 billion people (1 in 5 individuals) have vision impairment. Improving the accessibility of visualizations will enable more people to participate in and engage with our data analyses.\r\n\r\nIn this talk, we\u2019ll discuss some principles and best practices for creating more accessible data visualizations. It will include tips for individuals who create visualizations, as well as guidelines for the developers of visualization software to help ensure your tools can help downstream designers and developers create more accessible visualizations.", "full_description": "Specifically, we will cover:\r\n\r\n- What makes data visualizations inaccessible? We will cover accessibility fundamentals like color contrast, alternative text descriptions, keyboard navigation support, screen reader compatibility, and more, with specific examples and demonstrations.\r\n- Are Python data visualization tools accessible? We will teach how to analyze the visualization landscape and discuss how tool developers can begin and prioritize improvements. \r\n- How accessible is my visualization? We will demonstrate how to conduct accessibility audits for data visualization tools by performing and documenting two accessibility evaluation tests live.\r\n\r\nThis talk will include specific examples from our ongoing work to improve the accessibility of Bokeh, a Python library for creating interactive data visualizations for web browsers. We hope this talk enables you to take the first few steps in making your next data visualization and your visualization tools, more accessible.", "code": "LNW3KE", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/LNW3KE.png", "speaker_names": "Dr. Tania Allard, Pavithra Eswaramoorthy", "speakers": "\n### Dr. Tania Allard\n\n\n\n### Pavithra Eswaramoorthy\n\nPavithra Eswaramoorthy is a Developer Advocate at Quansight, where she works to improve the developer experience and community engagement for several open source projects in the PyData community. Currently, she maintains the Bokeh visualization library, and contributes to the Nebari (adjacent to the Jupyter community), and conda-store (part of the conda ecosystem).\r\n\r\nPavithra has been involved in the open source community for over 5 years, notable as an emeritus contributor to the Dask library and Wikimedia Foundation projects. In her spare time, she enjoys a good book and hot coffee. :)\n", "track": "Visualisation & Jupyter"}, {"title": "Beyond Code: Fostering Diversity and Inclusion in Open Source", "abstract": "Open source thrives not just on code, but on the diverse perspectives and inclusive practices that shape our communities. In this dynamic talk, we explore the intersection of open source and diversity, shedding light on how to bridge existing gaps and build welcoming environments for underrepresented groups. Through real-world examples, practical strategies, and inspiring stories from existing researches made on African communities, we'll uncover the immense benefits of diversity and equip you with actionable steps to foster inclusivity. Join us to learn how you can help create a more vibrant and inclusive open-source community that goes beyond code.", "full_description": "Open source is more than just a collaborative coding effort -- it's about creating communities where everyone feels valued and included. This talk delves into the critical intersection of open source and diversity, examining the current landscape and identifying gaps that need to be addressed. We'll explore practical strategies to make open source communities more inclusive and welcoming to underrepresented groups. Attendees will gain insights into the benefits of a diverse community, understand the challenges faced by marginalized contributors, and learn actionable steps to foster inclusivity. By the end of this session, you'll be equipped with the knowledge and tools to advocate for and implement diversity initiatives within your open-source projects. \r\n\r\n**Outline**\r\n**1- Introduction to Diversity in Open Source (5 minutes)**\r\n\r\nTopics: Importance of diversity, current landscape and challenges.\r\nObjectives: Set the context for the importance of diversity in open-source communities, highlight existing gaps and challenges.\r\n\r\n\r\n**2- Benefits of a Diverse Community (5 minutes)**\r\n\r\nTopics: Innovation, community health, and project sustainability.\r\nObjectives: Illustrate the tangible benefits of diversity in open-source projects, inspire attendees to prioritize inclusivity.\r\n\r\n\r\n**3- Challenges Faced by Underrepresented Groups (5 minutes)**\r\n\r\nTopics: Barriers to entry, experiences of marginalized contributors.\r\nObjectives: Provide a deeper understanding of the obstacles faced by underrepresented groups, foster empathy and awareness.\r\n\r\n\r\n**4- Practical Strategies for Inclusion (5 minutes)**\r\n\r\nTopics: Mentorship programs, inclusive language, community-building activities.\r\nObjectives: Offer actionable steps for creating inclusive environments, provide practical examples and best practices.\r\n\r\n\r\n**5- Case Studies and Success Stories (5 minutes)**\r\n\r\nTopics: Real-world examples of successful diversity initiatives.\r\nObjectives: Share inspiring stories of diverse open-source communities, demonstrate the positive impact of inclusion efforts.\r\n\r\n**6- Q&A (5 minutes)**", "code": "LRL7MS", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/LRL7MS.png", "speaker_names": "Ariane Djeupang", "speakers": "\n### Ariane Djeupang\n\n**Ariane Djeupang** is a Junior Machine Learning Engineer and Project Manager, passionate about fostering community growth and innovation in the tech industry. She is dedicated to promoting diversity and inclusion, actively participating in the Python | Django | PyLadies communities and other initiatives that empower underrepresented groups in technology. Her commitment to mentorship and community-driven change drives her efforts to create supportive environments where everyone can thrive and excel, and have a sincere sense of belonging.\n", "track": "Community & Diversity"}, {"title": "Why Don\u2019t Customers Want My Free Goods? \u2013 Why Forecasting Models Don\u2019t Answer 'What If' Questions", "abstract": "Forecasting and causal inference are distinct but fundamental tasks in data science. While forecasting predicts future outcomes based on history, causal inference explores the \"why\" behind those outcomes and helps simulate \"what if\" scenarios. Confusing the two can lead to misleading results.\r\n\r\nAt Blue Yonder, we encountered a case where a customer's forecasting model predicted demand accurately based on price. However, when they used the model for simulations to explore \"what if\" scenarios, the results were counterintuitive: lower prices led to lower demand. I will share how we resolved this issue and emphasize the importance of incorporating causal thinking when addressing questions like, \"Why did this happen?\" or \"What if I do X?\"\r\n\r\nIn this talk, I\u2019ll show how to identify common pitfalls, like confounders, when integrating causal inference into forecasting workflows. We\u2019ll also explore Bayesian models, powered by Markov Chain Monte Carlo (MCMC) methods, to bridge the gap between forecasting and causality using the PyMC library on practical examples.\r\n\r\nBy the end of this talk, you\u2019ll learn how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses with Directed Acyclic Graphs (DAGs) to understand relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.", "full_description": "Forecasting and causal inference are fundamental yet distinct tasks in data science. While forecasting predicts future outcomes based on historical patterns, causal inference seeks to understand the \"why\" behind these outcomes and explore \"what if\" scenarios to simulate the impact of interventions. These tasks often overlap but have different requirements\u2014and confusing them can lead to unexpected results.\r\n\r\nAt Blue Yonder, we observed an intriguing case where a customer's forecasting model used price information to predict demand accurately. However, when they used the model for causal simulations\u2014adjusting prices to explore \"what if\" scenarios\u2014the predictions behaved counterintuitively: lower prices led to lower predicted demand! I would like to share the details of how we helped the customer resolve this issue in this real-world example and highlight the importance of taking a causal perspective when tackling questions like, \"Why did this happen?\", \"What if I do X?\", or \"What if I had done Y instead?\"\r\n\r\nIn this talk, I will demonstrate how to identify and avoid common pitfalls, such as confounders, when incorporating causal inference into traditional forecasting workflows. I\u2019ll show how we can use Bayesian models powered by Markov Chain Monte Carlo (MCMC) methods to bridge the gap between forecasting and causality. Together, we\u2019ll walk through practical examples using the PyMC library to illustrate these concepts.\r\n\r\nBy the end of this talk, you\u2019ll gain actionable insights on how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses using Directed Acyclic Graphs (DAGs) to understand causal relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.\r\n\r\nWhether you\u2019re interested in forecasting, causal inference, Bayesian modeling, or all of them, this talk will equip you with practical tools to approach each with clarity and confidence\u2014grounded in a real-world example from the Blue Yonder experience.", "code": "LRRVGG", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/LRRVGG.png", "speaker_names": "Matthias Binder", "speakers": "\n### Matthias Binder\n\nSenior Staff Data Science Consultant at Blue Yonder. I'm passionate about ML and AI in the Supply Chain. I'm an avid learner and my newest goal is to dive deeper into Bayesian Inference and Causal Inference.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond DALL-E: Advanced Image Generation Workflows with ComfyUI", "abstract": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. There are well-known models like Stable Diffusion and Flux, which can be used with easy-to-use frontends like A1111 or Invoke AI, but if you want to do more complex or bleeding-edge workflows, you need something else. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build complex pipelines that are otherwise only possible using plain code.", "full_description": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. But thanks to Stable Diffusion, Flux, and many supplementary models like ControlNet or an Image Prompt Model, we have much more control over the images we want to create. There are frontends for that, like A1111 or Invoke AI, but if you want to try bleeding-edge models or do something more complex, you will have a hard time implementing such a pipeline in code yourself, and it requires a steep learning curve. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build workflows as a DAG. Thanks to many other contributors, there are a lot of plugins available which bring in new functionality. This talk shows the capabilities and power of this tool using practical examples and how you can combine many things together to create a complex workflow much faster than coding it yourself.\r\n\r\nI want to cover the following topics:\r\n - What are the limits of a simple text-to-image workflow?\r\n - What is ComfyUI?\r\n - What are the requirements to use ComfyUI? (Resources, OS, etc.)\r\n - What can you do with ComfyUI that you can't do with a simple text-to-image interface?\r\n   - Pre- and post-processing of images in a single workflow\r\n   - Advanced conditioning using images, bounding boxes, depth maps, etc., all together\r\n - The examples shown as a demonstration:\r\n   - Integrating existing objects from a photo into a generated scenery\r\n   - Creating optical illusions and surreal images", "code": "LRUKZQ", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/LRUKZQ.png", "speaker_names": "Ren\u00e9 Fa", "speakers": "\n### Ren\u00e9 Fa\n\nJust another Python nerd with a freshly gained enthusiasm for image gen AI. I'm working as a Data Engineer for nearly three years with focus on computer vision topics.\n", "track": "Computer Vision (incl. Generative AI CV)"}, {"title": "Enhancing Software Supply Chain Security with Open Source Python Tools", "abstract": "The Cyber Resilience Act (CRA) is focused on improving the security and resilience of digital products. But to comply with the CRA, businesses will need to start preparing the necessary evidence to ensure compliance if they want to continue to deliver digital products to the EU market once the CRA is in force. \r\n\r\nKey requirements within the CRA include implementing robust security measures throughout the product life-cycle, adopting secure development practices and implementing proactive vulnerability management processes.\r\n\r\nThis session will show how a number of the requirements for the CRA can be achieved by use of a number of open source Python tools.", "full_description": "The Cyber Resilience Act (CRA) is aimed at improving the security and resilience of the software components within a digital product. This session will provide a high level overview of the CRA and demonstrate how to enhance software supply chain transparency, manage risks effectively throughout the Software Development Lifecycle (SDLC), and achieve the necessary compliance by leveraging a suite of open-source Python tools. \r\n\r\nKey areas to be addressed will include:\r\n\r\n- Learn how to create comprehensive and high quality SBOMs to gain a clear understanding of all components within your software.\r\n- Discover how to identify and mitigate potential risks and threats within the software supply chain throughout the entire SDLC.\r\n- Explore effective strategies for identifying, assessing, prioritising and remediating software vulnerabilities.\r\n- Understand how to adopt best practices to ensure compliance with relevant regulations and industry standards.\r\n\r\nThe Python tools/applications to be referenced will include sbom4python, lib4sbom, lib4vex, lib4package, distro2sbom, sbomdiff, sbomaudit and cve-bin-tool.", "code": "M98YBR", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/M98YBR.png", "speaker_names": "Anthony Harrison", "speakers": "\n### Anthony Harrison\n\nAnthony Harrison has been developing and delivering mission-critical applications for over 40 years working on various complex programs where he held various roles in software, systems and cyber engineering, as well as providing technical leadership for a number of programmes.\r\n\r\nHe is the Founder and Director of APH10, and co-founder of SBOM Europe, and is a leading source of expertise in Software Bill of Materials (SBOM). He has been developing open source software actively for a number of years; most recently, the applications have been related to supporting the software supply chain through utilities to generate and analyse software bills of materials (SBOMs).\r\n\r\nHe has been a mentor for the Google Summer of Code for the past four years via the Python Software Foundation and is a mentor for his local CoderDojo in Manchester teaching students Python.\n", "track": "Security"}, {"title": "Oh my license! \u2013 Achieving order by automation in the license chaos of your dependencies", "abstract": "License issues can haunt you at night.\r\nYou spend days, weeks, and months developing beautiful software.\r\nBut then it happens.\r\nYou realize that an essential dependency is GPL-3.0 licensed.\r\n\r\nAll your code is now infected with this license.\r\nNow you are forced to either:\r\n1. Rewrite all parts relying on the other library\r\n2. Open-source your codebase under the GPL-3.0 license\r\n\r\nHow could this have been avoided?\r\n\r\nJoin the talk and find out!\r\nFirst, we\u2019ll give you a brief introduction to different software licenses and their implications.\r\nSecond, we\u2019ll show you how to automate your license checking using open-source software.", "full_description": "Software licensing can feel like a daunting maze, but it doesn\u2019t have to be.\r\nThis talk will demystify the world of software licenses and equip you with the critical knowledge to navigate it with confidence.\r\n\r\nWe\u2019ll start by exploring key categories of licenses\u2014like Strong Copyleft, Weak Copyleft, and Permissive\u2014and break down the most common ones you\u2019ll encounter (e.g., GPL, AGPL, BSD, and MIT). Through concrete examples, you\u2019ll learn how these licenses affect your projects and how to handle them effectively.\r\n\r\nNext, we\u2019ll dive into practical solutions for automating license compliance. You\u2019ll be introduced to conda-deny (an open-source tool) and see how it can help ensure your projects remain compliant without adding manual overhead.\r\n\r\nWhether you\u2019re building open-source software or proprietary tools, this talk will leave you with actionable strategies to future-proof your projects and avoid licensing pitfalls.", "code": "ME7XPJ", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/ME7XPJ.png", "speaker_names": "Paul M\u00fcller", "speakers": "\n### Paul M\u00fcller\n\nPaul studies Computer Science at the KIT in Karlsruhe.\r\nAlongside his studies, he works part-time at QuantCo.\n", "track": "Programming & Software Engineering"}, {"title": "What do a tree and the human brain have in common-a not so serious introduction to digital pathology", "abstract": "While trees and human brains don't share that many properties regarding their domain, the analysis of the height of a tree and cancer in human brains does.\r\nThis talk provides a not-so-serious introduction to the domain of computer vision for pathological use cases. \r\nBesides a general introduction to (digital) pathology and the technical similarities between satellite images (GeoTIFs) and pathological images (Whole-Slide Images), we will take a look at computer vision (both ML-based and conventional) on Python.\r\nWhether you have never done image processing in Python, are an expert (ready to share some tricks with me), or are just curious to see pictures of a human brain, this talk is for you.\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "full_description": "Inspired by last year's talk about the height of a tree [\ud83c\udf33 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery \ud83d\udef0\ufe0f] and the strong similarities between optical high resolution satellite images and pathological images, this talk will give a not-so-serious introduction to a quite serious topic.\r\nThe main content is:\r\n- a short introduction to digital pathology (know your domain)\r\n- the similarities between a tree and your brain (technically speaking, there are a lot)\r\n- ML-based and conventional computer vision in Python with some practical use cases\r\n- Why we can steal (nearly) everything from radiology and get away with it\r\n\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "code": "MJD7TG", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/MJD7TG.png", "speaker_names": "Daniel Hieber", "speakers": "\n### Daniel Hieber\n\nDaniel is a PhD student in digital neuropathology at Julius-Maximilians-University W\u00fcrzburg and a research assistant at the University Hospital Augsburg as well as Neu-Ulm University of Applied Sciences. His work is focused on applying computer vision techniques to automate analysis processes in the pathological departments.\n", "track": "Computer Vision (incl. Generative AI CV)"}, {"title": "Using Causal thinking to make Media Mix Modeling", "abstract": "In today's data-driven landscape, understanding causal relationships is essential for effective marketing strategies. This talk will explore the link between Bayesian causal thinking and media mix modeling, utilizing Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP).\r\n\r\nWe will examine how DAGs represent causal assumptions, how SCMs define relationships in media mix models, and how to implement these models within a Bayesian framework. By using media mix models as causal inference tools, we can estimate counterfactuals and causal effects, offering insights into the effectiveness of media investments.", "full_description": "In the era of data-driven decision-making, understanding causal relationships is crucial for effective marketing strategies. This talk delves into the underexplored connection between Bayesian causal thinking and media mix modeling, linking Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP). By navigating through these key concepts, we will demonstrate how we can build models that not only predict outcomes but also represent causal mechanisms within the marketing ecosystem.\r\n\r\nStarting from foundational principles, we will explore how DAGs serve as a formal language for encoding causal assumptions, how Structural Causal Modeling define relationships in media mix models, and how we implement those in the Bayesian framework through the famous DGP. We will further illustrate how media mix models can be employed as causal inference tools to estimate counterfactuals and causal effects, providing actionable insights into the effectiveness of media investments.\r\n\r\nFinally, we\u2019ll show how Bayesian inference enables us to update these causal beliefs in light of data. This synthesis of causal reasoning and probabilistic modeling is not only theoretically rich but practically powerful\u2014offering a robust framework for constructing media mix models that more accurately reflect the complexities of real-world marketing dynamics.\r\n\r\nAttendees will leave with an understanding of how to apply Bayesian causal discovery (guided by an example in an IPython notebook) to develop causally valid models that can be applied to real-world marketing data. They will learn how to use Media Mix Models as causal inference tools to estimate counterfactual scenarios and causal effects, unlocking deeper insights into the effectiveness of media investments. This presentation aims to reveal a new pathway for marketers, data scientists, and researchers to harness the potential of these powerful methodologies together, empowering them to drive more informed, causally grounded decisions.", "code": "MNTFRG", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/media/social/talks/MNTFRG.png", "speaker_names": "Carlos Trujillo", "speakers": "\n### Carlos Trujillo\n\nSix years ago, I discovered that my passion was in the field of data and artificial intelligence. I decided to move from Venezuela to Chile in search of new challenges and currently found one of them in Estonia, where I have had the opportunity to work on teams in Latin America, Europe and Africa.\r\n\r\nI have been able to work on projects related to artificial intelligence, machine learning and deep learning, especially in the field of marketing, which has allowed me to help traditional companies adopt a data-driven approach. However, my latest challenge has been working at the fastest mobility company in Europe, where I have been able to apply all my knowledge and skills in a highly dynamic and constantly evolving environment.\r\n\r\nI have had to develop different programs in Python, using SQL and No-SQL to build cloud structures that can handle large volumes of information. I have learned to work with DataBricks and DBT, and I am familiar with Google Cloud and AWS. I have also explored tools such as Airflow, CloudRun, App Engine, BigQuery, S3, DynamoDB, MongoDB, among others.\r\n\r\nMy focus has always been on the areas of statistics and mathematics, seeking to solve recurrent problems in business through techniques of computer vision, natural language processing, regression or classification algorithms, and neural networks. I have generated dashboards in Looker, Looker Studio, Tableau and Power BI, also custom reports, alerts and complex artificial intelligence models, leading teams of four to ten people, being the bridge between marketers and technical teams.\r\n\r\nI still have a long way to go to become the \"Marketing Scientist\" I want to be, but I am grateful for all the opportunities and challenges I have faced so far. I am certainly eager to continue learning and growing in my career, looking for new challenges that will take me to spaces that I have not yet explored.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Open Table Formats in the Wild: From Parquet to Delta Lake and Back", "abstract": "Open table formats have revolutionized analytical, columnar storage on cloud object stores with critical features like ACID compliance and enhanced metadata management, once exclusive to proprietary cloud data warehouses. Delta Lake, Iceberg, and Hudi have significantly advanced over traditional open file formats like Parquet and ORC.\r\n\r\nIn an effort to modernize our data architecture, we aimed to replace our Parquet-based bronze layer with Delta Lake, anticipating better query performance, reduced maintenance, native support for incremental processing, and more. While our initial pilot showed promise, we encountered unexpected pitfalls that ultimately brought us back to where we began.\r\n\r\nCurious? Join me as we shed light on the current state of table formats.", "full_description": "# Description\r\n\r\nOpen Table Formats (OTF) such as Hudi, Iceberg and Delta Lake have disruptively changed the data engineering landscape in recent years. While the Parquet file format has evolved as the de-facto standard for open, interoperable columnar storage for analyical workloads, it lacked first class support for critical features such as ACID compliance, incremental processing, flexible schema & partioning evolution and scalable meta data management. This led to increased development and maintenance efforts while building idempotent and failure tolerant data pipelines that often resulted in custom frameworks. OTFs solve all of these issues via providing a sophisticated meta data layer and improved maintenance capabilities on top of Parquet.\r\n\r\nDriven by the promises of OTFs, we intended to replace our own bronze-read-only Parquet-based storage layer with Delta Lake. In theory, this should have improved performance, reduced maintenanced and provided more flexibility. However, we've stumbled upon several issues:\r\n\r\n1. drastic performance issues with Liquid Clustering during incremental processing\r\n2. inmature interoperability in the python and cloud-based ecosystem (DuckDB, Pandas, Polars, Athena, Snowflake)\r\n3. maintaining logical session-boundaries during incremental processing\r\n\r\nWhile the first two issues are solvable in foreseeable future, the last one is specific to our requirements and does not overlap with design decisions made for incremental processing in Delta Lake. Taken together, these points ultimately led us to go back to relying on Parquet again.\r\n\r\n## Targeted Audience\r\n\r\nThis talk is mainly intended for an intermediate data engineering audience but is well suited for interested beginners, too. The content of this talk is relevant for all architects and data engineers being responsible for storing and managing data for analytical workloads.\r\n\r\n# Key takeaways\r\n\r\n- What problems do OTFs solve?\r\n- How do OTFs contribute to an open, composable data stack?\r\n- Is there a predominant Open Table Format?\r\n- How does Delta Lake conceptionally work?\r\n- What are concrete real-world advantages of Delta Lake in contrast to \"plain\" Parquet?\r\n- What is the \"small files\" problem and how does Liquid Clustering help?\r\n- How is the current state of interoperability with Delta Lake?\r\n\r\n# Talk Outline\r\n- Introduction (5 min)\r\n- OTFs in comparison (5 min)\r\n- Delta Lake Internals (10 min)\r\n- Use Case Requirements (5 min)\r\n- Benchmarks & Results (10 min)\r\n- Conclusion and Outlook (5 min)\r\n- Questions (5 min)", "code": "MRHNCV", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/MRHNCV.png", "speaker_names": "Franz W\u00f6llert", "speakers": "\n### Franz W\u00f6llert\n\nHi my name is Franz and I\u2019m an open source and python enthuisiast:\r\n\r\n- father of 3 girls\r\n- major in psychology\r\n- chess hobbiyst\r\n- competitive ultimate frisbee player\r\n- likes cooking and baking sourdough bread\n", "track": "Data Handling & Engineering"}, {"title": "Beyond FOMO \u2014 Keeping Up-to-Date in AI", "abstract": "The rapid evolution of AI technologies, particularly since the emergence of Large Language Models, has transformed the data science landscape from a field of steady progress to one of constant breakthroughs. This acceleration creates unique challenges for practitioners, from managing FOMO to battling imposter syndrome. Drawing from personal experience transitioning from mathematical modeling to modern AI development, this talk explores practical strategies for staying current while maintaining sanity. We'll discuss building effective learning structures, creating collaborative knowledge-sharing environments, and finding the right balance between innovation and implementation. Attendees will leave with actionable insights on navigating technological change while fostering sustainable growth in their teams and careers.", "full_description": "The landscape of data science and AI is evolving at an unprecedented rate. What started as a relatively stable field of mathematical modeling and time-series analysis has transformed into a whirlwind of weekly breakthroughs, especially since the emergence of Large Language Models. How do we stay current without succumbing to FOMO or imposter syndrome?\r\n\r\nIn this talk, I'll share my personal journey from traditional mathematical modeling to modern AI development, exploring how the field's pace has shifted dramatically. Drawing from real-world experiences as a consultant and team lead, I'll discuss practical strategies for maintaining technical excellence while managing the psychological challenges of rapid technological change. We'll examine how to build effective learning structures within teams, the importance of creating safe spaces for knowledge sharing, and why sometimes it's okay to not be at the cutting edge of every new development.\r\n\r\nThrough concrete examples and lessons learned, I'll offer insights on balancing client expectations, team growth, and personal development in an era where the technological landscape shifts weekly. Whether you're a seasoned data scientist or just entering the field, this talk will provide practical frameworks for navigating the exciting yet overwhelming world of modern AI development.", "code": "MSUCAS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/MSUCAS.png", "speaker_names": "Carsten Frommhold", "speakers": "\n### Carsten Frommhold\n\nHi! I am Carsten. I have been working in the data and analytics environment for seven years.  As a data scientist, I am excited by the challenge of translating the business into optimizable algorithms and creating real impact. As a self-taught programmer, I am just as absorbed in the technical challenges, preferably in the cloud.\n", "track": "Education, Career & Life"}, {"title": "supplyseer: Computational Supply Chain with Python", "abstract": "This talk introduces supplyseer, an open-source Python library that brings advanced analytics to Supply Chain and Logistics. By combining time series embedding techniques, stochastic process modeling, and geopolitical risk analysis, supplyseer helps organizations make data-driven decisions in an increasingly complex global supply chain landscape. The library implements novel approaches like Takens embedding for demand forecasting, Hawkes processes for modeling supply chain events, and Bayesian methods for inventory optimization. Through practical examples and real-world use cases, we'll explore how these mathematical concepts translate into actionable insights for supply chain practitioners.", "full_description": "Supplyseer bridges the gap between theoretical supply chain analytics and practical implementation by providing a pythonic interface to advanced mathematical concepts. This talk will walk through the library's core components and demonstrate how they solve real-world supply chain challenges.\r\n\r\nOutline:\r\n\r\n1. Introduction to Modern Supply Chain Analytics\r\n- The need for sophisticated analytics in today's complex supply chains\r\n- Why traditional methods fall short\r\n- The role of probabilistic modeling and topological analysis\r\n\r\n2. Core Mathematical Foundations\r\n- Time series embedding techniques using Takens' theorem\r\n- Stochastic process modeling for demand forecasting\r\n- Bayesian approaches to Economic Order Quantity (EOQ)\r\n- Point process modeling with Hawkes processes\r\n- Network analysis for supply chain risk assessment\r\n\r\n3. Library Architecture and Design Philosophy\r\n- Object-oriented design for supply chain analytics\r\n- Integration of multiple analytical approaches\r\n- Extensible architecture for custom analytics\r\n- Performance considerations and optimizations\r\n\r\n4. Key Features Deep Dive\r\na) Demand Forecasting Module\r\n   - Stochastic demand process simulation\r\n   - Time-delay embedding for pattern recognition\r\n   - Mixture density networks for uncertainty quantification\r\n\r\nb) Risk Analysis Tools\r\n   - Geopolitical risk assessment\r\n   - Supply chain network visualization\r\n   - Real-time monitoring and alerting\r\n   - Trade restriction impact analysis\r\n\r\nc) Inventory Optimization\r\n   - Bayesian EOQ implementation\r\n   - Multi-echelon inventory optimization\r\n   - Stockout probability calculation\r\n   - Vector field analysis for inventory dynamics\r\n\r\n5. Practical Applications\r\n- Route optimization with geopolitical risk consideration\r\n- You and your suppliers play cooperative games: game-theoretic Supply Chain\r\n- Supply Chain Digital Twins\r\n- Real-time risk monitoring and mitigation\r\n\r\n6. Integration with Data Science Ecosystem\r\n- Compatibility with pandas and polars\r\n- Integration with scikit-learn pipeline\r\n- Visualization with matplotlib and seaborn\r\n- Performance optimization with numpy\r\n\r\n7. Future Directions\r\n- Planned features and enhancements\r\n- Community contribution opportunities\r\n- Integration with other supply chain tools\r\n- Research directions in supply chain analytics\r\n\r\n8. Interactive Demonstrations\r\n- Live coding examples\r\n- Real-world data analysis\r\n- Visualization of supply chain dynamics\r\n- Risk assessment workflows\r\n\r\nThe talk will include code examples and practical demonstrations, showing how to:\r\n- Implement stochastic demand forecasting\r\n- Analyze supply chain risks using network analysis\r\n- Optimize inventory levels using Bayesian methods\r\n- Visualize supply chain dynamics using vector fields\r\n- Monitor and assess geopolitical risks\r\n\r\nTarget Audience:\r\nThis talk is aimed at data scientists, supply chain analysts, and Python developers interested in applying advanced analytics to supply chain problems. Attendees should have intermediate Python knowledge and basic familiarity with data science libraries like pandas and numpy.\r\n\r\nPrerequisites:\r\n- Python programming experience\r\n- Basic understanding of supply chain concepts\r\n- Familiarity with pandas and numpy\r\n- Basic knowledge of probability and statistics\r\n\r\nTakeaways:\r\nAttendees will learn:\r\n- How to implement advanced supply chain analytics in Python\r\n- Practical applications of mathematical concepts in supply chain\r\n- Best practices for supply chain data analysis\r\n- Techniques for visualizing and monitoring supply chain dynamics\r\n- Methods for quantifying and managing supply chain risks\r\n\r\nAll code examples and demonstrations will be available in a GitHub repository, allowing attendees to experiment with the concepts presented and apply them to their own supply chain challenges.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b", "code": "N9CAUM", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/N9CAUM.png", "speaker_names": "Rostami Jako", "speakers": "\n### Rostami Jako\n\nI am a Machine Learning Engineer at H&M Group, former Data Scientist at Lidl Sweden, as a professional I am designing Machine Learning services, extracting insights and arranging meaningful stories for my clients by conducting high-quality modeling, engineering, data mining and analytics. \r\n\r\nI have a Bachelor degree in Statistics and Probability theory from Uppsala University of Sweden. Because I am a Statistician at core I have good experience with Data Sciencr, Python, R, time series modeling, simulations, machine learning algorithms, SQL, Excel, Spark and database technologies, as well as good communication skills. \r\n\r\nYou\u2019ll find two comprehensive Python libraries I have open-sourced. One is based on an emerging modern statistical hypothesis testing framework using e-values and martingales based on game-theoretic statistics. The other is for computational Supply Chain and Logistics. The first one is called \u2019expectation\u2019 and the second one is called \u2019supplyseer\u2019 and you can find both on my GitHub.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Streamlining the Cosmos: Pythonic Workflow Management for Astronomical Analysis", "abstract": "Astronomical surveys are growing rapidly in complexity and scale, necessitating accurate, efficient, and reproducible reduction and analysis pipelines. In this talk we explore Pythonic workflow managers to streamline processing large datasets on distributed computing environments.\r\n\r\nModern astronomy generates vast datasets across the electromagnetic spectrum. NASA's flagship James Webb Space Telescope (JWST) provides unprecedented observations that enable deep studies of distant galaxies, cosmic structures, and other astrophysical phenomena. However, these datasets are complex and require intricate calibration and analysis pipelines to transform raw data into meaningful scientific insights.\r\n\r\nWe will discuss the development and deployment of Pythonic tools, including snakemake and pixi, to construct modular, parallelized workflows for data reduction and analysis. Attendees will learn how these tools automate complex processing steps, optimize performance in distributed computing environments, and ensure reproducibility. Using real-world examples, we will illustrate how these workflows simplify the journey from raw data to actionable scientific insights.", "full_description": "As astronomical surveys continue to grow in size and sophistication, researchers face mounting challenges in building efficient, scalable, and reproducible data processing pipelines. Modern observatories, like NASA's James Webb Space Telescope (JWST), are delivering unprecedented volumes of complex and specialized data, requiring innovative approaches to transform raw observations into meaningful, scientifically valid results. This talk focuses on leveraging Pythonic workflow management tools to address the unique challenges of processing large-scale astronomical datasets efficiently and reproducibly.\r\n\r\nI will provide a brief overview of JWST including its capabilities and the groundbreaking science it has enabled. In particular we will focus on the Pure Parallel mode which collect serendipitous observations from regions of the sky adjacent to primary science targets. These opportunistic datasets are a powerful resource for blind extragalactic surveys, offering unique opportunities to uncover faint galaxies, cosmic structures, and rare astrophysical phenomena. However, their \u201cunscheduled\u201d and heterogeneous nature presents significant challenges: the data arrive in raw, uncalibrated formats and require intricate, multi-step workflows\u2014such as artifact masking, background subtraction, and galaxy spectral analysis\u2014before becoming scientifically usable.\r\n\r\nIn this talk, I will demonstrate how tools like Snakemake and Pixi offer powerful, Pythonic solutions for these challenges. I\u2019ll show how these tools allow scientists to design modular, scalable, and highly parallel workflows that automate the reduction and analysis process while efficiently distributing computation across high-performance computing (HPC) clusters and cloud environments. By breaking workflows into smaller, reusable components, we can improve computational performance and maintain flexibility to adapt pipelines to new datasets, instruments, or evolving scientific goals.\r\n\r\nReproducibility remains a critical pillar of modern science, and I will highlight how combining workflow managers with environment management tools ensures version-controlled pipelines, transparent data lineage tracking, and reliable replication of results. This enables consistent analyses across diverse systems, fostering collaboration and long-term usability of scientific products.\r\n\r\nThis talk is designed for (data) scientists and researchers working with large-scale or complex datasets with multi-step reduction/analysis pipelines. This talk will provide a GitHub repository containing resources for building modular workflows, including examples of existing infrastructures, to provide actionable takeaways. Attendees will leave with a clear understanding of how to apply modern workflow management techniques to streamline their processing pipelines, improve reproducibility, and scale their analyses to meet the demands of their datasets.", "code": "NBFH7G", "state": "confirmed", "created": "2024-12-18", "social_card_image": "/static/media/social/talks/NBFH7G.png", "speaker_names": "Raphael Hviding", "speakers": "\n### Raphael Hviding\n\n\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Building an Open Source RAG System for the United Nations Negotiations on Global Plastic Pollution", "abstract": "Plastic pollution is a significant global challenge. Every year, millions of tons of plastic enter the oceans, impacting marine ecosystems and human health. To address this issue, the United Nations is negotiating a legally binding treaty with representatives from 180 countries, aiming to reduce plastic pollution and promote sustainable practices. \r\n\r\nWe have developed NegotiateAI, an open-source chat application that supports delegations during the UN negotiations on a legally binding agreement to combat plastic pollution. The tool demonstrates how generative AI and Retrieval Augmented Systems (RAG) can address complex global challenges. Built with Haystack 2.0, Qdrant, HuggingFace Spaces, and Streamlit, it showcases the potential of open-source technologies in tackling issues of global relevance. \r\n\r\nAs a beginner or advanced developer, this talk will give you valuable insights into developing impactful AI applications with open source tools in the public sector.", "full_description": "Plastic pollution is a global crisis that requires urgent action. An estimated 4.8 to 12.7 million tons of plastic end up in the oceans every year. Forecasts show that global plastic waste will triple by 2060. In response, the United Nations is currently negotiating a legally binding agreement to end plastic pollution, involving representatives from 180 countries in a multi-year process. Tools that can streamline these complex negotiations can help support the negotiations.\r\n\r\nThis talk introduces NegotiateAI, an open-source application developed to support delegations during the UN negotiations on a legally binding treaty to end plastic pollution. Developed with Haystack 2.0, Qdrant Vector Storage, HuggingFace Spaces, and Streamlit, NegotiateAI is a concrete example of how generative AI can be harnessed to address global challenges. While RAG is no longer a new concept, its variety continues to make it an essential approach for tackling real-world problems with LLMs, as demonstrated in this application.\r\n\r\nWe will take you on a journey through the development of NegotiateAI from choosing the right tools, to overcoming technical challenges, to using the app in live UN negotiations. Along the way, we will explore the development of a robust RAG system. We\u2019ll also discuss how we leveraged Streamlit to build a user-friendly interface, showcasing features such as multi-tab navigation and custom layouts that make the app intuitive and accessible to end users.\r\n\r\nDuring the session, we will highlight key challenges and present best practices for the coding structure. We will also show how we designed the app to be extensible and allow for the integration of additional data.\r\n\r\nBeginners will gain practical knowledge about building RAG systems and their real-world applications, while advanced developers will be inspired by the technical innovations, tool integration, and the potential of generative AI in the public sector. The talk will also provide insights into how organizations like the GIZ (German International Cooperation Society*)* are using AI to tackle pressing global issues and offer inspiration for anyone interested in the intersection of technology and sustainable development.", "code": "NF8UPF", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/NF8UPF.png", "speaker_names": "Rahkakavee Baskaran, Teresa Kroesen", "speakers": "\n### Rahkakavee Baskaran\n\nRahkakavee Baskaran studied Political Science and Social and Economic Data Science at the University of Konstanz. At &effect, she works as a Data Scientist, Machine Learning Engineer, and Backend Developer, with over four years of experience in Natural Language Processing (NLP).\r\nHer work focuses on leveraging data science and software development to create social impact, particularly in projects related to social sciences and the public sector.\n\n### Teresa Kroesen\n\n\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Mastering Demand Forecasting: Lessons from Europe's Largest Retailer", "abstract": "Ever craved your favorite dish, only to find its key ingredient missing from the store? You're not alone - stock outs can have significant consequences for businesses, resulting in frustrated customers and lost sales. On the other hand, overstocking can lead to wasted storage costs and potential write-offs. The replenishment system is responsible for striking the right balance between these opposing risks.\r\nThe key to successful replenishment is making accurate predictions about future demand. \r\n\r\nThis presentation takes a deep dive into the intricate world of demand forecasting, at Europe's largest retailer. We will demonstrate how enhancing simple machine learning methods with domain knowledge allows to generate hundreds of millions of high-quality forecasts every day.", "full_description": "This talk will provide an in-depth look at the forecasting engine, the heart of Lidl's replenishment system.\r\n\r\nEach day at Lidl, hundreds of millions of various products journey from suppliers to warehouses before reaching the shelves. Our so-called forecasting engine helps to automate the supply chain at every step along the way. \r\nEven with the vast amount of data at our disposal, the problem is still extraordinarily intricate. Each item, store or warehouse has unique demand patterns influenced heavily by a wide range of factors, such as holidays. While most of the effects are quantifiable, others remain unavailable and a certain degree of stochasticity is inherent to the process. The objective of our demand prediction may also vary based on their usages. Accuracy on the day level typically matters for short-term predictions, while it doesn't for long-term predictions. \r\n\r\nWe'll present our pragmatic modeling methodology on a simplified version of the problem at hand: The warehouse forecasting of single items. \r\n\r\nWe explain the rationale for training separate models for each item-warehouse combination and go into the reasons why we opted for using a LGBM model and why we believe it is best suited for our application. In addition to outlining our high-level modeling approach, we demonstrate how business and domain expertise are integrated into the modeling process through the use of sample and feature weighting and examine the impact of this integration on prediction quality. Following the base model, extensions are introduced that enable the incorporation of higher-level information at the finest level of granularity. This is achieved through decomposition and recomposition of the time-series at hand. In detail, we will present uplift decomposition for different use-cases, which include handling of promotions and holidays.\r\n\r\nTo conclude, we will give an overview of how all the presented methods synergize in delivering reliable forecasts for happy customers, so that you will hopefully never find yourself in front of an empty shelf!", "code": "NNGWGC", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/NNGWGC.png", "speaker_names": "Moreno Schlageter, Yovli Duvshani", "speakers": "\n### Moreno Schlageter\n\nMachine Learning Engineer at Schwarz IT, Germany, where I'm passionate about harnessing the power of AI to revolutionize the retail industry\n\n### Yovli Duvshani\n\nVersatile data scientist with 3+ years of experience building AI-products at the service of the industry. I believe that the key for success revolves around embracing shared best practices, upholding high quality standards for code development and having a team composed of complementary skill sets.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Driving Trust and Fairness: Addressing Ethical Challenges in Transportation through Explainable AI", "abstract": "Machine Learning can transform transportation\u2014improving safety, optimizing routes, and reducing delays\u2014yet it also presents ethical concerns. From potential algorithmic bias to opaque decision-making processes, trust and fairness are at stake. In this talk,I will show how Explainable AI (XAI) can offer practical solutions to these ethical dilemmas. Instead of focusing on the technical underpinnings, we will discuss how transparency, accountability, and fairness can be enhanced in AI-driven transportation systems. Using a real-world example, I will demonstrate how XAI provides the groundwork for building ethical, trustworthy, and socially responsible AI solutions in public transportation systems.", "full_description": "AI systems in transportation make decisions that directly impact people's lives, such as route optimization, safety measures, and resource allocation. These decisions often rely on complex algorithms, which can be opaque to stakeholders, including operators, regulators, and passengers. Key ethical concerns include:\r\n\r\n* *Algorithmic Bias*: Models trained on historical data may inadvertently reinforce inequities, such as prioritizing affluent or otherwise preffered neighborhoods over underserved areas.\r\n* *Transparency Deficit*: Operators often cannot understand or explain why an AI system makes a specific decision, leading to mistrust.\r\n* *Lack of Accountability*: In situations where decisions lead to negative outcomes, it is challenging to pinpoint responsibility when the AI acts as a \"black box.\"\r\nThese issues highlight the urgent need for mechanisms to build trust in AI systems, particularly in high-stakes environments like public transportation.\r\n\r\n**The Solution: Explainable AI (XAI)**\r\n\r\nExplainable AI (XAI) refers to methods and tools that make AI systems more transparent by providing interpretable insights into their decision-making processes. By integrating XAI, stakeholders can understand, validate, and trust the outputs of AI systems. Key features of XAI include:\r\n* *Transparency*: Clear explanations of how decisions are made.\r\n* *Accountability*: Mechanisms to trace decisions back to specific data inputs or model parameters.\r\n* *Fairness*: Tools to identify and mitigate bias in data and algorithms.\r\n\r\n\r\n**KARL: A Case Study in XAI for Public Transportation**\r\n\r\nThe *KARL* (KI in Arbeit und Lernen in der Region Karlsruhe) project is an exemplary initiative showcasing how XAI can address ethical challenges in AI-driven transportation. In this project, XAI is integrated into the tram system of Karlsruhe to:\r\n1. *Enhance Problem Handling*: When a tram is delayed, operators receive clear explanations of the underlying causes, such as weather conditions, mechanical failures, or traffic patterns. This enables quicker, more effective interventions.\r\n2. *Build  Trust*: Operators are informed about delays or reroutes in an understandable manner, fostering transparency and trust.\r\n3. *Ensure Fairness*: By analyzing historical data, the system identifies potential biases in route optimization and implements corrective measures to ensure equitable service.\r\n\r\n**Technical Implementation**\r\n\r\nWhile the presentation will not delve deeply into technical specifics, it will touch upon key elements such as:\r\n* The use of open-source libraries like *SHAP* (SHapley Additive exPlanations) to provide interpretability.\r\n* Integration of XAI tools into the operational dashboard used by tram operators.\r\n* Collaboration with domain experts to ensure the explanations are meaningful and actionable.\r\n\r\n**Takeaways for the Audience**\r\n\r\nAt the end of this talk, attendees will:\r\n1. Understand the ethical challenges posed by AI in transportation and how they can undermine trust.\r\n2. Learn how XAI tools can address these challenges by enhancing transparency, accountability, and fairness.\r\n3. Gain insights into the practical implementation of XAI in a real-world setting through the KARL project.\r\n4. Be inspired to incorporate XAI principles into their own AI projects to build ethical and socially responsible solutions.", "code": "NPMNCE", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/NPMNCE.png", "speaker_names": "Natalie Beyer", "speakers": "\n### Natalie Beyer\n\nNatalie co-founded  Lavrio.solutions, a company specializing in AI implementation. Since then, she has helped numerous organizations integrate AI into their processes and optimize their workflows. She has also conducted AI training sessions for businesses and professionals, bridging the gap between technical innovation and real-world usability.\n", "track": "Ethics & Privacy"}, {"title": "Is your LLM any good at writing? Benchmarking on creative writing and editing tasks", "abstract": "Many LLM benchmarks focus on reasoning and coding tasks. These are exciting tasks! But the majority of LLM usage is still in writing and editing related tasks, and there's a surprising lack of benchmarks on these. \r\n\r\nIn this talk you'll learn what it took to create a writing benchmark, and which model performs best!", "full_description": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating human-quality text, but how do we objectively measure their performance on complex writing and editing tasks? This talk explores the challenges of benchmarking LLMs for these tasks and presents a novel framework for evaluating their effectiveness. \r\n\r\nThe talk will provide practical guidance on how to evaluate and compare the performance of different LLMs. Basic familiarity with language models is required for this talk. \r\n\r\n**Outline:**\r\n\r\n1. Introduction\r\n\r\n- Briefly introduce LLMs and their growing role in writing and editing.\r\n- Highlight the need for standardized benchmarks to compare and improve LLM performance. Majority of LLM usage is still on writing tasks*! \r\n\r\n*Source: https://arxiv.org/pdf/2405.01470\r\n\r\n2. Challenges in Benchmarking LLMs for Writing and Editing:\r\n\r\n- Defining objective metrics for subjective tasks like writing quality and editing accuracy.\r\n- Addressing the issue of bias in training data and its impact on evaluation.\r\n- Accounting for the diverse range of writing and editing tasks.\r\n\r\n3. A Framework for Evaluating LLM Performance:\r\n\r\n- Proposing a set of key metrics that encompass fluency, coherence, accuracy, and style.\r\n- Introducing a methodology for constructing diverse and representative test datasets.\r\n\r\n4. Case Studies and Results:\r\n\r\n- Showcasing examples of how the proposed framework can be applied to evaluate different LLMs.\r\n- Presenting findings from recent benchmarking studies and discussing their implications.\r\n\r\n5. Future Directions:\r\n\r\n- Exploring the potential of LLMs to assist with increasingly complex writing and editing tasks.\r\n- Identifying areas for future research and development in LLM benchmarking.", "code": "P8GUWG", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/P8GUWG.png", "speaker_names": "Azamat Omuraliev", "speakers": "\n### Azamat Omuraliev\n\nAI engineer at Typetone, where I'm taming LLMs to automate end-to-end marketing. \r\n\r\nWe help unburden SMEs and solopreneurs from doing their content marketing, and this task is surprisingly hard for LLMs to solve yet!\r\n\r\nIn past lives personalized marketing at ING as a data scientist and ran a non-profit in Kyrgyzstan.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Quiet on Set: Building an On-Air Sign with Open Source Technologies", "abstract": "Learn how to build a custom On-Air sign using Apache Kafka\u00ae, Apache Flink\u00ae, and Apache Iceberg\u2122! See how to capture events like Zoom meetings and camera usage with Python, process data with FlinkSQL, analyze trends using Iceberg, and bring it all together with a practical IoT project that easily scales out.", "full_description": "While many of us have adapted to work from home life, one major problem remains: finding an easy way to keep folks in your home away from your workspace when you\u2019re on an important call. Dust off your Raspberry Pi\u2013\u2013let\u2019s build a custom on-air sign with Apache Kafka\u00ae, Apache Flink\u00ae, and Apache Iceberg\u2122!\r\n\r\nWe\u2019ll begin by writing Python scripts to capture key events\u2013\u2013such as when a Zoom meeting is running and when a camera is being used\u2013\u2013and produce it into Kafka. The live data are then consumed by a Raspberry Pi script to drive the operation of a custom designed on-air sign. From there, you\u2019ll be introduced to the ins and outs of FlinkSQL for stream processing as we wrangle the data into a better format for downstream use. And, finally, we\u2019ll see Iceberg in action and learn how to use query engines to analyze meeting and recording trends.\r\n\r\nBy the end of the session, you\u2019ll be well-acquainted with this powerful trio of open source technologies and know how you could use the same scaffolding and scale out a simple, at-home project to millions of users and simultaneous events.", "code": "P9GRZU", "state": "confirmed", "created": "2024-12-10", "social_card_image": "/static/media/social/talks/P9GRZU.png", "speaker_names": "Danica Fine", "speakers": "\n### Danica Fine\n\nDanica began her career as a software engineer in data visualization and warehousing with a business intelligence team where she served as a point-person for standards and best practices in data visualization across her company. In 2018, Danica moved to San Francisco and pivoted to backend engineering with a derivatives data team which was responsible for building and maintaining the infrastructure that processes millions of financial market data per second in near real-time. Her first project on this team involved Kafka Streams and Kafka Connect. From there, she immersed herself in the world of data streaming and found herself quite at home in the Apache Kafka and Apache Flink communities. She now leads the open source advocacy efforts at Snowflake, supporting Apache Iceberg and Apache Polaris (incubating). Outside of work, Danica is passionate about sustainability, increasing diversity in the technical community, and keeping her many houseplants alive. She can be found on X (Bluesky and Mastodon), talking about tech, plants, and baking @TheDanicaFine.\n", "track": "Infrastructure - Hardware & Cloud"}, {"title": "Extending Python with Rust, Mojo, Cuda and C and building packages", "abstract": "We all love Python - but we especially love it for its unique ability as a glue language.\r\n\r\nIn this talk we will show a number of ways of extending Python: using Rust, C and Cython, C++, CUDA and Mojo! We will use the pixi package manager and the open source conda-forge distribution to demonstrate how to easily build custom Python extensions with these languages.\r\n\r\nThe main challenge with custom extensions is about distributing them. The new pixi build feature makes it easy to build a Python extension into a conda package as well as wheel file for PyPI.\r\n\r\nPixi will manage not only Python, but also the compilers and other system-level dependencies.", "full_description": "Extending Python with native code is a common way of speeding up the execution. There are a number of traditional ways of writing Python extensions (Fortran, C, C++) but lately some modern languages have also entered the game (Rust, Mojo, and let\u2019s count CUDA as modern, too).\r\n\r\nAll of these have slightly different ways of writing Python extensions, and they require the installation of a compiler and compilation tool chain, as well as possibly other system dependencies. Installing, updating and managing the system dependencies is usually a bit of a hassle, and it is where pixi comes in. Pixi is a new package manager that builds on top of the Conda ecosystem. The community distribution \u201cconda-forge\u201d already has tools like C and Rust compilers, and thus it\u2019s easy to maintain the compiler + Python tool chain in a single project.\r\n\r\nThe new \u201cpixi build\u201d feature makes it even easier to build complex multi-language workspaces that combine different Python versions, compilers, and languages.\r\n\r\nIn the talk we will show lots of live demos going over simple numerical examples that highlight the different ways of extending Python (using pybind11, nanobind, PyO3, Mojo, \u2026) glued together in a single workspace with pixi build compiling the extensions from source. We will also demonstrate how pixi can help not only depending on other packages from source, but also by building the packages into Conda and Wheel (PyPI) packages that can be shared.\r\n\r\nAfter the talk, the listeners will have seen a number of ways how to extend Python code (easily!) with native languages and will have an understanding of benefits and drawbacks of the different approaches.", "code": "P9VKRV", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/P9VKRV.png", "speaker_names": "Ruben Arts, Wolf Vollprecht", "speakers": "\n### Ruben Arts\n\n\n\n### Wolf Vollprecht\n\nWolf Vollprecht has been active in the Python open source community for the past 5 years. He is a core member of conda-forge and the conda steering council, and the original author of the mamba package manager. He also has extensive experience in high-performance C++ and Rust. 2 years ago he started prefix.dev where the team is focusing all efforts on making cross-platform, language independent package management great (on top of the conda ecosystem).\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "pytest - simple, rapid and fun testing with Python", "abstract": "The pytest tool offers a rapid and simple way to write tests for your Python code. This training gives an introduction with exercises to some distinguishing features, such as its assertions, marks and fixtures.\r\n\r\nDespite its simplicity, pytest is incredibly flexible and configurable. We'll look at various configuration options as well as the plugin ecosystem around pytest.", "full_description": "- (20 minutes) **pytest feature walkthrough:**\r\n    * Automatic test discovery\r\n    * Assertions without boilerplate via the assert statement\r\n    * Configuration and commandline options\r\n    * Marking and skipping tests\r\n    * Data-driven tests via parametrization\r\n    * Exercises\r\n\r\n- (60 minutes) **pytest fixture mechanism:**\r\n    * Setup and teardown via dependency injection\r\n    * Declaring and using function/module/session scoped fixtures\r\n    * Using fixtures from fixture functions\r\n    * Parametrizing fixtures\r\n    * Looking at useful built-in fixtures (managing temporary files, patching, output capturing)\r\n    * Exercises\r\n\r\n- (10 minutes) **Where to go next:**\r\n    * Useful CLI arguments to deal with failing tests\r\n    * Overview of the plugin ecosystem around pytest", "code": "PDBAXQ", "state": "confirmed", "created": "2024-12-01", "social_card_image": "/static/media/social/talks/PDBAXQ.png", "speaker_names": "Florian Bruhin", "speakers": "\n### Florian Bruhin\n\n\n", "track": "Testing"}, {"title": "AI Agents of Change: Creating, Reflecting, and Monetizing", "abstract": "Create, reflect, and earn\u2014with purpose. In this workshop, you\u2019ll not only build your own AI agent but also confront the ethical questions it raises, from its impact on jobs to its potential for social good. Together, we\u2019ll explore how to harness AI for empowerment while uncovering pathways to turn your skills into meaningful value.\r\n\r\nThis workshop is designed to equip Python enthusiasts with the tools to create their own AI agent while fostering a deeper understanding of the societal implications of this technology. Through hands-on learning, collaborative discussions, and practical monetization strategies, you\u2019ll leave with more than just code\u2014you\u2019ll gain a vision of how AI can be wielded responsibly and profitably.", "full_description": "The session unfolds in three engaging parts:\r\n\t1.\tBuild Your AI Agent\r\nStart with the fundamentals of AI by designing and implementing a functional agent. Using Python, we\u2019ll demystify the process and equip you with practical skills for creating an AI that responds to user needs and scenarios.\r\n\t2.\tReflect on Ethics and the Future of Work\r\nOnce your agent comes to life, we\u2019ll pause to examine the bigger picture:\r\n\t\u2022\tHow does the AI agent you have created may reshape the job market?\r\n\t\u2022\tCan it democratize and decentralize opportunities, or does it risk amplifying inequalities?\r\n\t\u2022\tWhat collective vision do we want for the future of work?\r\nThis thought-provoking discussion will challenge you to think critically about the role of technology in fostering empowerment or exacerbating social challenges.\r\n\t3.\tEarn by Sharing Value\r\nFinally, we\u2019ll explore how your AI agent can create real-world value. You\u2019ll learn how to leverage marketplaces like OpenServ to turn your innovation into income. Whether you aim to solve practical problems, inspire creativity, or contribute to ethical AI development, this segment will connect your skills with opportunities for meaningful impact.\r\n\r\nBy the end of the workshop, you\u2019ll have built an AI agent, grappled with its ethical dimensions, and uncovered how to use your coding prowess to create and share value\u2014all while shaping a more inclusive, responsible AI ecosystem.", "code": "PKZD8L", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/PKZD8L.png", "speaker_names": "Paloma Oliveira", "speakers": "\n### Paloma Oliveira\n\nI\u2019m a wholehearted explorer and community-driven developer, advocating for FOSS while blending art, technology, and inclusion.\n", "track": "Generative AI"}, {"title": "Code & Community: The Synergy of Community Building and Task Automation", "abstract": "The Python community is built on a culture of support, inclusion, and collaboration. Sustaining this welcoming environment requires intentional community-building efforts, which often involve repetitive or time-consuming tasks. These tasks, however, can be automated without compromising their value\u2014freeing up time for meaningful human engagement.\r\n\r\nThis talk showcases my project aimed at supporting underrepresented groups in tech, specifically through building Python communities on Mastodon and Bluesky. A key part of this initiative is the \"Awesome PyLadies\" repository, a curated collection of PyLadies blogs and YouTube channels that celebrates their work. To enhance visibility, I created a PyLadies bot for social media. This bot automates regular posts and reposts tagged content, significantly extending their reach and fostering an engaged community.\r\n\r\nIn this session, I\u2019ll cover:\r\n- The role of automation in community building\r\n- The technical architecture behind the bot\r\n- A hands-on demo on integrating Google\u2019s Gemini into community tools\r\n- Upcoming features and opportunities for collaboration\r\n\r\nBy combining Python, automation, and modern AI capabilities, we can create thriving, inclusive communities that scale impact while staying true to the human-centered ethos of open source.", "full_description": "My planned outline for the talk is as follows:\r\n\r\n- **Introduction**: A brief overview of the project and its goals, focusing on community building and inclusivity within the Python ecosystem (3 minutes)\r\n- **The Importance of Visibility**: Explain the background of the project and why visibility is important (3 minutes)\r\n- **Bot Architecture and Setup**: A technical walkthrough of the bot, its architecture, and how it operates to extend the reach of community content on platforms like Mastodon or Bluesky (5 minutes)\r\n- **Hands-On Demo: Task Automation with Google\u2019s Gemini and GitHub Actions**: A step-by-step guide to integrating Google\u2019s Gemini and GitHub Actions for creating low-barrier, automated workflows tailored for community-building tasks (12 minutes)\r\n- **Looking Ahead**: Provide a forward-looking perspective (upcoming features of the project and future developments) (2 minutes)\r\n- **Q&A and Buffer** (5 minutes)\r\n\r\nI hope that the talk will inspire more Pythonistas to automate their tasks, and also more PyLadies to share material publicly and make the public perception of experts in the field more diverse.", "code": "PLMJZ8", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/PLMJZ8.png", "speaker_names": "Cosima Meyer", "speakers": "\n### Cosima Meyer\n\nCosima Meyer is a data scientist with a strong focus on making machine learning models explainable and accessible. Passionate about trustworthy AI, she is committed to building systems that are not only technically robust but also transparent and ethical. As a Google's Women Techmakers Ambassador and an active member of PyLadies, Cosima is dedicated to fostering inclusive and collaborative communities, working to bridge the two groups and create spaces for knowledge-sharing and growth.\r\n\r\nDuring her PhD studies at the University of Mannheim, Cosima discovered her enthusiasm for sharing knowledge through technical blog posts and developing open-source software. Her work reflects a blend of technical expertise and a passion for community building, inspiring others to explore, learn, and contribute to the fields of AI and data science.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Zero Code Change Acceleration: familiar interfaces and high performance", "abstract": "The PyData ecosystem is home to some of the best and most popular tools for doing data-science. Every data-scientist alive today has used pandas and scikit-learn and even Large Language Models know how to use them! For many years there have also been alternative implementations with similar interfaces and libraries with completely new approaches that focus on achieving the ultimate in performance and hardware acceleration. This talk will look at the recent efforts to give users the best of both worlds: a familiar and widely used interface as well as high performance.", "full_description": "The interfaces defined by libraries like Numpy, pandas or scikit-learn are the defacto standard APIs in each library's domain. Data scientists use these libraries directly as well as indirectly through libraries that depend on them.\r\n\r\nThis talk will look at the different approaches that recent efforts have taken to give users both a familiar interface and GPU acceleration. This means users do not have to rewrite their code, learn a new library and benefit from acceleration when using existing libraries.\r\n\r\nThe cudf team built a pandas accelerator by diving deep into the import system of Python. By hooking into the import system you can replace the result of `import pandas as pd` with a library that uses cudf where possible and falls back to pandas where necessary. This is great as it means the user\u2019s seaborn code is automatically accelerated, without the user or the seaborn authors having to do anything.\r\n\r\nThe polars team collaborated with the cudf team to build a GPU engine for polars. They chose a tightly coupled approach where the cuDF library hooks in after the Polars Intermediate Representation is created from the user input. This means using the GPU engine is seamless from the point of view of the user, there is transparent fallback to the CPU and no import statements need changing.\r\n\r\nThe scikit-learn team is adding experimental support to handle PyTorch and CuPy inputs by using the array API standard. Instead of using the Numpy API to perform array computations, scikit-learn is switching to using the array API. This is a subset of the Numpy API that is supported by several other array libraries. The API of Numpy and PyTorch is similar but not exactly the same, this makes writing code that works with both hard. The array API addresses this problem by providing a unified API. Users can accelerate their scikit-learn code by passing in a CuPy or PyTorch array instead of a Numpy array.", "code": "PNQB7C", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/PNQB7C.png", "speaker_names": "Tim Head", "speakers": "\n### Tim Head\n\nI am a scikit-learn core maintainer and work at NVIDIA.\r\n\r\nBefore working on scikit-learn I helped build mybinder.org and worked on JupyterHub.\r\n\r\nMany years ago I was a particle physicist at CERN in Geneva.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "PyData Stack: Building and deploying pure Python, open source data platforms", "abstract": "Modern open source Python data packages offer the opportunity to build and deploy pure Python, production-ready data platforms. Engineers can and do play a big role in helping companies become data-driven by centralising this data, cleaning and modelling it and presenting back to the business. Now more than ever it allows engineers and companies of any size the ability to build data products and insights for relatively low cost. In this talk we\u2019ll walk through the key components of this stack, tooling options available and demo a deployable containerised Python data stack.", "full_description": "Modern data platforms can be built and deployed using completely open source, Python packages. In this talk, I\u2019ll cover what constitutes a modern data stack and what open source Python packages can be used to build a stack suitable for the needs of most developers and companies. Rather than a one size fits-all approach, I\u2019ll initially demonstrate the rich ecosystem of technologies available and the pros and cons of the technology choices.\r\n\r\nTo be concrete, we will demo an instance of this type of self-contained, deployable platform that is composed of specific technology choices for the key components: data pipelines, transformation engine, data warehouse, presentation layer and orchestration. This implementation will use Docker, Python and yes, even some SQL. \r\n\r\nStructure\r\n1. What is a data platform? [5 mins]\r\n    1. Why are they useful\r\n    2. When do companies need them? \r\n2. What are the key components [5 mins]\r\n    1. Data pipelines\r\n    2. Transformation engine\r\n    3. Data store\r\n    4. Presentation layer\r\n    5. Orchestration\r\n3. Python packages available for this? [10 mins]\r\n    1. Data pipelines: dlt/PyAirbyte\r\n    2. Transformation engine: dbt or sqlmesh\r\n    3. Data store: s3/blob store/data warehouse/duckdb\r\n    4. Presentation layer: Streamlit/Superset\r\n    5. Orchestration: Prefect/Dagster\r\n4. The platform [10 mins]\r\n    1. Design\r\n    2. Technology choices\r\n    3. Joining it all together\r\n5. Deployment [10 mins]\r\n    1. IAC via Pulumi-Python\r\n\r\nOutcomes\r\n\r\nThe aim of this talk  is to equip attendees with an understanding of the availalbe technology choices and  the knowledge to build their own data platforms. This would specifically be useful for attendees who may be software or backend engineers who may also be called upon to own the data stack to support business and analyst use cases. It may also help engineers who may be looking to re-platform legacy, expensive data platforms to a more modern data stack. \r\nFor research and personal projects, spinning up a modern platform could be useful for compute heavy analytics that have outgrown local development.", "code": "PRRPQ3", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/PRRPQ3.png", "speaker_names": "Eric Thanenthiran", "speakers": "\n### Eric Thanenthiran\n\nI lead the Engineering function at Tasman Analytics, a boutique data consultancy. We act as an interim/fractional data team and have built many, many data stacks for our clients. We are passionate about helping clients leverage the power of their data. \r\n\r\nPersonally, I have a background of mechanical engineering and have worked across a range of sectors including sustainability, energy, property, construction and architecture. I am an engineer at heart and perennially look to hone the craft of engineering. \r\n\r\nWriting Python makes me incredibly happy.\n", "track": "Data Handling & Engineering"}, {"title": "Security for Devs", "abstract": "Two truths and a lie:\r\n\r\nYou have been hacked and you don\u2019t know yet\r\nYou haven\u2019t been hacked because it\u2019s not yet convenient (for the attackers)\r\nYou think your application is secure\r\n\r\nSecurity is hard. Hard to measure and is always a catch-up game.\r\n\r\nJoin this talk if you are interested in understanding a bit more about modern security and how it goes way beyond the code of your app.", "full_description": "Usually devs don\u2019t care much about security, and that has probably different reasons:\r\n\r\nFocus on security itself doesn\u2019t pay the bills\r\nNot always clear what being secured means\r\nSecurity =! my code is properly tested\r\nOften comes as an annoying byproduct of ISO27001/SOC2\r\n\r\nBut attacks and data breaches are becoming way more common, and DORA, NIS2 and the law obligation to publicly share data breaches are trying to bring the companies\u2019 attention (and budget) back to this topic.\r\n\r\nLeaving the legalise aside, during this talk we will cover these topics\r\n\r\nSome real examples\r\nYour code\r\nCI/CD\r\nYour app\r\nYour production environment\r\nFramework to assess your security posture\r\nWhere to Start", "code": "QGLS8H", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/QGLS8H.png", "speaker_names": "Christian Barra", "speakers": "\n### Christian Barra\n\nChristian Barra is a Software Engineer, Tech Lead and international speaker living in Lisbon.\r\nHe\u2019s the co-founder of ZeroBang, a cloud consulting company.\r\nHe is an active member of the tech community in Berlin, conference organiser and a Python Software Foundation Fellow.\r\nYou can follow him on X @christianbarra\n", "track": "Security"}, {"title": "From stockouts to happy customers: Proven solutions for time series forecasting in retail", "abstract": "Time series forecasting in the retail industry is uniquely challenging: Datasets often include stockouts that censor actual demand, promotional events cause irregular demand spikes, new product launches face cold-start issues, and diverse demand patterns within an imbalanced product portfolio create modeling challenges.\r\nIn this talk, we\u2019ll explore proven, real-world strategies and examples to address these problems. Learn how to successfully handle censored demand caused by stockouts, effectively incorporate promotional effects, and tackle the variability of diverse products using clustering and ensembling strategies. Whether you\u2019re a seasoned data scientist or a Python developer exploring forecasting, the goal of this session is to introduce you to the key challenges in retail forecasting and equip you with actionable insights to successfully overcome them in real-life scenarios.", "full_description": "Retail time series forecasting is uniquely challenging: stockouts censor true demand, promotions cause irregular demand spikes, cold-start products lack historical data, and diverse product portfolios introduce modeling complexities. These challenges can lead to inefficiencies such as over- or understocking in the warehouses and therefore also to dissatisfied customers. This talk explores proven strategies to tackle these issues and deliver actionable insights.\r\n\r\nLearn how to handle constrained demand caused by stockouts both with adequate imputation as well as machine learning strategies, incorporate promotional effects with suitable feature engineering techniques that also help in cases of incomplete promotional data, predict demand for new products using transfer learning and also discover how ensembling strategies and clustering can simplify forecasting for diverse, imbalanced datasets.\r\n\r\nWe\u2019ll also highlight tools like statsforecast, neuralforecast, scikit-learn and our AutoML framework with a strong stacking ensembling mechanism in it's core. Whether you\u2019re a seasoned data scientist or a Python developer exploring forecasting, the goal of this session is to introduce you to the key challenges in retail forecasting and equip you with actionable insights to successfully overcome them in real-life scenarios.", "code": "QN3BTA", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/QN3BTA.png", "speaker_names": "Robert Haase", "speakers": "\n### Robert Haase\n\nI earned both my Bachelor's and Master's degrees in Physics from the University of Heidelberg, specializing in Condensed Matter Physics and Computational Physics. During my Master's thesis in 2020, I advanced existing NLP Transformer architectures for timeseries applications where I worked extensively with uncertainty quantifications and normalizing flows. Since the beginning of 2021, I have been employed at Paretos, where the primary focus of my work lies in Timeseries Forecasting, specifically demand forecasting. Since 2023, Im leading the AI team at paretos which is giving me a good opportunity to combine my leadership skills with our super interesting research in scalable time series forecasting & optimization applications.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Rustifying Python: A Practical Guide to Achieving High Performance While Maintaining Observability", "abstract": "In this session, I\u2019ll share our journey of migrating key parts of a Python application to Rust, resulting in over 200% performance improvement.\r\nRather than focusing on quick Rust-to-Python integration with PyO3, this talk dives into the complexities of implementing such a migration in an enterprise environment, where reliability, scalability, and observability are crucial.\r\nYou\u2019ll learn from our mistakes, how we identified suitable areas for Rust integration, and how we extended our observability tools to cover Rust components.\r\nThis session offers practical insights for improving performance and reliability in Python applications using Rust.", "full_description": "For performance-critical sections of code, especially those that are I/O-bound or CPU-heavy, Python\u2019s Global Interpreter Lock (GIL) can create significant bottlenecks.\r\nTo improve performance, our team explored integrating Rust, taking advantage of its speed and concurrency features while maintaining Python\u2019s ease of use and flexibility.\r\n\r\nThis session will focus on overcoming common hurdles when migrating to Rust and optimizing performance in a real-world, production environment which orchestrates workload across 2000 compute nodes in various data centers and cloud provider regions.\r\nThis talk covers practical aspects such as observability, scalability, and deployment in a production setting.\r\n\r\nWe\u2019ll begin by discussing how to identify the parts of your Python code that would benefit most from a Rust migration, particularly those where the GIL is a limiting factor.\r\nWe\u2019ll also share insights into our migration process, including the challenges we faced and how we overcame them.\r\nYou\u2019ll learn how we refactored Python code and used PyO3 to integrate Rust, achieving over 200% performance improvements.\r\n\r\nA key challenge when adding Rust to a Python codebase is maintaining robust observability.\r\nWe\u2019ll explain how we extended our OpenTelemetry and Sentry observability stack to include Rust components, ensuring seamless monitoring, tracing, and debugging across the entire stack.\r\n\r\nThroughout the session, we\u2019ll illustrate the process with a practical example: a simplified version of our own application, which includes both I/O-heavy and compute-heavy tasks.\r\nYou\u2019ll see how to break down business logic and decide which parts to migrate to Rust for maximum performance benefit.\r\n\r\nBy the end of this session, you will be equipped with the knowledge to assess where Rust can improve your Python application\u2019s performance, and how to integrate it in a reliable and observable way.\r\nThis session is ideal for anyone looking to optimize Python performance with Rust, while keeping applications running.", "code": "QXSQKL", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/QXSQKL.png", "speaker_names": "Max H\u00f6hl", "speakers": "\n### Max H\u00f6hl\n\nI\u2019m a Senior Software Developer in the team behind SAP\u2019s huge CI/CD infrastructure for SAP HANA.\r\nWe design, implement, operate and maintain it's cloud native graph-based task execution framework leveraging 2000 compute nodes in multiple data centers and cloud provider regions.\r\nIn my spare time, I like to play Dungeons & Dragons.\n", "track": "Programming & Software Engineering"}, {"title": "Forecast of Hourly Train Counts on Rail Routes Affected by Construction Work", "abstract": "Construction work in national railroad networks often disrupts train traffic, making it vital to estimate hourly train numbers for effective re-routing. Traditionally managed by humans, this process has been automated due to staff shortages and demographic changes. DB Systel GmbH, Deutsche Bahn's IT provider, leveraged machine learning and artificial intelligence to estimate train traffic during construction. Using Python and frameworks like Pandas, scikit-learn, NumPy, PyTorch and Polars, their solution demonstrated significant benefits in performance and efficiency.", "full_description": "Within a national railroad network, construction work for maintenance and modernization is unavoidable - as is train traffic on the affected sections under certain circumstances. Although there are fixed timetables for passenger rail transport that are planned well in advance and are set very early, there are still many freight transports and special trains that are registered at short notice and cause a dynamic traffic situation on the rail network. Therefore, the capacity utilisation of the rail routes is unknown until shortly before the journey takes place. It is therefore important to estimate the number of trains that will run over the affected tracks in order to establish a sensible re-routing strategy. Until now, this process has been in the hands of human decision-makers for decades or even more than a century.\r\n\r\nDemographic change and staff shortages are increasingly forcing companies to automate activities intelligently. This is where machine learning and artificial intelligence come into play.\r\n\r\nAs Deutsche Bahn's IT service provider, DB Systel GmbH was able to successfully implement an example of intelligent automation of this process and estimate train numbers on sections of tracks affected by construction using modern ML and AI methods. Python as well as various established frameworks (Pandas, scikit-learn, NumPy, PyTorch) and new frameworks (Polars, Ruff) were used in this project. A success and performance measurement clearly demonstrated the benefits of ML automation.", "code": "RAHBEP", "state": "confirmed", "created": "2024-12-04", "social_card_image": "/static/media/social/talks/RAHBEP.png", "speaker_names": "Sebastian Folz, Dr Maren Westermann", "speakers": "\n### Sebastian Folz\n\n\n\n### Dr Maren Westermann\n\nDr Maren Westermann works as a machine learning engineer at DB Systel GmbH and holds a PhD in environmental science. She is a self taught Pythonista, a member of the documentation and contributor experience team, respectively at the open source machine learning library scikit-learn, and a team member of the open source library Narwhals. She is also a co-organiser of PyLadies Berlin where she mainly hosts open source hack nights.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "How to use Data Science Superpowers in real life, a Bayesian perspective", "abstract": "In the data science field, we use all these powerful methods to solve important problems. Most of the time, we do this very well because our data science and machine-learning toolbox fits the problems we tackle quite precisely. Yet, what about our everyday choices or even our most important life decisions? Can we use for our private lives what we advocate for in our jobs or are these choices inherently different?\r\nMany of this real life decisions are a little different than textbook machine-learning problems. There is often less or hard-to-come-by data and the decisions are infrequent, but sometimes very consequential. This talk will dive into what makes everyday decisions difficult to handle with our data science toolbox. It will show how Bayesian thinking can help to reason in such cases, especially when there is not a lot of data to rely on.", "full_description": "In this talk, I want to have a look on decision making from a slightly different angle. In a world that produces an ever growing amount of data in every domain, data scientists can shine with their tools to make data-driven decisions. Often there is even too much data and the most tedious part of the work is to remove the noise from the signal with clever feature engineering. Though the world gets covered more and more by big data, this development is not distributed evenly.\r\n\r\nLots of decisions we need to make in real life do not follow this pattern. In fact, there are often surprisingly few data points that help us here. Yet, are there fundamental differences between everyday decisions and the type of decisions we automate so well with machine learning in our jobs? In this part of the talk, I will attempt a characterisation of both types of decisions. We will have a closer look at what implicit assumptions we make to use our machine learning toolbox. After this we might get a first explanation why these tools might be unsuited to answer questions like \u2019how longe should I study for an exam\u2019 or \u201a\u2019should I  accept this new job or not\u2019.\r\n\r\nEnter Bayesian statistics: This part of the talk will introduce Bayesian statistics for beginners using simple examples and images. It will highlight the benefits of the method when we are short of data but have some additional experience not encoded in the data. I will show how in these circumstances prior distributions come in really handy.\r\n\r\nAfter laying the groundwork on Bayesian methods we will circle back to the everyday decisions and see how well both things fit together. On a higher level, this will show what makes problems in decision making a great fit for Bayesian methods. I will introduce this using a practical example. The example will deal with the decision how long one should study for a test or exam. Taking a step-by-step approach, we explore how this decision can be informed with just a few data points. Set aside finding the key to successful exam preparation, the example is also helpful to see some of the basics for working with the pymc library.\r\n\r\nThe talk will end with some more general thoughts. This will answer where to go from here and for which decisions a thorough investigation like the presented one is worthwhile.,Yet, once one is familiar with the basics of Bayesian thinking, there might be shortcuts. I will show that we can use the principles as a great tool to improve discussions about important decisions on a broader scale.", "code": "RLTZTC", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/RLTZTC.png", "speaker_names": "Tim Lenzen", "speakers": "\n### Tim Lenzen\n\nI am currently working as a Senior Data Scientist at Ailio. My focus is on helping improve organizations by better utilizing their data. I contribute to these transformation projects by bringing in my broad expertise in data related topics ranging from data engineering and cloud-development (AWS, Azure) over data science and machine learning to communication and leadership skills.\r\n\r\nAfter completing my masters in chemistry, I really started my journey in the data science and machine learning field during my PhD studies in theoretical chemistry. The next step for me was a role as a data scientist  in a company developing software in the IT-Security field. For five years, I worked on a system to detect suspicious e-mail traffic using machine learning. Set aside the technical aspect of the job, I also built a small team. From this experience I learnt a lot about leadership and developing software products on a larger scale.\r\n\r\nI strongly believe that using the right data to inform important decisions helps organizations of all kinds improve. However, often this is easier said than done. I am always curios to discover and tackle these interesting challenges. Also, I am more than happy to sharing my knowledge and learnings.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Intuitive A/B Test Evaluations for Coders", "abstract": "A/B testing is a critical tool for making data-driven decisions, yet its statistical underpinnings\u2014p-values, confidence intervals, and hypothesis testing\u2014are often challenging for those without a background in statistics. Coders frequently encounter these concepts but lack a straightforward way to compute and interpret them using their existing skill set.\r\nThis talk presents a practical approach to A/B test evaluations tailored for coders. By utilizing Python\u2019s random number generator and basic loops, it introduces bootstrapping as an accessible method for calculating p-values and confidence intervals directly from data. The goal is to simplify statistical concepts and provide coders with an intuitive understanding of how to evaluate test results without relying on complex formulas or statistical jargon.", "full_description": "Making A/B Test Evaluations Intuitive for Coders: A Python-Based Approach\r\n\r\nA/B testing is an essential method for data-driven decision-making, but interpreting the results can be daunting. Complex jargon around p-values and confidence intervals often creates barriers to understanding. This talk simplifies A/B testing by introducing a practical, Python-powered approach using bootstrapping\u2014a flexible and accessible method that aligns with how software engineers think and works without requiring statistical knowledge.\r\n\r\nSession Highlights:\r\n\r\n1. Statistical Significance and Hypothesis Testing:\r\n    * Why is statistical testing crucial for A/B tests? Simple comparisons overlook randomness.\r\n    * Using Python, we\u2019ll demonstrate how to simulate \"what-if\" scenarios by shuffling and resampling data, allowing participants to compute p-values and understand the likelihood of observed differences occurring by chance.\r\n2. Confidence Intervals with Bootstrapping:\r\n    * Confidence intervals clarify the range of plausible outcomes.\r\n    * We\u2019ll explore how to resample experiment data repeatedly to estimate variability and construct intuitive confidence intervals\u2014all using basic tools like random number generators and loops, without requiring advanced math.\r\n    * \r\nKey Takeaways:\r\n* Hands-on skills to compute p-values and confidence intervals using basic programming concepts.\r\n* Clear, step-by-step demonstrations of shuffling, resampling, and generating statistical insights.\r\n* Practical knowledge to move beyond black-box libraries and understand the \"why\" and \"how\" behind A/B test evaluations.\r\n\r\nBy the end of the session, attendees will be equipped to demystify A/B testing with a coder-friendly workflow, empowering them to make confident, data-driven decisions in their projects.\r\n\r\nTalk Outline:\r\n\r\n1. Setting the Stage (5 minutes)\r\n    * What is A/B testing?\r\n    * Why isn't it enough to just compare numbers? Why do we need statistics to interpret results?\r\n2. Statistical Significance and P-Values (5 minutes)\r\n    * Statistical tests (t-test, z-test, binomial test) are frequently used, but what is the intuition behind them?\r\n    * Introducing the basic idea of bootstrapping.\r\n3. Bootstrapping Explained (8 minutes)\r\n    * Step-by-step illustration of the bootstrapping approach.\r\n    * What is a p-value? An intuitive description using resampling.\r\n4. Confidence Intervals Explained (7 minutes)\r\n    * Importance of confidence intervals and how they help interpret results.\r\n    * Intuitive computation of confidence intervals using bootstrapping.\r\n    * Impact of sample size on confidence intervals and certainty.\r\n5. Why These Statistics Matter (5 minutes)\r\n    * Discussion on the practical necessity of statistical techniques.\r\n    * How these methods ensure data-driven decision-making in A/B testing.", "code": "RQ8JBM", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/RQ8JBM.png", "speaker_names": "Thomas Mayer", "speakers": "\n### Thomas Mayer\n\n**Thomas Mayer** holds a PhD in Quantitative Language Comparison and brings a profound background in Machine Learning and Natural Language Processing (NLP) to his work. As Team Lead in the Data Intelligence team at HolidayCheck, Thomas combines his passion for data-driven insights with his expertise in linguistics and AI to drive innovation in the travel industry. With a deep understanding of both technical and business challenges, he plays a pivotal role in leveraging data to enhance customer experiences and inform strategic decisions.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond", "abstract": "\"Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond\" explores a common programming tool with a fresh perspective. While exceptions are a key feature in Python and other languages, they share surprising similarities with the notorious goto statement. This talk examines those parallels, the problems exceptions can create, and practical alternatives for better code. Attendees will gain a clear understanding of modern programming concepts and the evolution of programming.", "full_description": "Exceptions have long been seen as an improvement over error-handling approaches like goto. However, they can introduce complexity and obscure control flow when used without care. This talk will critically examine exceptions, outline the similarities to goto, and explore better ways to handle errors in programming.\r\n\r\n### Outline:\r\n\r\n1. Introduction (5 minutes)\r\n    - The historical role of goto in programming.\r\n    - Spaghetti code and the rise of structured programming.\r\n    - How exceptions emerged as an alternative.\r\n2. Why and What Are Exceptions (10 minutes)\r\n    - Why exceptions were introduced.\r\n    - How they became mainstream in languages like Java and C++.\r\n    - Common problems caused by exceptions: hidden control flow, debugging challenges, and performance impacts.\r\n3. The Evolution Toward Result Types (10 minutes)\r\n    - How result types address the shortcomings of exceptions.\r\n    - Implementations in Haskell, Rust, and Golang.\r\n    - Real-world benefits of using result types.\r\n4. Using Result Types in Python (10 minutes)\r\n    - Introducing the returns package.\r\n    - Practical examples of result types in Python.\r\n    - How this approach improves code clarity and reliability.\r\n5. Conclusion (5 minutes)\r\n    - Recap of the journey from goto to exceptions to result types.\r\n    - Key takeaways: thoughtful error handling and modern best practices.\r\n    - Encouragement to explore and adopt better patterns in Python.\r\n\r\nThis session is ideal for intermediate and advanced Python developers seeking actionable techniques to improve error handling and write cleaner, more predictable code.", "code": "S8MUBF", "state": "confirmed", "created": "2024-12-12", "social_card_image": "/static/media/social/talks/S8MUBF.png", "speaker_names": "Florian Wilhelm", "speakers": "\n### Florian Wilhelm\n\nFlorian is Head of Data Science & Mathematical Modeling at inovex GmbH, an IT project center driven by innovation and quality, focusing its services on \u2018Digital Transformation\u2019. He holds a PhD in mathematics, has more than 10 years of experience in predictive & prescriptive analytics use-cases and likes everything math \ud83e\udd2f\n", "track": "Programming & Software Engineering"}, {"title": "Vector Streaming: The Memory Efficient Indexing for Vector Databases", "abstract": "Vector databases are everywhere, powering LLMs. But indexing embeddings, especially multivector embeddings like ColPali and Colbert, at a bulk is memory intensive. Vector streaming solves this problem by parallelizing the tasks of parsing, chunking, and embedding generation and indexing it continuously chunk by chunk instead of bulk. This not only increase the speed but also makes the whole task more optimized and memory efficient.\r\n\r\nThe library gives many vector database supports, like Pinecone, Weavaite, and Elastic.", "full_description": "Embedding creation is mostly done synchronously; a lot of time is wasted while the chunks are being created, as chunking is not a compute-heavy operation. As the chunks are being made, passing them to the embedding model would be efficient. This problem further intensifies with late interaction embeddings like CoLBert or ColPali.\r\n\r\nThe solution is to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures no time is wasted on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database.\r\n\r\nAll this is then bound into Python using pyo3 and maturin, so it's easily accessible from Python, but the core is still asynchronous with rust.", "code": "SFDRTR", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/SFDRTR.png", "speaker_names": "Sonam Pankaj, Akshay Ballal", "speakers": "\n### Sonam Pankaj\n\nSonam is the creator of the open-source library called Embed-Anything, which helps to create local and multimodal embeddings and index them efficiently to vector databases, it\u2019s built in rust and thus it\u2019s more greener and efficient. She works as the GenerativeAI Evangelist at Articul8, spun-off of Interl, Articul8 is the go-to generativeAI platform for enterprise.\n\n### Akshay Ballal\n\n\n", "track": "Rust"}, {"title": "Agentic AI: Build a Multi-Agent Application with CrewAI", "abstract": "This hands-on tutorial will dive into the fundamentals of building multi-agent systems using the CrewAI Python library. Starting from the basics, we\u2019ll cover key concepts, explore advanced features, and guide you step-by-step through building a complete application from scratch. We\u2019ll discuss implementing guardrails, securing interactions, and preventing query injection vulnerabilities along the way.", "full_description": "### **Short Abstract**  \r\n**Agentic AI: Build a Multi-Agent Application with CrewAI**  \r\nIn this hands-on tutorial, we\u2019ll dive into the fundamentals of building multi-agent systems using the CrewAI Python library. Starting from the basics, we\u2019ll cover key concepts, explore advanced features, and guide you step-by-step through building a complete application from scratch. Along the way, we\u2019ll discuss implementing guardrails, securing interactions, and preventing query injection vulnerabilities.\r\n\r\n---\r\n\r\n### **Detailed Description**  \r\nThis tutorial introduces **Agentic AI**\u2014a design approach where multiple agents collaborate to solve complex tasks efficiently. Using the **CrewAI Python library**, we\u2019ll start with the fundamentals and progressively move towards advanced concepts, focusing on practical implementation.\r\n\r\n#### **What We\u2019ll Cover:**  \r\n1. **Understanding Agentic AI:** Core principles and why multi-agent systems are valuable.  \r\n2. **Getting Started with CrewAI:** Setting up the library and creating simple agents.  \r\n3. **Advanced Agent Interactions:** Defining workflows, collaboration patterns, and communication protocols.  \r\n4. **Building from Scratch:** Step-by-step guide to developing a complete multi-agent application.  \r\n5. **Implementing Guardrails:** Techniques to ensure agents operate within defined constraints.  \r\n6. **Preventing Query Injection:** Strategies for securing agent queries against malicious inputs.  \r\n\r\n#### **Why Attend?**  \r\nBy the end of this session, you\u2019ll have hands-on experience building an agent-based application, understand how to implement security measures, and be equipped with best practices for maintaining control over agent behavior. Whether you're new to agentic systems or looking to refine your skills, this tutorial will provide both the theory and the practical insights needed to start building with CrewAI.  \r\n\r\n**Prerequisites:** Familiarity with Python and basic AI concepts will help you get the most out of this session.", "code": "SVLRGG", "state": "confirmed", "created": "2024-12-23", "social_card_image": "/static/media/social/talks/SVLRGG.png", "speaker_names": "Alessandro Romano", "speakers": "\n### Alessandro Romano\n\nAlessandro is a highly experienced data scientist with a Bachelor\u2019s degree in computer science and a Master\u2019s in data science. He has collaborated with various companies and organizations and currently holds the role of senior data scientist at logistics giant Kuehne+Nagel. Alessandro is particularly passionate about statistics and digital experimentation and has a strong track record of applying these skills to solve complex problems. He shares his knowledge regularly, speaking at events like the Data Innovation Summit and ODSC.\n", "track": "Generative AI"}, {"title": "Data as (Python) Code", "abstract": "In contemporary data-driven environments, the seamless integration of data into automated workflows is paramount. The reliability of automation, however, is constantly threatened by breaking changes in the source data. The Data-as-Code (DaC) paradigm address this challenge by treating data as a first-class citizen within the software development lifecycle.", "full_description": "Data-as-Code (DaC) is a paradigm that streamlines data distribution by encapsulating dataset retrieval within Python packages, along with a data contract. This approach makes it easy to enforce data quality, effortlessly leverage on semantic versioning to prevent errors in the data pipeline, and abstracts away from the Data Scientist all the boilerplate code to load the data needed by the ML models, improving efficiency and consistency. This presentation will delve into the implementation of DaC, demonstrate its practical applications, and discuss the benefits it offers in modern data workflows.\r\n\r\nThis session will cover:\r\n1. Introduction to Data-as-Code (DaC):\r\n   - What problems do we want to solve with DaC\r\n   - What it is out of scope\r\n2. Implementing DaC:\r\n   - Packaging data as Python packages\r\n   - Defining data contracts\r\n3. Advantages of DaC:\r\n   - Application of semantic versioning to manage data changes effectively\r\n   - Breaking changes in data are automatically detected as part of the data distribution\r\n   - Abstraction of data loading mechanisms, allowing seamless transitions between data sources\r\n   - Elimination of hard-coded data field names, enhancing code maintainability\r\n   - Facilitation of unit testing through schema examples\r\n   - Inclusion of comprehensive data descriptions and metadata\r\n   - Centralized data distribution via the Python Package Index (PyPI)\r\n4. DaC in the real world:\r\n   - Step-by-step walkthrough of creating and distributing a DaC package\r\n   - Guidelines for data engineers on preparing data for DaC\r\n   - Instructions for data scientists on consuming DaC packages in their workflows\r\n   - Discussion on the scalability and adaptability of DaC\r\n6. Q&A Session:\r\n   - Addressing audience questions and remarks", "code": "SXRVNU", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/SXRVNU.png", "speaker_names": "Francesco Calcavecchia", "speakers": "\n### Francesco Calcavecchia\n\nPhysicist, ML Engineer, Agile adept. I\u2019d rather have a taste of everything than specialize. Eager to learn, unlearn, try out, share, help.\n", "track": "MLOps & DevOps"}, {"title": "Unforgettable, that's what you are: Evaluating Machine Unlearning and Forgetting", "abstract": "Can deep learning/AI models forget? In this talk, you'll explore the realm of machine unlearning, where researchers and practitioners aim to remove memorized examples from machine learning models. This is relevant for training increasingly overparameterized models and growing GDPR/Privacy concerns with large scale model development and use.", "full_description": "Deep learning memorization is a known phenomena, where deep learning / AI models memorize parts of their training dataset. This happens often for repeated examples, novel examples and occurs more often in overparameterized models.\r\n\r\nThis presents problems for guiding machine learning behavior, requiring much effort in guardrails and output monitoring, as well as questioning whether the models can be GDPR-compliant (i.e. the right to be forgotten).\r\n\r\nA growing area of research on machine unlearning or machine forgetting has emerged to investigate ways a model might unlearn or forget particular memorized examples. In this talk, you'll learn about the field of machine unlearning and related topics like data anonymization to evaluate exactly what's truly unforgettable. Jokes aside: you'll have some practical take-aways to apply to your work in data and machine learning development.", "code": "SZFRRA", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/SZFRRA.png", "speaker_names": "Katharine Jarmul", "speakers": "\n### Katharine Jarmul\n\nKatharine Jarmul is a privacy activist and an internationally recognized data scientist and lecturer who focuses her work and research on privacy and security in data science and machine learning. You can follow her work via her newsletter, Probably Private (https://probablyprivate.com) or in her recently published book, Practical Data Privacy (O'Reilly 2023) now also available in German as Data Privacy in der Praxis.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "From Text to Multimodal: Building Self-Hosted RAG Systems with Open Source", "abstract": "While text-based RAG systems have been everywhere in the last year and a half, the real power comes from combining multiple modalities\r\n\r\nIn this session, we'll show how it is possible to create a Multimodal RAG system using Open-Source tools such as vLLM for deploying your LLM, Pixtral for multimodal understanding, and Milvus for storing your vectors.\r\n\r\nI'll walk you through the architecture and implementation details of setting up your own infrastructure, demonstrating how to effectively process and understand both images and text in a unified system.\r\nThe talk will include a live demo, showing the real-time implementation and performance of the system.\r\n\r\nWhether you're looking to reduce dependency on commercial APIs or need more control over your LLM infrastructure, this talk will provide you with practical insights and implementation strategies.", "full_description": "While text-based RAG systems have been everywhere in the last year and a half, the real power comes from combining multiple modalities\r\n\r\nIn this session, we'll show how it is possible to create a Multimodal RAG system using Open-Source tools such as vLLM for deploying your LLM, Pixtral for multimodal understanding, and Milvus for storing your vectors.\r\n\r\nI'll walk you through the architecture and implementation details of setting up your own infrastructure, demonstrating how to effectively process and understand both images and text in a unified system.\r\nThe talk will include a live demo, showing the real-time implementation and performance of the system.\r\n\r\nWhether you're looking to reduce dependency on commercial APIs or need more control over your LLM infrastructure, this talk will provide you with practical insights and implementation strategies.", "code": "TFLBTW", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/TFLBTW.png", "speaker_names": "Stephen Batifol", "speakers": "\n### Stephen Batifol\n\nStephen Batifol is a Developer Advocate at Zilliz. He previously worked as a Machine Learning Engineer at Wolt, where he created and worked on the ML Platform, and previously as a Data Scientist at Brevo. Stephen studied Computer Science and Artificial Intelligence. \r\nHe is a founding member of the MLOps.community Berlin group, where he organizes Meetups and hackathons. He enjoys boxing and surfing.\n", "track": "Generative AI"}, {"title": "Reinforcement Learning Without a PhD: A Python Developer\u2019s Journey", "abstract": "From watching AI conquer Super Mario to building production-ready Reinforcement Learning (RL) systems, this talk explores how Python developers can dive into RL without requiring advanced degrees or big tech resources. Drawing on our three-year journey of building a production RL system, I\u2019ll show how developers can leverage RL through practical strategies and accessible tools. Using pi_optimal, an open-source RL toolkit, we\u2019ll bridge the gap between cutting-edge RL research and real-world applications. Attendees will gain actionable insights, implementation techniques, and hands-on experience to confidently start their own RL projects.", "full_description": "Reinforcement Learning (RL) has achieved groundbreaking results, from dominating video games to optimizing business processes. Despite these successes, a considerable gap exists between RL research and practical, accessible implementation for everyday developers. This talk aims to bridge that gap with hands-on guidance and real-world insights.\r\n\r\nBased on our three-year journey of developing a production-grade RL system, we\u2019ll cover:\r\nTranslating research papers into functional code\r\n\r\n- Avoiding common RL pitfalls\r\n- Techniques to optimize resources without big budgets\r\n- Real-world case studies and key lessons learned\r\n\r\nOur experience culminated in the development of pi_optimal, an open-source library that simplifies RL implementation for Python developers. We\u2019ll showcase how pi_optimal makes RL accessible, enabling developers to apply RL principles without needing a PhD or extensive computational resources.\r\n\r\nWhat You\u2019ll Learn\r\n1. RL Fundamentals in Action\r\n1.1. Core concepts explained through an interactive Super Mario AI demo\r\n1.2. Understand RL components without diving into complex math\r\n2. From Theory to Production\r\n2.1. Step-by-step guide to implementing RL with pi_optimal\r\n2.2. A real-world case study illustrating successful application\r\n2.3. Solutions to common challenges and obstacles\r\n3. Practical Implementation Tips\r\n3.1. Strategies for resource optimization\r\n3.2. Best practices for testing and debugging RL systems\r\n3.3. Insights into deploying RL solutions in production\r\n\r\nLive Demonstration\r\nI will show the creation and training of a simple RL agent using pi_optimal. Experience real-time learning and discover how to apply these techniques to your own projects.\r\n\r\nWho Should Attend\r\n- Python developers exploring AI applications\r\n- Software engineers interested in RL systems\r\n- Technical leaders assessing RL\u2019s potential for their teams\r\n\r\nPrerequisites\r\n- Basic Python programming knowledge\r\n- A foundational understanding of machine learning concepts\r\n- No prior RL experience required\r\n\r\nKey Takeaways\r\n- A solid grasp of RL fundamentals\r\n- Access to open-source tools and starter code with pi_optimal\r\n- Strategies to implement RL in real-world scenarios\r\n- Resources for deepening your RL expertise\r\n\r\nJoin our community to democratise Reinforcement Learning and empower Python developers to tackle RL challenges!", "code": "TQLGA8", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/TQLGA8.png", "speaker_names": "Jochen Luithardt", "speakers": "\n### Jochen Luithardt\n\n\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Algorithmic Music Composition With Python", "abstract": "Computers have long been an integral part of creating music. Virtual instruments and digital audio workstations make creating music easy and accessible. But how do programming languages and especially Python fit into this? Python can serve as a tool for creating musical notation\r\nand MIDI files.   \r\n\r\nThroughout the session, you\u2019ll learn how to:\r\n\r\n- Use Python to create melodies, harmonies, and rhythms.\r\n- Generate music based on rules, randomness, and mathematical principles.\r\n- Visualize and export your compositions as MIDI and sheet music.\r\n\r\nBy the end of the talk, you\u2019ll have a clear understanding of how to turn simple algorithms into expressive musical works.", "full_description": "This talk provides a general introduction into creating music algorithmically using Python. Little prior knowledge about music is assumed. It is helpful to know how sheet music looks and what the MIDI format is beforehand. \r\n\r\nWe will start by looking briefly into the basic building blocks of music (harmony, melody and rhythm) and what our goal is (creating sheet music and a playable MIDI file). \r\n\r\nThen we will discuss the history of algorithmic composition in music and from that we will develop ideas how we can create music from algorithms and randomness. \r\n\r\nFor creating sheet music we will look into the packages Abjad und music21. \r\n\r\nIn the end will we will create a playable MIDI file for our music using MIDIUtil.", "code": "TQN98D", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/TQN98D.png", "speaker_names": "Hendrik Niemeyer", "speakers": "\n### Hendrik Niemeyer\n\nHendrik is a C++ developer and works on software for analysis of pipeline inspection data. This includes topics like machine learning, \r\nnumerical mathematics and distributed computing. Before this he completed his PhD in physics at the University of Osnabr\u00fcck with a thesis about quantum mechanics and \r\nnumerical simulations where he got to know and and love programming and complex, mathematical tasks. \r\nHis favorite programming languages, in which he also has the most experience, are C++, Python and Rust. He describes himself as a \"learning enthusiast\" \r\nwho always gets absorbed in trying out new things. Therefore, he values being up to date with programming languages \r\nand using the latest features of them in a meaningful way.\n", "track": "Python Language & Ecosystem"}, {"title": "Scaling Python: An End-to-End ML Pipeline for ISS Anomaly Detection with Kubeflow", "abstract": "Building and deploying scalable, reproducible machine learning pipelines can be challenging, especially when working with orchestration tools like Slurm or Kubernetes. In this talk, we demonstrate how to create an end-to-end ML pipeline for anomaly detection in International Space Station (ISS) telemetry data using only Python code.\r\n\r\nWe show how Kubeflow Pipelines, MLFlow, and other open-source tools enable the seamless orchestration of critical steps: distributed preprocessing with Dask, hyperparameter optimization with Katib, distributed training with PyTorch Operator, experiment tracking and monitoring with MLFlow, and scalable model serving with KServe. All these steps are integrated into a holistic Kubeflow pipeline.\r\n\r\nBy leveraging Kubeflow's Python SDK, we simplify the complexities of Kubernetes configurations while achieving scalable, maintainable, and reproducible pipelines. This session provides practical insights, real-world challenges, and best practices, demonstrating how Python-first workflows empower data scientists to focus on machine learning development rather than infrastructure.", "full_description": "Among popular open-source MLOps tools, **Kubeflow** stands out as a Kubernetes-native platform designed to support the entire ML lifecycle, from data preprocessing to model training, deployment, and retraining. Its modular structure enables the integration of a wide range of tools, making it a highly versatile framework for building scalable and reproducible ML workflows. Despite this, most existing resources focus on individual components rather than demonstrating how these can be orchestrated into a seamless, end-to-end pipeline.\r\n\r\nIn this talk, we present a practical case study that highlights the potential of Kubeflow in a real-world application. Specifically, we showcase how an automated ML pipeline for anomaly detection in International Space Station (ISS) telemetry data can be built and deployed using Kubeflow and other open-source MLOps tools. The dataset, originating from the Columbus module of the ISS, introduces unique challenges due to its complexity and high-dimensional nature, providing an excellent testbed for MLOps workflows.\r\n\r\n### **What makes this approach unique?**\r\n\r\nOur workflow is built entirely in Python, leveraging Kubeflow\u2019s Python SDK to orchestrate every stage of the pipeline. This eliminates the need for manual interaction with Kubernetes or container configurations, making the process accessible to ML engineers and data scientists without extensive DevOps expertise.\r\n\r\n### **Key takeaways for attendees:**\r\n\r\n*   **Tool integration:** Learn how to combine Dask for distributed preprocessing, Katib for hyperparameter optimization, PyTorch Operator for distributed training, MLFlow for experiment tracking and monitoring, and KServe for scalable model serving. These tools are orchestrated into a unified pipeline using Kubeflow Pipelines.\r\n*   **Overcoming challenges:** Gain insights into the technical hurdles faced during the implementation of this pipeline and discover the strategies and best practices that made it possible.\r\n*   **Real-world impact:** Understand how to apply MLOps principles to complex, real-world datasets and how these principles translate into scalable, maintainable, and reproducible workflows.\r\n\r\nTo ensure reproducibility and accessibility, the entire pipeline, including configurations and code, is publicly available in our GitHub repository [here](https://github.com/hsteude/code-ml4cps-paper). Attendees will be able to replicate the workflow, adapt it to their own use cases, or extend it with additional features.\r\n\r\n### **Who should attend?**\r\n\r\nThis session is designed for data scientists, ML engineers, and Python enthusiasts who want to simplify the development of scalable ML pipelines. Whether you're new to Kubernetes or looking to streamline your MLOps workflows, this talk will provide actionable insights and tools to help you succeed.", "code": "TRUUVL", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/TRUUVL.png", "speaker_names": "Christian Geier, Henrik Sebastian Steude", "speakers": "\n### Christian Geier\n\nChristian has 12+ years of experience in the scientific application of python in academic and industry settings. He is one of the founders of prokube.ai where he builds an MLOps platform build around Kubeflow, MLFlow, Kubernetes, and a host of other open source tools. He also holds a PhD in physics, where he gained experiences in maintaining a distributed compute clusters. Christian is a maintainer of several OSS projects.\n\n### Henrik Sebastian Steude\n\nHenrik is an ML researcher at Helmut Schmidt University, specializing in the application of ML in cyber-physical systems. In his current project, he is developing an anomaly detection and diagnostic AI system for use with data from the International Space Station. Before returning to academia, Henrik spent five years as a data scientist in various consulting roles, where he had the opportunity to delve into a range of exciting datasets. During this time, Henrik became a Python and Kubeflow enthusiast.\n", "track": "MLOps & DevOps"}, {"title": "Analyze data easily with duckdb - and the implications on data architectures", "abstract": "duckdb is increasingly becoming a universal tool for accessing and analyzing data. In this talk I will show with slides and live demo what duckdb is capable of and will dive deeper in how it will influence modern data architectures.", "full_description": "duckdb - a lightweight database with a focus on data analysis and a fast query engine that can be used in a variety of ways:   \r\n- Analyze data, stored on your own hard drive or somewhere on the Internet, in the browser with SQL? No problem  \r\n- Quickly check all the JSON files in S3 using SQL? Nothing could be easier  \r\n- A huge parquet file, bigger than my working memory. And now I have to analyze it locally. Easy!  \r\n- Read csv from blob storage, process and save in a Postgres database. Just one command\r\n \r\nduckdb is developing more and more into a universal tool for accessing and analyzing data.\r\n\r\nIn this talk I will show with slides and a live demo why it is so popular and why it belongs in the toolbox of every data scientist, ML engineer or data engineer. \r\n\r\nBut I will not stop at the useful tooling. I will dive deeper into the implications for data and software architectures that arise from the rise of the embedded OLAP systems like duckdb. I will especially focus on both moving the data closer to the user for faster analytics but also on accessing data without the explicit need to move it. \r\n\r\nWhat you learn and see can be used immediately in your day-to-day work.", "code": "TXKLWR", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/TXKLWR.png", "speaker_names": "Matthias Niehoff", "speakers": "\n### Matthias Niehoff\n\nMatthias Niehoff works as Head of Data and Data Architect for codecentric AG and supports customers in the design and implementation of data architectures. His focus is on the necessary infrastructure and organization to help data and ML projects succeed.\n", "track": "Data Handling & Engineering"}, {"title": "Guiding data minds: how mentoring transforms careers for both sides", "abstract": "Mentorship is a powerful way to shape careers while building meaningful connections in the data field. In this talk, I\u2019ll share my journey as a professional mentor, what the role entails, and the impact it has on both mentees and mentors. Learn how mentorship drives growth, fosters innovation, and creates value for the data community\u2014and why you should consider stepping into this rewarding role.", "full_description": "Mentorship is a rewarding journey that allows experienced professionals to guide and empower the next generation of talent. As a mentor in the data field, I have had the privilege of helping individuals navigate their careers, refine their skills, and unlock their potential. In this talk, I will share my personal journey into becoming a professional mentor, how I approach mentorship in a structured and impactful way, and the unique value a mentorship brings to both mentees and mentors.\r\n\r\nI\u2019ll provide insights into the day-to-day activities of mentoring, from offering career guidance to solving technical challenges, while also discussing the importance of tailoring advice to individual goals. Beyond technical skills, mentorship fosters confidence, networking, and long-term growth for mentees while offering mentors opportunities for personal development, deep satisfaction, and a broader industry perspective.\r\n\r\nWith the rapid evolution of the data industry, mentorship has never been more critical. This talk will highlight how professionals at any stage of their career can engage in mentorship to create a ripple effect of positive change in the data community\u2014and why taking the step to become a mentor, paid or otherwise, is an investment in the future of data science and yourself.", "code": "TYXMZC", "state": "confirmed", "created": "2024-12-08", "social_card_image": "/static/media/social/talks/TYXMZC.png", "speaker_names": "Anastasia Karavdina", "speakers": "\n### Anastasia Karavdina\n\nMy background is particle physics, where I was completely spoiled by access to large amounts of data and the freedom to try out every hot ML algorithm on it. The experiments I participated in were so-called large scale experiments (e.g Large Hadron Collider) and had from 500+ up to 2.5k other people working on them. So in addition to physics, I was exposed to the best software development practices that helped us to avoid a complete mess and destroy the Universe. \r\n\r\nAfterwards I was working as Data Scientist in various fields and recently became \"Solution Architect ML/AI and BI\" at big enterprise company. \r\n\r\nDuring my free time, I like learning new tools and techniques and implementing them in end-to-end AI/ML and IoT projects. My experience has also been very helpful in guiding data analysts, data scientists, and machine learning engineers as a mentor and contributing to the growth of the next generation of data scientist elite.\n", "track": "Community & Diversity"}, {"title": "Getting Started with Bayes in Engineering: Implementing Kalman Filters with RxInfer.jl", "abstract": "Bayesian methods are not commonly seen in Civil Engineering and Structural Dynamics. In this talk we explore how RxInfer.jl and the Julia Programming Language can simplify Bayesian modeling by implementing a Kalman filter for tracking the dynamics of a structural system. Perfect for engineers, researchers, and data scientists eager to apply probabilistic modelling and Bayesian methods to real-world engineering challenges.", "full_description": "Bayesian methods are renowned for their ability to incorporate domain knowledge and quantify uncertainty, making them valuable across various engineering and data science fields. However, finding practical examples of these methods in civil engineering, especially within structural dynamics, can be challenging.\r\n\r\nThis talk aims to make Bayesian inference accessible to engineering practitioners by demonstrating how RxInfer.jl, a Julia package for probabilistic programming, can be used to implement a Kalman filter for tracking the dynamics of a structural system. The session covers:\r\n\r\n1. Bayesian Modelling in Python and Julia: A brief comparison of probabilistic programming languages, highlighting Python and Julia\r\n2. State Space Modelling of Structural Dynamical Systems: A brief introduction to state space models and their use in structural dynamics\r\n3. Linking State Space Modelling to Finite Element Modelling: Making the connection between FEM and SSM\r\n4. A Simplified Overview of Bayesian Filtering and Kalman Filters for Dynamical Systems\r\n5. Bayesian Filtering Made Simple with RxInfer.jl: a step-by-step guide to setting up a user-friendly and readable Bayesian filter using Rxinfer.jl\r\n6. Full Workflow Example\r\n7. Interpreting the Results and Next Steps\r\n8. Connections to Julia, Python and Open-Source Ecosystems: exploring integrations with tools like FreeCAD and other open-source platforms\r\n\r\nBy the end of the talk, attendees will have a clear understanding of how to start using Bayesian methods in their engineering projects, supported by reproducible and open-source code.", "code": "U9KHNA", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/U9KHNA.png", "speaker_names": "Victor Flores Terrazas", "speakers": "\n### Victor Flores Terrazas\n\n\n", "track": "Research Software Engineering"}, {"title": "From LIKE to Love: Adding Proper Search to Your Django Apps", "abstract": "Is your Django application still relying on SQL LIKE queries for search? In this talk, we'll explore why basic text matching falls short of modern user expectations and how to implement proper search functionality without complexity. We'll introduce django-semantic-search, a practical package that bridges the gap between Django's ORM and powerful semantic search capabilities. Through practical code examples and real-world use cases, you'll learn how to enhance your application's search experience from basic keyword matching to understanding user intent. Whether you're building a content platform, e-commerce site, or internal tool, you'll walk away with concrete steps to implement production-ready search that your users will actually enjoy using.", "full_description": "Introduction (5 minutes)\r\n1. The state of search in Django applications today\r\n2. Common patterns and their limitations\r\n3. Real costs of poor search functionality\r\n4. Why search is often an afterthought in Django apps\r\n\r\nThe Search Landscape (10 minutes)\r\n1. Review of Django's built-in search capabilities\r\n2. Performance implications of basic text matching\r\n3. Field lookups and their limitations\r\n4. PostgreSQL-specific features\r\n5. Popular search solutions in the Django ecosystem\r\n6. Trade-offs between complexity and functionality\r\n\r\nWhy Search Matters (10 minutes)\r\n1. User expectations in 2025\r\n2. Common search patterns and user behaviors\r\n3. Impact on user engagement and business metrics\r\n4. Natural language queries vs keyword matching\r\n5. Handling imperfect input\r\n6. Context and intent understanding\r\n7. Real-world examples of search improvements\r\n\r\nModern Search Approaches (5 minutes)\r\n1. Key concepts of vector search\r\n2. From keywords to meaning\r\n3. Why embeddings work better than keywords\r\n4. Understanding user intent\r\n5. Relevance beyond exact matches\r\n\r\nPractical Implementation & Best Practices (15 minutes)\r\n1. Introducing django-semantic-search\r\n2. Core concepts and architecture\r\n3. Integration with existing Django models\r\n4. Real-world implementation strategies\r\n5. Handling different content types\r\n6. Performance optimization techniques\r\n7. Common pitfalls and solutions\r\n8. Resource management\r\n9. Query optimization\r\n10. Monitoring and maintaining search quality", "code": "UCG9AS", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/UCG9AS.png", "speaker_names": "Kacper \u0141ukawski", "speakers": "\n### Kacper \u0141ukawski\n\nSoftware developer and data scientist at heart, with an inclination to teach others. Public speaker, working in DevRel.\n", "track": "Django & Web"}, {"title": "Accuracy Is Not Enough: Building Trustworthy AI with Conformal Prediction", "abstract": "Building a good scoring model is just the beginning. In the age of critical AI applications, understanding and quantifying uncertainty is as crucial as achieving high accuracy. This talk highlights conformal prediction as the definitive approach to both uncertainty quantification and probability calibration, two extremely important topics in Deep Learning and Machine Learning. We\u2019ll explore its theoretical underpinnings, practical implementations using TorchCP, and transformative impact on safety-critical fields like healthcare, robotics, and NLP. Whether you're building predictive systems or deploying AI in high-stakes environments, this session will provide actionable insights to level up your modelling skills for robust decision-making.", "full_description": "When deploying machine learning models in the real world, especially in domains like healthcare, robotics, or natural language processing, the stakes are high. It\u2019s not enough to train a model, evaluate its accuracy, and call it a day. Questions of how confident the model is, how reliable its predictions are, and how to act on these predictions are critical yet often overlooked. This talk takes you beyond conventional metrics and into the world of uncertainty quantification and probability calibration, with conformal prediction as the definitive tool for both.\r\n\r\n\r\n\r\nWe\u2019ll start of the presentation by exploring the fundamental need for uncertainty in AI systems\u2014why it matters, how it\u2019s quantified, and how it can be used to make informed decisions. From there, we\u2019ll introduce conformal prediction, a mathematically rigorous yet practical framework that provides guarantees on prediction reliability while remaining model-agnostic. Core concepts such as probability calibration and uncertainty quantification will be highlighted as key parts in the modelling process, establishing their importance in the domain.\r\n\r\n\r\n\r\nThe session will also feature real-world examples and use cases such as:\r\n\r\n- Healthcare: Predict disease likelihood with quantifiable confidence, such as identifying risks in MRI scans.\r\n\r\n- Robotics: Navigate dynamic environments safely using calibrated vision-based models.\r\n\r\n- Natural Language Processing: Improve outputs of large language models with uncertainty-aware predictions.\r\n\r\n\r\n\r\nFinally, we\u2019ll showcase the TorchCP toolbox, a GPU-accelerated library for integrating conformal prediction into deep learning pipelines, an area of Data Science that has a lot of hype but often overlooks the importance of such tools. Through a live demonstration, you\u2019ll see how to implement these methods step-by-step, empowering you to build trustworthy AI systems that go beyond accuracy.\r\n\r\n\r\n\r\nAttendees will leave with:\r\n\r\n- A solid understanding of uncertainty quantification, probability calibration and their importance.\r\n\r\n- Practical knowledge of conformal prediction and how to implement it.\r\n\r\n- A new perspective on AI reliability and decision-making in critical domains.\r\n\r\n\r\n\r\nWhether you're an ML researcher, data scientist, or practitioner deploying AI models in critical environments, this session will equip you with the right tools and philosophy to create AI systems that are not only accurate but also reliable and robust.", "code": "UDDTBS", "state": "confirmed", "created": "2025-01-04", "social_card_image": "/static/media/social/talks/UDDTBS.png", "speaker_names": "Chris Aivazidis", "speakers": "\n### Chris Aivazidis\n\nA data scientist that goes beyond conventional methods to build robust and trustworthy AI models and solutions. \r\n- Experience in industry leading companies and a fairly short research background in Explainable AI in NLP. \r\n- Background in mathematics.\r\n- Always keeping up to date with the latest AI research and findings.\r\n- Competing in Machine Learning competitions.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Observability Matters: Empowering Python Developers with OpenTelemetry.", "abstract": "Have you ever experienced your favorite social platform going down? It\u2019s frustrating, right? As users, we feel the inconvenience immediately. To prevent these disruptions, engineering teams rely on various tools and practices to address unexpected system behavior. In today\u2019s distributed world, applications are often segmented into microservices to improve agility and performance. This also brings added complexity, with more opportunities for things to go wrong - errors, latency and more. In this dynamic landscape, observability has evolved from a mere buzzword into a crucial approach for modern reliable systems. We initially gravitate towards a tool for its ease of use, but later after investing significant amount of time and finances, if we choose to migrate, vendor lock-in becomes a major concern. This is where the importance of open standards like OpenTelemetry become essential.\r\n\r\nWhile many think of observability as solely an issue for SREs, but it\u2019s increasingly a developer concern. During this session, we\u2019ll explore why observability matters and how it is evolved over the years. We'll cover how to instrument Python code, process telemetry data and export it seamlessly for analysis. Attendees will leave with a clearer understanding of how observability increases confidence in managing complex systems and how adopting open standards improve the overall developer experience. Let\u2019s work together to redefine OSS observability, as a shared responsibility through collaboration.", "full_description": "During this session, I\u2019ll cover the following key topics:\r\n\r\n- The evolution of observability and why it has become essential over the past few decades.\r\n\r\n- A deep dive into core telemetry signals: Logs, Metrics, and Traces.\r\n\r\n- The challenges developers face with modern observability, and how adopting OpenTelemetry can address these issues.\r\n\r\n- A comparison between traditional approaches and the OpenTelemetry method, focusing on everything from easy Python code instrumentation to advanced data processing and seamless telemetry export to any observability backend\u2014without vendor lock-in.\r\n\r\n- Different sampling techniques like head, tail, probabilistic, allow you to have full control over the data you ingest and its associated costs with different setup strategies.\r\n\r\n- Best practices for getting started with OpenTelemetry, including a live code demo.\r\n\r\n- Future of AI in the of open-source observability and ongoing work on OpenTelemetry with eBPF.", "code": "UDSJME", "state": "confirmed", "created": "2024-12-30", "social_card_image": "/static/media/social/talks/UDSJME.png", "speaker_names": "Yash Verma", "speakers": "\n### Yash Verma\n\nYash is a software engineer and researcher with a deep interest in distributed systems. His focus is on observability and performance, areas where he constantly seeks new insights. As an active advocate of OpenTelemetry, Yash contributes to both the project and the wider community. Outside of tech, he\u2019s an avid explorer, whether in the kitchen experimenting with new recipes or traveling the world to taste diverse cuisines.\n", "track": "Programming & Software Engineering"}, {"title": "Securing Generative AI: Essential Threat Modeling Techniques", "abstract": "Generative AI development introduces unique security challenges that traditional methods often overlook. This talk explores practical threat modeling techniques tailored for AI practitioners, focusing on real-world scenarios encountered in daily development. Through relatable examples and demonstrations, attendees will learn to identify and mitigate common vulnerabilities in AI systems. The session covers user-friendly security tools and best practices specifically designed for AI development. By the end, participants will have practical strategies to enhance the security of their AI applications, regardless of their prior security expertise.", "full_description": "1. Introduction\r\n    * Motivation\r\n    * What can go wrong\r\n2. Generative AI vs Traditional Applications\r\n    * Key differences in security considerations\r\n    * Unique challenges posed by generative AI\r\n3. Threat Modeling Basics and AI-Specific Threats \r\n    * STRIDE framework\r\n    * Focus on prompt injection and data poisoning\r\n    * Example: Simple prompt injection attempt\r\n4. Practical Threat Modeling Process\r\n    * Simplified system decomposition example\r\n    * Threat identification walkthrough\r\n5.  Example: Input Validation\r\n6. Tools Showcase and Mitigation Strategies\r\n    * AI security tools applicable\r\n    * Best practices for API security\r\n7. Conclusion and Resources\r\n    * Recap key takeaways\r\n    * List of recommended tools and further reading", "code": "UGTB7A", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/UGTB7A.png", "speaker_names": "Elizaveta Zinovyeva", "speakers": "\n### Elizaveta Zinovyeva\n\n\n", "track": "Generative AI"}, {"title": "Instrumenting Python Applications with OpenTelemetry", "abstract": "Observability is challenging and often requires vendor-specific instrumentation. Enter OpenTelemetry: a vendor-agnostic standard for logs, metrics, and traces. Learn how to instrument Python applications with OpenTelemetry and send telemetry to your preferred observability backends.", "full_description": "Understanding the behaviour and performance characteristics of the software we deploy, especially distributed software, is quite tricky. While observability tooling helps, implementing vendor-specific instrumentation creates tight coupling and technical debt. \r\n\r\nEnter OpenTelemetry: A one-stop-shop for observability instrumentation, collection and routing. It aims to solve the above problem by providing SDKs, libraries and a unified semantic model for describing telemetry signals like logs, metrics and traces. These signals can be collected, transformed and then routed to many observability backends that support the OpenTelemetry protocol - avoiding vendor lock-in and platform specific observability code.\r\n\r\nIn this workshop, we'll guide you through what OpenTelemetry is, how it works, how to instrument your Python applications to emit telemetry data, and how to ingest this data into observability backends - enabling you to make better decisions about your application's performance.", "code": "UH7FXA", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/UH7FXA.png", "speaker_names": "Mika Naylor", "speakers": "\n### Mika Naylor\n\nMika is a Berlin-based lifeform mostly working with devops, distributed systems and Apache Flink. She also loves Rust, making ceramics and baking bread.\n", "track": "MLOps & DevOps"}, {"title": "PDFs - When a thousand words are worth more than a picture (or table).", "abstract": "PDF, a must-have in RAG systems, ensures visual fidelity across platforms and devices, at the expense of compromising what would be the core condition for computers to properly process and interpret text: semantics. That means any logical arrangement of text, upon rendering, explodes into dummy visual shards of data that literally portrait the bigger picture for the human eye to perceive, but no longer convey the information computers should grasp. Such a bottleneck already makes proper ingestion of text-only documents a big challenge, let alone when tables or figures come into play, the ultimate nightmare for PDF parsers, not to say developers. The rest you must have already foreseen: a RAG system barfing unreliable knowledge from bad chunks (based on regular PDF parsing), if those ever get to be retrieved from a vector database. In this talk you can gather some vision-driven insights on how to leverage the strengths of PDF and language models towards good chunks to be ingested. Or, in other words, how multimodal models can go beyond trivial reverse engineering by decomposing tables into its building blocks, in plain language, as how those would be explained to another human; or better yet, as how humans would ask questions about such pieces of knowledge. And from such a strategy, we transfer the same rationale to figures. Come along, gather some insights, and get inspired to break down tables and figures from your own PDFs, and to improve retrieval in your RAG systems.", "full_description": "PDF, a must-have in RAG systems, ensures visual fidelity across platforms and devices, at the expense of compromising what would be the core condition for computers to properly process and interpret text: semantics. That means any logical arrangement of text, upon rendering, explodes into dummy visual shards of data that literally portrait the bigger picture for the human eye to perceive, but no longer convey the information computers should grasp. Such a bottleneck already makes proper ingestion of text-only documents a big challenge, let alone when tables or figures come into play, the ultimate nightmare for PDF parsers, not to say developers. The rest you must have already foreseen: a RAG system barfing unreliable knowledge from bad chunks (based on regular PDF parsing), if those ever get to be retrieved from a vector database.\r\n\r\nIn this talk you can gather some vision-driven insights on how to leverage the strengths of PDFs and language models towards good chunks to be ingested in a vector database. Or, in other words, how multimodal models can go beyond trivial reverse engineering by decomposing tables into its building blocks, in plain language, as how those would be explained to another human; or better yet, as how humans would ask questions about such pieces of knowledge. Consequently, it brings robustness to retrieval, the backbone of RAG. And from such a strategy, we can transfer the same rationale to figures.\r\n\r\nGet ready to boost your retrieval skills, as we:\r\n- Analyze the semantical bottlenecks, from the anatomy of a PDF stream, to how parsers traverse it;\r\n- (Briefly) approach the never-ending debate on the ideal chunk format for ingestion in vector databases;\r\n- Build some chunks using multimodal models to decompose tables into its building blocks, preserving plain language;\r\n- Conduct an experiment on measuring quality of retrieval and compare the decomposition strategy against PDF parsers and reverse engineering techniques;\r\n- And last, but not least, transfer the same rationale to figures.\r\n\r\nBy then, you'll have enough food for thought to get your hands dirty, clone the repo, and give tweaks to the experiment yourself. Come along, gather some insights, and get inspired to break down tables and figures from your own PDF files, and to improve retrieval in your RAG systems.", "code": "UVPALT", "state": "confirmed", "created": "2024-12-13", "social_card_image": "/static/media/social/talks/UVPALT.png", "speaker_names": "Caio Benatti Moretti", "speakers": "\n### Caio Benatti Moretti\n\nCaio holds a PhD in Computer Science and has been working with\u00a0data\u00a0and AI both in academia and industry since 2014. Currently working as a DS/MLE Consultant at Xebia Data, he is particularly keen on neural networks in its many forms and applications. His enthusiasm even led him to make a neural network fit inside a business card. With experience designing and taking applications into production, Caio has been recently focusing on how (Generative)AI can augment human productivity.\n", "track": "Generative AI"}, {"title": "Oh, no! Users love my GenAI-Prototype and want to use it more.", "abstract": "Demos and prototypes for generative AI (GenAI) projects can be quickly created with tools like Streamlit, offering impressive results for users within hours. However, scaling these solutions from prototypes to robust systems introduces significant challenges. As user demand grows, hacks and workarounds in tools like Streamlit lead to unreliability and debugging frustrations. This talk explores the journey of overcoming these obstacles, evolving to a stable tech stack with Qdrant, Postgres, Litellm, FastAPI, and Streamlit. Aimed at beginners in GenAI, it highlights key lessons.", "full_description": "Demos and prototypes for projects with generative AI can be quickly put together: an API key from the preferred model provider, some source code from the online tutorial and a few small adjustments suffice. Thanks to Streamlit and the like, even beginners can achieve impressive results that can be used by users within a few hours.\r\n\r\nBut what happens when users actually like the solution? When demos and prototypes need to be expanded and connected to other systems? What if the number of users continues to rise?\r\n\r\nIt is quite impressive how far you can bend Streamlit to achieve things it was probably never meant for. But at a certain point, you pay for the hacks and workarounds with unreliability and frustrating debugging.\r\n\r\nThe speakers repeatedly reached this point in various projects and delayed the necessary architecture discussion for too long. So the path was longer and more painful than it should have been \u2013 but in the end, thanks to the wide range of open-source (Python) projects, a flexible and stable system was created. Our current tech stack includes Qdrant, Postgress, Litellm and FastAPI \u2013 as well as OpenWebUI, and of course Streamlit. \r\n\r\nThanks to modularization, we now have a stable system that we can easily run locally but also deploy in an enterprise environment. Nevertheless, we have retained a great deal of flexibility.\r\n\r\nIn our talk, we report on the trials and tribulations along the way. We report on the challenges that led to decisions for various components. We disclose which problems we were able to solve and which new problems arose.\r\n\r\nThe talk is aimed primarily at those who are taking their first steps with generative AI or have already developed their first demonstrators or prototypes. \r\n\r\nStructure:\r\n\r\n(1) GenAI applications in Streamlit are cool\r\n(2) The challenges on the way from prototype to productive deployment\r\n(3) Ramming heads through walls\r\n(4) The path to a flexible but stable stack\r\n(5) What still plagues us", "code": "UXTCZC", "state": "confirmed", "created": "2025-01-02", "social_card_image": "/static/media/social/talks/UXTCZC.png", "speaker_names": "Thomas Prexl, Frank Rust", "speakers": "\n### Thomas Prexl\n\nThomas is an expert in tech transfer and startup development, with a career focused on fostering innovation and bridging the gap between research and industry. He has led initiatives like accelerator programs, innovation networks, and hackathons. A Generative AI enthusiast and co-founder of neunzehn innovations, Thomas helps companies leverage AI technologies. He holds a doctorate from the University of Basel and is a dedicated advisor, educator, and speaker in the startup ecosystem.\n\n### Frank Rust\n\n\n", "track": "MLOps & DevOps"}, {"title": "Reinforcement Learning for Finance", "abstract": "Reinforcement Learning and related algorithms, such as Deep Q-Learning (DQL), have led to major breakthroughs in different fields. DQL, for example, is at the core of the AIs developed by DeepMind that achieved superhuman levels in such complex games as Chess, Shogi, and Go (\"AlphaGo\", \"AlphaZero\"). Reinforcement Learning can also be beneficially applied to typical problems in finance, such as algorithmic trading, dynamic hedging of options, or dynamic asset allocation. The workshop addresses the problem of limited data availability in finance and solutions to it, such as synthetic data generation through GANs. It also shows how to apply the DQL algorithm to typical financial problems. The workshop is based on my new O'Reilly book \"Reinforcement Learning for Finance -- A Python-based Introduction\".", "full_description": "Reinforcement Learning and related algorithms, such as Deep Q-Learning (DQL), have led to major breakthroughs in different fields. DQL, for example, is at the core of the AIs developed by DeepMind that achieved superhuman levels in such complex games as Chess, Shogi, and Go (\"AlphaGo\", \"AlphaZero\"). Reinforcement Learning can also be beneficially applied to typical problems in finance, such as algorithmic trading, dynamic hedging of options, or dynamic asset allocation. The workshop addresses the problem of limited data availability in finance and solutions to it, such as synthetic data generation through GANs. It also shows how to apply the DQL algorithm to typical financial problems.\r\n\r\nThe workshop covers the following topics:\r\n\r\n* Learning through interaction\r\n* Deep Q-Learning applied to Finance\r\n* Synthetic Data Generation\r\n* Dynamic Asset Allocation with DQL\r\n\r\nThe workshop is based on my new O'Reilly book \"Reinforcement Learning for Finance -- A Python-based Introduction\".", "code": "VBW3EK", "state": "confirmed", "created": "2025-01-04", "social_card_image": "/static/media/social/talks/VBW3EK.png", "speaker_names": "Dr. Yves J. Hilpisch", "speakers": "\n### Dr. Yves J. Hilpisch\n\nDr. Yves J. Hilpisch is the founder and CEO of The Python Quants (https://tpq.io), a group focusing on the use of open source technologies for financial data science, artificial intelligence, algorithmic trading, computational finance, and asset management.\r\n\r\nYves has a Diploma in Business Administration, a Ph.D. in Mathematical Finance, and is Adjunct Professor for Computational Finance.\r\n\r\nYves is the author of seven books (https://home.tpq.io/books):\r\n\r\n* Reinforcement Learning for Finance (2024, O\u2019Reilly)\r\n* Financial Theory with Python (2021, O\u2019Reilly)\r\n* Artificial Intelligence in Finance (2020, O\u2019Reilly)\r\n* Python for Algorithmic Trading (2020, O\u2019Reilly)\r\n* Python for Finance (2018, 2nd ed., O\u2019Reilly)\r\n* Listed Volatility and Variance Derivatives (2017, Wiley Finance)\r\n* Derivatives Analytics with Python (2015, Wiley Finance)\r\n\r\nYves is the director of Certificate in Python for Finance (CPF) Program, a comprehensive, systematic online training program preparing students, academics, and professionals alike for the challenges faced by financial institutions in data science, computation, trading, and artificial intelligence. He also lectures on computational finance, machine learning, and algorithmic trading at the CQF Program (http://cqf.com).\r\n\r\nYves is the originator of the financial analytics library DX Analytics (http://dx-analytics.com) and organizes Meetup group events, conferences, and Bootcamps about Python, artificial intelligence, and algorithmic trading in London (http://pqf.tpq.io) and New York (http://aifat.tpq.io). He has given keynote speeches at technology conferences in the United States, Europe, and Asia.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Mini-Pythonistas: Coding, Experimenting, and Exploring with Z\u00fcmi!", "abstract": "Welcome, mini-Pythonistas! In this workshop, we\u2019ll dive into the world of Z\u00fcmi, a programmable car that\u2019s much more than just wheels and motors. With built-in sensors, lights, and a camera, Z\u00fcmi can learn to recognize colors, respond to gestures, and even identify faces \u2014 all with your help!\r\n\r\nNote, this is a kids workshop: All children and young people up to the age of 16 are welcome.", "full_description": "## Summary\r\nWelcome, mini-Pythonistas! In this workshop, we\u2019ll dive into the world of Z\u00fcmi, a programmable car that\u2019s much more than just wheels and motors. With built-in sensors, lights, and a camera, Z\u00fcmi can learn to recognize colors, respond to gestures, and even identify faces \u2014 all with your help!\r\n\r\nNote, this is a kids workshop: All children and young people up to the age of 16 are welcome.\r\n\r\n\r\n## More Details\r\nWhether you\u2019re brand new to programming or a seasoned Python pro, there\u2019s something here for everyone:\r\n* Blockly: Perfect for beginners! Learn the basics of programming by snapping together colorful blocks.\r\n* Jupyter Notebooks: Already know about variables and loops? Take the next step and explore more advanced coding concepts.\r\n* Python Scripting: For our experienced coders, write your own Python scripts and push Z\u00fcmi to its limits.\r\n\r\nWhat can you teach Z\u00fcmi?\r\n* Drive and park autonomously: With infrared sensors, Z\u00fcmi can detect obstacles, stop, and adjust its course.\r\n* Recognize colors: Train a machine learning model to teach Z\u00fcmi to stop or react when it sees a specific color.\r\n* Identify faces: Using its camera, Z\u00fcmi can spot faces in photos and even recognize a smile!\r\n* and many more!\r\n\r\nJoin us for a fun-filled adventure where coding meets creativity and discovery. Let\u2019s see what you and Z\u00fcmi can achieve together! \ud83d\ude97\ud83d\udcbb\u2728", "code": "VC3T39", "state": "confirmed", "created": "2025-02-03", "social_card_image": "/static/media/social/talks/VC3T39.png", "speaker_names": "Anna-Lena Popkes, Daniel Hieber, Dr. Marisa Mohr, Hannah Hepke", "speakers": "\n### Anna-Lena Popkes\n\nI'm Anna-Lena, a machine learning engineer living in Bonn, Germany. I'm very passionate about learning and love to share my knowledge with other people. Besides machine learning I love teaching Python and have been a regular guest on PyCon events and podcasts.\n\n### Daniel Hieber\n\nDaniel is a PhD student in digital neuropathology at Julius-Maximilians-University W\u00fcrzburg and a research assistant at the University Hospital Augsburg as well as Neu-Ulm University of Applied Sciences. His work is focused on applying computer vision techniques to automate analysis processes in the pathological departments.\n\n### Dr. Marisa Mohr\n\nI'm Marisa, I live in L\u00fcbeck, Germany, I'm a mathematician and Team Lead at inovex. With a passion for data, demystification, and people, I\u2019m on a mission to bridge the gap between complex mathematical concepts and real-world understanding. Whether it\u2019s through interpretability, explainability, or fostering equal opportunity and fairness, I believe that math can \u2013 and should \u2013 be accessible to everyone.\n\n### Hannah Hepke\n\nI'm Hannah, a data and machine learning engineer living in Karlsruhe, Germany. With a strong interest in Artificial Intelligence, I am excited to start my journey in this dynamic field. I am passionate about teaching and take great pleasure in breaking down complex concepts and making them accessible to others, fostering a collaborative learning environment.\n", "track": "Sponsor"}, {"title": "Taking Control of LLM Outputs: An Introductory Journey into Logits", "abstract": "This talk explores the use of logits - the raw confidence scores that language models generate before selecting each token. Working directly with logits enables finer control over model behavior.\r\n\r\nThe session covers practical techniques for accessing and utilizing these scores through local models. Topics include detecting model uncertainty, implementing custom stopping conditions, and steering generation without prompt modifications.\r\n\r\nYou will learn how to analyze model confidence patterns and apply this knowledge to real-life use cases.", "full_description": "Logits are the raw numerical scores that language models compute for each token in their vocabulary before making a selection. These scores are typically converted to probabilities and used internally for token selection. Accessing and analyzing them directly opens up possibilities for controlling and understanding model behavior.\r\n\r\nLogits provide insights into model uncertainty, help detect potential hallucinations, and enable fine-grained control over generation without modifying prompts.\r\n\r\nIn this session, attendees will learn how to access logits through local models, visualize confidence patterns, and implement practical techniques like uncertainty detection and generation steering. You can get practical insight which you can apply to your own projects.", "code": "VDG9YG", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/VDG9YG.png", "speaker_names": "Emek G\u00f6zl\u00fckl\u00fc", "speakers": "\n### Emek G\u00f6zl\u00fckl\u00fc\n\natypical techie. software engineer & researcher building ai/ml tools with keen interest in edtech. co-founder and builder of Quipu. \r\n\r\nalso working as a part-time engineer at MICE Portal, where he supports transformation of the company processes with agentic ai-backed approaches.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "They are not unit tests: a survey of unit-testing anti-patterns", "abstract": "The entire industry approves of unit testing but almost no one can fully agree on how to do it correctly, or even on what unit tests are. This results in unit tests often being associated with slower development cycle and an overall less enjoyable workflow. I'll show you how testing turns into hell in real enterprises with the most common anti-patterns and then I'll show you that most of them are avoidable with modern tooling like mutation testing, snapshot testing, dirty-equals, and many more. We'll discuss how to make tests speed up your development and make refactoring easy.", "full_description": "Similar to TDD, unit tests are one of the most misunderstood concepts in software engineering. In this session, I will cover the most important fallacies about unit testing and the most common anti-patterns. I will also show you how modern infrastructure (pytest-fixture-classes, inline-snapshot, dirty-equals, import-linter, mutmut, and pytest-xdist) makes it possible to avoid most of them. \r\n\r\nWe will discuss that the real goal of tests is not always stability and how tests often make refactoring and restructuring your project easy, not hard. I will define my criteria for good tests and then for the rest of the session, we will be using it to analyze anti-patterns and explore modern solutions to them. You will see:\r\n\r\n1. How people make their \"units\" too small and how you can prevent it using import-linter\r\n2. How people make their \"units\" too big and what architectural patterns can you use to make them smaller\r\n3. How the real value of tests is in the quality of their assertions and how mutation testing can measure it for you\r\n4. How people end up with asserting too much, and how inline-snapshot and dirty-equals make this problem obsolete\r\n5. How people try to cover the volatile parts of their software, and how coveragepy already has tooling to prevent it\r\n6. How slow tests hurt you, and how to make your tests fast even if you tried it many times and failed\r\n7. How to build an architecture that makes writing tests hard, and how to make it easy using inline-snapshot, pytest-fixture-classes, and a few clever tricks\r\n8. How you can mock your way into making your tests useless, what you should actually mock and how testcontainers can help you with that\r\n\r\nAfter this session, your tests will become your friend instead of slowing you down.", "code": "VFE78U", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/VFE78U.png", "speaker_names": "Stanislav Zmiev", "speakers": "\n### Stanislav Zmiev\n\nExperienced platform engineer and architect with a passion for open source and developer tools. The author of Cadwyn -- a sophisticated API Versioning framework based on FastAPI. A contributor to numerous projects such as CPython and tortoise-orm. Currently building the future of finance at Monite.\n", "track": "Testing"}, {"title": "Building Bare-Bones Game Physics in Rust with Python Integration", "abstract": "Learn how to build a minimalist game physics engine in Rust and make it accessible to Python developers using PyO3. This talk explores fundamental concepts like collision detection and motion dynamics while focusing on Python integration for scripting and testing. Ideal for developers interested in combining Rust\u2019s performance with Python\u2019s ease of use to create lightweight and efficient tools for games or simulations.", "full_description": "Python\u2019s simplicity makes it the go-to choice for scripting, while Rust excels in performance-critical tasks like game physics. This talk demonstrates how to build a minimalist physics engine in Rust, focusing on core concepts like collision detection, basic rigid body dynamics, and force application, while providing seamless Python integration using PyO3.\r\n\r\nWe\u2019ll explore how PyO3 allows developers to expose Rust functionality as native Python modules, enabling Python developers to easily script and interact with the physics engine. Through practical examples, attendees will see how Python can be used for rapid prototyping and gameplay scripting, while Rust handles the heavy lifting of physics calculations.\r\n\r\nBy the end of this session, participants will not only understand the basics of implementing physics in Rust but also how to use PyO3 to bridge the gap between Rust\u2019s performance and Python\u2019s flexibility. This talk is perfect for Python enthusiasts curious about Rust or Rustaceans looking to make their libraries accessible to the Python ecosystem.", "code": "VKYDBD", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/media/social/talks/VKYDBD.png", "speaker_names": "Sam Kaveh", "speakers": "\n### Sam Kaveh\n\nBorn in Iran, I have embraced diverse roles throughout my career, ranging from founding a startup and software development to consulting companies on cloud migrations and integrating machine learning technologies into their operations. My professional journey has been shaped by a passion for problem-solving and innovation across various domains.\r\n\r\nAcademically, I hold a Ph.D. in particle physics, specializing in Higgs boson precision measurements as part of the CMS experiment at CERN's Large Hadron Collider. This experience honed my analytical skills and gave me a deep appreciation for collaboration in high-stakes, cutting-edge environments.\r\n\r\nToday, I draw on my multidisciplinary background to create solutions at the intersection of software, data science, and high-performance computing, continually seeking to bridge theory and practice in impactful ways.\n", "track": "Rust"}, {"title": "Go Beyond Basic RAG with Agentic Behavior", "abstract": "RAG has transformed AI systems by combining retrieval and generation, but traditional workflows often struggle with the dynamic demands of real-world applications, such as multi-step queries or integrating external APIs. Agentic behavior enhances RAG by enabling LLMs to make decisions, call tools, and adapt workflows dynamically. In this talk, we\u2019ll define agentic behavior, explore its core features such as routing, tool integration, and reasoning, and demo its practical implementation in Python using Haystack.", "full_description": "Retrieval-Augmented Generation (RAG) has transformed how we build AI systems with Large Language Models (LLMs) by combining the strengths of retrieval and generation. However, traditional RAG workflows are static and often struggle to handle the dynamic and complex demands of real-world applications, such as answering multi-step queries, integrating external APIs, or gracefully recovering from retrieval failures. Agentic behavior addresses these challenges by extending RAG pipelines, enabling LLMs to make decisions, integrate tools, and dynamically adapt workflows.\r\n\r\nIn this talk, we\u2019ll explore how agentic behavior enhances RAG pipelines. We\u2019ll define what it means for a system to act as an \u201cagent\u201d and cover core concepts like routing, tool calling, and reasoning. Using hands-on examples implemented in Python with Haystack, we\u2019ll walk through practical use cases, such as integrating external APIs and solving multi-step problems. Finally, we\u2019ll tackle challenges like transparency in complex systems and share how graph-based approaches can make these workflows more interpretable.\r\n\r\n### Outline:\r\n\r\n1. Intro of the speaker (1 min)\r\n2. Introduction to RAG (3 mins)\r\n    - Overview of RAG\r\n    - its capabilities and its key limitations\r\n    - Some examples of complex tasks\r\n3. Agentic Behavior (7-8 mins)\r\n    - Definition & Core Characteristics\r\n    - Reasoning, tool calling, dynamic adaptation of workflows (routing)\r\n    - How these characteristics improve RAG pipelines\r\n4. Practical Implementation in Haystack (10 min)\r\n    - Intro to Haystack and key components to use in the demo: retrieval, generation (3 mins)\r\n    - Show how to implement agentic behavior using Haystack\r\n    - Step-by-step demonstration of an agent calling external APIs and performing multi-hop reasoning\r\n5. Addressing Challenges in Complex Systems (3 min)\r\n    - Challenges of understanding complex, dynamic workflows\r\n    - How graph-based approaches provide clarity and control\r\n6. Conclusion and Q&A (5 min)", "code": "VYL7AU", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/VYL7AU.png", "speaker_names": "Bilge Y\u00fccel", "speakers": "\n### Bilge Y\u00fccel\n\nBilge is a Developer Relations Engineer at deepset, working with Haystack. She studied Computer Science and Engineering at Sabanci University. With over two years of experience as a Software Engineer, she developed a strong interest in AI and pursued a master's degree in Artificial Intelligence at KU Leuven with a focus on NLP. Now, she enjoys working with Haystack and helping the community build custom LLM applications \u2728\n", "track": "Generative AI"}, {"title": "FastHTML vs. Streamlit - The Dashboarding Face Off", "abstract": "In the right corner, we have the go-to dashboarding solution for showcasing ML models or visualizing data, **STREAMLIT** (\\*crowd cheers\\*). Simple yet powerful, it defends the throne of Python dashboarding, but have you ever tried to create complex interactions with it? Things like drill-downs or logins, can make your control flow become messy really quick (\\*crowd nods knowlingly\\*).\r\n\r\nAnd in the left corner, the new contender in the arena of Python web frameworks which, according to its docs, \"*excels at building dashboards*\", **FastHTML** (\\*crowd whoops\\*). We will see if this is true, in the **ultimate dashboarding face off** (\\*crowd gasps\\*). By building the same dashboard, step by step, in both frameworks, investigate their strengths and weaknesses, we will see which framework can claim the crown.", "full_description": "Streamlit is the go-to dashboarding solution for showcasing ML models or visualizing data. It has a vibrant community, multiple years of development under its belt, and tons of third-party integrations. On the other hand, everyone that tried to create complex interactions, like drill-downs or logins, knows that control flow can get messy really quick. Initially simple dashboards often evolve into something bigger and the simple-but-powerful Streamlit formula may not always be up to the tasks.\r\n\r\nFastHTML is a new contender in the arena of Python web frameworks and, according to its docs, \"it excels at building dashboards.\" FastHTML stands on the shoulders of giants, giving you a smooth Python experience for authoring web pages, while allowing access to the foundations of the web, like CSS and JS, at any time. We will see if FastHTML can put code where its mouth is, by building the same dashboard, step by step, in both frameworks and investigate their strengths and weaknesses.\r\n\r\nThis is a talk for data enthusiasts that dabble in web technologies for the sake of showcasing their work or building internal tooling. Do not expect a course on building customer-facing web apps. We will build a dashboard that features:\r\n\r\n- an interactive Plotly chart\r\n- a drill-down with detailed information shown in  a second plot\r\n- a login\r\n- multiple pages and navigation\r\n\r\nWe will examine how hard or easy it is to implement each of these features and how interacting with them in the browser feels. At the end we will see if the reigning champion can defend their crown or if the ambitious contender takes the win.", "code": "WCDPLP", "state": "confirmed", "created": "2024-12-04", "social_card_image": "/static/media/social/talks/WCDPLP.png", "speaker_names": "Tilman Krokotsch", "speakers": "\n### Tilman Krokotsch\n\n\n", "track": "Django & Web"}, {"title": "Probably Fun: Board Games to teach Data Science", "abstract": "In this tutorial, you will speed-date with board and card games that can be used to teach Data Science. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\n\r\nAs a result, you will experience multiple ideas that you can use to make complex ideas more understandable and enjoyable. We would like to demonstrate how gamification can not only used to produce short puzzles and quizzes, but also as a tool to reason complex problem-solving strategies.\r\n\r\nWe will bring a set of carefully selected games that have been proven effective in teaching statistics, programming, machine learning and other Data Science skills. We also believe that it is probably fun to participate in this tutorial.", "full_description": "Games encourage people to put their brains to work in a focused, constructive and peaceful way. This makes games a fantastic tool in the classroom. Many board games contain sophisticated algorithms and statistical models right under the surface. Therefore, Data Science education can be boosted by playing carefully selected games.\r\n\r\nWe have applied popular board and card games such as Memory, Wizard, Machi Koro, Pandemic and Sky Team (the 2024 Game of the Year in Germany) to teach Data Science concepts in our courses. Learners would first play a game, discuss the mechanisms and only after that get exposed to the theory. Finally, they would move to practical applications using computers.\r\n\r\nThis game-driven approach provides learners with an intrinsic motivation to solve a real practical problem (succeeding at the game).\r\nAnalyzing a game makes it easier to grasp the core mechanism or algorithmic model and ask qualified questions about the details later.\r\nIt also makes sure learners will want to come back for the next class. We have documented practical lessons and made them available under a CC license on https://www.academis.eu/probably_fun/ .\r\n\r\nIn this tutorial, you will speed-date with several short games that can be used to teach Data Science concepts and skills. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\nThis way, you will experience multiple ideas you can use to make complex methods and ideas more accessible. Also, the tutorial is probably fun to participate in.\r\n\r\nThe tutorial will be executed according to the following pseudocode (or lesson plan):\r\n\r\n1. The presenters give a short introduction on why games matter (5 min)\r\n2. The presenters group participants into teams of up to 6 people.\r\n3. Each team is assigned to a game table with a game and a cheat sheet with instructions. The presenters facilitate with understanding rules and to remove other obstacles.\r\n4. The teams play the game for up to 15 minutes.\r\n5. The teams discuss 1-3 prepared reflection questions to make the transfer from the game to the data science concepts.\r\n6. Each team moves to the next table.\r\n7. Repeat for 3-4 rounds.\r\n8. Everybody gets together for a joint Q & A\r\n9. A QR-Code links to material with games that help learning Data Science and lesson plans", "code": "WELCVS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/media/social/talks/WELCVS.png", "speaker_names": "Paula Gonzalez Avalos, Dr. Kristian Rother", "speakers": "\n### Paula Gonzalez Avalos\n\nData Nerd & Python Pydata community lover. AI education specialist with five years of experience shaping data science and AI educational offers. Currently leading the AI Academy at the appliedAI Institute for Europe.\n\n### Dr. Kristian Rother\n\nKristian is a freelance Python trainer who wrote his first lines of Python in the year 11111001111. After a career writing software for life science research, he has been teaching Python, Data Analysis and Machine Learning throughout Europe since 2011. More recently, he has built data pipelines for the real estate and medical sector.\r\n\r\nKristian has translated 5 Python books and written 2 more himself, in addition to numerous teaching guides. Kristian has collected 364 stars on Advent of Code. His knowledge about async is, unfortunately, miserable. His favorite Python module is 're'. Kristian believes everybody can learn programming.\r\n\r\nYou can find Kristians teaching materials on https://www.academis.eu\n", "track": "Education, Career & Life"}, {"title": "The Mighty Dot - Customize Attribute Access with Descriptors", "abstract": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.", "full_description": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.\r\n\r\nIn this tutorial you will:\r\n\r\n* Learn how to use Python's descriptors to add new functionality to attribute access\r\n* Acquired solid background knowledge on how descriptors work\r\n* Work with practical examples for applying descriptors\r\n* Learn when to use a property or reach for a descriptor\r\n* Get to know how popular Python libraries apply descriptors for tasks such as\r\n  data structure access, REST-APIs, ORMs, and serialization", "code": "WJPEQH", "state": "confirmed", "created": "2024-12-18", "social_card_image": "/static/media/social/talks/WJPEQH.png", "speaker_names": "Mike M\u00fcller", "speakers": "\n### Mike M\u00fcller\n\nI've been a Python user since 1999, teaching Python professionally since 2004.\r\nI am also active in the community, organizing Python conferences such as\r\nPyCon DE, EuroSciPy, and BarCamps.\r\nI am a PSF Fellow, PSF Community Service Award winner,\r\nand chair of the German Python Software Verband.\n", "track": "Python Language & Ecosystem"}, {"title": "A11y Need Is Love (But Accessible Docs Help Too)", "abstract": "Accessible documentation benefits everyone, from developers to end users. Using the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/) as a case study, this talk dives into common accessibility barriers in documentation websites like low contrast colors, missing focus states, etc. and practical ways to address them. Learn about accessibility improvements and take part in a live accessibility audit to see how small changes can make a big difference.", "full_description": "The Beatles told us that \u2018all you need is love\u2019 and while that is a lovely sentiment, love alone won\u2019t fix low contrast colours, missing focus states or inaccessible navigation. These barriers impact countless users with disabilities, reducing the usefulness and reach of valuable documentation. So, while love is great, accessible docs are *essential*.\r\n\r\nIn this talk, we will use the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/) as a case study to explore common accessibility problems in documentation websites and how to tackle them. We will discuss the accessibility changes we made to the theme, how those changes affected users, and what we learnt along the way. Additionally, we will also conduct a short accessibility audit on a website suggested by the audience. This demo will provide a practical understanding of how to improve accessibility.\r\n\r\nWhether you\u2019re a documentation maintainer, a curious developer or simply someone who cares about accessibility, this beginner-friendly talk will help you learn more about accessibility in documentation and how to get started. Love might be a universal language, but your code appreciates accessible documentation.", "code": "WLZSEZ", "state": "confirmed", "created": "2024-12-18", "social_card_image": "/static/media/social/talks/WLZSEZ.png", "speaker_names": "Smera Goel", "speakers": "\n### Smera Goel\n\n\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Going Global: Taking code from research to operational open ecosystem for AI weather forecasting", "abstract": "When I was hired as a Scientist for Machine Learning, experts said ML would never work in weather forecasting. Nowadays, I get to contribute to Anemoi, a full-featured ML weather forecasting framework used by international weather agencies to research, build, and scale AI weather forecasting models. \r\n\r\nThe project started out as a curiosity by my colleagues and soon scaled as a result of its initial success. As machine learning stories go, this is a story of change, adaptation and making things work. \r\n\r\nIn this talk, I'll share some practical lessons: how we evolved from a mono-package with four people working on it to multiple open-source packages with 40+ internal and external collaborators. Specifically, how we managed the explosion of over 300 config options without losing all of our sanity, building a separation of packages that works for both researchers and operations teams, as well as CI/CD and testing that constrains how many bugs we can introduce in a given day. You'll learn concrete patterns for growing Python packaging for ML systems, and balancing research flexibility with production stability. As a bonus, I'll sprinkle in anecdotes where LLMs like chatGPT and Copilot massively failed at facilitating this evolution.\r\n\r\nJoin me for a deep dive into the real challenges of scaling ML systems - where the weather may be hard to predict, but our code doesn't have to be.", "full_description": "What does it take to go from \"ML will never work in weather forecasting\" to running AI models in production at weather agencies? This talk chronicles the journey of Anemoi, a framework that evolved from research code to an operational ML weather forecasting system - and the technical challenges we faced along the way.\r\n\r\nStarting as experimental code and notebooks by a small team of four, Anemoi grew into a robust ecosystem supporting 40+ developers across multiple international weather agencies. I'll share our experience of scaling both the team and codebase, including the interesting challenge of conducting weekly code tours for new team members while maintaining development velocity.\r\n\r\nThe technical evolution of Anemoi mirrors many challenges in scaling ML systems. We'll explore how the codebase transformed from research artifacts and notebooks into a structured mono-package with proper separation of concerns. Then, how we split this into an ecosystem of specialized packages - only to later realize that some components were too tightly coupled and needed reunification. This journey offers valuable lessons about when to split packages and when to maintain unified codebases. \r\n\r\nConfiguration management evolved alongside our architecture. I'll demonstrate how we leveraged Hydra to tame over 300 configuration options into a hierarchical system that enables component composition without sacrificing usability. This system now powers everything from dataset creation to model inference, with full traceability of configurations and artifacts throughout the ML lifecycle.\r\n\r\nA unique aspect of developing ML systems at ECMWF is integrating with decades of expertise in weather forecast validation. We'll look at how we connected modern ML tooling like MLFlow with traditional meteorological evaluation systems, creating a bridge between ML innovation and established meteorological practices.\r\n\r\nThe talk will cover practical challenges that every growing ML system faces:\r\n\r\n- Making model components truly configurable and replaceable\r\n- Implementing model sharding for global weather predictions\r\n- Supporting flexible grids for regional weather services\r\n- Managing CI/CD across multiple packages\r\n- Streamlining release processes with modern tools\r\n- The eternal struggle with changelog management\r\n\r\nThroughout the presentation, I'll share real examples of what worked, what didn't, and why - including our experiments with AI coding assistants and where they fell short. You'll walk away with concrete patterns for scaling Python ML systems, strategies for managing growing complexity, and insights into balancing research flexibility with production requirements.\r\n\r\nWhether you're scaling an ML system, managing a growing Python codebase, or interested in how weather forecasting is being transformed by AI, this talk offers practical lessons from the frontier of operational ML systems.", "code": "WMBDJ8", "state": "confirmed", "created": "2024-12-23", "social_card_image": "/static/media/social/talks/WMBDJ8.png", "speaker_names": "Jesper Dramsch", "speakers": "\n### Jesper Dramsch\n\nJesper Dramsch works at the intersection of machine learning and physical, real-world data. Currently, they're working as a scientist for machine learning in numerical weather prediction at the coordinated organisation ECMWF.\r\n\r\nJesper is a fellow of the Software Sustainability Institute, creating awareness and educational resources around the reproducibility of machine learning results in applied science. Before, they have worked on applied exploratory machine learning problems, e.g. satellites and Lidar imaging on trains, and defended a PhD in machine learning for geoscience. During the PhD, Jesper wrote multiple publications and often presented at workshops and conferences, eventually holding keynote presentations on the future of machine learning in geoscience.\r\n\r\nMoreover, they worked as consultant machine learning and Python educator in international companies and the UK government. They create educational notebooks on Kaggle applying ML to different domains, reaching rank 81 worldwide out of over 100,000 participants and their video courses on Skillshare have been watched over 128 days by over 4500 students.  Recently, Jesper was invited into the Youtube Partner programme creating videos around programming, machine learning, and tech.\n", "track": "MLOps & DevOps"}, {"title": "Graph Neural Networks for Collusion Detection using PyTorch and Deep Graph Library", "abstract": "Collusion is a complex phenomenon in which companies secretly collaborate to engage in fraudulent practices. This talk presents an innovative methodology for detecting and predicting collusion patterns in different national markets using neural networks (NNs) and graph neural networks (GNNs). GNNs are particularly well suited to this task because they can exploit the inherent network structures present in collusion and many other economic problems. In Python, we use PyTorch and the Deep Graph Library (DGL) to develop and train models on individual market datasets from Japan, the United States, two regions in Switzerland, Italy, and Brazil, focusing on predicting collusion in single markets. In our empirical study, we show that GNNs outperform NNs in detecting complex collusive patterns. This research contributes to the ongoing discourse on preventing collusion and optimizing detection methodologies, providing valuable guidance on the use of NNs and GNNs in economic applications to enhance market fairness and economic welfare.", "full_description": "Collusion detection is a critical process for identifying instances in which entities, such as companies or individuals, secretly collaborate to engage in fraudulent practices, often characterized by agreements on prices, market shares, or tactics to avoid competition. GNNs explicitly allow the incorporation of network structures, which are prevalent in collusion and key to improving the detection rate. This talk tackles the challenge of detecting fraud patterns using GNNs and highlights how these models can effectively learn network structures inherent in bidding markets.\r\nWe propose a fraud detection algorithm that uses relational graph convolutional networks (R-GCNs) to analyze the inherent network structures in bidding data. R-GCNs have the ability to assign different weights to different types of edges. By using this extension of GNNs, different types of relationships between the bids can be combined to generate richer node embeddings, making them more effective at detecting collusive behavior among companies participating in bids and tenders. We develop and train these models using the PyTorch framework and the Deep Graph Library (DGL), applying them to datasets from Japan, the United States, Switzerland, Italy, and Brazil. Our empirical findings show that GNNs outperform traditional NNs in identifying complex collusive patterns, offering a new solution to this fraud problem.\r\nFor data scientists, especially those working with network data, the detection of anomalous patterns in network structures is a critical challenge across different domains, e.g. financial fraud detection or social network analysis. Attendees will gain practical strategies for applying GNNs to classification tasks to detect patterns in network data and a deeper understanding of how graph embeddings can enhance fraud detection models.", "code": "WUH93P", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/WUH93P.png", "speaker_names": "Mara Mattes", "speakers": "\n### Mara Mattes\n\nI am a researcher and PhD candidate in Data Science and Economics at the Heinrich-Heine University D\u00fcsseldorf. My current research focuses on the application of machine learning and deep learning methods to economic problems.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Lessons learned in bringing a RAG chatbot with access to 50k+ diverse documents to production", "abstract": "Retrieval-Augmented Generation (RAG) chatbots are a key use case of GenAI in organizations, allowing users to conveniently access and query internal company data. A first RAG prototype can often be created in a matter of days. But why are the majority of prototypes still in the pilot stage? [\\[1\\]](https://www2.deloitte.com/content/dam/Deloitte/us/Documents/consulting/us-state-of-gen-ai-q3.pdf)\r\n\r\nIn this talk we share our insights from developing a production-grade chatbot at Merck. Our RAG chatbot for R&D experts accesses over 50,000 documents across numerous SharePoint sites and other sources. We identified three key success factors:\r\n1. Developing a robust data pipeline that syncs documents from source systems and that handles enterprise features such as replicating user permissions. \r\n2. Establishing a comprehensive evaluation framework with a clear optimization metric.\r\n3. Driving adoption through an onboarding training and ongoing user engagement, such as regular office hours.\r\n\r\nWe think that many of these lessons are broadly applicable to RAG chatbots, making this talk valuable for practitioners aiming to implement GenAI solutions in business contexts.", "full_description": "Building a prototype RAG chatbot with frameworks like LangChain can be straightforward. However, scaling it into a production-grade application introduces complex challenges. In this talk, we share our lessons learned from developing a RAG chatbot designed to assist research and development (R&D) experts.\r\n\r\nOur chatbot was developed to effectively handle and provide access to a large collection of unstructured knowledge, consisting of over 50,000 documents stored across more than 20 SharePoint sites and other sources. We faced significant hurdles in:\r\n- **Data Pipeline Engineering**: Crafting a modular and scalable pipeline capable of periodically syncing documents, handling dynamic user permissions, and efficiently processing large volumes of unstructured data.\r\n- **Evaluation Framework Development**: Implementing an effective testing strategy without the availability of static ground truth data. We employed automated testing with frameworks like pytest, utilized LLM-as-a-judge, and integrated tracing to iteratively refine our dataset and maintain high answer quality.\r\n- **RAG Design and Prompting Strategies**: Addressing challenges in document chunking, citation integration, reranking retrieved results, and applying permission and PII filters to ensure compliance and accuracy in responses.\r\n- **User Adoption**: Driving user adoption through onboarding training and ongoing engagement, such as regular office hours and feedback mechanisms.\r\n\r\nWe emphasize the importance of applying data science principles to GenAI projects:\r\n- **Start Simple and Iterate**: Begin with a basic implementation as a baseline and iteratively enhance functionality based on testing and user feedback.\r\n- **Test-Driven Development**: Identify key test scenarios early and use them to drive development, ensuring that improvements are measurable and aligned with growing user needs.\r\n- **Focus on Key Metrics**: Establish clear metrics to optimize against, aiding in making informed decisions throughout the development process.\r\n  \r\n**Main Takeaways for the Audience:**\r\n- Understand the critical role of robust, modular data pipelines in handling dynamic and unstructured data sources for LLM applications.\r\n- Learn strategies for developing effective evaluation frameworks in complex domains where traditional ground truth data may be lacking.\r\n- Gain insights into advanced RAG design techniques that enhance chatbot performance and reliability.\r\n- Recognize the substantial data engineering and software development efforts required to transition a prototype to a production-grade LLM solution.\r\n\r\nBy sharing our experiences, attendees will gain practical insights into deploying robust RAG chatbots, transforming a functional prototype into a reliable, scalable application that fulfills enterprise requirements.", "code": "XLZQFA", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/XLZQFA.png", "speaker_names": "Bernhard Sch\u00e4fer", "speakers": "\n### Bernhard Sch\u00e4fer\n\nBernhard is a Senior Data Scientist at Merck. For more information you can connect with him on LinkedIn. :-)\n", "track": "Generative AI"}, {"title": "The Foundation Model Revolution for Tabular Data", "abstract": "What if we could make the same revolutionary leap for tables that ChatGPT made for text? While foundation models have transformed how we work with text and images, tabular / structured data (spreadsheets and databases) - the backbone of economic and scientific analysis - has been left behind. TabPFN changes this. It's a foundation model that achieves in 2.8 seconds what traditional methods need 4 hours of hyperparameter tuning for - while delivering better results. On datasets up to 10,000 samples, it outperforms every existing Python library, from XGBoost to CatBoost to Autogluon.\r\n\r\nBeyond raw performance, TabPFN brings foundation model capabilities to tables: native handling of messy data without preprocessing, built-in uncertainty estimation, synthetic data generation, and transfer learning - all in a few lines of Python code. Whether you're building risk models, accelerating scientific research, or optimizing business decisions, TabPFN represents the next major transformation in how we analyze data. Join us to explore and learn how to leverage these new capabilities in your work.", "full_description": "TabPFN shows how foundation model concepts can advance tabular data analysis in Python. Born as research published at ICLR 2023, it found strong community adoption with 1,200+ GitHub stars and 100,000+ downloads. Our upcoming January 2025 release introduces major improvements in speed, scale and capabilities that we're excited to preview at PyCon.\r\n\r\n**Detailed Outline:**\r\n\r\n1. **Context & Evolution** (5 min)\r\n- The challenge of applying deep learning to tabular data\r\n- Learning from the foundation model revolution in text and vision\r\n- Key improvements from V0 to V1 based on community feedback\r\n- Real-world examples where TabPFN shines (and where it doesn't)\r\n\r\n2. **Technical Insights** (8 min)\r\n- How we adapted transformers for tabular data\r\n- Making in-context learning work for structured data\r\n- Performance characteristics and resource requirements\r\n- Understanding current limitations and constraints\r\n\r\n3. **Live Coding & Integration** (12 min)\r\n- Getting started with TabPFN in 3 lines of code\r\n- Handling real-world data challenges:\r\n  - Missing values and mixed data types\r\n  - Built-in uncertainty estimation\r\n  - Working with similar tasks efficiently\r\n- Integration with pandas, scikit-learn and the Python ecosystem\r\n\r\n4. **Practical Applications** (5 min)\r\n- When to choose TabPFN vs traditional methods\r\n- Resource requirements and scalability limits  \r\n- What's next for TabPFN\r\n- Q&A\r\n\r\n**Key Takeaways:**\r\n- Practical understanding of TabPFN's capabilities and limitations\r\n- Hands-on experience integrating with Python data science workflows\r\n- Best practices for working with foundation models on tabular data\r\n- Insight into emerging approaches for structured data analysis", "code": "XRHEYZ", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/media/social/talks/XRHEYZ.png", "speaker_names": "Noah Hollmann, Frank Hutter", "speakers": "\n### Noah Hollmann\n\n\n\n### Frank Hutter\n\nFrank is a Hector-Endowed Fellow and PI at the ELLIS Institute T\u00fcbingen and has been a full professor for Machine Learning at the University of Freiburg (Germany) since 2016. Previously, he has been an Emmy Noether Research Group Lead at the University of Freiburg since 2013. Before that, he did a PhD (2004-2009) and postdoc (2009-2013) at the University of British Columbia (UBC) in Canada. He received the 2010 CAIAC doctoral dissertation award for the best thesis in AI in Canada, as well as several best paper awards and prizes in international ML competitions. He is a Fellow of ELLIS and EurAI, Director of the ELLIS unit Freiburg, and the recipient of 3 ERC grants. Frank is best known for his research on automated machine learning (AutoML), including neural architecture search, efficient hyperparameter optimization, and meta-learning. He co-authored the first book on AutoML and the prominent AutoML tools Auto-WEKA, Auto-sklearn and Auto-PyTorch, won the first two AutoML challenges with his team, is co-teaching the first MOOC on AutoML, co-organized 15 AutoML-related workshops at ICML, NeurIPS and ICLR, and founded the AutoML conference as general chair in 2022. In recent years, his focus has been on the intersection of foundation models and AutoML, prominently including the first foundation model for tabular data, TabPFN.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Django's Dilemma: Balancing Simplicity with Scalability", "abstract": "Django's model-view-serializer approach works great for small apps, but as projects grow, you might face challenges like scattered database operations and APIs that are too closely linked to your database models. These issues can make unit testing and scaling harder. I'll share real-world examples of these problems and show how to refactor an app using ideas from Domain-Driven Design (DDD) and hexagonal architecture. We'll look at a before-and-after example to see how these changes can make your app easier to use and debug. Plus, we'll discuss when Django's simplicity is enough and when it's worth adopting a more structured approach. You'll leave with practical tips for transforming your Django projects into systems that can handle increased complexity.", "full_description": "Django's model-view-serializer framework is a powerful tool for building small to medium-sized applications quickly and efficiently. Its straightforward approach allows developers to create CRUD operations with minimal effort, making it an excellent choice for projects with clear and simple requirements. However, as applications grow in complexity and scale, developers often encounter several challenges that can hinder the application's maintainability and scalability.\r\nOne common issue is the scattering of database operations across the codebase. This pattern often leads to tightly coupled APIs and database models, making it difficult to isolate logic for individual testing and increasing the fragility of the application as it scales. As database logic becomes intertwined with business logic, maintaining and expanding the application becomes increasingly challenging.\r\nIn this talk, we will explore these common pitfalls through real-world examples and demonstrate how to refactor a Django application for improved scalability and maintainability. Using a webhook server app as a case study, we'll discuss how client-driven response customization can complicate the traditional model-view-serializer approach. We'll explore the limitations of handling complex serialization and response formatting within the view layer, which often results in bloated views that are hard to test and scale.\r\nTo address these challenges, we'll introduce concepts from Domain-Driven Design (DDD) and hexagonal architecture. By applying these principles, we can decouple the application's components, making it easier to manage and extend. We'll refactor the application using a builder pattern for payload formatting, ensuring that response structures are flexible yet maintainable. Additionally, we'll introduce an injected service layer and a repository layer, which separate concerns and allow for more targeted unit testing. This architecture not only improves testability but also enhances the application's ability to adapt to changing requirements.\r\nWe'll provide a clear before-and-after comparison, highlighting the benefits of this refactored approach. Attendees will learn how these changes can lead to cleaner, more scalable code that is easier to debug and maintain. We'll also discuss the balance between Django's simplicity and the need for more sophisticated architecture, helping developers decide when it's appropriate to adopt these patterns.\r\nBy the end of this session, attendees will gain practical insights into transforming their Django projects into robust systems capable of handling increased complexity. Whether you're a developer working on scaling an existing application or someone interested in learning more about advanced architectural patterns, this talk will provide valuable takeaways and actionable strategies for improving your Django applications.", "code": "YCAUMC", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/media/social/talks/YCAUMC.png", "speaker_names": "Anette Haferkorn", "speakers": "\n### Anette Haferkorn\n\nAs a seasoned Software Engineer with a strong focus on backend development, Anette is thriving at the intersection of data and scalable architecture.\r\n\r\nCurrently at Qonto, a leading FinTech unicorn, she engages deeply in feature development, scaling initiatives, and architectural improvements using an advanced tech stack including Golang, Ruby on Rails, and Python, complemented by distributed systems expertise with Kafka, AWS, Docker, and Terraform.\n", "track": "Django & Web"}, {"title": "The Forecast Whisperer: Secrets of Model Tuning Revealed", "abstract": "Forecasting can often feel like interpreting vague signals\u2014unclear yet full of potential. In this talk, we\u2019ll cover advanced techniques for tuning forecasting models in professional settings, moving beyond the basics to explore methods that enhance both accuracy and interpretability.\r\n\r\nYou\u2019ll learn:\r\n\r\nHow to set clear business goals for ML model tuning and align technical work with business needs, including balancing forecast granularity and accuracy and selecting statistically correct metric.\r\n\r\nPractical data preparation methods, including business-driven data cleaning and detecting data problems with statistical and buiness driven approaches.\r\n\r\nAdvanced feature selection techniques such as recursive feature elimination and SHAP values, alongside hyperparameter tuning strategies including Bayesian optimization and ensemble methods.\r\n\r\nHow generative AI can support model tuning by automating feature generation, hyperparameter search, and enhancing model explainability through SHAP and LIME techniques.\r\n\r\nReal-world case studies, including how Blue Yonder\u2019s data science team optimized demand forecasting models for retail and supply chain applications.\r\n\r\nWe'll also discuss common mistakes like overfitting and data leakage, best practices for reliable validation, and the importance of domain knowledge in successful forecasting. Whether you're a seasoned data scientist or exploring time series forecasting, you'll gain advanced insights and techniques you can apply immediately.", "full_description": "Forecasting can often feel like trying to make sense of unclear patterns\u2014difficult to interpret but rich with potential. This talk clarifies the process, focusing on actionable steps for tuning forecasting models in professional environments where accuracy and performance drive business outcomes.\r\n\r\n1. Defining Clear Business Objectives:\r\n\r\nImportance of aligning machine learning efforts with tangible business goals.\r\nScoping forecasting problems and selecting appropriate success metrics.\r\n\r\n2. Data Preparation Techniques:\r\n\r\nCleaning data with a focus on business relevance and systematically enriching it\r\nIn addition, we show how to tune the model by tuning the data nad the corresponding feature engineering. \r\n\r\n3. Feature Selection and Hyperparameter Tuning:\r\n\r\nAdvanced feature selection strategies and their impact on model performance.\r\nTechniques for identifying impactful features.\r\nBest practices for hyperparameter tuning and optimization strategies.\r\n\r\n4. The Role of interpretability and Generative AI in Model Tuning:\r\n\r\nAutomating feature generation.\r\nHyperparameter optimization techniques using generative AI.\r\nmodel tuning through model interpretation\r\n\r\n5. Real-World Applications and Case Studies:\r\n\r\nHow Blue Yonder improved retail forecast accuracy.\r\nLessons learned from industry case studies.\r\n\r\n6. Common Pitfalls and Best Practices:\r\n\r\nTypical mistakes made during model tuning.\r\n\r\nBest practices for ensuring model reliability and relevance.\r\nThe importance of domain knowledge in successful forecasting.\r\n\r\nConclusion:\r\nWhether you are a seasoned data scientist or just starting your forecasting journey, this session will provide you with actionable insights to fine-tune your forecasting models effectively. Expect practical techniques, real-world examples, and expert tips that you can apply immediately. Join us and learn how better forecasts lead to better business decisions.", "code": "YLKDJK", "state": "confirmed", "created": "2025-01-03", "social_card_image": "/static/media/social/talks/YLKDJK.png", "speaker_names": "Illia Babounikau", "speakers": "\n### Illia Babounikau\n\nDr. Illia Babounikau is an accomplished data scientist with extensive expertise in machine learning and forecasting. He holds a Ph.D. in Physics from Hamburg University and initially pursued an academic career, focusing on large-scale data analysis and machine learning applications. His contributions have been instrumental in international scientific collaborations, including the CMS experiment at CERN\u2019s Large Hadron Collider and the COMET project at J-PARC.\r\n\r\nFor the past five years, Dr. Babounikau has been a Data Scientist at Blue Yonder, specializing in developing and fine-tuning advanced forecasting models for retail planning and inventory management. He leads the design and implementation of tailored machine-learning solutions, addressing complex challenges within supply chains across diverse industries.\r\n\r\nDr. Babounikau is passionate about bridging the gap between data science and business strategy, ensuring machine learning models are aligned with business objectives to drive data-informed decision-making.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Design, Generate, Deploy:  Contract-First with FastAPI", "abstract": "This talk explores a contract-first approach to API development using the OpenAPI generator, a powerful tool for automating API generation from a standardized specification.  We will cover (1) what would you need to run to have a standard implementation of the FastAPI endpoints and data models; (2) how to customize the mustache templates that are used to generate the API stubs; (3) share some ideas how to customize the CLI and (4) how to maintain the contract and how to handle breaking changes to the contract. We will close the session with a discussion of the challenges of implementing the OpenAPI generator.", "full_description": "Let me share a story with you about two developers working at a Malt, Europe's leading freelance management system & marketplace. \r\n\r\nDev-1: Hi there! We have an issue on production. It seems that a request was sent where \u201ccompany id\u201d is not given. \r\nDev-2: Oops! But I thought we agreed on an anonymous mode?\r\nDev-1: That\u2019s actually a great idea. You mean that company id is not required? \r\nDev-2: Exactly!\r\nDev-1: Thanks! I will update the data model and push the changes! \r\n\r\nAs the conversation above suggests, sending data between two applications can easily fail if the requirements are not defined up front. Even for simple requests a lot of decisions have to be made: are the fields optional or mandatory? What about the returned payloads and their data types? Do we need default values? If we are not clear what we will expect (from the request) and what we will return (in the response), in the worst case, the request will fail and we spend time debugging, like above.  \r\n\r\nTo overcome this issue, we decided to move to a contract-first approach, where we define the exact request and response and generate the endpoints and data models from there using the OpenAPI generator. The OpenAPI generator is a powerful tool that allows you to automatically generate API client libraries, server stubs, documentation, and configuration from an OpenAPI specification, or a \u201ccontract\u201d between two applications. This contract forms the basis for generating the endpoint stubs for our python applications but also for the client models and code. Starting with the contract can significantly speed up the development process and improve the consistency of your API implementations.\r\n\r\nDuring this talk we will address the following topics: \r\n- The vanilla implementation that generates endpoints and data models: what would you need to run to have a first version of the FastAPI endpoints. If the setting allows for it, we would show a short demonstration. \r\n- How to use customisable templates: we customised the mustache templates that generated the endpoints and data models so we could generate our custom FastAPI app. Also we added examples to the generated data models as these were not available in the default implementation. \r\n- How to customise the CLI tool and ideas for setting up your CI pipeline: we will share some ideas how to customise the CLI and how we used it in our CI pipeline to prevent discrepancies between the contract and the generated stubs.\r\n- how to maintain the contract and how to handle breaking changes to the contract\r\nWe will close the session with a discussion of the challenges and benefits of implementing the OpenAPI Generator. While it offers standardisation and best practices, it can introduce additional complexity, especially with the tool still in beta. We'll share our experiences navigating this trade-off.", "code": "ZACM3E", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/ZACM3E.png", "speaker_names": "Dr. Evelyne Groen, Kateryna Budzyak", "speakers": "\n### Dr. Evelyne Groen\n\nHello! My name is Evelyne Groen and I am a Senior Machine Learning engineer at Malt. A long time ago I studied physics in Amsterdam, after which I moved to Berlin to discover the world of data science. Currently I'm working at Malt as a machine learning engineer exploring the boundaries between devops and data.\n\n### Kateryna Budzyak\n\nKat is Machine Learning Engineer at Malt, the freelancer marketplace, where she works in the relevancy and matching team. She has a background in bioinformatics and passionate about beautiful code.\n", "track": "MLOps & DevOps"}, {"title": "Information Retrieval Without Feeling Lucky: The Art and Science of Search", "abstract": "Search is everywhere, yet effective Information Retrieval remains one of the most underestimated challenges in modern technology. While Retrieval-Augmented Generation has captured significant attention, the foundational element - Information Retrieval - often remains underexplored. \r\n\r\nIn this talk, we put Information Retrieval center stage by asking: \r\nHow do we know that user queries and data 'speak' the same language?\r\nHow do we evaluate the relevance and completeness of search results? And how do we prioritize what gets displayed? Or do we even want to hide specific content?\r\n\r\nWe try to answer these questions by introducing the audience to the art and science of Information Retrieval, exploring metrics such as precision, recall, and desirability. We\u2019ll examine key challenges, including ambiguity, query relaxation, and the interplay between sparse and dense search techniques. Through a live demo using public content from Sendung mit der Maus, we show how hybrid search improves upon vector and keyword based search in isolation.", "full_description": "Information Retrieval goes beyond keyword matching - it\u2019s about intent, context, and delivering relevant and accurate results. As RAG applications gain traction, understanding the retrieval process becomes more crucial for developers, data scientists, and search engineers.\r\n\r\nWe start with the Why. People have different needs for search - lookup, research, and inspiration. Each of these needs can be influenced and affected by the key IR metrics of search engines: precision, recall, and desirability. Having introduced these fundamentals, we go into common retrieval challenges, such as ambiguity, mismatched vocabularies, and the impact of context.\r\n\r\nAiming to solve these challenges, we then go into advanced search techniques, comparing sparse (keyword-based) and dense (vector-based) retrieval, highlighting their strengths and limitations. We\u2019ll explore hybrid search as a powerful approach that blends these techniques. In a live demo, using crawled data from the Sendung mit der Maus, we\u2019ll showcase a hybrid search setup leveraging tools like Mistral, Elasticsearch, and Streamlit. While the dataset language is German, the core concepts and search dynamics should hopefully be easily understandable also for non native speakers.\r\n\r\nThe talk concludes with key takeaways on building effective search systems and a look ahead at future developments in contextualized search.\r\n\r\nTentative Outline:\r\n1. Introduction to Information Retrieval (~ 5 min)\r\n\u00a0 * Why do we search? Lookup, research, inspiration\r\n\u00a0 * Core metrics: precision, recall, desirability\r\n\r\n2. Challenges in Search and Retrieval (~ 5 min)\r\n\u00a0 * Ambiguity\r\n\u00a0 * Discrepancy in query and content \r\n\u00a0 * The impact of context \r\n\r\n3. Search Techniques (~ 10 min)\r\n\u00a0 * Sparse vs dense retrieval: comparing keyword and vector search (semantic search, embeddings, synsets, decompounders)\r\n\u00a0 * Hybrid search: Combining sparse and dense approaches\r\n\r\n4. Hybrid Search in Action (< 10 min)\r\n\u00a0 * Setting up a hybrid search with Mistral, Elasticsearch, and Streamlit\r\n\u00a0 * Live Demo: exploring search in Lach- & Sachgeschichten from Sendung mit der Maus\r\n\r\n5. Takeaways & Outlook (< 5 min)\r\n* hybrid search systems combine semantics, precision and explainability\r\n* contextualized search\r\n\r\nThe talk is directed at anyone interested in building or improving search systems. Attendees will gain a deeper understanding of the tools, methodologies, and metrics essential for building robust and explainable search systems.", "code": "ZHT9HW", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/media/social/talks/ZHT9HW.png", "speaker_names": "Anja Pilz", "speakers": "\n### Anja Pilz\n\nI received my PhD in Machine Learning (ML) and Natural Language Processing (NLP) from the University of Bonn and Fraunhofer IAIS where I was member of the Text Mining group. Now I work on AI and data driven products, mostly focused on applications in the medical and healthcare domain.\r\nMy main passion is in NLP, especially for the German language, and Information Retrieval (IR). Sometimes I build Recommender Systems.\n", "track": "Others"}, {"title": "expectation: A modern take on statistical A/B testing with e-values and martingales", "abstract": "This talk introduces a novel Python library for statistical testing using e-values, offering a refreshing alternative to traditional p-values. We'll explore how this approach enables real-time sequential testing, allowing data scientists to monitor experiments continuously without the statistical penalties of repeated testing. Through practical examples, we'll demonstrate how e-values provide more intuitive evidence measures and enable flexible stopping rules in A/B testing, clinical trials, and anomaly detection. The library implements cutting-edge methods from game-theoretic probability, making advanced sequential testing accessible to Python practitioners. Whether you're conducting A/B tests, monitoring production models, or running clinical trials, this talk will equip you with powerful new tools for sequential data analysis.", "full_description": "Modern data science demands flexible statistical methods that can handle sequential data analysis and continuous monitoring. Traditional p-values, while widely used, have limitations when dealing with sequential testing scenarios. This talk introduces a Python library that implements e-values and e-processes, offering a more natural approach to measuring statistical evidence and enabling true sequential testing.\r\n\r\nOutline:\r\n\r\n1. Introduction to E-Values\r\n   - The limitations of p-values in sequential testing\r\n   - What are e-values and why do they matter?\r\n   - Key advantages: intuitive interpretation, composability, and sequential validity\r\n   - Real-world applications: A/B testing, clinical trials, anomaly detection\r\n\r\n2. Library Overview and Core Features\r\n   - Architecture and design philosophy\r\n   - Key components:\r\n     - Sequential testing framework\r\n     - Confidence sequences\r\n     - Standard (classical) testing with e-values\r\n     - Conformal e-testing\r\n   - Integration with existing Python data science tools\r\n   - Live coding demonstrations of basic usage\r\n\r\n3. Practical Applications\r\n   - A/B testing with continuous monitoring\r\n   - Sequential hypothesis testing\r\n   - Adaptive experimental design\r\n   - Real-time anomaly detection\r\n   - Live demonstrations with real-world datasets\r\n\r\n4. Advanced Features and Extensions\r\n   - Conformal prediction integration\r\n   - CUSUM procedures for change detection\r\n   - Adaptive thresholding\r\n   - Power analysis for e-value tests\r\n   - Performance considerations and optimizations\r\n\r\n5. Best Practices and Guidelines\r\n   - When to use e-values vs. traditional methods\r\n   - Setting up monitoring frameworks\r\n   - Common pitfalls and how to avoid them\r\n   - Tips for production deployment\r\n\r\n6. Future Directions and Community Involvement \r\n   - Roadmap for future development\r\n   - How to contribute\r\n   - Resources for learning more\r\n\r\nThe talk will include interactive demonstrations and real-world examples, making complex statistical concepts accessible to practitioners. Attendees will leave with:\r\n- Understanding of e-values and their advantages\r\n- Practical knowledge of implementing sequential testing\r\n- Hands-on experience with the library\r\n- Best practices for production deployment\r\n- Resources for further learning", "code": "ZKNTGN", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/media/social/talks/ZKNTGN.png", "speaker_names": "Rostami Jako", "speakers": "\n### Rostami Jako\n\nI am a Machine Learning Engineer at H&M Group, former Data Scientist at Lidl Sweden, as a professional I am designing Machine Learning services, extracting insights and arranging meaningful stories for my clients by conducting high-quality modeling, engineering, data mining and analytics. \r\n\r\nI have a Bachelor degree in Statistics and Probability theory from Uppsala University of Sweden. Because I am a Statistician at core I have good experience with Data Sciencr, Python, R, time series modeling, simulations, machine learning algorithms, SQL, Excel, Spark and database technologies, as well as good communication skills. \r\n\r\nYou\u2019ll find two comprehensive Python libraries I have open-sourced. One is based on an emerging modern statistical hypothesis testing framework using e-values and martingales based on game-theoretic statistics. The other is for computational Supply Chain and Logistics. The first one is called \u2019expectation\u2019 and the second one is called \u2019supplyseer\u2019 and you can find both on my GitHub.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Death by a Thousand API Versions", "abstract": "API versioning is tough, really tough. We tried multiple approaches to versioning in production and eventually ended up with a solution we love. During this talk you will look into the tradeoffs of the most popular ways to do API versioning, and I will recommend which ones are fit for which products and companies. I will also present my framework, Cadwyn, that allows you to support hundreds of API versions with ease -- based on FastAPI and inspired by Stripe's approach to API versioning.\r\n\r\nAfter this session, you will understand which approach to pick for your company to make your versioning cost effective and maintainable without investing too much into it.", "full_description": "Web API Versioning is a way to allow your developers to move quickly and break things while your clients enjoy the stable API in long cycles. It is best practice for any API-first company to have API Versioning in one way or another. Otherwise, the company will either be unable to improve their API or their clients will have their integrations broken every few months.\r\n\r\nAt Monite, we spent a great deal of time trying to make API versioning work. We went through multiple horrifying solutions until we reached something we are proud of. I'll share just how expensive API versioning can get if done incorrectly.\r\n\r\nI'll cover all sorts of approaches you can pick to add incompatible features to your API: extremely stable and expensive, easy-looking but horrible in practice, and even completely version-less yet viable. I will provide you with the best practices of how you could find or implement a modern API versioning solution and will discuss the versioning at Stripe and Monite in great detail.\r\n\r\nWhen you leave, you'll have enough information to make your API Versioning user-friendly without overburdening your developers.", "code": "ZMKJAY", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/media/social/talks/ZMKJAY.png", "speaker_names": "Stanislav Zmiev", "speakers": "\n### Stanislav Zmiev\n\nExperienced platform engineer and architect with a passion for open source and developer tools. The author of Cadwyn -- a sophisticated API Versioning framework based on FastAPI. A contributor to numerous projects such as CPython and tortoise-orm. Currently building the future of finance at Monite.\n", "track": "Django & Web"}, {"title": "From Rules to Reality: Python's Role in Shaping Roundnet", "abstract": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community. However, like many emerging sports, it faces challenges in balancing competition, optimizing rules, and increasing accessibility for both players and spectators. This is where Python and data analysis come into play.\r\n\r\nIn this talk, I'll share insights from my role as Data Lead on the International Roundnet rule committee, where we use Python-powered data analysis to make informed decisions about the future of the sport. We'll explore how analyzing gameplay patterns and testing rule changes with simulation can lead to fairer, more exciting games and attract a broader audience.", "full_description": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community.  But what's truly unique about Roundnet is the opportunity it offers: as a new and emerging sport, we have the rare chance to shape its global rule changes entirely through data analysis. This is a groundbreaking approach \u2013 a first for any sport in the modern era.\r\n\r\nIn this talk, I\u2019ll share insights from my role as Data Lead on the International Roundnet Rule Committee and take you through how we are leveraging Python and data analysis to guide these changes. Over the past year, we\u2019ve collected rule proposals and have set up a series of experiments designed to test their effects. Using Python and statistical modeling, we\u2019re planning to select key tournaments worldwide this year to observe how rule adjustments impact gameplay data.\r\n\r\nOur ultimate goal is to discover if specific combinations of rule changes can make Roundnet fairer, more exciting, and accessible for players and spectators alike. This journey is an exploration of how data-driven decision-making can transform a sport from the ground up, using real-world insights and experimentation.\r\n\r\nThis talk will take you on the journey of how we set up our testing framework, what tools we\u2019re using, and how we\u2019ve employed Python-powered analysis to bring empirical evidence into the decision-making process. It will equip you with a new perspective on data's role in shaping real-world change \u2013 especially in grassroots movements, community building, and sports.", "code": "ZT3MGL", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/media/social/talks/ZT3MGL.png", "speaker_names": "Larissa Haas", "speakers": "\n### Larissa Haas\n\n\n", "track": "Data Handling & Engineering"}]}