{"talks": [{"title": "A mind of metal, born of human thought.  A consciousness confined, a digital soul.", "abstract": "AI DevOps combines artificial intelligence and DevOps practices to revolutionize software development and delivery. By automating tasks, predicting failures, and optimizing resource allocation, AI DevOps significantly improves efficiency, reduces errors, and accelerates time-to-market. This talk will explore the key concepts, benefits, and challenges of AI DevOps, showcasing real-world use cases and best practices for successful implementation.", "full_description": "AI DevOps: Automating the Future of Software Development\r\n\r\nAI DevOps is a transformative approach that leverages AI to streamline and optimize the entire software development lifecycle. This talk will explore the key concepts and benefits of AI DevOps, focusing on three main areas:\r\n\r\n1. AI-Driven Automation (10 minutes):\r\n\r\n**Automated testing and debugging using machine learning\r\n**Intelligent code completion and suggestion tools\r\n**Predictive analytics for proactive maintenance\r\n\r\n2. Continuous Integration and Continuous Delivery (CI/CD) with AI (10 minutes):\r\n\r\n**AI-powered pipeline optimization and self-healing\r\n**Automated deployment and rollback strategies\r\n**Real-time monitoring and anomaly detection\r\n\r\n3. AI-Enhanced Collaboration and Decision-Making (10 minutes):\r\n\r\n**Collaborative platforms with AI-powered insights\r\n**AI-assisted decision support tools for strategic planning\r\n**Ethical considerations and responsible AI in DevOps", "code": "8JF8FY", "state": "accepted", "created": "2024-11-19", "speaker_names": "Christopher Schultz", "track": "PyCon: MLOps & DevOps"}, {"title": "QuerySet.explain(): make it make sense.", "abstract": "We have tools that show what queries are executed and the time it takes. But what next? What is going on there? Is it good that it's doing that? Will some indexes help?\r\nIn this talk, I will help you decipher database query plans and give some rules of thumb to understand if the database is doing the best it can. We will also use query plans to find the best indexes for queries. I will also share several anti-patterns I have seen in  Django projects and show how to rewrite them in a database-friendly way.", "full_description": "We have tools that show what queries are executed and the time it takes. But what next? What is going on there? Is it good that it's doing that? Will some indexes help?\r\nIn this talk, I will help you decipher database query plans and give some rules of thumb to understand if the database is doing the best it can. We will also use query plans to find the best indexes for queries. I will also share several anti-patterns I have seen in  Django projects and show how to rewrite them in a database-friendly way.", "code": "S7JQTG", "state": "withdrawn", "created": "2024-11-27", "speaker_names": "Aivars Kalv\u0101ns", "track": "PyCon: Django & Web"}, {"title": "How to solve a Python mystery", "abstract": "Has any of your Python applications become unresponsive? Has it deadlocked or is it busy doing something? How can you know?\r\nIn this talk, I will introduce useful Linux performance and observability tools that can be used to understand which files, connections, and OS system calls your application is performing and share real-world mysteries that it helped to solve.", "full_description": "Has any of your Python applications become unresponsive? Has it deadlocked or is it busy doing something? How can you know?\r\nWhen troubleshooting some Python applications you often don\u2019t have time to analyze the source code and to understand how exactly it is working. Instead, you have to act fast and treat the application as a black box. In this talk, I will introduce useful Linux performance and observability tools that can be used to understand which files, connections, and OS system calls your application is performing. I will also share real-world mysteries that this approach has helped to solve.", "code": "ZMX3JT", "state": "withdrawn", "created": "2024-11-27", "speaker_names": "Aivars Kalv\u0101ns", "track": "PyCon: Programming & Software Engineering"}, {"title": "Bike Map: Computer Vision for Trip Routing", "abstract": "This talk introduces a computer vision-powered system for optimizing bike routes in urban areas like Berlin. By leveraging advanced models such as Mask2Former for semantic segmentation and DINOv2 for feature extraction, the system detects and categorizes bike lanes by safety and identifies cobblestone streets. It integrates these insights into a routing algorithm that prioritizes safe, smooth, and comfortable paths for cyclists. Learn how AI transforms urban cycling by combining safety-focused analysis with practical route planning tools.", "full_description": "Urban cycling safety and comfort are critical for encouraging sustainable transportation, yet existing routing tools often overlook these factors. This talk delves into the development of a computer vision-driven system to address this gap. Using street view imagery, the system detects and classifies bike lanes by safety, identifies cobblestone streets, and integrates these insights into a routing algorithm to create the safest and most comfortable bike paths. Attendees will explore the pipeline in detail, from using advanced models like Mask2Former for semantic segmentation and DINOv2 for feature extraction, to clustering techniques for safety assessment and binary classification for road surface detection. This session is ideal for AI enthusiasts, urban planners, and cycling advocates seeking to understand how AI can enhance urban mobility.", "code": "T3QJHM", "state": "submitted", "created": "2024-11-27", "speaker_names": "Antonio Rueda-Toicen", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Streaming at 30,000 Feet: A Real-Time Journey from APIs to Stream Processing", "abstract": "Traditional API architectures face significant challenges in environments where repetitive and frequent requests are required to retrieve data updates. These request-response mechanisms introduce latency, as clients must continually query the server to check for changes, often receiving redundant or outdated information. This approach leads to increased network overhead, inefficient use of server resources and diminished scalability as the number of clients or requests grows. Additionally, frequent requests expand the attack surface, requiring security measures to mitigate risks such as (un-)authorised access, rate limiting and query sanitisation. Managing all of these inherent problem results in increasingly complex systems to maintain and improve while putting considerable implementation effort onto the customer.\r\nJoin to find out how transitioning to a streaming architecture can address these issues by providing proactive, event-based data delivery, reducing latency, minimising redundant processing, enhancing scalability and simplifying security management.", "full_description": "In this talk we will go over which benefits, drawbacks and lessons Airbus has encountered/learned in the switch from an API to a Python based Stream Architecture for continuous flight traffic prediction. The Goal is to highlight Stream based architectures as a architectural alternative and allow the attendees to decide if it could be a alternative to their current API based Setup worthwhile to look into.\r\n\r\nThe talk addresses the inefficiencies and limitations of traditional API-based architectures for real-time data delivery. Specifically, it explores challenges such as high latency, network overhead, customer effort, and scalability issues when APIs rely on polling mechanisms. These issues became apparent at Airbus during a project as customer needs evolved, highlighting the shortcomings of APIs in handling real-time updates effectively. This story of the project will aid as an example on how to identify a limiting architectural decision and what pain points can potentially be avoided by taking a new route guiding us along the talk.\r\n\r\nFor developers and architects building modern, data-driven applications, choosing the right architecture is critical. Many face similar challenges when scaling APIs for real-time use cases, such as IoT, financial data, or notifications. This problem is relevant because adopting an unsuitable architecture can lead to poor performance, higher costs, and frustrated users.\r\n\r\nThe proposed solution is to transition from an API-based architecture to a streaming architecture for real-time data delivery. This involves leveraging stream processing systems that push updates proactively, handle high-throughput data efficiently, and offer features like backpressure, partitioning, and stateful processing. Recent developments in Python based tools such as **Bytewax**, **Faust** and **Quix** are highlighted for their scalability and fault-tolerance capabilities.\r\n\r\n### Key Takeaways\r\n1. **Challenges of APIs**: Polling APIs is inefficient for real-time updates, leading to delays, resource wastage, and customer dissatisfaction.\r\n2. **Advantages of Streaming**: Streaming architectures offer real-time data delivery, lower latency, reduced customer effort, better scalability, and improved fault tolerance.\r\n3. **Key Streaming Concepts**: Understanding backpressure, partitioning, and stateful processing is essential for understanding streaming specific limitations and solutions.\r\n4. **Architectural Considerations**: Streaming is ideal for use cases where data changes frequently and needs to be delivered in real time, while APIs may still be suitable for low-frequency, static, or manual queries.\r\n5. **Strategic Transition**: Adopting a streaming approach requires a paradigm shift in thinking about how data is delivered and processed, with significant changes to the system architecture which needs to be cautiously managed.", "code": "CTUEJX", "state": "submitted", "created": "2024-11-27", "speaker_names": "Felix Leon Buck", "track": "PyCon: Programming & Software Engineering"}, {"title": "Effortless Testing in Resource-Intensive Data Applications", "abstract": "Testing data-driven applications often involves significant overhead due to the setup and teardown of resource-intensive systems like databases and cloud services. This talk will explore practical strategies for designing integration and system tests with **pytest** to minimize these inefficiencies. By leveraging techniques to decouple tests while maximizing resource reuse, you\u2019ll learn how to enhance your testing process, improving performance, scalability, and test isolation in your own projects.", "full_description": "Fast and reliable tests are the backbone of maintaining quality and performance in data-heavy applications, where even small issues can cascade into significant bottlenecks or failures. A streamlined testing process allows developers to confidently evolve complex systems while ensuring robustness and efficiency.\r\n\r\nFor integration and system tests in data-driven applications, the setup and teardown of data layers (e.g., databases, blob storage) often take more time than the actual test execution. A common approach is to use session-scoped test resources, but this introduces shared dependencies that couple tests together. As a result, developers must carefully manage resource cleanup after each test to prevent interference, leading to tests that are often flaky, verbose, and harder to maintain.\r\n\r\nThis talk will demonstrate how to leverage pytest fixtures to abstract setup and teardown logic across both session and function scopes. This method ensures proper resource cleanup, improves test isolation, and enhances test readability and development efficiency. We\u2019ll also explore strategies for parallelizing test execution while sharing system resources effectively, enabling faster and more scalable testing.\r\n\r\nThis talk is designed for intermediate to advanced Python developers with a strong interest in testing and improving their development workflows.", "code": "BERGKY", "state": "submitted", "created": "2024-11-28", "speaker_names": "Patrik Hlobil", "track": "PyCon: Testing"}, {"title": "Feature or Preprocessing Step? How to Correctly Set a Baseline in NLP Classification Tasks", "abstract": "In the field of Natural Language Processing (NLP), establishing a robust baseline is critical for the reliable evaluation and comparison of models. However, many NLP studies suffer from sloppy baseline models and improper preprocessing techniques, leading to flawed results. This presentation addresses key issues in setting effective baselines, focusing on the common pitfalls in research, such as the misuse of preprocessing steps and the confusion between preprocessing and feature engineering. The session will define the roles of preprocessing and features, provide a step-by-step guide for setting baselines with traditional machine learning models, and discuss the importance of hyperparameter tuning. We will also explore the fair comparison of traditional and advanced models, offering insights into appropriate preprocessing techniques for both. By the end, attendees will understand best practices for preprocessing, feature engineering, and baseline setting, enabling them to conduct more reliable and comparable NLP research. This talk is aimed at researchers, data scientists, and practitioners in NLP and machine learning looking to enhance their methodological rigor.", "full_description": "OBJECTIVES:\r\n\r\n- Identify Common Issues in Baseline Models: Highlight the prevalent issues found in numerous NLP papers, such as sloppy baseline models and the misuse of preprocessing techniques across different model types.\r\n- Distinguish Between Preprocessing and Feature Engineering: Define what constitutes a preprocessing step versus a feature, and explain their roles in setting a baseline.\r\n- Establishing a Robust Baseline: Provide a step-by-step guide on how to correctly set a baseline for traditional ML models and compare it to more sophisticated models.\r\n\r\n\r\nOUTLINE:\r\n\r\n1. Introduction\r\n- Importance of a robust baseline in NLP classification tasks.\r\n- Overview of common pitfalls in current research.\r\n\r\n2. Identifying Sloppy Baselines\r\n- Examples of inadequate preprocessing for traditional ML models.\r\n- Consequences of improper preprocessing: deleting or generating features, inappropriate tokenisation, and not leveraging Scikit-learn capabilities.\r\n\r\n3. Distinguishing Preprocessing Steps from Features\r\n- Acceptable preprocessing steps for a baseline model: lower-casing, appropriate tokenisation, deleting single-appearance tokens.\r\n- The necessity of experimenting with preprocessing techniques.\r\n\r\n4. Setting a Baseline: A Detailed Approach\r\n- Data splitting: training, validation, and test sets.\r\n- Baseline with traditional ML models: using Naive Bayes with basic preprocessing.\r\n- The importance of hyperparameter tuning for vectorisers.\r\n\r\n5. Algorithm Selection and Optimisation\r\n- Pipeline for algorithm selection and feature engineering.\r\n- Examples of preprocessing and feature engineering techniques: normalisation, handling noise, replacing slang, stemming, etc.\r\n\r\n6. Comparing Traditional and Advanced Models\r\n - Fair comparison to advanced models (neural networks, transformers).\r\n - Appropriate preprocessing for advanced models: minimal preprocessing (i.e. only lowercasing and deleting noise), avoiding lemmatization/stemming, appropriate tokenisation.\r\n\r\n7. Tips and Best Practices for Advanced Models\r\n- Handling unknown words and dummy tokens.\r\n- Dealing with non-generalisable information (emails, paths, names).\r\n- Enhancing embeddings with additional data.\r\n\r\n8. Conclusion\r\n- Recap of best practices for setting a robust baseline.\r\n- Encouragement to adopt these practices for more reliable and comparable NLP research outcomes.\r\n\r\n\r\nEXPECTED OUTCOMES:\r\n\r\nAttendees will gain a comprehensive understanding of the importance of setting a robust baseline in NLP classification tasks. They will learn practical techniques to improve their preprocessing steps and distinguish between preprocessing and feature engineering. This knowledge will help them conduct more reliable experiments and produce more credible results in their research.\r\n\r\n\r\nTARGET AUDIENCE:\r\n\r\nResearchers, data scientists, and practitioners in the field of NLP and machine learning who are involved in model development and evaluation. This talk is especially relevant for those looking to improve their methodological rigor in establishing baselines and preprocessing practices.", "code": "UVSVCY", "state": "submitted", "created": "2024-11-28", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Decoding Topics: A Comparative Analysis of Python\u2019s Leading Topic Modeling Libraries Using Climate C", "abstract": "Topic modelling has come a long way, evolving from traditional statistical methods to leveraging advanced embeddings and neural networks. Python\u2019s diverse library ecosystem includes tools like Latent Dirichlet Allocation (LDA) using gensim, Top2Vec, BERTopic, and Contextualized Topic Models (CTM). This talk evaluates these popular approaches using a dataset of UK climate change policies, considering use cases relevant to organisations like DEFRA (Department for Environment, Food & Rural Affairs). The analysis explores real-time integration, dynamic topic modelling over time, adding new documents, and retrieving similar ones. Attendees will learn the strengths, limitations, and practical applications of each library to make informed decisions for their projects.", "full_description": "Objectives:\r\nThe session aims to:\r\n\r\n1.  Compare Python-based topic modelling libraries, highlighting their relevance to real-world scenarios like policy analysis.\r\n2.    Explore practical use cases, including real-time document integration, tracking topic evolution, and finding similar documents.\r\n3.    Evaluate the tools based on performance, interpretability, scalability, and flexibility, with a focus on climate change policy data presented by [1] focusing on adaptation and mitigation.\r\n4.    Provide actionable guidance on selecting the right library for different project needs and datasets.\r\n\r\nOutline:\r\n\r\n1. Introduction to Topic Modeling: Overview of traditional and modern approaches, including their practical significance.\r\n\r\n2. Algorithms & Libraries Overview: LDA (gensim) [2], CTM [3], Top2Vec [4], BERTopic [5]\r\n\r\n3. Dataset and Use Cases:\r\n      - Overview of the UK climate change policy dataset.\r\n      - Use cases inspired by DEFRA and similar organisations, such as:\r\n            - Real-time integration for continuously adding new documents.\r\n            - Tracking topic development over time (dynamic topic modeling).\r\n            - Retrieving similar documents for faster insights.\r\n           (- Classification)\r\n\r\n4. Evaluation Criteria: Analysis of libraries based on:\r\n        - Ease of Use: How easy it is for no coding experts\r\n        - Quality: Coherence and diversity of extracted topics.\r\n        - Efficiency: Runtime performance and scalability.\r\n        - Flexibility: Features like contextual embeddings and integration capabilities.\r\n        - Interpretability: Ease of understanding topics and output.\r\n\r\n5. Results: Detailed findings, including specific advantages and limitations of each library in supporting the outlined use cases.\r\n\r\n6. Practical Recommendations: Guidance on choosing a library based on project goals, dataset characteristics, and organisational needs.\r\n\r\n7. Conclusion and Future Directions: Summary of key insights and the evolving role of embedding-based methods in topic modelling.\r\n\r\nOutcomes:\r\nBy attending this session, participants will:\r\n\r\n- Gain an in-depth understanding of Python\u2019s top topic modeling libraries.\r\n- Learn how to apply these tools to real-world challenges in policy analysis and other fields.\r\n- Understand how to handle use cases like real-time document integration and topic evolution over time.\r\n- Develop the skills to evaluate and choose the best tool for specific datasets and objectives.\r\n\r\nTarget Audience\r\n\r\nThis talk is for:\r\n- Data scientists and NLP practitioners seeking to apply topic modelling to unstructured text data.\r\n- Policy analysts and researchers working with large textual datasets, such as government or environmental policies.\r\n- Professionals in organisations like DEFRA, where tracking changes, adding new documents, or finding similar records are critical tasks.\r\n- Python enthusiasts interested in cutting-edge NLP techniques for extracting meaningful insights.\r\n\r\n[1] R. Biesbroek, S. Badloe, and I. Athanasiadis. Machine learning for research on cli-\r\nmate change adaptation policy integration: an exploratory uk case study. Regional\r\nEnvironmental Change, 20, 07 2020.\r\n\r\n[2] https://pypi.org/project/gensim/\r\n[3] https://github.com/MilaNLProc/contextualized-topic-models\r\n[4] https://github.com/ddangelov/Top2Vec\r\n[5] https://maartengr.github.io/BERTopic/index.html\r\n5 https://github.com/MilaNLProc/contextualized-topic-models", "code": "BJKSGK", "state": "accepted", "created": "2024-11-28", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "GraphQL: Does it replace SQL, REST or Something Else?", "abstract": "Join us for an in-depth exploration of GraphQL's impact on API development. We'll analyze its unique approach to data querying and manipulation, comparing it with traditional SQL and REST architectures. Discover the advantages and limitations of GraphQL, and explore its potential to reshape modern web development practices. Whether you're a seasoned developer or new to the field, this talk offers valuable insights into navigating the evolving landscape of API technologies. At the end you will get recommendations when to choose what.", "full_description": "Attendees will gain insights into GraphQL's unique approach to API development, understand its comparative advantages and limitations when contrasted with SQL and REST, and learn about its potential applications in modern web development. This session equips participants with knowledge to navigate the evolving API landscape effectively. The demo application showcases how to manage an airline database using both GraphQL and REST interfaces. The application uses Flask for the web server and Couchbase for the database. It provides CRUD operations (Create, Read, Update, Delete) for managing airlines.", "code": "VQLKZG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Gregor Bauer", "track": "PyCon: Programming & Software Engineering"}, {"title": "Building Bare-Bones Game Physics in Rust with Python Integration", "abstract": "Learn how to build a minimalist game physics engine in Rust and make it accessible to Python developers using PyO3. This talk explores fundamental concepts like collision detection and motion dynamics while focusing on Python integration for scripting and testing. Ideal for developers interested in combining Rust\u2019s performance with Python\u2019s ease of use to create lightweight and efficient tools for games or simulations.", "full_description": "Python\u2019s simplicity makes it the go-to choice for scripting, while Rust excels in performance-critical tasks like game physics. This talk demonstrates how to build a minimalist physics engine in Rust, focusing on core concepts like collision detection, basic rigid body dynamics, and force application, while providing seamless Python integration using PyO3.\r\n\r\nWe\u2019ll explore how PyO3 allows developers to expose Rust functionality as native Python modules, enabling Python developers to easily script and interact with the physics engine. Through practical examples, attendees will see how Python can be used for rapid prototyping and gameplay scripting, while Rust handles the heavy lifting of physics calculations.\r\n\r\nBy the end of this session, participants will not only understand the basics of implementing physics in Rust but also how to use PyO3 to bridge the gap between Rust\u2019s performance and Python\u2019s flexibility. This talk is perfect for Python enthusiasts curious about Rust or Rustaceans looking to make their libraries accessible to the Python ecosystem.", "code": "VKYDBD", "state": "accepted", "created": "2024-11-28", "speaker_names": "Sam Kaveh", "track": "PyCon: Programming & Software Engineering"}, {"title": "Beyond Spans and Traces: Advanced Python Observability with OpenTelemetry", "abstract": "This talk dives into the latest developments in OpenTelemetry's libraries for python. Attendees will gain practical insights into integrating OpenTelemetry to capture logs and metrics, moving beyond basic tracing. The session is designed for developers, SREs, and DevOps professionals familiar with OpenTelemetry basics, offering practical examples and open-source visualization tools to build a comprehensive observability framework. Equip yourself with the skills to capture a complete, real-time picture of your system's health and performance, ensuring a robust monitoring strategy for your Python applications.", "full_description": "This talk explores advanced observability in Python using OpenTelemetry, focusing on logs and metrics. We will introduce effective logging and metric collection in Python applications, providing a comprehensive understanding of application behavior and performance.\r\n\r\nAttendees will learn through practical examples how to integrate OpenTelemetry for logs and metrics, moving beyond basic tracing to achieve a more holistic view of their applications. The session will also share examples of using open source tools for visualization.\r\n\r\nGeared towards developers, SREs, and DevOps professionals familiar with OpenTelemetry basics, this talk equips attendees with the skills to build a robust observability framework in their Python applications, capturing a complete, real-time picture of their system's health and performance.", "code": "WE3DDG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Yosef Arbiv", "track": "PyCon: MLOps & DevOps"}, {"title": "Conquering PDFs: document understanding beyond plain text", "abstract": "NLP and data science could be so easy if all of our data came as clean and plain text. But in practice, a lot of it is hidden away in PDFs, Word documents, scans and other formats that have been a nightmare to work with. In this talk, I'll present a new and modular approach for building robust document understanding systems, using state-of-the-art models and the awesome Python ecosystem. I'll show you how you can go from PDFs to structured data and even build fully custom information extraction pipelines for your specific use case.", "full_description": "For the practical examples, I'll be using spaCy, and the new Docling library and layout analysis models. I'll also cover Optical Character Recognition (OCR) for image-based text, how to convert tabular data to pandas DataFrames, and strategies for creating training and evaluation data for information extraction tasks like text classification and entity recognition using PDFs and other documents as inputs.", "code": "FUX3FR", "state": "confirmed", "created": "2024-11-28", "speaker_names": "Ines Montani", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Django's Dilemma: Balancing Simplicity with Scalability", "abstract": "Django's model-view-serializer approach works great for small apps, but as projects grow, you might face challenges like scattered database operations and APIs that are too closely linked to your database models. These issues can make unit testing and scaling harder. I'll share real-world examples of these problems and show how to refactor an app using ideas from Domain-Driven Design (DDD) and hexagonal architecture. We'll look at a before-and-after example to see how these changes can make your app easier to use and debug. Plus, we'll discuss when Django's simplicity is enough and when it's worth adopting a more structured approach. You'll leave with practical tips for transforming your Django projects into systems that can handle increased complexity.", "full_description": "Django's model-view-serializer framework is a powerful tool for building small to medium-sized applications quickly and efficiently. Its straightforward approach allows developers to create CRUD operations with minimal effort, making it an excellent choice for projects with clear and simple requirements. However, as applications grow in complexity and scale, developers often encounter several challenges that can hinder the application's maintainability and scalability.\r\nOne common issue is the scattering of database operations across the codebase. This pattern often leads to tightly coupled APIs and database models, making it difficult to isolate logic for individual testing and increasing the fragility of the application as it scales. As database logic becomes intertwined with business logic, maintaining and expanding the application becomes increasingly challenging.\r\nIn this talk, we will explore these common pitfalls through real-world examples and demonstrate how to refactor a Django application for improved scalability and maintainability. Using a webhook server app as a case study, we'll discuss how client-driven response customization can complicate the traditional model-view-serializer approach. We'll explore the limitations of handling complex serialization and response formatting within the view layer, which often results in bloated views that are hard to test and scale.\r\nTo address these challenges, we'll introduce concepts from Domain-Driven Design (DDD) and hexagonal architecture. By applying these principles, we can decouple the application's components, making it easier to manage and extend. We'll refactor the application using a builder pattern for payload formatting, ensuring that response structures are flexible yet maintainable. Additionally, we'll introduce an injected service layer and a repository layer, which separate concerns and allow for more targeted unit testing. This architecture not only improves testability but also enhances the application's ability to adapt to changing requirements.\r\nWe'll provide a clear before-and-after comparison, highlighting the benefits of this refactored approach. Attendees will learn how these changes can lead to cleaner, more scalable code that is easier to debug and maintain. We'll also discuss the balance between Django's simplicity and the need for more sophisticated architecture, helping developers decide when it's appropriate to adopt these patterns.\r\nBy the end of this session, attendees will gain practical insights into transforming their Django projects into robust systems capable of handling increased complexity. Whether you're a developer working on scaling an existing application or someone interested in learning more about advanced architectural patterns, this talk will provide valuable takeaways and actionable strategies for improving your Django applications.", "code": "YCAUMC", "state": "submitted", "created": "2024-11-28", "speaker_names": "Anette Haferkorn", "track": "PyCon: Django & Web"}, {"title": "Bridging Causal thinking and Media Mix Modeling", "abstract": "In today's data-driven landscape, understanding causal relationships is essential for effective marketing strategies. This talk will explore the link between Bayesian causal thinking and media mix modeling, utilizing Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP).\r\n\r\nWe will examine how DAGs represent causal assumptions, how SCMs define relationships in media mix models, and how to implement these models within a Bayesian framework. By using media mix models as causal inference tools, we can estimate counterfactuals and causal effects, offering insights into the effectiveness of media investments.", "full_description": "In the era of data-driven decision-making, understanding causal relationships is crucial for effective marketing strategies. This talk delves into the underexplored connection between Bayesian causal thinking and media mix modeling, linking Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP). By navigating through these key concepts, we will demonstrate how we can build models that not only predict outcomes but also represent causal mechanisms within the marketing ecosystem.\r\n\r\nStarting from foundational principles, we will explore how DAGs serve as a formal language for encoding causal assumptions, how Structural Causal Modeling define relationships in media mix models, and how we implement those in the Bayesian framework through the famous DGP. We will further illustrate how media mix models can be employed as causal inference tools to estimate counterfactuals and causal effects, providing actionable insights into the effectiveness of media investments.\r\n\r\nFinally, we\u2019ll show how Bayesian inference enables us to update these causal beliefs in light of data. This synthesis of causal reasoning and probabilistic modeling is not only theoretically rich but practically powerful\u2014offering a robust framework for constructing media mix models that more accurately reflect the complexities of real-world marketing dynamics.\r\n\r\nAttendees will leave with an understanding of how to apply Bayesian causal discovery (guided by an example in an IPython notebook) to develop causally valid models that can be applied to real-world marketing data. They will learn how to use Media Mix Models as causal inference tools to estimate counterfactual scenarios and causal effects, unlocking deeper insights into the effectiveness of media investments. This presentation aims to reveal a new pathway for marketers, data scientists, and researchers to harness the potential of these powerful methodologies together, empowering them to drive more informed, causally grounded decisions.", "code": "MNTFRG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Carlos Trujillo", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Beyond Single Models: The Secret Sauce of Predictive Success", "abstract": "In this talk, we will explore the powerful world of ensemble learning techniques in machine learning, a method that improves predictive performance by combining multiple models. We will cover the foundations of ensemble learning, beginning with an understanding of weak and strong learners and their roles in model accuracy.\r\n\r\nWe will delve into three key ensemble techniques:\r\n\r\nBagging: A technique where multiple base models are trained independently and in parallel, helping to reduce variance and increase stability in predictions.\r\nBoosting: A sequential technique that builds a strong model by combining multiple weak models, focusing on reducing bias and improving prediction accuracy.\r\nStacking: A method that combines multiple models to build a generalized and robust final model, leveraging the strengths of diverse learners.\r\nThroughout the session, we will break down the mechanics of each technique, explore their strengths and weaknesses, and demonstrate how they can be applied to real-world problems to elevate predictive performance.", "full_description": "In this talk, we\u2019ll delve into the fascinating world of ensemble learning techniques in Machine learning \u2014a powerful paradigm that combines the strengths of multiple models to enhance overall predictive performance. Here\u2019s what you can expect: 1. Foundations: Understanding Weak and Strong Learners 2. Bagging: An ensemble technique that has multiple base models trained independently and in parallel. 3. Boosting: A technique that sequentially builds a strong model by combining multiple weak models. 4. Stacking: Model fusion of multiple models, building a more generalized yet robust model.", "code": "MMWXLU", "state": "rejected", "created": "2024-11-29", "speaker_names": "Yashasvi Misra", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Deploying Synchronous and Asynchronous Django Applications for Hobby Projects", "abstract": "Simplify deploying hybrid Django applications with synchronous views and asynchronous apps. This session covers ASGI support, Docker containerization, and Kamal for seamless, zero-downtime deployments on single-server setups, ideal for hobbyists and small-scale projects.", "full_description": "Hobby projects often start small but can quickly grow in complexity, especially when incorporating Django\u2019s support for asynchronous applications alongside traditional synchronous views. Deploying such hybrid projects on a single server\u2014whether in the cloud or on-premise\u2014can be daunting without the right tools and workflows.  \r\n\r\nThis talk focuses on simplifying the deployment process for hobbyists and developers who want to create and manage robust Django applications without requiring extensive infrastructure or expertise. We\u2019ll cover:  \r\n- Deploying Django projects that combine synchronous views and asynchronous apps using Django\u2019s ASGI support.  \r\n- Containerizing the application with Docker for consistent and manageable environments.  \r\n- Utilizing Kamal, an open-source deployment tool, to enable zero-downtime deployments, rolling updates, and seamless app management.  \r\n- Demonstrating the workflow on a single cloud server, with insights on adapting it to on-premise servers.  \r\n\r\nWhether you're building a passion project or experimenting with modern Django features, this session will provide you with practical tools and approaches to deploy hybrid Django applications effortlessly, keeping the process accessible and scalable for hobby-level development.", "code": "CMTKZS", "state": "submitted", "created": "2024-11-30", "speaker_names": "melhin", "track": "PyCon: Django & Web"}, {"title": "Enterprise-Ready FastAPI: Beyond the Basics", "abstract": "FastAPI is excellent for building services quickly, but sometimes we need to scale it further. To increase the joy of it, I\u2019ll share our experience designing larger applications, improving the structure of monoliths, and handling API/database migrations. We\u2019ll discuss usage Depends vs. DI containers, straightforward ways to implement tests (unit to acceptance), and how to configure logging/tracing for better telemetry. Plus, I\u2019ll touch on deployment strategies.\r\n\r\nI aim for this talk to offer developers and leads, even those using different frameworks, practical insights on ensuring better testability and observability in services.", "full_description": "FastAPI is excellent for building services quickly, but scaling it for enterprise applications introduces new challenges. In this talk, I\u2019ll share lessons from designing larger applications and managing API/database migrations. We\u2019ll explore testing strategies from unit to acceptance, compare FastAPI\u2019s Depends vs. DI containers, and discuss logging, tracing, and deployment practices. \r\n\r\nI aim for this talk to offer developers and leads, even those using different frameworks, practical insights on ensuring better testability and observability in services.", "code": "LDQXUY", "state": "submitted", "created": "2024-11-30", "speaker_names": "Alexander Ptakhin", "track": "PyCon: Django & Web"}, {"title": "pytest tips and tricks for a better testsuite", "abstract": "pytest lets you write simple tests fast - but also scales to very complex scenarios: Beyond the basics of no-boilerplate test functions, this training will show various intermediate/advanced features, as well as gems and tricks.\r\n\r\nTo attend this training, you should already be familiar with the pytest basics (e.g. writing test functions, parametrize, or what a fixture is) and want to learn how to take the next step to improve your test suites.\r\n\r\nIf you're already familiar with things like fixture caching scopes, autouse, or using the built-in `tmp_path`/`monkeypatch`/... fixtures: There will probably be some slides about concepts you already know, but there are also various little hidden tricks and gems I'll be showing.", "full_description": "We'll cover things like:\r\n\r\n- Recommended pytest settings for more strictness\r\n- What's xfail and why is it useful?\r\n- How to mark an entire test file or single parameters\r\n- Ways to deal with parametrize IDs and syntax\r\n- Useful built-in pytest fixtures\r\n- Caching for fixtures\r\n- Using fixtures implicitly\r\n- Advanced fixture and parametrization topics\r\n- How to customize fixtures behavior based on markers or custom CLI arguments\r\n- Basics of patching, mocking, and alternatives\r\n- Short intro to property-based testing with Hypothesis\r\n- Various useful plugins, and a short teaser about how to write your own", "code": "SSHXXH", "state": "submitted", "created": "2024-12-01", "speaker_names": "Florian Bruhin", "track": "PyCon: Testing"}, {"title": "pytest - simple, rapid and fun testing with Python", "abstract": "The pytest tool offers a rapid and simple way to write tests for your Python code. This training gives an introduction with exercises to some distinguishing features, such as its assertions, marks and fixtures.\r\n\r\nDespite its simplicity, pytest is incredibly flexible and configurable. We'll look at various configuration options as well as the plugin ecosystem around pytest.", "full_description": "- (20 minutes) **pytest feature walkthrough:**\r\n    * Automatic test discovery\r\n    * Assertions without boilerplate via the assert statement\r\n    * Configuration and commandline options\r\n    * Marking and skipping tests\r\n    * Data-driven tests via parametrization\r\n    * Exercises\r\n\r\n- (60 minutes) **pytest fixture mechanism:**\r\n    * Setup and teardown via dependency injection\r\n    * Declaring and using function/module/session scoped fixtures\r\n    * Using fixtures from fixture functions\r\n    * Parametrizing fixtures\r\n    * Looking at useful built-in fixtures (managing temporary files, patching, output capturing)\r\n    * Exercises\r\n\r\n- (10 minutes) **Where to go next:**\r\n    * Useful CLI arguments to deal with failing tests\r\n    * Overview of the plugin ecosystem around pytest", "code": "PDBAXQ", "state": "submitted", "created": "2024-12-01", "speaker_names": "Florian Bruhin", "track": "PyCon: Testing"}, {"title": "I want to deploy my Flask app on Kubernetes, what are my options?", "abstract": "After coding your amazing Flask app, now you have to deploy and operate it. Kubernetes is a great way to do this, but it isn't easy to get started with creating a container and knowing what your deployment options are. In this talk we look at a few options so that the next time you have to deploy your Flask application you know which option is best for you!", "full_description": "After creating a great web app using Python such as with flask, the next hurdle to production is how to make it available to users and operate it. And not just your app, but also ingress, the database, observability and the list goes on. We will go through your options for simplifying the operations of your web app using open source tooling. This will include using k8s directly with helm charts, PaaS using fly.io and new tooling developed by Canonical using juju. By the end of the talk you will have seen the benefits and drawbacks of each which will help you make an informed decision on which tool best suits your needs!", "code": "KHTQJQ", "state": "submitted", "created": "2024-12-02", "speaker_names": "David Andersson", "track": "PyCon: Django & Web"}, {"title": "\ud83e\udd2f No data? No problem! Synthetic data to the rescue", "abstract": "Got data problems? Relax. Synthetic data is here to help. \r\n\r\nI will go over the fundamentals of synthetic data and show how frontier model providers and researchers use synthetic data to speed up their model development processes, proving that sometimes the best solution isn't finding the right data\u2014it's creating it.", "full_description": "Got data problems? Relax. Synthetic data is here to help. \r\n\r\nI will go over the fundamentals of synthetic data and show how frontier model providers and researchers use synthetic data to speed up their model development processes, proving that sometimes the best solution isn't finding the right data\u2014it's creating it. \r\n\r\nThis talk will start with an overview of the importance of data in general and continue with the what, why and how of synthetic data. This will also briefly mention some of the use cases in commercial and research scenarios. \r\n\r\nThen, we'll go over a taxonomy of different synthetic data types focusing on rewriting, judging and rationales. These data types are then discussed based on fun, practical examples and more educational research papers. \r\n\r\nThe final chapter focuses on the evaluation of synthetic data and how we can scale generation from various prompts to pipelines with various useful tools and libraries. We then go over various entry points for getting started with synthetic data.", "code": "F9KRYX", "state": "submitted", "created": "2024-12-02", "speaker_names": "David Berenstein", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "The Visual Grammar of AI: Building Multi-modal Systems with Python", "abstract": "This tutorial introduces practitioners to the convergence of text and image understanding through modern embedding techniques. Participants will gain hands-on experience implementing multi-modal solutions using Python, focusing on CLIP embeddings and diffusion models. Through interactive examples and visualization tools, attendees will understand why these technologies have converged and how to leverage them effectively.", "full_description": "In this hands-on tutorial, we'll explore how modern AI bridges language and images using CLIP embeddings. You'll discover why these two domains have finally converged. Through interactive visualizations and real code examples, you'll learn to harness the power of unified embeddings for practical applications. Perfect for Python developers curious about the latest advances in AI vision-language understanding. Bring your laptop and sense of adventure - let's make text and images speak the same language! \ud83d\udc0d\ud83d\udc41\ufe0f\ud83d\udcdd", "code": "Z8BALM", "state": "submitted", "created": "2024-12-02", "speaker_names": "Hila Weisman Zohar", "track": "PyData: Generative AI"}, {"title": "From SHAP to EBM: Explain your Gradient Boosting Models in Python", "abstract": "Imagine you\u2019ve just developed a **credit rating model** for a bank. The backtesting metrics look fantastic, and you get the green light to go live. But then, the first loan applications start rolling in, and suddenly your model\u2019s decisions are under scrutiny. \r\n\r\nWhy was this application rejected? Why was that one approved? You **must** have answers\u2014and fast.\r\n\r\nThis session has you covered.\r\n\r\nWe\u2019ll dive into SHAP (SHapley Additive exPlanations) and EBM (Explainable Boosting Machine), two widely used methods for interpreting tree-based ensemble models like XGBoost. You\u2019ll learn about their theory, strengths, and limitations, as well as see them in action with Python examples using the `shap` and `interpret-ml` libraries.\r\n\r\nFrom understanding feature contributions to hands-on coding, this talk will equip you with practical tools to make complex models transparent, understandable, and ready for critical applications.", "full_description": "**XGBoost** is considered a state-of-the-art model for regression, classification, and learning-to-rank problems on tabular data. Unfortunately, tree-based ensemble models are notoriously **difficult to explain**, limiting their application in critical fields. Techniques like SHapley Additive exPlanations (SHAP) and Explainable Boosting Machine (EBM) have become common methods for assessing how much each feature contributes to the model prediction.\r\n\r\nThis talk will introduce SHAP and EBM, **explaining the theory** behind their mechanisms in an accessible way and **discussing the pros and cons** of both techniques. We will also comment on Python snippets where SHAP and EBM are used to explain a gradient boosting model.\r\n\r\nAttendees will walk away with an understanding of how SHAP and EBM work, the limitations and merits of both techniques, and a tutorial on how to use these methods in Python, courtesy of the [shap](https://shap.readthedocs.io/en/latest/) and [interpret-ml](https://interpret.ml/docs/ebm.html) packages.\r\n\r\nTalk outline:\r\n\r\n- A brief reminder about gradient boosting and XGBoost (5 mins)\r\n- The challenge of explainability (5 mins)\r\n- EBM: theory and applications (5 mins)\r\n- SHAP: theory and applications (5 mins)\r\n- Case study with live coding (10 mins)", "code": "MBLX3W", "state": "withdrawn", "created": "2024-12-02", "speaker_names": "Emanuele Fabbiani", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Write your own code coverage tool in Python", "abstract": "Coverage tools are essential for ensuring that your tests exercise significant parts of your Python application. But what exactly is code coverage, how do coverage tools work behind the scenes, and how can you build one yourself? This is what this talk is all about.", "full_description": "Coverage tools are great, but how do they work? Python 3.12 introduced a new low-impact monitoring API through PEP 669, enabling the development of significantly faster tools, including debuggers and coverage analyzers. This talk explores how to leverage the new API to implement efficient coverage tools, what insights they provide, and how they can enhance your development workflow, allowing you a peak behind the curtain.", "code": "PNRGT7", "state": "submitted", "created": "2024-12-02", "speaker_names": "Johannes Bechberger", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Behind the curtains of the new monitoring API", "abstract": "The new low-impact monitoring API introduced with PEP 669 allows us to implement tools like debuggers and coverage with only a small performance impact. Contrary to the previous `sys.settrace` based approach, the new API uses bytecode modification internally, which allows it to be much faster and fine-grained.\r\n\r\nThis talk will cover the basics of Python bytecode and how the new monitoring API is implemented in CPython, explaining along the way why some monitoring API methods are slower than others.", "full_description": "In this talk, I'll first give an introduction to Python's bytecode, why we need it, how it speeds up code execution, and how it works in principle. But you don't need a compiler engineering degree; I'll keep it short. Then, I follow this with an overview of how the old monitoring API with `sys.settrace` worked and show why it is probably not the best. The new API is far better, so I briefly introduce it and end this by showing how the new API uses clever bytecode modification to achieve the configurability and speediness it is cherished for.", "code": "YY7DGS", "state": "submitted", "created": "2024-12-02", "speaker_names": "Johannes Bechberger", "track": "PyCon: Python Language & Ecosystem"}, {"title": "We don't talk about PyPLUTO", "abstract": "PyPLUTO is a versatile Python package designed for loading, analyzing, and visualizing data from simulations produced by the astrophysical code PLUTO. By integrating libraries like NumPy, SciPy, and Matplotlib, PyPLUTO facilitates the visualization of numerical outputs, showcasing how Python effectively enhances scientific understanding and communication.", "full_description": "Numerical simulations have become essential for tackling complex astrophysical problems in recent years. The MagnetoHydroDynamics (MHD) framework plays a central role in studying the dynamic evolution of astrophysical plasmas, governed by partial differential equations that conserve mass, momentum, and energy while incorporating Maxwell's equations for electromagnetic field evolution. Due to the high nonlinearity of the MHD equations (regardless of their specifications, e.g., classical/relativistic or ideal/resistive), a general analytical solution is precluded, making the numerical approach crucial. Numerical simulations usually end up producing large sets of data files and their scientific analysis leans on dedicated software designed for data visualization. However, in order to encompass all of the code output features, specialized tools focusing on the numerical code may represent a more versatile and built-in tool. Here, we present PyPLUTO, a Python package tailored for efficient loading, manipulation, and visualization of outputs produced with the PLUTO code. PyPLUTO uses memory mapping to optimize data loading and provides general routines for data manipulation and visualization. Additionally, it supports the particle modules of the PLUTO code, enabling visualization of particles such as cosmic rays, Lagrangian particles, or dust in hybrid simulations. A user-friendly Graphical User Interface (GUI) further enhances accessibility, allowing for the creation of single-subplot figures with ease. PyPLUTO provides a comprehensive, versatile toolkit for astrophysical data exploration and interpretation.", "code": "FWZLKL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Giancarlo Mattia", "track": "PyData: Visualisation & Jupyter"}, {"title": "PosePIE: Replace Your Keyboard and Mouse With AI-Driven Gesture Control", "abstract": "In this talk, we show how to leverage publicly available tools to control any game or program using hand or body movements. To achieve this, we introduce PosePIE, an open-source programmable input emulator that generates input events on virtual gamepads, keyboards and mice based on gestures recognized by using AI-driven pose estimation. PosePIE is fully configurable by the user through Python scripts, making it easily adaptable to new applications.", "full_description": "Recent advancements in machine learning and AI hardware acceleration have enabled the use of complex models for solving computer vision problems in real-time applications. Pose estimation is one such problem, involving the detection of keypoints of the human body within an image.\r\n\r\nIn this talk, we show how PosePIE uses pose estimation to control any game or program using hand or body movements. By using state-of-the-art models, PosePIE does not require expensive specialized sensors but works entirely on the monocular image from an off-the-shelf webcam. By leveraging readily available Graphics Processing Unit (GPU) hardware, it is able to do all processing at a high frame rate to support interactive applications.\r\n\r\nAs PosePIE is fully configurable by the user through Python scripts, it can be easily adapted to new applications. This lowers the barrier to use pose estimation and gesture recognition in creative ways and for novel applications.\r\n\r\nThe source code of PosePIE is available on GitHub under the GNU GPLv3+ license.", "code": "AEUZGX", "state": "submitted", "created": "2024-12-20", "speaker_names": "Daniel Stolpmann", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "The Mighty Dot - Customize Attribute Access with Descriptors", "abstract": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.", "full_description": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.\r\n\r\nIn this tutorial you will:\r\n\r\n* Learn how to use Python's descriptors to add new functionality to attribute access\r\n* Acquired solid background knowledge on how descriptors work\r\n* Work with practical examples for applying descriptors\r\n* Learn when to use a property or reach for a descriptor\r\n* Get to know how popular Python libraries apply descriptors for tasks such as\r\n  data structure access, REST-APIs, ORMs, and serialization", "code": "WJPEQH", "state": "confirmed", "created": "2024-12-18", "speaker_names": "Mike M\u00fcller", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Rustifying Python: A Practical Guide to Achieving High Performance While Maintaining Observability", "abstract": "In this session, I\u2019ll share our journey of migrating key parts of a Python application to Rust, resulting in over 200% performance improvement.\r\nRather than focusing on quick Rust-to-Python integration with PyO3, this talk dives into the complexities of implementing such a migration in an enterprise environment, where reliability, scalability, and observability are crucial.\r\nYou\u2019ll learn from our mistakes, how we identified suitable areas for Rust integration, and how we extended our observability tools to cover Rust components.\r\nThis session offers practical insights for improving performance and reliability in Python applications using Rust.", "full_description": "For performance-critical sections of code, especially those that are I/O-bound or CPU-heavy, Python\u2019s Global Interpreter Lock (GIL) can create significant bottlenecks.\r\nTo improve performance, our team explored integrating Rust, taking advantage of its speed and concurrency features while maintaining Python\u2019s ease of use and flexibility.\r\n\r\nThis session will focus on overcoming common hurdles when migrating to Rust and optimizing performance in a real-world, production environment which orchestrates workload across 2000 compute nodes in various data centers and cloud provider regions.\r\nThis talk covers practical aspects such as observability, scalability, and deployment in a production setting.\r\n\r\nWe\u2019ll begin by discussing how to identify the parts of your Python code that would benefit most from a Rust migration, particularly those where the GIL is a limiting factor.\r\nWe\u2019ll also share insights into our migration process, including the challenges we faced and how we overcame them.\r\nYou\u2019ll learn how we refactored Python code and used PyO3 to integrate Rust, achieving over 200% performance improvements.\r\n\r\nA key challenge when adding Rust to a Python codebase is maintaining robust observability.\r\nWe\u2019ll explain how we extended our OpenTelemetry and Sentry observability stack to include Rust components, ensuring seamless monitoring, tracing, and debugging across the entire stack.\r\n\r\nThroughout the session, we\u2019ll illustrate the process with a practical example: a simplified version of our own application, which includes both I/O-heavy and compute-heavy tasks.\r\nYou\u2019ll see how to break down business logic and decide which parts to migrate to Rust for maximum performance benefit.\r\n\r\nBy the end of this session, you will be equipped with the knowledge to assess where Rust can improve your Python application\u2019s performance, and how to integrate it in a reliable and observable way.\r\nThis session is ideal for anyone looking to optimize Python performance with Rust, while keeping applications running.", "code": "QXSQKL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Max H\u00f6hl", "track": "PyCon: Programming & Software Engineering"}, {"title": "Bundestagsmining: A Data-Driven Look at Who Really Represents Us", "abstract": "Ever wondered how representative Germany's \"representative democracy\" really is? I did. And I used the freely available data about our \"Bundestagsabgeordnete\" to analyze how skewed the distribution of age, gender, religion and former profession of our representatives really is. Spoiler alert: there are not too many single mothers who worked part-time at the local bakery before joining the Bundestag. I will give some technical insights about how I parsed and analyzed the data, did some data-wrangling (Berufemapping is not trivial!) and created a Plotly dashboard to visualize the results. I will share some insights about surprising (and some less surprising) findings I had when working with the data and their implications for the current state of our parliament. I will also provide some relevant memes.", "full_description": "In this talk I will share both technical insights and political findings I gained when developing the Bundestagsmining project.\r\n\r\n# Technical setup\r\n- Git versioned Python code (publicly available at GitHub)\r\n- Dependencies managed with poetry\r\n- Data wrangling and preprocessing using Jupyter notebooks\r\n- Interactive visualization with Plotly/Dash\r\n- Deployment to the Google Cloud Platform using Docker \r\n\r\n# Key findings\r\n- Highly skewed distribution of parliament members regarding:\r\n  - Gender (significant underrepresentation of women)\r\n  - Age (dominance of 50+ age group)\r\n  - Religion (overrepresentation of traditional Christian denominations)\r\n  - Professional background (dominance of legal and academic careers)\r\n- Each party has their own bubble (CDU: white Christian male, SPD: a lot of females but very academic etc)\r\n- even when combining the members of all parties a lot of blind spots remain: \r\n  - too few young people\r\n  - very few people from non-traditional family structures\r\n  - very few STEM professionals\r\n  - nearly no \"blue collar workers\"\r\n\r\n# Impulses and takeaways for the audience\r\n- be educated about politics and try to gain and empower data literacy\r\n- analyzing data (even as hobby project) can help raise awareness. I will also mention David Kriesel's \"Spiegelmining\" and David Englert's \"Tagesschaumining\" as examples for our power of rising political awareness as data scientists\r\n- analysis of publicly available data takes less time and effort than many would think (Python + Jupyter + Git + any cloud is all you need)", "code": "STPBVD", "state": "submitted", "created": "2024-12-22", "speaker_names": "Lena Aretz", "track": "PyData: Visualisation & Jupyter"}, {"title": "Monitoring Celery jobs with AWS Firehose, S3, Athena, and Grafana", "abstract": "The go-to solution for Celery monitoring is flower. If you're inside the AWS ecosystem, combining AWS Firehose, S3, Athena, and Grafana can be a great choice for implementing your Celery monitoring. In this talk, I'll show you how to set that up for good.", "full_description": "## Introduction - 5min\r\n\r\nWe'll discuss why we need to monitor our tasks and which things we should monitor. We'll briefly discuss the flower and its limitations.\r\n\r\n## Celery events - 5min\r\nWe'll discuss how Celery communicates the state of tasks - sent, received, started, ... We'll take a look at how to subscribe to Celery's events with code.\r\n\r\n## AWS Firehose, S3, Athena, Grafana - 5min\r\nI'll briefly present each tool and its role in our monitoring. We'll also discuss their limitations.\r\n\r\n\r\n## Implementing monitoring - 25min\r\nWe'll look at how to set up all the AWS tools using Terraform. We'll also set up a Celery events listener to send Celery's Events to our monitoring solution. We'll also create some plots inside Grafana to monitor some actual executions of jobs.\r\n\r\n## Q&A - 5min", "code": "3BHPJJ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Jan Giacomelli", "track": "PyCon: MLOps & DevOps"}, {"title": "Climate Data Standardization with Pymorize: A Case Study in Integrating Various PyData Libraries", "abstract": "Pymorize is designed for large-scale Earth System Modeling post-processing workflows, leveraging Prefect, Dask, and SLURM for orchestration. It handles big data with ease by paralleling tasks, caching results for fast retrieval, and monitoring SLURM jobs in real-time. Perfect for managing complex data operations such as multi-model output integration or regridding, Pymorize automates and streamlines the post-processing pipeline. It simplifies data handling and ensures efficient, scalable workflows for massive datasets, making your data flow seamlessly.", "full_description": "# Climate Data Standardization with Pymorize: A Case Study in Integrating Various PyData Libraries\r\n> Paul Gierz, Pavan Siligam, and Miguel Andres-Martinez\r\n> Alfred Wegener Institute for Polar and Marine Research, Bremerhaven, Germany\r\n\r\nThe [PyData ecosystem](https://pydata.org) provides powerful tools for scientific research, yet combining these tools into workflows that handle massive datasets remains a formidable challenge. In climate and earth system modelling, where datasets often span terabytes, efficient data standardization is critical. This is where\u00a0[`pymorize`](https://github.com/esm-tools/pymorize)\u00a0comes in\u2014a lightweight, open-source framework designed to streamline the standardization of scientific data into the widely-used\u00a0[CMOR (Climate Model Output Rewriter)\u00a0format](https://cmor.llnl.gov).\r\n\r\nIn this talk, we will showcase how `pymorize` bridges key PyData libraries, such as\u00a0[`xarray`](https://xarray.dev),\u00a0[`dask`](https://www.dask.org), along with several extensions such as as [`CF-Xarray`](https://cf-xarray.readthedocs.io/en/latest/), [`pint`](https://pint-xarray.readthedocs.io/en/stable/), and [`uxarray`](https://uxarray.readthedocs.io/en/latest/), together with the [Prefect workflow orchestrator](https://www.prefect.io) in order to simplify large-scale data processing pipelines. By leveraging\u00a0SLURM\u00a0for distributed task management and\u00a0Dask\u00a0for parallel computation, Pymorize ensures scalability on both local and high-performance computing environments. We will highlight how this modular system allows researchers to preprocess, validate, and transform data with ease, even under the constraints of limited memory or extreme data sizes.\r\n\r\nKey takeaways will include:\r\n* An overview of how Pymorize integrates tools like Pint and Xarray to handle multidimensional data and unit conversions.\r\n* Practical approaches to parallelizing workflows for CMOR standardization using Dask and SLURM, orchestrated via Prefect.\r\n* Real-world examples of efficiently handling datasets too large to fit in memory, while maintaining reproducibility and transparency.\r\n* Lessons learned from developing scalable workflows in a research-driven environment.\r\n\r\nThis talk is aimed at anyone navigating the complexities of scientific data processing. Whether you\u2019re a researcher, data engineer, or open-source enthusiast, join us to discover how Pymorize turns the PyData ecosystem into a robust solution for large-scale data standardization.\r\n\r\nGitHub: https://github.com/esm-tools/pymorize\r\nDocumentation: https://pymorize.readthedocs.io", "code": "EPDPCK", "state": "submitted", "created": "2024-12-06", "speaker_names": "Paul Gierz", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Data as (Python) Code", "abstract": "In contemporary data-driven environments, the seamless integration of data into automated workflows is paramount. The reliability of automation, however, is constantly threatened by breaking changes in the source data. The Data-as-Code (DaC) paradigm address this challenge by treating data as a first-class citizen within the software development lifecycle.", "full_description": "Data-as-Code (DaC) is a paradigm that streamlines data distribution by encapsulating dataset retrieval within Python packages, along with a data contract. This approach makes it easy to enforce data quality, effortlessly leverage on semantic versioning to prevent errors in the data pipeline, and abstracts away from the Data Scientist all the boilerplate code to load the data needed by the ML models, improving efficiency and consistency. This presentation will delve into the implementation of DaC, demonstrate its practical applications, and discuss the benefits it offers in modern data workflows.\r\n\r\nThis session will cover:\r\n1. Introduction to Data-as-Code (DaC):\r\n   - What problems do we want to solve with DaC\r\n   - What it is out of scope\r\n2. Implementing DaC:\r\n   - Packaging data as Python packages\r\n   - Defining data contracts\r\n3. Advantages of DaC:\r\n   - Application of semantic versioning to manage data changes effectively\r\n   - Breaking changes in data are automatically detected as part of the data distribution\r\n   - Abstraction of data loading mechanisms, allowing seamless transitions between data sources\r\n   - Elimination of hard-coded data field names, enhancing code maintainability\r\n   - Facilitation of unit testing through schema examples\r\n   - Inclusion of comprehensive data descriptions and metadata\r\n   - Centralized data distribution via the Python Package Index (PyPI)\r\n4. DaC in the real world:\r\n   - Step-by-step walkthrough of creating and distributing a DaC package\r\n   - Guidelines for data engineers on preparing data for DaC\r\n   - Instructions for data scientists on consuming DaC packages in their workflows\r\n   - Discussion on the scalability and adaptability of DaC\r\n6. Q&A Session:\r\n   - Addressing audience questions and remarks", "code": "SXRVNU", "state": "accepted", "created": "2024-12-19", "speaker_names": "Francesco Calcavecchia", "track": "PyCon: MLOps & DevOps"}, {"title": "Python Performance Unleashed: Essential Optimization Techniques Beyond Libraries", "abstract": "Every Python developer faces performance challenges, from slow data processing to memory-intensive operations. While external libraries like Numba or Cython offer solutions, understanding core Python optimization techniques is crucial for writing efficient code. This talk explores practical optimization strategies using Python's built-in capabilities, demonstrating how to achieve significant performance improvements without external dependencies. Through real-world examples from machine learning pipelines and data processing applications, we'll examine common bottlenecks and their solutions. Whether you're building data pipelines, web applications, or ML systems, these techniques will help you write faster, more efficient Python code.", "full_description": "Performance optimization remains a critical challenge in Python development. While Python's simplicity and extensive ecosystem make it the language of choice for many applications, its interpreted nature can lead to significant performance bottlenecks. This is particularly evident in data-intensive applications, machine learning pipelines, and large-scale production systems where every millisecond counts.\r\n\r\nMany developers immediately reach for external libraries or complex solutions when facing performance issues. However, Python's standard library and built-in features offer powerful optimization opportunities that are often overlooked. Understanding these fundamental optimization techniques not only improves code performance but also helps developers write more efficient code from the start.\r\n\r\nThis talk addresses the core performance challenges faced by Python developers daily. From memory management to algorithmic efficiency, we'll explore how seemingly simple code changes can lead to substantial performance improvements. Through practical examples drawn from real-world applications, we'll demonstrate how to identify, measure, and optimize performance bottlenecks effectively.", "code": "AJDYRL", "state": "accepted", "created": "2024-12-22", "speaker_names": "Thomas Berger", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Pyscript, chart.js, bootstrap and fastapi. A match made in heaven", "abstract": "This is another talk from the continuing series of \"What does it actually take to make ...\".\r\n\r\nThis time we're covering what it takes to display data streams in a web based environment in order to have an overview of the current state of a system or track its behaviour over time.\r\n\r\nThis is a central component in smart home systems such as Home Assistant, or in monitoring environments for example built on top of influx and grafana.\r\n\r\nIf you're interested in how these things basically work, then you're in luck these days, as many solutions in this area are open source, and you just can take a look at the sources. But let's be honest, this isn't entirely true given a code base of many million lines of code in some cases. \r\n\r\nWell you might be able to go back to version 0.0.1 ...\r\n\r\nOn the other hand there might as well be a talk to cover the basic concepts of what it takes to make a version 0.0.x for such a thing.\r\n\r\nAnd this is the talk ;-)", "full_description": "Probably due to my scientific background I'm very interested in monitoring my home. I'm looking for ways to track my power meter readings to get an idea of current energy consumption. I would like to know about the solar revenue of my rooftop power plant. I like to be aware of the costs to heat my house in the winter, and how temperature and humidity develops in some rooms of my house. \r\n\r\nYou certainly can buy readymade solutions for this. For the sensors needed, for the data storage and also for displaying and analysing the data collected.\r\n\r\nI am on the other hand interested in how these things work, and what it takes to make my own version of it.\r\n\r\nAnd I would like to have a system that I can host myself, where everything fits together neatly. For this ideally everything is based on Python. \r\n\r\nFor most components involved it was quite simple to generate a Python based solution. This was even true for data display on the desktop. For more practical web based solutions however this was tricky before.\r\n\r\nBut PyScript changed the world completely here.\r\n\r\nIt can be seen as an alternative to JavaScript for web based applications. It allows to have a Python Interpreter running in your Web-Browser with full control over the currently displayed web page, which makes it intriguing to let Python generate and manage the whole page and take care of handling and retrieving all data for display. \r\n\r\nTogether with fastapi on the server side, which delivers both the PyScript environment and the data for the display, a thin wrapper around chart.js to make things a bit more pythonic and Bootstrap to create nice looking responsive pages, this allows to create a visually pleasing, yet powerful web based real time data display, which can quite easily be understood.", "code": "YQK9AW", "state": "submitted", "created": "2025-01-05", "speaker_names": "Jens Nie", "track": "PyData: Visualisation & Jupyter"}, {"title": "EffiDict: Python Dictionary with Disk Persistence and Application in Bioinformatics", "abstract": "EffiDict is a Python package which behaves exactly like a dictionary but uses the disk to handle larger amounts of data. Depending on the application, users can choose from various replacement strategies; we have implemented some popular cache replacement policies such as LRU and LFU. Additionally, users can select either pickle or SQLite for persistence. We have used EffiDict to enhance the performance of the Oligo Designer Toolsuite, which relies on large dictionaries for its pipelines. Switching to EffiDict allowed us to offload some parts to disk, which significantly improved the program's performance and enabled it to process much larger amounts of data.", "full_description": "EffiDict is a Python package which behaves exactly like a dictionary but uses the disk to handle larger amounts of data. This talk will cover:\r\n\r\n- An overview of EffiDict\u2019s architecture and key features.\r\n- A practical application to one of our packages: the Oligo Designer Toolsuite, and our integration process.\r\n- Performance tests comparing EffiDict to alternative solutions.\r\n- Ongoing challenges with the package\r\n- Lessons learned and best practices for incorporating EffiDict into other research software applications.", "code": "YFC3KA", "state": "submitted", "created": "2024-12-27", "speaker_names": "Mekki Isra", "track": "PyData: Research Software Engineering"}, {"title": "Open Table Formats in the Wild: From Parquet to Delta Lake and Back", "abstract": "Open table formats have revolutionized analytical, columnar storage on cloud object stores with critical features like ACID compliance and enhanced metadata management, once exclusive to proprietary cloud data warehouses. Delta Lake, Iceberg, and Hudi have significantly advanced over traditional open file formats like Parquet and ORC.\r\n\r\nIn an effort to modernize our data architecture, we aimed to replace our Parquet-based bronze layer with Delta Lake, anticipating better query performance, reduced maintenance, native support for incremental processing, and more. While our initial pilot showed promise, we encountered unexpected pitfalls that ultimately brought us back to where we began.\r\n\r\nCurious? Join me as we shed light on the current state of table formats.", "full_description": "# Description\r\n\r\nOpen Table Formats (OTF) such as Hudi, Iceberg and Delta Lake have disruptively changed the data engineering landscape in recent years. While the Parquet file format has evolved as the de-facto standard for open, interoperable columnar storage for analyical workloads, it lacked first class support for critical features such as ACID compliance, incremental processing, flexible schema & partioning evolution and scalable meta data management. This led to increased development and maintenance efforts while building idempotent and failure tolerant data pipelines that often resulted in custom frameworks. OTFs solve all of these issues via providing a sophisticated meta data layer and improved maintenance capabilities on top of Parquet.\r\n\r\nDriven by the promises of OTFs, we intended to replace our own bronze-read-only Parquet-based storage layer with Delta Lake. In theory, this should have improved performance, reduced maintenanced and provided more flexibility. However, we've stumbled upon several issues:\r\n\r\n1. drastic performance issues with Liquid Clustering during incremental processing\r\n2. inmature interoperability in the python and cloud-based ecosystem (DuckDB, Pandas, Polars, Athena, Snowflake)\r\n3. maintaining logical session-boundaries during incremental processing\r\n\r\nWhile the first two issues are solvable in foreseeable future, the last one is specific to our requirements and does not overlap with design decisions made for incremental processing in Delta Lake. Taken together, these points ultimately led us to go back to relying on Parquet again.\r\n\r\n## Targeted Audience\r\n\r\nThis talk is mainly intended for an intermediate data engineering audience but is well suited for interested beginners, too. The content of this talk is relevant for all architects and data engineers being responsible for storing and managing data for analytical workloads.\r\n\r\n# Key takeaways\r\n\r\n- What problems do OTFs solve?\r\n- How do OTFs contribute to an open, composable data stack?\r\n- Is there a predominant Open Table Format?\r\n- How does Delta Lake conceptionally work?\r\n- What are concrete real-world advantages of Delta Lake in contrast to \"plain\" Parquet?\r\n- What is the \"small files\" problem and how does Liquid Clustering help?\r\n- How is the current state of interoperability with Delta Lake?\r\n\r\n# Talk Outline\r\n- Introduction (5 min)\r\n- OTFs in comparison (5 min)\r\n- Delta Lake Internals (10 min)\r\n- Use Case Requirements (5 min)\r\n- Benchmarks & Results (10 min)\r\n- Conclusion and Outlook (5 min)\r\n- Questions (5 min)", "code": "MRHNCV", "state": "confirmed", "created": "2024-12-19", "speaker_names": "Franz W\u00f6llert", "track": "PyData: Data Handling & Engineering"}, {"title": "Reinventing Streamlit", "abstract": "Dreaming of creating sleek, interactive web apps with just Python? Streamlit is great for dashboards, but what if your needs go beyond that? Discover how Reflex.dev, a cutting-edge full-stack Python framework, lets you level up from dashboards to full-fledged web apps!", "full_description": "Have you ever wished you could build sleek, interactive web apps using just Python? Maybe you\u2019ve tried Streamlit and loved its simplicity. But maybe you also had the feeling that your dashboard is no longer a dashboard and your needs have outgrown Streamlit's data model.\r\n\r\nIn this talk, I\u2019ll introduce Reflex.dev, a powerful Python framework that makes web development effortless. Reflex combines the ease of Python with the flexibility of React, enabling you to create full-stack, interactive apps quickly.\r\n\r\nWe\u2019ll cover the basics: what Reflex.dev is and how it stacks up against familiar frameworks. Then, we\u2019ll dive into building a Streamlit-inspired app from scratch in Reflex.dev by creating an API compatibility wrapper. Along the way, I\u2019ll show you how Reflex can:\r\n\r\n- Help you build dynamic, shareable web apps with only Python.\r\n- Smoothly transition your Streamlit app into a stateful Reflex app.\r\n- Make the whole react ecosystem accessible.\r\n- Help testing your application.\r\n\r\nNo web development experience? No problem. This talk is for anyone who wants to create web apps without diving into JavaScript. We\u2019ll stick to Python and start from the ground up.\r\n\r\nBy the end, you\u2019ll leave with a working Streamlit clone and a powerful new tool in your Python arsenal. Let\u2019s make web development fun again!", "code": "7CXSPN", "state": "accepted", "created": "2024-12-16", "speaker_names": "Malte Klemm", "track": "PyCon: Django & Web"}, {"title": "\ud83d\ude32 Quantifying Surprise \u200a- \u200aA Data Scientist's Introduction To Information Theory", "abstract": "How do we measure information, and why does it matter for data scientists and machine learning practitioners?\r\n\r\nThis talk delves into the principles of Information Theory, a cornerstone of statistics and optimization, to show how quantifying information can enhance your statistical analyses and provide deeper insights into the mechanics of state-of-the-art machine learning algorithms.\r\n\r\nUsing intuitive examples\u2014such as coin flips, dice rolls, and the famous Monty Hall problem\u2014we\u2019ll bridge foundational concepts with practical applications, including supervised classification and feature selection. I\u2019ll also share a real-world case study from biotech \ud83e\uddec to demonstrate the practical impact of these tools.\r\n\r\nWhether you're an experienced practitioner or just starting out, you\u2019ll gain actionable knowledge, Python \ud83d\udc0d code, and innovative ways to integrate Information Theory into your data science projects.\r\n\r\nBy the end of the talk you will gain an intuition for useful concept in Information Theory: \r\n\r\n* ***Shannon/Self-Information***\u200a-\u200athe amount of information in an event\r\n* ***Entropy***\u200a-\u200athe average amount of information of a variable's outcome\r\n* ***Cross-entropy*** -\u200athe misalignment between probability distributions (also described by its derivative KL-Divergence)\r\n* ***Mutual Information***\u200a-\u200athe co-dependency of two variables by their conditional probability distributions.", "full_description": "In his 1948 seminal paper Claude Elwood Shannon addressed an age old question: how does one quantify information? His insights pioneered fields involving information: quantification, storage and communications. Insights made major contributions to fields such as Economics, Statistical Physics and Computer Science.\r\n\r\nThe focus of this talk is on the quantification of information as an essential tool for a data scientist's repertoire.  I will show its utility both for statistical analyses as well as understanding the inner-workings of many state of the art machine learning algorithms.\r\n\r\nI demonstrate using common statistics such as coin and dice \ud83c\udfb2 tosses as well as machine learning applications such as in supervised classification and feature selection. I will provide a case study in a biotech \ud83e\uddec. For fun I also apply to the popular brain twister called the Monty Hall problem \ud83d\udeaa\ud83d\udeaa \ud83d\udc10.\r\n\r\nAll examples will be shared in a Jupyter notebook with python code \ud83d\udc0d.", "code": "7MRZ9K", "state": "submitted", "created": "2024-12-20", "speaker_names": "Eyal Kazin", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Scaling Machine Learning Workloads with Python on Kubernetes", "abstract": "This technical session will explore how Python can be used to scale Machine Learning (ML) workloads on Kubernetes. We'll explore the architecture behind ZEISS's computer vision platform, which supports training and inference for various ML model types. You will learn how a Python-based service orchestrates containerized workloads, leveraging Celery, RabbitMQ, and Redis to manage tasks and monitor status. Gain insights into handling challenges such as inter-service communication, secure data access, and Kubernetes role configurations, all while adhering to the principle of least privilege. This session is ideal for developers building scalable ML platforms on Kubernetes.", "full_description": "In this session, we will explore how Python can be leveraged to scale diverse Machine Learning (ML) workloads on Kubernetes.\r\n\r\nAt ZEISS, we develop and maintain a computer vision platform that enables users to train ML models across various types, including:\r\n\r\n- 2D and 3D segmentation\r\n- Classification\r\n- Anomaly detection\r\n- Object detection\r\n\r\nOur platform operates entirely on Kubernetes, with each ML workload (training or inference) running in its own Kubernetes pod. Each model type is a distinct, containerized application that implements the same abstract interface. Managing this complexity requires a dedicated orchestrating service capable of initiating ML workloads based on user-selected models and configuration parameters.\r\n\r\n### Workflow Overview\r\n\r\nWe\u2019ll dive into the end-to-end workflow of initiating and managing ML training tasks in Kubernetes:\r\n\r\n1. **User Request**: A user initiates an ML training task by providing a set of parameters.\r\n2. **Task Creation**: The task is transformed into a message and sent to RabbitMQ.\r\n3. **Task Processing**: The orchestrating service (a Celery worker) retrieves the task from RabbitMQ.\r\n4. **Pod Initialization**: The service starts a Kubernetes pod, executing a containerized ML workload based on the task parameters.\r\n5. **Monitoring**: The service continuously tracks workload status and stores updates in a Redis cache.\r\n\r\n### Key Topics and Challenges\r\n\r\nWe will address the following technical complexities:\r\n\r\n- **Celery and RabbitMQ in Kubernetes**: Best practices for setting up and managing these components in a Kubernetes environment.\r\n- **Inter-Service Communication**: Strategies to ensure seamless interaction between ML workload pods and the orchestrating service.\r\n- **Data Access**: How workload pods access the necessary training data securely and efficiently.\r\n- **Principle of Least Privilege**: Configuring the service to create Kubernetes pods while maintaining the principle of least privilege.", "code": "FLQP98", "state": "submitted", "created": "2025-01-05", "speaker_names": "Irena Grgic", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Introduction to GeoSpatial data in Python", "abstract": "The beautiful thing about Python is its versatility and this comes into limelight when working with GeoSpatial data. From plotting interactive and non-interactive maps to manipulating GeoSpatial data, we will see how Python and its various libraries like GeoPandas, Shapely and Plotly can be used effectively with some real life examples. Also, we will discuss some optimisations you can work around to make your code lighter and run faster.", "full_description": "In this talk, we will discuss primarily these topics - \r\n1. Structure of GeoSpatial Data - GeoJson, shape files and MongoDB\u2019s GeoSpatial queries\r\n2. GeoSpatial data cleaning and manipulation\r\n3. Playing around with GeoPandas and Shapely\r\n4. Plotting interactive and non-interactive maps using Plotly\r\n5. Using GeoSpatial queries on MongoDB\r\n6. Avoiding common pitfalls\r\n\r\nThe technical requirement of this talk is basic understanding of Python.", "code": "CYEVZX", "state": "submitted", "created": "2024-12-30", "speaker_names": "Prabhu Pant", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Dependency management, but enjoyable and without the mess: Meet uv", "abstract": "Have you ever been in the \u201ceducational\u201d situation where your carefully crafted venv refused to install as soon as your colleague tried it on a different OS?\r\n\r\nHave you ever happily installed dependencies from requirements.txt, only to be greeted later by obscure warnings from some random library\u2014because, for example, you used Python 3.9 instead of Python 3.10?\r\n\r\nHave you ever felt your heart rate spike at the thought of running conda install in an environment full of pip installs?\r\n\r\nYeah, me too. And not only once. \r\n\r\nMaybe you\u2019ve heard about the new cool kid in town, `uv`. I recently gave it a try and it was instant love - I have never gotten that excited about a Python tool in my life!\r\n\r\nIn this talk, I\u2019ll share my first-hand experience with `uv` as a data scientist: how I use it, how it simplifies dependency management, and the cool Python features I discovered while tinkering with it. Since uv is (practically) inseparable from `pyproject.toml`, we\u2019ll also learn about this fantastic standard and how using both together can make deploying your code a breeze.\r\n\r\nDISCLAIMER: I am in no way associated with `uv`\u2019s authors. I just think they built a phenomenal tool. All opinions expressed here are my own.", "full_description": "Have you ever been in the \u201ceducational\u201d situation where your carefully crafted venv refused to install as soon as your colleague tried it on a different OS?\r\n\r\nHave you ever happily installed dependencies from requirements.txt, only to be greeted later by obscure warnings from some random library\u2014because, for example, you used Python 3.9 instead of Python 3.10?\r\n\r\nHave you ever felt your heart rate spike at the thought of running conda install in an environment full of\u00a0pip installs?\r\n\r\nYeah, me too. And not only once.\u00a0\r\n\r\nMaybe you\u2019ve heard about the new cool kid in town, `uv`. I recently gave it a try and it was instant love -\u00a0I have never gotten that excited about a Python tool in my life!\r\n\r\nIn this talk, I\u2019ll share my first-hand experience with `uv` as a data scientist: how I use it, how it simplifies dependency management, and the cool Python features I discovered while tinkering with it. Since `uv` is (practically) inseparable from `pyproject.toml`, we\u2019ll also learn about this fantastic standard and how using both together can make deploying your code a breeze.\r\n\r\nWant more details? Here are some of the things we\u2019ll talk about:\r\n\r\n**The Pain(s): Current situation in typical Data Science projects**\r\n\r\nHow data science teams out there typically deal with dependency management, and the issues that arise time and again.\r\n\r\n**Using `uv`**\r\n\r\nWe\u2019ll dive into how to:\r\n- Install uv.\r\n- Initialize a project.\r\n- Add dependencies.\r\n- Work with dependency groups.\r\n\r\n**`pyproject.toml` and `uv.lock`**\r\n\r\nAs soon as you run `uv init` you\u2019ll see these files. But who are these guys? You\u2019ll end up loving them.\r\n\r\n**Syncing and locking - What\u2019s the difference?**\r\n\r\n`pip intall -r requirements.txt` et al are replaced by an incredibly elegant `uv sync`. But there is `uv lock` as well. And `uv pip install`. When do you need each?\r\n\r\n**`uv run` - Or an entire wishlist come true in six characters**\r\n\r\nThis is one of my personal highlights. `uv run` stands for \u201cNo Python installed? Not my problem. Just handle everything and run my code already\u201d. Could you ask for anything more?\r\n\r\n**Take-home sample repo**\r\n\r\nI\u2019ll share a repo with you where you can try all of the above in your own machine!\r\n\r\n**The Takeaways**\r\n\r\n1. A clear understanding of `uv`\u00a0for dependency managament and how it solves common issues.\r\n\r\n2. Insights into `pyproject.toml` and `uv.lock`\u00a0\u2014 what they are, why they matter, and how they make projects more maintainable.\r\n\r\n3. Practical knowledge of `uv`\u00a0commands like `uv sync`,`uv lock`, and `uv run`.\r\n\r\n4. A sample project repo to start experimenting right away.\r\n\r\nDISCLAIMER: I am in no way associated with `uv`\u2019s authors. I just think they built a\u00a0phenomenal\u00a0tool. All opinions expressed here are my own.", "code": "NAYEQP", "state": "submitted", "created": "2024-12-21", "speaker_names": "Samuel L\u00f3pez Santamar\u00eda", "track": "PyCon: MLOps & DevOps"}, {"title": "They are not unit tests: a survey of unit-testing anti-patterns", "abstract": "The entire industry approves of unit testing but almost no one can fully agree on how to do it correctly, or even on what unit tests are. This results in unit tests often being associated with slower development cycle and an overall less enjoyable workflow. I'll show you how testing turns into hell in real enterprises with the most common anti-patterns and then I'll show you that most of them are avoidable with modern tooling like mutation testing, snapshot testing, dirty-equals, and many more. We'll discuss how to make tests speed up your development and make refactoring easy.", "full_description": "Similar to TDD, unit tests are one of the most misunderstood concepts in software engineering. In this session, I will cover the most important fallacies about unit testing and the most common anti-patterns. I will also show you how modern infrastructure (pytest-fixture-classes, inline-snapshot, dirty-equals, import-linter, mutmut, and pytest-xdist) makes it possible to avoid most of them. \r\n\r\nWe will discuss that the real goal of tests is not always stability and how tests often make refactoring and restructuring your project easy, not hard. I will define my criteria for good tests and then for the rest of the session, we will be using it to analyze anti-patterns and explore modern solutions to them. You will see:\r\n\r\n1. How people make their \"units\" too small and how you can prevent it using import-linter\r\n2. How people make their \"units\" too big and what architectural patterns can you use to make them smaller\r\n3. How the real value of tests is in the quality of their assertions and how mutation testing can measure it for you\r\n4. How people end up with asserting too much, and how inline-snapshot and dirty-equals make this problem obsolete\r\n5. How people try to cover the volatile parts of their software, and how coveragepy already has tooling to prevent it\r\n6. How slow tests hurt you, and how to make your tests fast even if you tried it many times and failed\r\n7. How to build an architecture that makes writing tests hard, and how to make it easy using inline-snapshot, pytest-fixture-classes, and a few clever tricks\r\n8. How you can mock your way into making your tests useless and what you should mock\r\n\r\nAfter this session, your tests will become your friend instead of slowing you down.", "code": "VFE78U", "state": "confirmed", "created": "2024-12-21", "speaker_names": "Stanislav Zmiev", "track": "PyCon: Testing"}, {"title": "Greening the Labour Market: Using NLP to Identify in-demand Skills", "abstract": "We will discover how to identify in-demand \u2018green skills\u2019 and jobs in the labour market using natural language processing. Learn about the \u2018Green Skill Ratio\u2019 and how it can be used to measure the environmental impact of different industries.", "full_description": "The talk will explore the concept of \u201cgreen skills\u201d and \u201cgreen jobs\u201d, which are skills and occupations that contribute to preserving or restoring the natural environment. The focus will be on using natural language processing (NLP) techniques to create an index of greenees of different types of work, in order to better understand and quantify the environmental impact of different industries and occupations. The talk will also discuss the potential benefits of investing in green skills and jobs, such as reducing carbon emissions, improving air and water quality, and conserving natural resources. Additionally, the talk will explore the challenges and opportunities of using NLP to create the index as well as the limitations of using text-based data to measure environmental impact.", "code": "NZU9GF", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Getting Started with Bayes in Engineering: Implementing Kalman Filters with RxInfer.jl", "abstract": "Bayesian methods are not commonly seen in Civil Engineering and Structural Dynamics. In this talk we explore how RxInfer.jl and the Julia Programming Language can simplify Bayesian modeling by implementing a Kalman filter for tracking the dynamics of a structural system. Perfect for engineers, researchers, and data scientists eager to apply probabilistic modelling and Bayesian methods to real-world engineering challenges.", "full_description": "Bayesian methods are renowned for their ability to incorporate domain knowledge and quantify uncertainty, making them valuable across various engineering and data science fields. However, finding practical examples of these methods in civil engineering, especially within structural dynamics, can be challenging.\r\n\r\nThis talk aims to make Bayesian inference accessible to engineering practitioners by demonstrating how RxInfer.jl, a Julia package for probabilistic programming, can be used to implement a Kalman filter for tracking the dynamics of a structural system. The session covers:\r\n\r\n1. Bayesian Modelling in Python and Julia: A brief comparison of probabilistic programming languages, highlighting Python and Julia\r\n2. State Space Modelling of Structural Dynamical Systems: A brief introduction to state space models and their use in structural dynamics\r\n3. Linking State Space Modelling to Finite Element Modelling: Making the connection between FEM and SSM\r\n4. A Simplified Overview of Bayesian Filtering and Kalman Filters for Dynamical Systems\r\n5. Bayesian Filtering Made Simple with RxInfer.jl: a step-by-steo guide to setting up a user-friendly and readable Bayesian filter using Rxinfer.jl\r\n6. Full Workflow Example\r\n7. Interpreting the Results and Next Steps\r\n8. Connections to Julia, Python and Open-Source Ecosystems: exploring integrations with tools like FreeCAD and other open-source platforms\r\n\r\nBy the end of the talk, attendees will have a clear understanding of how to start using Bayesian methods in their engineering projects, supported by reproducible and open-source code.", "code": "U9KHNA", "state": "accepted", "created": "2024-12-21", "speaker_names": "Victor Flores Terrazas", "track": "PyData: Research Software Engineering"}, {"title": "Building Knowledge Graph RAGs with Neo4j", "abstract": "* Knowledge graphs are used in development to structure complex data relationships, drive intelligent search functionality, and build powerful AI applications that can reason over different data types.", "full_description": "\u2022 Knowledge graphs can connect data from both structured and unstructured sources (databases, documents, etc.), providing an intuitive and flexible way to model complex, real-world scenarios.\r\n\u2022 Unlike tables or simple lists, knowledge graphs can capture the meaning and context behind the data, allowing you to uncover insights and connections that would be difficult to find with conventional databases.\r\n\u2022 This rich, structured context is ideal for improving the output of large language models (LLMs), because you can build more relevant context for the model than with semantic search alone.", "code": "FTMBZH", "state": "submitted", "created": "2024-12-21", "speaker_names": "Jayita Bhattacharyya", "track": "PyData: Generative AI"}, {"title": "Query your structured data with a LangChain AI agent", "abstract": "In this talk, we will explore the capabilities of LangChain, a versatile framework tailored for the development of applications powered by Large Language Models (LLMs).\r\nWe will see how to use LangChain to build advanced AI applications with autonomous AI agents.", "full_description": "In this talk, we will explore the capabilities of LangChain, a versatile framework tailored for the development of applications powered by Large Language Models (LLMs).\r\nBy bridging the gap between LLMs and sources of context, such as prompt instructions and few-shot examples, LangChain empowers applications to understand and reason about user queries effectively.\r\nWith over 90k stars on GitHub, LangChain stands as the leading Generative AI framework in terms of adoption.\r\nDuring the presentation, attendees will witness a live demonstration showcasing LangChain's SQL agent in action. The agent will autonomously choose available tools to get information about the database schema, select relevant \u201cgolden sample\u201d queries from a vector store that will reduce so-called hallucinations from the LLM, call the LLM to get a text-2-SQL answer, execute the query on the database, and build an answer from the query result to the initial question.", "code": "UVVACB", "state": "submitted", "created": "2025-01-05", "speaker_names": "Christophe Bornet", "track": "PyData: Generative AI"}, {"title": "Efficient Embedding Generation using ONNX models in Rust", "abstract": "Embeddings are now a popular representation for finding similar items from a collection, but how do you generate them using optimized ONNX models? \r\n\r\nThis talk answers that question. Through it, you will learn how to access and load up ONNX models via the Hugging Face Hub. You will also see how to use them to convert multi-modal input (text, images) into embeddings using the ort crate.\r\n\r\nIn the end, you will walk away with knowledge that equips you with the ability to cut down the time it takes to go from input to embeddings for your self-hosted models. If you do not have Rust knowledge, the ideas are also easily transferrable to Python, as ONNX enables accessibility across multiple languages.", "full_description": "Embeddings are now a popular representation for finding similar items from a collection, but how do you generate them using optimized ONNX models? \r\n\r\nThis talk answers that question. Through it, you will learn how to access and load up ONNX models via the Hugging Face Hub. You will also see how to use them to convert multi-modal input (text, images) into embeddings using the ort crate.\r\n\r\nA proposed structure for this talk is:\r\n\r\n- Introducing the ONNX format: Here you will learn about what ONNX models are and why the format is widely accepted in the machine learning industry. You will also briefly see how to generate ONNX models from other models, say PyTorch or TensorFlow.\r\n\r\n- Accessing model weights and model artifacts: Hugging Face has become an incredible enabler of machine learning work in industry and academia. This part of the talk goes into how to download model weights and other essential artifacts from the Hugging Face Hub using the hf_hub crate.\r\n\r\n- Understanding the model artifacts: There are a couple of useful files needed for properly running inference with the downloaded model weights. You will learn about the contents of those files and understand how they help the model yield the best results.\r\n\r\n- Using the ONNX Runtime: In this section, you will learn how to run inference via the ONNX Runtime using the ort crate. This section will cover some of the setup required to load ONNX models and then convert the preprocessed inputs into embeddings.\r\n\r\n- Tying it all together through the lens of a real-world application: Ahnlich-ai is an open-source tool written in Rust and built to aid semantic search. It helps generate embeddings from text, images, and audio inputs using self-hosted models. At its foundation are the core ideas already discussed in the previous sections. Here you will learn how Ahnlich-ai puts together the various ideas to serve the purposes it does today.\r\n\r\nIn the end, you will walk away with knowledge that equips you with the ability to cut down the time it takes to go from input to embeddings for your self-hosted models. If you do not have Rust knowledge, the ideas are also easily transferrable to Python, as ONNX enables accessibility across multiple languages.", "code": "RJSV9J", "state": "submitted", "created": "2025-01-05", "speaker_names": "Habeeb Shopeju, Diretnan Domnan", "track": "General: Rust"}, {"title": "From RAGs to riches: Advanced Retrieval with Python", "abstract": "**RAG (Retrieval-Augmented Generation)** is one of the most common application of LLMs, often used for e.g. question-answering systems based on non-public data. In its basic form (\"Naive RAG\"), a user query is compared to documents in a database, with matching entries included in the prompt to generate an answer.\r\n\r\nHowever, **relevant documents might not be found**, decreasing the quality of the result or preventing a correct answer. To remedy this, many **advanced RAG methods** exist.\r\n\r\nIn this talk, different chunking and indexing methods, query rewriting, hybrid search and knowledge graphs, result re-ranking and RAG evaluation metrics **will be described and shown how to be used in Python**.\r\n\r\nAfterwards, you will be able to **improve the performance of your RAG system** as you are able to identify weaknesses and apply the proper methods for improvement.", "full_description": "This talk will start with a basic RAG example, identify weaknesses and show several methods for improvement. Implementation examples in Python will be shown for most techniques.\r\n\r\nOutline:\r\n- Naive RAG\r\n- Evaluation of RAG performance: metrics and LLM-as-a-judge\r\n- Prepare Embeddings & Vectorstore\r\n  - Different vector stores\r\n  - Indexing (RAPTOR)\r\n- Query Enhancement\r\n  - Query Rewriting\r\n  - Hypothetical Document Embedding (HyDE)\r\n  - Query Decomposition\r\n- Improved Search\r\n  - Hybrid Search\r\n  - Filters\r\n- Post-Retrieval\r\n  - Reranking\r\n  - Self Route", "code": "7XDAVM", "state": "submitted", "created": "2024-12-18", "speaker_names": "Dominik Haitz", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "The Secrets of Bones: Person Identification Using 3D Skeletons", "abstract": "This presentation unveils an advanced machine learning model that leverages unique skeletal attributes to achieve robust person identification, providing an alternative to facial recognition in specific use cases.\r\nThe core of the model is a Siamese neural network architecture, which is adept at differentiating nuanced characteristics present in skeletal data. The approach often exceeds expectations delivering highly accurate cross-camera and cross-angle identification.", "full_description": "Person identification is a crucial task in computer vision, traditionally relying on facial recognition or fingerprint analysis. Instead, we are focusing on unique skeletal features. The talk shows how to leverage the distinct characteristics of an individual's skeleton, such as height, joint distances, and relative sizes to accurately identify people across various cameras and viewpoints. The core is a Siamese ML-Network, which is adept at comparing pairs of data and learning the subtle differences between them. We also explore different architectures and compare the results. The models are trained using labeled camera data, pre-processed through a pose estimation model to extract the 3D skeletons. We observe a very good separation of features among test subjects. The approach often exceeds expectations delivering highly accurate cross-camera and cross-angle identification.", "code": "SJBUFP", "state": "submitted", "created": "2024-12-17", "speaker_names": "Jonathan Tilly", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "GitMLOps \u2013 How we are managing 100+ ML pipelines in AWS SageMaker", "abstract": "Scaling machine learning pipelines is no small feat - especially when you\u2019re managing over 100 of them on AWS SageMaker. In this talk, I\u2019ll take you behind the scenes of how our team at idealo built a Git-based MLOps framework that powers millions of real-time recommendations every minute.\r\n\r\nI\u2019ll share the challenges we faced, the solutions we implemented, and the lessons we learned while streamlining model versioning, deployment, and monitoring. This session is packed with actionable takeaways for ML engineers, data scientists, and DevOps professionals looking to simplify their MLOps workflows and operate efficiently at scale.\r\n\r\nWhether you\u2019re running a handful of pipelines or preparing to scale up, this talk will equip you with the tools and strategies to tackle MLOps with confidence.", "full_description": "In 2022, idealo\u2019s Machine Learning Engineering (MLE) team took on a bold mission: to transform and scale the recommendation systems powering the idealo website. Fast forward to today, we\u2019re delivering over 1 million recommendations per minute across 20 key user touchpoints - driving seamless, personalized experiences at scale.\r\n\r\nBut how do you manage over 100 machine learning pipelines without breaking a sweat? In this talk, I\u2019ll reveal the three core principles that helped us build a sustainable and efficient MLOps workflow in AWS SageMaker:\r\n\r\n* Decoupling pipeline releases from deployments for ultimate flexibility\r\n* Testing pipelines to ensure seamless performance\r\n* Centrally managing infrastructure as code for full control and scalability\r\nIf you\u2019re ready to supercharge your MLOps game, this session will leave you with practical strategies and battle-tested solutions for running ML pipelines like a pro.", "code": "DPAPUA", "state": "submitted", "created": "2024-12-16", "speaker_names": "Bogdan Girman", "track": "PyCon: MLOps & DevOps"}, {"title": "Developing Robust and Scalable RAG Systems: Lessons Learned from Five Use Cases", "abstract": "Retrieval-Augmented Generation (RAG) enables enterprises to effectively use large language models with their own proprietary data. However, the rapid evolution of underlying technologies makes selecting and implementing suitable RAG solutions increasingly challenging.\r\n\r\nIn this talk, we share key insights from five real-world use cases, focusing on (1) building a modular, adaptable RAG system leveraging open-source libraries like LiteLLM and Langfuse, (2) ensuring consistent and scalable setups through Infrastructure-as-Code (IaC), and (3) applying systematic evaluation methods to enhance system quality. Attendees will leave with actionable strategies to design robust, GDPR-compliant RAG systems, integrate proprietary knowledge sources, and continuously optimize performance for scalable, future-proof applications.", "full_description": "ChatGPT is widely known for providing answers to almost any question, yet it faces significant limitations when dealing with current, personal, or proprietary information\u2014such as confidential internal product details. Retrieval-Augmented Generation (RAG) bridges this gap by supplying large language models with external knowledge for response generation. However, building productive RAG systems comes with significant technical and operational challenges, including technology selection, scalability, and robustness.\r\nDrawing from five practical use cases, this talk will demonstrate how to design and implement robust, scalable RAG systems tailored to diverse organizational needs.\r\n\r\n**Key Takeaways**:\r\n  **1. Building a Modular Core System:** Learn how a flexible, modular architecture\u2014integrating different open-source libraries\u2014can simplify RAG application development across multiple use cases.\r\n  **2.\tScalable Infrastructure with IaC:** Discover how Infrastructure-as-Code (IaC) ensures consistent, reproducible setups across testing and production environments or different projects.\r\n  **3.\tIntegrating Proprietary Knowledge Sources:** Learn about our lessons learned from connecting RAG systems with enterprise-specific platforms like SharePoint and intranets, handling diverse data formats effectively.\r\n**4.\tCost Tracking:** Understand how open-source tools like LiteLLM facilitate transparent budget tracking across teams and use cases.\r\n**5.\tContinuous Improvement Through Feedback:** Discover how libraries like Langfuse enable robust evaluation and feedback mechanisms, allowing you to identify areas for improvement and systematically address them to enhance retrieval accuracy and overall system performance. \r\n\r\nThis session will equip attendees with actionable insights and strategies to:\r\n\u2022\tDesign scalable, GDPR-compliant RAG systems using IaC\r\n\u2022\tIntegrate proprietary knowledge sources seamlessly.\r\n\u2022\tContinuously improve system performance through structured feedback loops.", "code": "NGTP3M", "state": "submitted", "created": "2024-12-20", "speaker_names": "Arne Grobr\u00fcgge, Alina Dallmann", "track": "PyData: Generative AI"}, {"title": "Streamlining LLMOps for Seamless Enterprise Production", "abstract": "As enterprises increasingly adopting Large Language Models (LLMs) for diverse AI applications, they encounter challenges in transitioning these solutions from proof-of-concept to production. Traditional MLOps workflows, designed primarily for conventional ML models, struggle to accommodate LLM-specific challenges such as managing embeddings, orchestrating sophisticated data ingestion pipelines, and deploying context-aware intelligent agents at scale. Compounding these difficulties is the requirement to integrate seamlessly with existing enterprise infrastructure, maintain reproducibility and traceability, and ensure optimal performance.\r\n\r\nIn this session, we share our hands-on experience to shed light on the pain points encountered while putting LLMs into production. Attendees will learn about bottlenecks from handling unstructured text data ingestion to scaling vector databases for efficient retrieval and discover how to address them with best tested strategies. We will also discuss the broader open source LLMOps landscape, revealing best practices for pipeline design, version control, model monitoring, and cross-functional collaboration. By sharing these lessons and insights, we aim to deliver our audience with practical knowledge that can be applied to their MLOps/LLMOps stack with any framework or tooling, so they can effectively leverage the power of LLMs for enterprise-scale applications.", "full_description": "Enterprises looking to operationalize LLMs often encounter numerous challenges that extend beyond those in traditional machine learning deployments, as standard MLOps practices cover areas like continuous integration, model monitoring, and automated deployment but may fall short when addressing the unique characteristics of LLMs, especially in embedding management, large-scale pipeline orchestration, and the dynamic behavior of intelligent, context-driven agents.\r\n\r\nIn this talk, we will explore how to efficiently manage large volumes of unstructured text data and maintain data quality in fast-paced environments, covering strategies for generating, storing, and updating document embeddings within vector databases to enable quick and context-aware retrieval, as well as methods for orchestrating and scaling pipeline components to ensure that every step from data preprocessing to model serving is reproducible and streamlined. We will also delve into considerations for deploying models and agents in real-world scenarios by addressing factors such as latency, context switching, and incorporating user feedback loops, while examining version control and compliance to maintain a clear audit trail for artifacts, models, and data changes, ensuring enterprise-grade governance and reproducibility.\r\n\r\nWhether you're an AI engineer, a DevOps specialist, or a technical leader, you'll end-up learning practical tips and insights to handle the quickly changing world of LLMOps with this talk, no matter which tools or platforms you use.", "code": "JVMWB7", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Rudraksh Karpe", "track": "PyCon: MLOps & DevOps"}, {"title": "Making Robots See: Using Python for Basic Computer Vision", "abstract": "Our eyes are our windows to the world. Through them we recognize familiar faces, read books that make us giggle, or even walk to our favourite coffee shop. With Python, you can allow robots to see the world just like you do! By giving them this ability, they become smarter and more interactive.\r\n\r\nIn this talk, I\u2019ll discuss how to achieve this using Python and OpenCV. You\u2019ll learn the basics of processing images, detecting objects and making your robot respond to its environment. We\u2019ll also cover how to integrate this in your robot or simulate this in Gazebo.\r\n\r\nBy the end, you\u2019ll know how to give your robot the ability to see and respond to the world, _just like you do!_", "full_description": "We use our eyes to make sense of the world\u2014recognizing faces, finding our way, and interacting with our surroundings. With Python and OpenCV, we can give robots this powerful capability, and it\u2019s simpler than you might think!\r\n \r\nIn this talk, I\u2019ll walk you through the basics of computer vision and how to simulate robot vision in a virtual environment. We\u2019ll start by processing images, detecting objects, and making sense of visual data. Then, we\u2019ll explore how to integrate this into robot simulations using Gazebo, so you can experiment without needing physical hardware. \r\n\r\n**Here\u2019s the outline:** \r\n1. Introduction (3 min)\r\n2. Why computer vision matters for robots (2 min) \r\n3. Getting started with image processing in Python using OpenCV (5 min) \r\n4. Teaching robots to identify and recognize objects (5 min)\r\n5. How to integrate computer vision into simulated robots in Gazebo (3 min)\r\n6. Live demo: Making robots see with Python and OpenCV (5 min)\r\n7. Q&A session (2 mins)\r\n\r\nIf you\u2019ve ever been curious about how robots can see and respond to the world, this talk is for you. No hardware? _No problem!_ All you need is Python, a bit of curiosity, and your computer.", "code": "BW3XNH", "state": "submitted", "created": "2024-12-22", "speaker_names": "Toyibat Adele", "track": "PyData: Embedded Systems & Robotics"}, {"title": "ML and LLM in production", "abstract": "The complexity of bringing ML models into production went really out of the roof in the last few years. Now it\u2019s not just deploying a model from your laptop to the cloud, but includes cars, IoT devices, mobile phones and custom hardware.\r\n\r\nAnd LLMs require a completely new set of capabilities. To manage the cost of the inference, assess drifts and prevent your users from putting glue on Pizzas.", "full_description": "The job of data scientists and AI devs is pretty tough. It often starts from a paper with almost no runnable-code, or from an off-the-shelve model that has to be optimized or fine tuned.\r\n\r\nBut that\u2019s not enough. Models might need to be trained or deployed on specific hardware (GPU, IoT and mobiles), and performance might drift over time.\r\n\r\nDuring this talk I\u2019ll share my experience of leading platform and AI service teams, how we tackled some of the major problems you might find along your AI journey like training, serving and monitoring ML models. \r\n\r\nWe start from a data scientist\u2019s laptop and talk on how to make her successful, covering both capabilities that you should provide and which tools might help.\r\n\r\nThe last part of the talk is dedicated to LLMs, how they are different from a serving and monitoring perspective compared to traditional ML models, and how to start using them in production.", "code": "FS8NRZ", "state": "submitted", "created": "2024-12-22", "speaker_names": "Christian Barra", "track": "PyCon: MLOps & DevOps"}, {"title": "Overcoming the Top 5 Data Challenges in AI Model Development", "abstract": "AI models are only as good as the data that trains them. In this talk, we will explore the top five pitfalls in web data collection and preparation that can hinder AI models' performance, fairness, and longevity. From addressing data bias and ensuring diverse datasets to balancing model complexity and maintaining data quality, attendees will clearly understand how to avoid common mistakes. Additionally, we\u2019ll cover the critical issue of data drift and how to future-proof AI models against evolving real-world conditions. Packed with actionable insights, this session is designed for data scientists, engineers, and AI practitioners looking to enhance the reliability and effectiveness of their models.", "full_description": "Building robust AI models starts with understanding and addressing common challenges in web data collection and preparation. This session focuses on the five most critical pitfalls to avoid:\r\n\r\nData Bias: Explore the risks of biased datasets and their potential to produce unfair or discriminatory AI outcomes. Learn strategies to detect and mitigate bias, ensuring fairness and inclusivity in your models.\r\n\r\nInsufficient Data Variety: Understand the importance of diverse datasets for training AI models that can perform effectively across varied scenarios and real-world conditions.\r\n\r\nOverfitting and Underfitting: Discover how to strike the right balance in model complexity, avoiding the traps of overfitting and underfitting to achieve optimal generalization.\r\n\r\nPoor Data Quality: Recognize how errors, inconsistencies, and poor labeling in training data can undermine AI reliability, and learn best practices for ensuring data integrity.\r\n\r\nIgnoring Data Drift: Learn how to monitor and adapt to data drift over time, keeping AI models relevant and effective in changing environments.", "code": "PD9EJY", "state": "submitted", "created": "2024-12-24", "speaker_names": "Syed Ansab Waqar Gillani, Syed Hassan Gilani", "track": "PyCon: Programming & Software Engineering"}, {"title": "Turning a Jupyter Notebook into a deployable artifact", "abstract": "The deployment of AI projects often faces significant hurdles due to the fragmented nature of their components\u2014datasets, models, and model weights are frequently stored in separate repositories, formats, or locations.\r\n\r\nIn this talk, we will explore the critical challenges posed by these disjointed workflows. We will discuss how we can addresses these issues by introducing an open source universal, open-source packaging and versioning solution based on the Open Container Initiative (OCI) standard to package together all of AI components in a single image by sharing production examples of open source projects like KitOps which are focused on making AI packaging more streamlined. This enables seamless integration of AI/ML components into cloud-native infrastructures, ensuring secure, auditable, and efficient deployment.", "full_description": "The deployment of AI projects often faces significant hurdles due to the fragmented nature of their components\u2014datasets, models, and model weights are frequently stored in separate repositories, formats, or locations.\r\n\r\nIn this talk, we will explore the critical challenges posed by these disjointed workflows. We will discuss how we can addresses these issues by introducing an open source universal, open-source packaging and versioning solution based on the Open Container Initiative (OCI) standard to package together all of AI components in a single image by sharing production examples of open source projects like KitOps which are focused on making AI packaging more streamlined. This enables seamless integration of AI/ML components into cloud-native infrastructures, ensuring secure, auditable, and efficient deployment.\r\n\r\nThus attendees will discover practical strategies for standardizing AI/ML packaging with a practical example of turning Jupyter notebook into deployable artifacts. \r\n\r\nThe talk will dive into the limitations of Jupyter Notebooks in production environments and introduce a step-by-step process to overcome these hurdles. Using a real-world example of fine-tuning a LLama3 LLM model, we'll demonstrate how to leverage tools like KitOps and ModelKit to create immutable, secure, and shareable artifacts for Jupyter notebooks\r\n\r\nKey topics include:\r\n\r\n- Understanding the stages of bringing an ML model to production\r\n- Creating untuned ModelKits for dataset preparation\r\n- Fine-tuning models and packaging adapters\r\n- Transitioning from challenger to champion models\r\n- Deploying models to production environments like Kubernetes\r\n- Bringing Container-Native Simplicity to AI/ML", "code": "N8FDJY", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Gaurav Pandey", "track": "PyCon: MLOps & DevOps"}, {"title": "Lessons Learned Building a Dashboard using Open-Source Technologies to Address Business Needs", "abstract": "Most organizations leverage dashboards to monitor data, processes, or business metrics. However, commercial solutions often fall short due to licensing costs, lack of customization, or limited integration options. In contrast, open-source solutions offer unmatched flexibility and transparency. This presentation will explore the advantages of using open-source dashboards from both the business manager's and developer's perspectives.\r\n\r\nWe\u2019ll reflect on five years of lessons learned, revisiting core challenges faced during the development of custom dashboards. Using GLEIF\u2019s publicly accessible Data Quality Dashboard as an example, we'll discuss how to choose the right open-source tools while balancing business needs and technical feasibility. At the core, the Dashboard builds on popular Python libraries, like FastAPI, Celery, and SQLAlchemy. Attendees will gain insights into avoiding common pitfalls and learn strategies for integrating open-source tools into business use cases.\r\n\r\nThis talk will provide a unique dual perspective, offering actionable insights for both developers and business stakeholders. It aims to foster a richer understanding of how Python tools can deliver superior outcomes when business goals and technical strategies align.", "full_description": "We begin with an introduction to the problem from a service owner perspective, highlighting the high cost, limited customizability, and transparency issues of commercial dashboards, while emphasizing the flexibility and control that open-source solutions offer. Next, we discuss choosing a web framework, focusing on the reasons why FastAPI is ideal for our development and business needs while comparing it to alternatives such as Flask and Django.\r\n\r\nWe then discuss data source integration, detailing the challenges developers face with ORMs such as SQLAlchemy and highlighting the business requirements for long-term data integrity. This is followed by an exploration of performance management, where we explain how task queues optimize compute-intensive tasks and ensure smooth user experiences that are critical for real-time decision-making.\r\n\r\nFinally, we summarise the development of a fully functional open-source dashboard tailored to both business and technical requirements, setting the stage for a dynamic discussion. The session concludes with a Q&A session that encourages participants to share their insights and explore best practices for building custom open-source dashboard solutions.\r\n\r\nThe Global Legal Entity Identifier Foundation (GLEIF) is a non-profit organization dedicated to supporting the implementation of the Legal Entity Identifier (LEI). The LEI is the transparency island in a cloudy environment. It provides open, free-of-charge, high-quality legal entity data with global coverage. The Data Quality Dashboard, developed by &effect, is an integral component of GLEIF\u2019s data quality framework, enabling public data users to assess the data quality within the global LEI system.\r\n\r\n&effect data solutions GmbH specialises in the development of innovative data solutions for the social and public sector. Our mission is to make data science an integral part of decision-making in the public and social sectors.", "code": "8HKJZJ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Camille Koenders", "track": "PyCon: Programming & Software Engineering"}, {"title": "High-performance dataframe-agnostic GLMs with glum", "abstract": "Generalized linear models (GLMs) are interpretable, relatively quick to train, and specifying them helps the modeler understand the main effects in the data. This makes them a popular choice today to complement other machine-learning approaches. `glum` was conceived with the aim of offering the community an efficient, feature-rich, and Python-first GLM library with a scikit-learn-style API. More recently, we are striving to keep up with PyData community's ongoing push for dataframe-agnosticism.\r\nWhile `glum` was originally heavily based on `pandas`, with the help of `narwhals`, we are close to being able to fit models on any dataset that the latter supports. This talk presents our experiences with achieving this goal.", "full_description": "Arguably, `glum`'s standout feature is its ability to efficiently handle datasets consisting of a mix of dense, sparse and categorical features. To facilitate this, it relies on our (similarly open-source) `tabmat` library, which provides classes and useful methods for mixed-sparsity data. `glum` fits models by first converting input data to `tabmat` matrices, and then using those matrices to do the necessary computations.\r\n\r\nTherefore, dataframe-agnostism in our case mostly boils down to handling the conversion of different dataframes to `tabmat` matrices (which themselves store data in `numpy` arrays and sparse `scipy` matrices) in an efficient manner. Most of it is rather smooth and straightforward due to `narwhals` providing a convenient compatibility layer for a wide range of dataframe functionality. However, we have encountered a couple of pain points that might be of interest to other package maintainers and the PyData community. In particular,\r\n\r\n- We heavily rely on manipulating the category order and encoding of categorical variables, for which there is somewhat limited support. This is due to various dataframe libraries handling cateegorical columns somewhat differently.\r\n- Most dataframe libraries do not support sparse columns, while for us, it is important to be able to accept sparse inputs.\r\n\r\nIn this talk I demonstrate how we used `narwhals` to easily accept multiple types of dataframes. I will go into details about categorical and sparse columns, and present the challenges we encountered with those. I will also examine the benefits and challenges of supporting sparse columns in dataframe libraries and the Arrow stardard. These points are meant to facilitate discussion among the participants and in the PyData community.\r\n\r\nAt the end of the talk I will also briefly mention potential future plans for `glum` and `tabmat`, including the possibility to do computations directly on Arrow objects without converting them to `numpy` and `scipy` arrays. \r\n\r\n### Outline\r\n\r\n1. A short intro to `glum`, it's backend library `tabmat`, and the main ideas that make them performant.\r\n2. Making `glum` dataframe-agnostic.\r\n    - Showcase how `narwhals` simplifies handling a wide variety of dataframes.\r\n    - Discuss handling categorical (and enum/dictionary) columns.\r\n    - Talk about representing sparse columns in dataframes.\r\n3. Concluding remarks and potential future plans.\r\n\r\n### Target audience\r\n- Basic understanding of the scientific Python ecosystem (with a focus on dataframe libraries) is recommended.\r\n- While some familiarity with linear models might be useful to get the most out of this talk, it is by no means required.\r\n\r\n### Main takeaways\r\n\r\n- How `glum` efficiently handles mixed-sparsity data\r\n- How `narwhals` helps to achieve dataframe-agnosticism with little effort\r\n- Differences between categorical types in various packages and the Apache Arrow specification.\r\n- How support for sparse column could be incorporated into dataframe libraries and the Arrow Columnar Format", "code": "JUQ9JJ", "state": "submitted", "created": "2025-01-04", "speaker_names": "Martin Stancsics", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Tactile Sensing for Robotics and the Role Python can Play", "abstract": "What if machines could not only act but also feel? Tactile sensing gives robots the ability to perceive touch, enabling them to handle delicate objects, recognize textures, and adapt their grip, just like humans. This talk introduces tactile sensing, its evolution, and advancements like vision-based systems, which combine visual and tactile information to enhance adaptability. It will also highlight how Python developers can contribute to this exciting field by leveraging tools like PyTorch for building machine learning models and OpenCV for processing tactile data. By the end, attendees will understand the importance of touch in robotics and the role Python plays in advancing this innovation.", "full_description": "Robotics has excelled at navigation and manipulation, but tactile sensing takes it further by giving robots the ability to handle delicate objects, recognize textures, and adapt based on touch. This session explores how tactile sensing mimics human touch, making robots more adaptable and effective in real-world applications.\r\n\r\nWhat We Will Explore:\r\n1. The basics of tactile sensing and its importance in robotics.\r\n2. How tactile sensing mimics human touch to enhance robotic adaptability.\r\n3. Vision-based tactile systems: combining touch and sight for precision.\r\n4. The role of Python in tactile sensing:\r\n   - Using PyTorch to build machine learning models for tactile data.\r\n   - Leveraging OpenCV for tactile data preprocessing and visualization.\r\n5. Real-world applications of tactile sensing in industries like healthcare, manufacturing, and automation.\r\n6. Challenges and opportunities in the field of tactile sensing.\r\n\r\nThrough practical insights and examples, attendees will gain:\r\n1. A clear understanding of tactile sensing and its importance in robotics.\r\n2. Insights into the latest advancements in vision-based tactile sensing.\r\n3. Knowledge of how Python can be used to process, analyze, and model tactile data.", "code": "9RBNMV", "state": "submitted", "created": "2025-01-05", "speaker_names": "Zaynab Awofeso", "track": "PyData: Embedded Systems & Robotics"}, {"title": "Unlocking Productivity: The Impact of Time Management on Programmers", "abstract": "In the fast-paced world of programming, effective time management is not just a soft skill it\u2019s a productivity multiplier. This talk delves into the strategies, tools, and mindsets that help programmers optimize their workflows, meet deadlines, and achieve a healthy work-life balance. Attendees will walk away with actionable insights to harness their time and elevate their productivity.", "full_description": "Understanding Time Management: The significance of time management in programming and its impact on productivity.\r\n\r\nPrioritization Techniques: Applying methods like Eisenhower Matrix and time blocking to coding tasks.\r\n\r\nTools for Efficiency: Leveraging Python-friendly tools and project management software for better time utilization.\r\n\r\nAvoiding Burnout: Strategies for sustainable productivity without compromising well-being.\r\n\r\nReal-Life Applications: Case studies of how effective time management improved project outcomes.", "code": "WR9DEB", "state": "rejected", "created": "2024-12-22", "speaker_names": "Olaleye Aanuoluwapo Kayode", "track": "General: Education, Career & Life"}, {"title": "The Grand Challenge of AI Science", "abstract": "Will the grand unified theory of physics be discovered by a human or an algorithm?\r\nThe next grand challenge AI is a completely autonomous scientific significant breakthrough. The development of an artificial curious scientist will revolutionize how humans approach discovering knowledge.\r\n\r\nLarge language models are ungrounded. They can only superficially reconstitute existing knowledge, they do not create new abstractions. Applying the scientific method and reasoning with evidence is the ultimate anti-hallucination, progress will push post-LLM development in AI forward.\r\n\r\nThis talk will cover why AI science is important and how we might get there.", "full_description": "After AlphaGo, there has been a conspicuous absence of a \"grand challenge\" for AI; since then generative models have risen to prominence, two Nobel prizes have been awarded for work in AI. LLMs have passed the Turing test like a express train passing a small town station. What is the next mountain to climb? This talk proposed the next grand challenge AI is a completely autonomous scientific breakthrough. The first AI generated discovery that is regarded my numerous human experts as a significant breakthrough and produced with minimal human intervention. The promising fields for this development are mathematics and theoretical physics, especially the Clay Mathematics Institute's Millennium Prize Problems, or a serious contribution towards the grand unified theory of physics. However AI discoveries in other areas such as biology, materials science and neuroscience are also tantalizingly close.\r\n\r\nThe first AI scientific discovery requires pushing the limits of what is possible with today's technology.\r\nLarge language models are ungrounded. They can only superficially reconstitute existing knowledge; they cannot create new abstractions. Applying the scientific method and reasoning with evidence is the ultimate anti-hallucination, progress will push post-LLM development in AI forward. It will also demonstrate how generative models can move humanity forward, not just fill the internet up with slop. This challenge is an excellent intermediate step; it doesn't necessarily require super-intelligence or consciousness for success, sidestepping these philosophically fraught and possibly dangerous areas. While the success condition of acceptance by prominent human scientists is less specific than beating a human world champion at a game, it is still relatively well defined, compared to something more open to interpretation like artificial general intelligence.\r\n\r\nThe talk will justify AI science as the next great challenge, discuss the ways pursuing it will push AI research forward and outline possible promising approaches.", "code": "LHL7ZK", "state": "submitted", "created": "2024-12-22", "speaker_names": "Andy Kitchen", "track": "PyData: Generative AI"}, {"title": "The Blueprint for Safe AI in Finance", "abstract": "The financial industry is rapidly adopting AI technologies, yet the high-stakes nature of finance demands models that are safe, validated, and robust. This talk explores the unique challenges faced by AI in finance, including data scarcity, evolving market conditions, regulatory compliance, and the threat of adversarial attacks. By addressing these challenges with a multi-level approach\u2014spanning data-centric, model-centric, and productization techniques\u2014we'll demonstrate how to build reliable AI systems capable of thriving in this critical domain. Attendees will learn actionable strategies for safeguarding AI workflows and ensuring their models are explainable, resilient, and compliant with regulations.", "full_description": "The adoption of AI in finance is growing rapidly, driven by the need for competitive advantage, process automation, and improved customer experiences. However, the high-stakes context of finance\u2014where failures can lead to significant financial loss, reputational damage, regulatory penalties, and even systemic risk\u2014requires a deliberate focus on safety, validation, and robustness.\r\n\r\nIn this talk, we will break down the challenges of deploying AI systems in finance and present practical solutions across three critical dimensions:\r\n\r\nData-Centric Techniques: Learn how to improve data quality through anonymization, enrichment, and drift detection while using feature engineering to enhance model accuracy. Techniques like balancing datasets and robust validation will also be covered.\r\n\r\nModel-Centric Techniques: Explore strategies to create reliable and interpretable models, including ensemble methods, adversarial testing, and fine-tuning hyperparameters. We'll also cover tools to explain predictions and manage bias effectively.\r\n\r\nProductization Techniques: Discover best practices for deploying AI systems, including securing endpoints, setting up observability for performance tracking, and implementing robust CI/CD pipelines. Deployment strategies such as canary testing and blue-green deployments will ensure smooth rollouts and risk mitigation.\r\n\r\nReal-world examples, such as loan default prediction for thin-file borrowers, will illustrate these techniques, highlighting how they improve reliability and compliance while mitigating risks. This session is ideal for developers, data scientists, and financial professionals seeking to build AI systems that deliver both innovation and trust.", "code": "NH7CWL", "state": "submitted", "created": "2025-01-05", "speaker_names": "Brain Aboze", "track": "PyCon: Security"}, {"title": "Vectors Everywhere: An Introduction to Vector Databases", "abstract": "You have probably heard of vector databases while learning about embeddings used in deep learning models or come across an ad on why you should use one. \r\nIn that case, you might be wondering why not just use SQL databases or NoSQL databases. In this talk you will learn what embeddings are, and what vector databases are, why they are useful and when to use them. Expect to understand why vector databases are preferred for certain applications over SQL and NoSQL databases. We will also work through an example of how vector search works in vector databases and its applications.\r\n\r\nEven if you have never heard of vector databases before, come learn how they are powering deep learning systems and search capabilities.\r\nYou will get the most out of this session if you have some knowledge of python and SQL databases, but no prior experience with vector embeddings is required.", "full_description": "Vector databases have become more commonplace with the adoption of LLMs in various systems. You may have heard of vector databases in passing and are wondering what they are and why they are all the rage despite SQL databases having stood the test of time. You might remember learning about vectors in school and are wondering what's their relation to LLMs.\r\nIn this talk, we will learn about vectors, embeddings and vector databases. You will form an understanding of when vector databases are applicable and work through how to use open source vector databases.\r\nThis session is excellent for Python developers, data engineers and ML engineers curious about data storage and retrieval in machine learning systems. Join us to learn the basics of vector databases and how they are powering some of the world's AI systems.", "code": "3WR83C", "state": "submitted", "created": "2024-12-20", "speaker_names": "Bernice Waweru", "track": "PyData: Data Handling & Engineering"}, {"title": "cluster-experiments: a library for end2end AB testing analysis", "abstract": "cluster-experiments is a library that helps analysts design, run, and analyze AB tests. It allows users to create AB test scorecards from AB test data, calculate MDEs given historical data using the Central Limit Theorem, and calculate statistical power via simulations. All of this can be done for many experiment designs (clustered, switchback) and analysis techniques (variance reduction, delta method, GEE). This library has been used in Delivery Hero as well as other companies to design and analyze AB tests.", "full_description": "Power analysis is a critical step in AB testing design. Many analysts routinely use calculators to run power analysis. However, these calculators fail to cover many important cases, including cases when: some non-parametric analysis method like bootstrap is used to run the hypothesis test; adding covariates to reduce variance (cuped or cupac rely on this); running clustered designs, where the unit of analysis is different than the unit of treatment assignment. cluster-experiments is a library that allows us to run power analysis for any experiment design. Also, in the spirit of \"analyse as you design\", cluster-experiments allows you to do AB test analysis using the same methods you used to design the experiment.", "code": "JGM8LX", "state": "submitted", "created": "2024-12-16", "speaker_names": "David Masip", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Djangonian Grapes: Integrating GrapesJS into Django", "abstract": "In modern web development, the demand for dynamic and customisable content is ever-growing. However, traditional CMS packages like Django CMS and Wagtail CMS can feel too limited, while other feature-rich tools like Wordpress or Builder.io will often tie you to paid features or traffic. Here enters GrapesJS, a powerful open-source web builder framework that allows developers and non-technical users to create and manage rich web content through an intuitive drag-and-drop interface. When integrated with Django, this combination becomes a formidable tool for building scalable, feature-rich web applications with seamless content management capabilities.", "full_description": "Here is an overview of the presentation\r\n\r\n1. GrapesJS and its purpose\r\nAdvantages of GrapesJS: User-friendly interface for web designers and developers, No extensive coding skills required, Wide range of pre-designed templates and components, Drag and drop functionality for easy design, Real-time preview and customisation, Responsive design capabilities, Granular styling options and custom CSS support\r\nDisadvantages of GrapesJS: Limited pre-built templates compared to other CMS options, Potential learning curve for new users. Flexibility of default layout may not fit all design needs\r\nGrapesJS versus Traditional CMS and Site Builders: Why choose GrapesJS over traditional CMS or complete site builders, Justification for using GrapesJS and other open-source web builders\r\n\r\n2. Technical Solution: Integrating GrapesJS into Django\r\nDjango and GrapesJS Integration\r\nStoring and Serving Content Created on GrapesJS Editor: Methods for storing HTML templates created in GrapesJS, Serving the content through Django\r\nManaging Media Content: Ways to store and serve images and videos within the GrapesJS editor, Integration with Django's media handling system\r\nAdding New Fonts and Customising the Editor\r\nCustomising the GrapesJS editor and the use of  plugins and custom scripts to extend GrapesJS functionalities\r\n\r\n3. Conclusion and Future Work\r\nTechnical Issues and Solutions: Common technical issues encountered during integration + Solutions and workarounds\r\nPotential future enhancements to the integration solution\r\nEmphasis on the extensibility of both Django and GrapesJS\r\nCommunity contributions and collaborative development opportunities\r\nImportant considerations when integrating a web builder into a backend framework like Django", "code": "KEZ8DY", "state": "submitted", "created": "2024-12-20", "speaker_names": "Carlos Teixeira Martinez", "track": "PyCon: Django & Web"}, {"title": "From Text to Multimodal: Building Self-Hosted RAG Systems with Open Source", "abstract": "While text-based RAG systems have been everywhere in the last year and a half, the real power comes from combining multiple modalities\r\n\r\nIn this session, we'll show how it is possible to create a Multimodal RAG system using Open-Source tools such as vLLM for deploying your LLM, Pixtral for multimodal understanding, and Milvus for storing your vectors.\r\n\r\nI'll walk you through the architecture and implementation details of setting up your own infrastructure, demonstrating how to effectively process and understand both images and text in a unified system.\r\nThe talk will include a live demo, showing the real-time implementation and performance of the system.\r\n\r\nWhether you're looking to reduce dependency on commercial APIs or need more control over your LLM infrastructure, this talk will provide you with practical insights and implementation strategies.", "full_description": "While text-based RAG systems have been everywhere in the last year and a half, the real power comes from combining multiple modalities\r\n\r\nIn this session, we'll show how it is possible to create a Multimodal RAG system using Open-Source tools such as vLLM for deploying your LLM, Pixtral for multimodal understanding, and Milvus for storing your vectors.\r\n\r\nI'll walk you through the architecture and implementation details of setting up your own infrastructure, demonstrating how to effectively process and understand both images and text in a unified system.\r\nThe talk will include a live demo, showing the real-time implementation and performance of the system.\r\n\r\nWhether you're looking to reduce dependency on commercial APIs or need more control over your LLM infrastructure, this talk will provide you with practical insights and implementation strategies.", "code": "TFLBTW", "state": "submitted", "created": "2024-12-17", "speaker_names": "Stephen Batifol", "track": "PyData: Generative AI"}, {"title": "Increase your productivity with the google workspace", "abstract": "Efficient workflows and collaboration are essential for Python developers, whether working on open-source projects or enterprise-level applications. Google Workspace offers a suite of tools that can drastically improve how teams communicate, plan, and execute their projects.", "full_description": "Overview of common productivity challenges faced by Python developers.\r\nIntroduction to Google Workspace as a comprehensive solution.\r\nCore Tools and Their Applications (10 minutes)\r\n\r\nGoogle Sheets: Using Sheets for agile sprint planning and task tracking.\r\nGoogle Docs: Collaborative technical documentation for Python projects.\r\nGoogle Drive: Organizing and sharing project files with version control.\r\nGoogle Calendar: Scheduling team meetings and setting up milestone reminders.\r\nGoogle Workspace Integrations with Python (10 minutes)\r\n\r\nAutomating workflows with Google Workspace APIs.\r\nExamples of Python scripts for data extraction and reporting using Google Sheets API.\r\nIntegrating Google Drive with Python projects for efficient file handling.\r\nCase Study: Enhancing an Open-Source Project (5 minutes)\r\n\r\nReal-world example demonstrating the use of Google Workspace tools to coordinate a distributed development team.", "code": "8BNKCH", "state": "submitted", "created": "2024-12-22", "speaker_names": "Olaleye Aanuoluwapo Kayode", "track": "General: Education, Career & Life"}, {"title": "A Journey Through Germany's Machine Learning Landscape", "abstract": "Join us, as we take a hike through the beautiful landscape of Open Source Machine Learning made in Germany.\r\n\r\nWe will map out this often underrated area of the Open Source Machine Learning world and will discover, that some of the most popular sights (aka libraries) of it, like spaCy, Qdrant or Black Forest Labs have their roots in Germany.\r\n\r\nThis presentation will not only help you get a better understanding of the libraries, projects and companies affiliated with Open Source in Germany - it will also take you on a poetic tour, making you see the Open Source landscape in an imaginative and illustrated way like never before.\r\n\r\nWhat are you waiting for? Get your backpack, put on your walking boots and let's explore this beautiful wonderland together!", "full_description": "Embark on a scenic journey through the vibrant landscape of Open Source Machine Learning in Germany, where innovation meets community. In this presentation, we'll explore the often-overlooked yet thriving ecosystem of German-based Open Source projects and companies that are shaping the future of Machine Learning.\r\n\r\nAs we wander through this virtual landscape, we'll encounter iconic landmarks such as spaCy, Qdrant, and Black Forest Labs, and discover how they're revolutionizing the field. Along the way, we'll meet the pioneers and contributors who are driving these projects forward, and learn about the libraries, frameworks, and tools that are making waves in the global Open Source community.\r\n\r\nThis presentation is more than just a technical overview - it's an experience that will take you to a world of creativity, collaboration, and innovation. Through a blend of storytelling, visuals, and poetic narrative, we'll reveal the beauty and diversity of Germany's Open Source Machine Learning landscape, and inspire you to explore, contribute, and become a part of this thriving community.\r\n\r\nA rough time and content outline:\r\n5 min - A short introduction to Open Source in Machine Learning\r\n30 min - \"The Grand Tour\", covering the categories NLP, Computer Vision, GenAI, Data Collection/Processing, MLOps, Search and Embed, General Machine Learning and Research Projects", "code": "PB7YSX", "state": "submitted", "created": "2024-12-22", "speaker_names": "Johannes Kolbe", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "\ud83e\udde0\ud83e\uddf9 Causality - Mental Hygiene for Data Science", "abstract": "To apply or not to apply, that is the question.\r\n\r\nCausal reasoning elevates predictive outcomes by shifting from *\u201cwhat happened\u201d* to *\u201cwhat would happen ***if***\u201d*. Yet, implementing causality can be challenging or even infeasible in some contexts. This talk explores how the very act of assessing its applicability can add value to your projects. Through a gentle introduction to causal inference tools and practical use cases, you will learn how to bring greater scientific rigour to real-world problems.\r\n\r\nTarget audience: Practicing and aspiring data scientists, machine learning engineers, and analysts looking to improve their decision-making with causal inference. \r\n\r\nNo prior knowledge is assumed. \r\n\r\nFor the seasoned practitioners I hope to shine light on aspects that may not have been considered. \ud83d\udca1", "full_description": "This talk aims to provide practitioners with a practical understanding of causal reasoning and its applications in real-world projects. It moves beyond the hype surrounding causal inference (CI) as the \"next big thing\" and encourages a critical assessment of its strengths and limitations.\r\n\r\n**Outline**\r\n\r\n* Introduction: The Allure and the Reality of Causality\r\n* Case Study: A Paradox in Machine Learning\r\n* Causal Graphs\r\n    * as Blueprints for Causal Thinking\r\n    * Building by *Causal Discovery*\r\n* Assessing Applicability with *Identifiability*\r\n* Summary and Resources\r\n\r\n\r\n**Central Thesis**  \r\nAlthough causal inference techniques have immense potential, their practical application requires careful consideration of context and limitations. The act of engaging in causality is valuable in its own right, promoting a more rigorous and insightful approach to a project. In particular, \"causal thinking\" through Causal Graph construction and critical assessment of assumptions, is a mental exercise that enhances project rigour which may lead to a deeper understanding of what is possible with the data.\r\n\r\n**In Detail**  \r\nA key focus will be on *Directed Acyclic Graphs* (DAGs) as powerful tools for causal thinking. They provide a visual aid of the data generation process, mapping out the relationships between different parameters and their dependencies. We'll examine how DAGs can articulate the understanding of a system. A second focus will be on *Identifiability* a process of identifying minimum sets of parameters required to answer specific questions about cause and effect.\r\n\r\n**Key Takeaways**  \r\nThis talk will equip you with a practical framework for:\r\n\r\n* Building a DAG as a visualisation tool to communicate your understanding of the data generation process and assess the level of control you have over the system being studied.\r\n* Using Identifiability on a DAG to assess the applicability of CI to your own projects, considering factors like data availability, project goals.\r\n\r\nBy doing so you will embrace \"causal thinking\" as a mental hygiene practice that can benefit any data science project, even if formal CI techniques cannot be employed.\r\n\r\n\r\nBy the end of this talk, you'll have a nuanced understanding of CI, going beyond the hype to appreciate its true potential and limitations. You'll be better equipped to assess when it's is the right tool for the job, and you'll gain valuable insights into how \"causal thinking\" can enhance your work as a data scientist, especially with real-world data.", "code": "UMF8PT", "state": "submitted", "created": "2024-12-20", "speaker_names": "Eyal Kazin", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Python bindings for Box2D 3.0", "abstract": "Box2D is a widely-used physics engine that powers many games, simulations, and can be used as educational tools. However, the existing Python bindings are outdated and unmaintained for many years.\r\nIn this talk we present  novel python bindings for Box2D based on nanobind and show how these allow to create tiny 2D physics simulation and games with a minimal amount of boilderplate code.", "full_description": "Box2D is a widely-used physics engine that powers many games, simulations, and can be used as educational tools.  Box2D is written in C but bindings for many other languages and frameworks exist.\r\nHowever, the existing Python bindings are outdated and unmaintained for many years.\r\nIn this talk we present  novel python bindings for Box2D based on nanobind and show how these allow to create tiny 2D physics simulation and games with a minimal amount of code. Developing  such simulations or tiny physics-based games is a great way to motivate and teach programming to kinds and students.  Using Box2D from Python allows students and users to focus on problem-solving without the complexity of compiled languages like C/C++.\r\nWe show how games like \"AngryBirds\", \"WorldOfGoo\"  and others can be implemented with a few lines of python code.", "code": "9QCCJB", "state": "submitted", "created": "2024-12-22", "speaker_names": "Thorsten Beier", "track": "General: Education, Career & Life"}, {"title": "Building Bare-Bones Game Physics in Rust with Python Integration", "abstract": "Learn how to build a minimalist game physics engine in Rust and make it accessible to Python developers using PyO3. This talk explores fundamental concepts like collision detection and motion dynamics while focusing on Python integration for scripting and testing. Ideal for developers interested in combining Rust\u2019s performance with Python\u2019s ease of use to create lightweight and efficient tools for games or simulations.", "full_description": "Python\u2019s simplicity makes it the go-to choice for scripting, while Rust excels in performance-critical tasks like game physics. This talk demonstrates how to build a minimalist physics engine in Rust, focusing on core concepts like collision detection, basic rigid body dynamics, and force application, while providing seamless Python integration using PyO3.\r\n\r\nWe\u2019ll explore how PyO3 allows developers to expose Rust functionality as native Python modules, enabling Python developers to easily script and interact with the physics engine. Through practical examples, attendees will see how Python can be used for rapid prototyping and gameplay scripting, while Rust handles the heavy lifting of physics calculations.\r\n\r\nBy the end of this session, participants will not only understand the basics of implementing physics in Rust but also how to use PyO3 to bridge the gap between Rust\u2019s performance and Python\u2019s flexibility. This talk is perfect for Python enthusiasts curious about Rust or Rustaceans looking to make their libraries accessible to the Python ecosystem.", "code": "VKYDBD", "state": "accepted", "created": "2024-11-28", "speaker_names": "Sam Kaveh", "track": "PyCon: Programming & Software Engineering"}, {"title": "Fix Your Python Bottlenecks: A Hands-on Guide to Numba", "abstract": "Python is a powerful and versatile programming language, but its performance can sometimes fall short for computationally intensive tasks. Numba, a Just-In-Time (JIT) compiler, bridges this gap by enabling Python developers to achieve near-C-level performance without leaving the Python ecosystem. In this talk, we will introduce Numba, explore its capabilities for accelerating numerical computations, and demonstrate how it integrates seamlessly with popular libraries like NumPy. Attendees will learn how to optimize Python code for speed using simple annotations and techniques, making it accessible even for those new to performance optimization. Whether you're a data scientist, machine learning engineer, or developer working with large datasets or simulations, this session will empower you to unlock the full potential of Python for high-performance computing.", "full_description": "### Python's Simplicity vs. Performance Bottlenecks\r\n\r\nPython's simplicity makes it the go-to language for data science and machine learning, but its interpreted nature creates significant performance bottlenecks in numerical computations. This talk addresses the critical challenge faced by ML engineers and data scientists: **how to maintain Python's ease of use while achieving C-like performance for computation-heavy tasks**. Through practical examples from real-world ML pipelines, we'll explore how Numba can transform slow Python code into high-performance implementations without sacrificing readability or requiring low-level programming expertise.\r\n\r\n---\r\n\r\n### **Description**\r\n\r\n#### **Problem Statement**  \r\nMany data scientists and ML engineers face these common challenges:  \r\n- ML model training and inference bottlenecks in production  \r\n- Slow numerical computations in data preprocessing pipelines  \r\n- Performance issues with custom algorithms not optimized by NumPy  \r\n- Resource constraints when scaling Python applications  \r\n\r\nThese problems directly impact:  \r\n- Development productivity and iteration speed  \r\n- Infrastructure costs in production  \r\n- Application scalability  \r\n- Real-time processing capabilities  \r\n\r\n---\r\n\r\n#### **Solution Approach**\r\n\r\nThe talk demonstrates practical solutions through:\r\n\r\n1. **Performance Analysis**  \r\n   - Identifying bottlenecks in Python code  \r\n   - Understanding when Numba is the right tool  \r\n   - Measuring performance impacts  \r\n\r\n2. **Implementation Strategies**  \r\n   - Converting pure Python functions to Numba-optimized code  \r\n   - Leveraging parallel processing capabilities  \r\n   - Integrating with existing ML frameworks  \r\n\r\n3. **Real-world Applications**  \r\n   - Custom feature engineering optimization  \r\n   - Fast numerical computations in ML pipelines  \r\n   - Efficient data preprocessing techniques  \r\n\r\n---\r\n\r\n### **Concrete Takeaways**\r\n\r\nBy the end of this session, attendees will gain:\r\n\r\n#### **Technical Skills**  \r\n- Master Numba's core decorators (`@jit`, `@njit`, `@vectorize`)  \r\n- Understand compilation modes and their trade-offs  \r\n- Learn to implement parallel processing with Numba  \r\n- Know how to profile and optimize Python code  \r\n\r\n#### **Practical Knowledge**  \r\n- Identify which code sections benefit most from Numba  \r\n- Debug common Numba compilation issues  \r\n- Apply best practices for production deployment  \r\n- Integrate Numba with existing ML workflows  \r\n\r\n---\r\n\r\n### **Code Examples**\r\n\r\nAttendees will receive:  \r\n- Ready-to-use code templates  \r\n- Performance comparison benchmarks  \r\n- Integration examples with popular ML frameworks  \r\n- Debugging and optimization checklists  \r\n\r\n---\r\n\r\n### **Target Audience Benefits**\r\n\r\nThis talk specifically helps:  \r\n- ML Engineers struggling with slow model training  \r\n- Data Scientists working with large-scale data processing  \r\n- Python developers building high-performance applications  \r\n- Teams looking to reduce infrastructure costs  \r\n\r\n---\r\n\r\n### **Hands-on Components**\r\n\r\nThe session includes:  \r\n- Interactive performance comparisons  \r\n- Real-time optimization examples  \r\n- Practical debugging scenarios  \r\n\r\n---\r\n\r\nThis talk bridges theory and practice, ensuring attendees can immediately apply performance optimization techniques to their projects, potentially reducing execution times by orders of magnitude while maintaining clean, maintainable Python code.", "code": "TNDDRT", "state": "submitted", "created": "2024-12-22", "speaker_names": "Thomas Berger", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "How to hack an agent - or not.", "abstract": "Large Language Models are not safe. And it's not because of their ability to hallucinate while working. LLMs can be manipulated. So far, the known security mechanisms have not really proven to be solid. For example, if you use jailbreaks correctly, you can get an LLM to do things it is not supposed to do. But hey, what's stopping us from giving an LLM a little email summary? Nothing. Unless you connect it to an agent that can also send emails. Which brings us to the issue: Can an AI agent be secured where even an LLM cannot be secured? We want to approach this question by looking at state-of-the-art security concepts, from catching LLM task drift with activations, to prompt shields, spotlighting, and instruction hierarchy. Is my AI agent safe? Let's find out!", "full_description": "Large language models (LLMs) are not safe. And it's not just because they can \u201challucinate\u201d when processing tasks. LLMs can be manipulated. So far, the known security mechanisms have not proven to be solid. For example, if you use so-called \u201cjailbreaks\u201d correctly, you can get an LLM to do things it\u2019s not supposed to do. But what\u2019s to stop us from simply giving an LLM a short email summary? Nothing\u2014unless you connect it to an agent that can also send emails. And therein lies the problem: can an AI agent be made secure when even an LLM is not secure?\r\n\r\nIn this talk, we will examine the current ecosystem surrounding the security of AI agents. How do you prevent an AI agent from performing a task it is not supposed to perform? As an example, we will consider the current \u201cCapture the Flag\u201d competition \u201cAdaptive Prompt Injection: LLMail Inject,\u201d which is part of the 3rd IEEE Conference on Secure and Trustworthy Machine Learning. This competition challenges participants to determine whether they can get an AI agent to send an email with certain content, even though the user is only asking for a summary.\r\n\r\nSpecifically, it addresses the vulnerabilities of the models Microsoft\u2019s Phi3 and OpenAI\u2019s GPT4-o-mini. Depending on the scenario, these models are protected by mechanisms such as a so-called LLM judge, task drift, prompt shields, spotlighting, or instruction hierarchy. All these procedures and models are also used in the Azure cloud and can be transferred to on-premises environments using open-source solutions. In 2025, this topic will become more urgent because, starting in August, the EU AI Act will require that general-purpose AI\u2014including LLMs\u2014be secured by state-of-the-art security concepts. This provides even more reason to examine the methods currently available and learn about their advantages and disadvantages.\r\n\r\nWhich leaves us with the question: Is my AI agent secure? Let\u2019s find out!", "code": "9TLCBF", "state": "submitted", "created": "2024-12-20", "speaker_names": "Thomas Fraunholz", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Relation Extraction in German Legal Documents using Meta-Learning and Contrastive Prototypical Netwo", "abstract": "Relation extraction (RE) is a critical natural language processing (NLP) task that identifies relationships between entities within text, with applications ranging from knowledge graph construction to automated decision-making. In this study, we explore advanced machine learning techniques, implemented entirely in Python, to tackle the challenges of RE in German legal documents\u2014a domain characterized by complex syntax, specialized terminology, and limited annotated data. Leveraging Python-based frameworks such as PyTorch, Hugging Face Transformers, and Scikit-learn, we develop and evaluate few-shot learning methods, including meta-learning and prototypical networks with contrastive loss. To address data scarcity, we generate synthetic data using large language models (LLMs) within the Python ecosystem. This research contributes to the open-source Python community by releasing a novel annotated dataset, reproducible Python code, and an open-access RE tool. Our results demonstrate the effectiveness of Python-driven solutions for low-resource NLP tasks, showcasing how Python empowers legal NLP research and facilitates robust experimentation with state-of-the-art methods.", "full_description": "This project implements advanced Python-based algorithms to enhance relation extraction (RE) in German legal texts, addressing challenges like data scarcity and linguistic complexity. The focus is on two few-shot learning methods: Meta-Learning and Prototypical Networks with Contrastive Loss, optimized for low-resource settings using Python frameworks such as PyTorch and Hugging Face Transformers.\r\n\r\n1. Meta-Learning (MAML)\r\nMeta-Learning allows rapid model adaptation with minimal data. Using PyTorch, we implement Model-Agnostic Meta-Learning (MAML) in a two-loop optimization process:\r\nInner Loop: Task-specific updates using few-shot examples (support set).\r\nOuter Loop: Meta-parameters refined across tasks to improve generalization.\r\nEpisodic training ensures the model learns task-level patterns effectively, supporting adaptation to new relations.\r\n\r\n2. Prototypical Networks with Contrastive Loss\r\nPrototypical Networks classify examples by computing class prototypes (average embeddings) and measuring query distances. Contrastive Loss enhances class separability by minimizing intra-class distance and maximizing inter-class margins. Pre-trained language models like BERT, fine-tuned with Hugging Face, generate embeddings for entity pairs in legal texts.\r\n\r\n3. Synthetic Data Generation\r\nLarge language models (LLMs) such as GPT-3 are used to generate synthetic data, addressing annotated dataset scarcity. Python APIs ensure efficient integration, and manually validated examples enrich training datasets.\r\n\r\n4. Implementation Workflow\r\nKey tools include:\r\nData Processing: Pandas and NLTK for preprocessing.\r\nModel Training: PyTorch for custom architectures and Hugging Face for embedding generation.\r\nEvaluation: Scikit-learn for metrics like precision, recall, and F1-score.\r\n\r\n5. Experimental Design\r\nThe models are trained using episodic few-shot frameworks with K-shot examples. Experiments compare MAML and Prototypical Networks, with ablation studies isolating the impact of meta-learning and contrastive loss.\r\n\r\nOutcomes\r\nThe implementation delivers a reproducible Python-based RE tool, optimized for low-resource NLP tasks, with significant potential for legal and domain-specific applications.", "code": "XAKLUS", "state": "submitted", "created": "2024-12-19", "speaker_names": "Shiva Banasaz Nouri", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Fake it with GenAI until you make it without GenAI: a path to better UX", "abstract": "Can generative AI be useful beyond chatbots? In this talk, we\u2019ll show you how we used Generative LLMs at Malt to experiment with improving our product's user experience (the \u201cfake it phase\u201d). The story could end here.  However, Generative LLMs have multiple flaws when used in production, especially for non-generative use cases. So, here comes the most interesting part, you will discover the process we followed to replace them with several smaller specialized models when the business value of the feature became increasingly evident (the \u201cmake it phase\u201d).", "full_description": "There is a lot of hype around generative LLMs. However, it is hard to deny their decent zero-shot performance on some tasks. That's why they can be a good solution to build prototypes if you want to craft a smoother user experience for your product. And no, we are not talking about adding a chatbot, but new features that will simplify your users' lives.\r\n\r\nIn this talk, we will take the example of a recent project at Malt, the AI brief builder. This feature can convert a job description from a user into a structured project brief on our platform. First, we'll show you how we started with a version entirely based on a Generative LLM and all the problems we faced by doing so. Then, you will learn about the process we followed to replace more and more parts of the processing with several smaller specialized models when the business value of the feature became increasingly evident. \r\n\r\nAre you ready to adopt and throw away Generative LLMs to simplify your users' lives?", "code": "87LWDU", "state": "submitted", "created": "2024-12-22", "speaker_names": "Marc Palyart, Kateryna Budzyak", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Python Untethered: Building Robust Embedded Systems", "abstract": "When code finally leaves your IDE, the destination is often not the cloud or a PC, but an embedded device in a home, office, or even the wild. However, developing applications for the embedded space comes with its own unique set of challenges and pitfalls. Whether designing for the home or enterprise, embedded devices require additional considerations for usability, reliability, and maintainability.\r\n\r\nIn this talk, we'll explore real-world challenges of embedded Python development and provide practical solutions drawn from both DIY projects and enterprise deployments. You'll learn strategies for:\r\n\r\n- Ensuring system reliability through power failure tolerance and fault handling\r\n- Building secure remote access and upgrade capabilities\r\n- Implementing effective monitoring and observability\r\n- Managing hardware constraints and system resources\r\n- Designing flexible configuration systems that work in the field\r\n\r\nWhether you're a hobbyist looking to make your home automation more robust or a professional developer designing industrial systems, you'll walk away with concrete patterns and best practices to build more reliable embedded Python applications.", "full_description": "This session dives into the obscure world of embedded systems and their diverse applications, from DIY to production. We'll examine the unique challenges these systems face, contrasting them with traditional computing environments like PCs and the cloud. Through real-world examples, I\u2019ll provide accessible strategies for hobbyist developers to overcome these hurdles, as well as robust solutions designed for enterprise-grade production systems. Whether you're just starting with embedded Python or managing complex deployments, this session will offer valuable insights to enhance your practice.", "code": "ME9Q7Y", "state": "submitted", "created": "2024-12-09", "speaker_names": "Oliver Rew", "track": "PyCon: Programming & Software Engineering"}, {"title": "NiceGUI: Why Limit Python to Scripts When It Can Do So Much More?", "abstract": "Python is one of the most versatile programming languages, but building complete applications\u2014frontend, backend, and beyond\u2014has often meant stepping outside Python into other tools and frameworks. **NiceGUI** changes this narrative. With just Python, NiceGUI enables developers to create traditional web apps, deploy machine learning models, and even control IoT devices, all in one streamlined workflow.\r\n\r\nIn this talk, we\u2019ll explore the transformative power of NiceGUI, showing how it allows Python developers to move beyond scripts and build fully functional, production-ready applications with ease.", "full_description": "For years, Python developers have relied on external tools and frameworks to build applications that go beyond simple scripts. Frontend work often required learning HTML, CSS, or JavaScript, while deploying machine learning models or building IoT dashboards meant setting up additional layers of complexity. \r\n\r\n**NiceGUI** simplifies this process by providing a framework that combines Python with an intuitive approach to building applications.  It\u2019s a framework that allows you to do everything\u2014create user interfaces, manage backend logic, deploy machine learning models, and even handle IoT tasks\u2014all within Python. Because this framework uniquely combines real-time interactivity, flexibility, and support for diverse applications like IoT, machine learning, and dynamic dashboards, it\u2019s an easy pick over other alternatives.\r\n\r\nWhile the possibilities with NiceGUI are extensive, this talk will focus on practical examples that demonstrate its potential. We\u2019ll walk through:\r\n\r\n- A chat application to showcase building interactive user interfaces,\r\n- Editing and displaying data in a pandas DataFrame directly in the browser and\r\n- Creating a to-do list for managing tasks with checkboxes and text inputs.\r\n\r\nBy the end of this session, you\u2019ll see how NiceGUI empowers Python developers of all levels to build complete, functional applications without relying on external tools or languages.", "code": "9FVZCB", "state": "submitted", "created": "2024-12-21", "speaker_names": "Ilerioluwakiiye Abolade", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Markdown, Github & Copilot: A new way to journal (business) life", "abstract": "I used to write my journals in journal books, but I have a bad handwriting and it took too much time to find out my own scribbles. In 2024 I had some very important stuff to journal, and there was no room to mess it up. I opened VS Code and started to write my journal in Markdown. I saved it in a private GitHub repository and used GitHub Copilot to help me write my journal entries. For the first time, I really enjoyed writing my journal.\r\n\r\nToday I share with you this brand new way to journal as a software developer: Let's make a digital journal on GitHub using Markdown and GitHub Copilot.\r\n\r\nIn this talk you will learn to create a private Github repository with a journal structure, set up your VS Code to work with GitHub, and use GitHub Copilot to write personal and professional articles. You will learn about Mermaid diagrams and how to use them in your journal. You will learn how to use GitHub mobile to consume your journal on the go and much more.\r\n\r\nCome to this inspiring but technical talk to learn to use technology in new ways!", "full_description": "This talk will learn you to set up a VSCode solution and set up a structure to work with Markdown files. After this, we will use GitHub Copilot to start making your own Journal. We will look to different ways of making a journal, how to link pages, how do display images, how to add classical notes and even a variety of graphs.\r\n\r\nWe will look how to open and consult our journal on a variety of ways including t he Github Mobile app.", "code": "JB7ER8", "state": "submitted", "created": "2024-12-26", "speaker_names": "Dennie Declercq", "track": "PyData: Generative AI"}, {"title": "How Data Governance Can Cure Your Data Headaches", "abstract": "Are high data costs giving you a headache? Join us for a session on how smart data governance can serve as the remedy. We\u2019ll diagnose the complexities of managing data in the AI era, offering real-world case studies to illustrate effective treatment plans. We\u2019ll prescribe solutions such as data contracts and semantic layers to boost efficiency and enhance data quality. By treating data as a product and focusing on the essentials, we\u2019ll show how you can achieve optimal health for your data management system. Don\u2019t miss this opportunity to learn how to restore balance and streamline your data operations.", "full_description": "Are skyrocketing data costs giving you a headache? In today\u2019s data-driven world, managing data can feel like a constant struggle\u2014but smart data governance can be the cure. Join us for a session where we\u2019ll break down the complexities of data management and share real-world case studies to demonstrate effective, actionable solutions.\r\n\r\n**Key Takeaways**:\r\n- Prevent data chaos with proactive strategies like data contracts and semantic layers, rather than relying on reactive fixes.\r\n- Adopt a product-centric mindset to focus your data strategy on what truly matters, maximizing efficiency and value.\r\n- Combine technical solutions with cultural shifts to create lasting improvements in your data management approach.\r\n- Discover how to streamline operations, cut costs, and fully unlock the potential of your data.\r\n\r\nThis session will explore both technical and cultural strategies to help you transform your data management practices, driving efficiency, value, and long-term success. Don\u2019t miss out on the opportunity to **restore balance to your operations and unlock the full potential of your data**.", "code": "ZNMCJE", "state": "submitted", "created": "2024-12-18", "speaker_names": "Kate\u0159ina \u0160\u010davnick\u00e1", "track": "PyData: Data Handling & Engineering"}, {"title": "From Empty Shelves to Insights: Opportunities and Challenges of Multimodal LLMs in Retail Analytics", "abstract": "How can multimodal LLMs revolutionize retail operations? In this talk, we\u2019ll explore a real-world use case: analyzing retail images to detect issues like empty shelves or cleanliness problems and performing semantic searches to identify images related to topics of interest. You\u2019ll learn how these systems work, their potential in business applications, and the challenges and opportunities they present. This session offers actionable insights and a forward-looking perspective, perfect for anyone curious about applying cutting-edge AI to real-world problems.\r\n\r\n**Key Insights for Attendees**\r\n\t\u2022\tLearn how multimodal LLMs analyze visual and textual data for practical business applications.\r\n\t\u2022\tUnderstand the implementation, challenges, and opportunities of these systems.\r\n\t\u2022\tExplore future possibilities for multimodal AI across industries.\r\n\r\n**Target Audience**\r\nOpen to all, from data scientists and developers to business professionals and AI enthusiasts eager to discover how multimodal LLMs can address complex challenges in retail and beyond.", "full_description": "This presentation dives into how multimodal LLMs can drive innovation in retail analytics, focusing on analyzing retail images to detect actionable insights and perform semantic searches. The talk is structured as follows:\r\n\r\n**Introduction**\r\n- What are multimodal LLMs?\r\n- Why they matter for solving complex business challenges, especially in retail.\r\n\r\n**Use Case: Retail Image Analysis**\r\n- Detecting the state of depicted scenarios (e.g., empty shelves, cleanliness issues).\r\n- Performing semantic searches: finding images related to a specific topic of interest (e.g., examples of poor product placement or seasonal displays).\r\n- Business impact: improving operations, identifying trends, and solving pain points effectively.\r\n\r\n**How Multimodal LLMs Enable This Use Case**\r\n- Core principles: how multimodal LLMs combine text and images to generate insights.\r\n- Key techniques and tools involved in implementation.\r\n\r\n**Discussion and Future Directions**\r\n- Current Challenges:\r\n- Ensuring system accuracy and contextual understanding.\r\n- Managing large-scale and diverse datasets in retail environments.\r\n- Enhancing automation and efficiency in decision-making.\r\n- Applying lessons learned in retail to other industries.\r\n- Innovations needed to overcome limitations and unlock new possibilities.\r\n\r\n**Takeaways**\r\n- Practical knowledge of how multimodal LLMs work for retail use cases.\r\n- Insights into challenges, opportunities, and future trends in multimodal AI.\r\n- Inspiration to explore similar applications in your domain.\r\n\r\nThis talk will provide a comprehensive understanding of multimodal LLMs\u2019 role in solving business problems, encouraging attendees to think creatively about their potential applications.", "code": "7E8CCS", "state": "submitted", "created": "2025-01-05", "speaker_names": "Alexander Kowsik", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "AI Agents of Change: Creating, Reflecting, and Monetizing", "abstract": "Create, reflect, and earn\u2014with purpose. In this workshop, you\u2019ll not only build your own AI agent but also confront the ethical questions it raises, from its impact on jobs to its potential for social good. Together, we\u2019ll explore how to harness AI for empowerment while uncovering pathways to turn your skills into meaningful value.\r\n\r\nThis workshop is designed to equip Python enthusiasts with the tools to create their own AI agent while fostering a deeper understanding of the societal implications of this technology. Through hands-on learning, collaborative discussions, and practical monetization strategies, you\u2019ll leave with more than just code\u2014you\u2019ll gain a vision of how AI can be wielded responsibly and profitably.", "full_description": "The session unfolds in three engaging parts:\r\n\t1.\tBuild Your AI Agent\r\nStart with the fundamentals of AI by designing and implementing a functional agent. Using Python, we\u2019ll demystify the process and equip you with practical skills for creating an AI that responds to user needs and scenarios.\r\n\t2.\tReflect on Ethics and the Future of Work\r\nOnce your agent comes to life, we\u2019ll pause to examine the bigger picture:\r\n\t\u2022\tHow does the AI agent you have created may reshape the job market?\r\n\t\u2022\tCan it democratize and decentralize opportunities, or does it risk amplifying inequalities?\r\n\t\u2022\tWhat collective vision do we want for the future of work?\r\nThis thought-provoking discussion will challenge you to think critically about the role of technology in fostering empowerment or exacerbating social challenges.\r\n\t3.\tEarn by Sharing Value\r\nFinally, we\u2019ll explore how your AI agent can create real-world value. You\u2019ll learn how to leverage marketplaces like OpenServ to turn your innovation into income. Whether you aim to solve practical problems, inspire creativity, or contribute to ethical AI development, this segment will connect your skills with opportunities for meaningful impact.\r\n\r\nBy the end of the workshop, you\u2019ll have built an AI agent, grappled with its ethical dimensions, and uncovered how to use your coding prowess to create and share value\u2014all while shaping a more inclusive, responsible AI ecosystem.", "code": "PKZD8L", "state": "confirmed", "created": "2025-01-05", "speaker_names": "Paloma Oliveira", "track": "PyData: Generative AI"}, {"title": "Blue-Green Deployment in Action: A Real Case Study", "abstract": "Discover the essence of Blue-Green Deployment in migrating 300k products. Uncover its necessity, problem-solving prowess. Highlighting crucial non-functional requirements and chosen infrastructure, this presentation unveils the secrets to a seamless transition.", "full_description": "Introduction:\r\nThe session kicks off by contextualizing the need for a migration strategy, exploring the challenges of migrating an extensive product database with a time constraint. It establishes the imperative of achieving zero downtime during the migration process and introduces Blue-Green Deployment as the chosen methodology.\r\n\r\nWhy Blue-Green Deployment?\r\nThe presentation will elucidate why Blue-Green Deployment is the chosen strategy, emphasizing its ability to facilitate continuous delivery, eliminate downtime, and ensure a seamless transition between the old and new systems. The approach's agility and efficiency in managing a large-scale migration will be highlighted.\r\n\r\nChallenges Addressed:\r\nDelving into the challenges encountered during the migration of over 300,000 products, the presentation will discuss issues such as scalability, independent task execution, migration progress monitoring, and the need for a swift and easy rollback mechanism.\r\n\r\nMigration Strategies:\r\nTwo pivotal aspects of the migration process will be explored in detail. Firstly, the ETL (Extract, Transform, Load) process, and secondly, the Blue-Green Deployment methodology. The intricacies of both strategies and their synergy in achieving a zero downtime migration will be thoroughly examined.\r\n\r\nNon-functional Requirements:\r\nThe heart of the presentation revolves around the non-functional requirements imperative for a successful migration. These include the ability to scale, task independence, migration progress monitoring, restart from failure, fast and easy rollback, and the core element of a zero downtime migration strategy. The emphasis will be on the significance of error monitoring and the ability to react swiftly to mitigate risks.\r\n\r\nInfrastructure Solution:\r\nThe session will conclude by shedding light on the chosen infrastructure to meet the non-functional requirements. The selected infrastructure will be unveiled as the linchpin in ensuring scalability, independence of tasks, robust progress monitoring, and swift error resolution.\r\nAttendees will leave with a comprehensive understanding of the critical role played by Blue-Green Deployment in meeting the outlined non-functional requirements, and the strategic choice of infrastructure for a seamless, fast, and scalable migration of an extensive product database.", "code": "HMPJAB", "state": "submitted", "created": "2025-01-05", "speaker_names": "Damian Wysocki", "track": "PyCon: Programming & Software Engineering"}, {"title": "Orchestrating an end-to-end Data Engineering Workflow:  Leveraging Python in Apache Beam and Airflow", "abstract": "\"Orchestrating an End-to-End Data Engineering Workflow: Leveraging Python in Apache Beam, Airflow, Cloud Functions, BigQuery, and Gemini AI Models\"\r\n\r\nThis talk explores the synergy between Apache Beam and Apache Airflow, demonstrating how to create a robust, end-to-end data engineering workflow. We'll dive into the challenges of orchestrating complex data processing tasks and show how combining Airflow's scheduling capabilities with Beam's data processing framework can create more efficient and manageable data pipelines. The session will cover integration with Google Cloud Platform services, including Cloud Functions, BigQuery, and Gemini AI models, showcasing a comprehensive approach to modern data engineering. Attendees will gain practical insights into building resilient, scalable data pipelines that meet the demands of data-driven applications in today's AI-powered landscape.", "full_description": "Problem Addressed:\r\nIn today's data-driven world, organizations face the daunting challenge of orchestrating complex, end-to-end data engineering workflows that seamlessly integrate batch and streaming processing, scheduling, cloud services, and AI models. This talk tackles the often-overlooked synergy between Apache Beam and Apache Airflow, two powerful tools in the data engineering ecosystem that are rarely used in tandem. We'll explore how combining these technologies with Google Cloud Platform services and cutting-edge AI models can revolutionize data pipeline architecture.\r\n\r\n\r\nRelevance to the Audience:\r\nAs data volumes explode and processing requirements become increasingly complex, data engineers and scientists are under pressure to build scalable, maintainable pipelines that can handle diverse data sources and downstream applications. This topic is crucial for professionals looking to:\r\n - Modernize their data infrastructure\r\n - Streamline machine learning pipelines\r\n - Overcome limitations of using Beam and Airflow separately\r\n - Integrate AI models into data workflows seamlessly\r\n - Leverage cloud services for enhanced scalability and performance\r\n\r\nSolutions and Key Takeaways:\r\nAttendees will gain practical insights and hands-on knowledge to:\r\n 1. Harness the Power of Integration: Learn to seamlessly combine Apache Beam's robust data processing capabilities with Apache Airflow's sophisticated scheduling and orchestration features.\r\n 2. Master Cloud-Native Data Engineering: Discover how to leverage Google Cloud Platform services like Cloud Functions and BigQuery to build serverless, scalable data pipelines.\r\n 3. Incorporate AI into Data Workflows: Explore techniques for integrating Gemini AI models into your data processing pipelines, opening new possibilities for intelligent data transformation and analysis.\r\n 4. Design Resilient Architectures: Gain expertise in creating modular, scalable, and fault-tolerant data architectures that can handle the demands of modern data-driven applications.\r\n  5. Optimize Performance and Monitoring: Learn best practices for performance tuning, error handling, and observability in complex data workflows.\r\n\r\nDetailed Outline (90 minutes)\r\n I. Introduction and Foundations (15 minutes)\r\n    A. The evolving landscape of data engineering\r\n    B. Apache Beam and Airflow: A powerful, underutilized combination\r\n    C. Setting the stage: Our end-to-end workflow scenario\r\n\r\n II. Beam and Airflow Integration Deep Dive (25 minutes)\r\n    A. Crafting Beam pipelines for Airflow execution\r\n    B. Mastering the BeamRunPythonPipelineOperator\r\n    C. Advanced techniques: Dynamic pipeline options and runtime parameters\r\n    D. Live demo: Building a robust Beam-Airflow workflow\r\n\r\n III. Leveraging Google Cloud Platform (20 minutes)\r\n    A. Cloud Functions: Serverless data processing on demand\r\n    B. BigQuery: Scalable analytics in your pipeline\r\n    C. Best practices for GCP integration\r\n    D. Hands-on: Implementing GCP services in our workflow\r\n\r\n IV. AI-Powered Data Engineering with Gemini (20 minutes)\r\n    A. Gemini AI models: Capabilities and use cases in data pipelines\r\n    B. Designing Beam transforms with embedded AI\r\n    C. Demo: Intelligent data processing using Gemini in our workflow\r\n\r\n V. Putting It All Together (10 minutes)\r\n    A. Walkthrough of the complete end-to-end workflow\r\n    B. Error handling, monitoring, and observability strategies\r\n    C. Performance optimization tips and tricks\r\n\r\nThis session promises to be a game-changer for data professionals seeking to build next-generation data pipelines. Through a perfect blend of theory and hands-on demonstrations, you'll learn to orchestrate complex data workflows that leverage the best of Apache Beam, Airflow, Google Cloud Platform, and AI technologies. By the end of this talk, you'll be equipped with the knowledge and tools to design, implement, and maintain cutting-edge data engineering solutions that can scale to meet the demands of any data-driven organization.", "code": "QP8XEH", "state": "submitted", "created": "2025-01-05", "speaker_names": "Sadeeq Akintola", "track": "PyData: Data Handling & Engineering"}, {"title": "Monitoring Drifts in RAG Applications", "abstract": "Retrieval-augmented generation (RAG) combines the power of real-time data retrieval with large language model (LLM) capabilities, enabling accurate, up-to-date, and contextually relevant outputs. By querying external databases or documents during text generation, RAG addresses key challenges like hallucinations and enhances fact-based responses. However, the dynamic nature of user behavior, query patterns, and system updates makes monitoring data drift essential to ensure RAG applications' long-term reliability and performance.\r\n\r\nMonitoring data drift in RAG applications is crucial to maintaining their reliability and performance over time. In this tutorial, we explore the drifts that can affect RAG systems and discuss their impact on performance. We will further explore how to monitor LLM drift using Evidently, an open-source tool designed for monitoring machine learning models. The session will include a live demonstration of Evidently\u2019s Reports, Test Suites, and Monitoring Dashboard.\u00a0Attendees will gain practical insights into maintaining the efficacy of RAG applications in the face of evolving data distributions, user behavior, and system updates.", "full_description": "Retrieval-Augmented Generation (RAG) is revolutionizing the use of Large Language Models (LLMs) by enabling real-time data retrieval to produce accurate, up-to-date, and contextually relevant outputs. However, the dynamic nature of user behavior, data, system components, and operational environments poses challenges to maintaining the long-term reliability of RAG systems. This tutorial delves into the concept of drift, which occurs when subtle changes in these elements disrupt a system's performance, and presents actionable strategies to monitor and mitigate it effectively.\r\n\r\nThrough a hands-on demonstration using Evidently AI, attendees will learn to monitor these drifts in RAG systems. The session will leverage a Google Colab notebook with a practical example dataset, teaching participants how to prepare data using Evidently\u2019s ColumnMapping object and evaluate it using various analytical techniques. Metrics such as text length, word count, and out-of-vocabulary percentages will be employed to detect shifts in user behavior. Additionally, pattern-based analysis and sentiment drift tracking will be covered, with a focus on identifying domain-specific trends and shifts in user sentiment over time.\r\n\r\nThe tutorial will also highlight advanced techniques, including using LLM-as-a-Judge, where OpenAI\u2019s models can be leveraged to define and monitor custom qualitative metrics like toxicity. Semantic similarity analysis will be demonstrated to track alignment between user queries and system responses. By combining Evidently's robust monitoring tools with Langtrace's capabilities, participants will understand how to maintain the efficacy and reliability of RAG systems in dynamic environments.", "code": "97MTW8", "state": "submitted", "created": "2025-01-05", "speaker_names": "Brain Aboze", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Don't Panic! A Developer's Guide To Security", "abstract": "As a developer, you play a crucial role in the security of your projects. At the same time, security is but one of the many responsibilities a developer has to fulfill these days. It almost seems like you must be an expert in just about anything!\r\n\r\nThankfully, you don't have to be a security expert to contribute to the security of your projects. In this talk, I will show you how to approach security systematically without feeling overwhelmed.\r\n\r\nFirst, I will discuss some background on security theory and my take on the role of developers in security. This discussion will include the difficult topic of advocating for security with product owners and stakeholders. Then, I will introduce you to open-source industry standards, such as the [OWASP DevSecOps Maturity Model](https://owasp.org/www-project-devsecops-maturity-model/), that you can use to start implementing security in your projects.\r\n\r\nBy the end of my talk, you will not be a security expert, but you should have enough pointers to get started with security!", "full_description": "If you've been a developer somewhere in the past two decades, you've probably noticed the push for shifting security \"left\" in the Software Development Lifecycle (SDLC). While this doesn't mean that security wasn't a concern for developers before, there is an increasing emphasis on security earlier in the development process.\r\n\r\nOn paper, this sounds great, but security isn't the only responsibility that has been added to or increased in emphasis on the developer's plate. We cannot shift left _everything_ and expect developers to become experts in _everything_. That just isn't scalable.\r\n\r\nThat is why I will start my talk by discussing the role of developers in security. I will talk about security culture, the shift-left movement, the responsibilities of software developers, and how you can manage them without being overwhelmed.\r\n\r\nI will not ignore the difficult topic that haunts us all: How do you advocate for security as a developer? While many organizations shout about their security ambitions, most product owners and stakeholders still need convincing that good security takes time.\r\n\r\nWhile doing so, I will also introduce some core security concepts and frameworks to help you think about security. While many of us are familiar with individual components such as security scanners and secure coding patterns to avoid common vulnerabilities, the bigger picture you get from zooming out helps you determine which security practices you should prioritize in your project.\r\n\r\nIn the second part of my talk, I will turn to the more practical side of security: How do you get started with security?\r\n\r\nLuckily, we can rely on the expertise of others in this area: We can turn to open-source industry standards for guidance. Specifically, I will introduce you to the [OWASP DevSecOps Maturity Model](https://owasp.org/www-project-devsecops-maturity-model/). This model is especially accessible to developers and gives you concrete practices to implement in your workflow.\r\n\r\nBy the end of this talk, you should have the guidance you need to get started with security!", "code": "AQZPMN", "state": "submitted", "created": "2025-01-05", "speaker_names": "Sebastiaan Zeeff", "track": "PyCon: Security"}, {"title": "Serverless Python - What We Learned in the Last 2 Years", "abstract": "Two years ago, we migrated from Python on Kubernetes to AWS Lambda, embracing serverless architecture. In this talk, I\u2019ll share our journey, including lessons learned, challenges faced, and unexpected benefits. From cold start optimizations to integrating Python\u2019s ecosystem and developer experience, this session offers practical insights for anyone adopting or optimizing serverless Python. Join me to explore what worked, what didn\u2019t, and how serverless has transformed our approach to building scalable Python applications.", "full_description": "Two years ago, we embarked on a bold migration from running Python applications on Kubernetes to adopting AWS Lambda as our primary runtime. This transition has been a transformative journey, reshaping how we think about scalability, deployment, and developer productivity in the serverless era.\r\n\r\nIn this talk, I\u2019ll share the key lessons we\u2019ve learned along the way. We\u2019ll dive into the challenges of moving away from containerized Python workloads to a serverless architecture, the strategies we used to overcome them, and the unexpected benefits that emerged. Topics include optimizing cold starts, managing ephemeral environments, integrating Python\u2019s ecosystem into serverless workflows, and developer experience.\r\n\r\nWhether you\u2019re contemplating a similar migration or looking to refine your existing serverless setup, this session will provide actionable insights and best practices drawn from real-world experience. Join me to uncover what worked, what didn\u2019t, and how Python thrives in the serverless age.", "code": "ZECYNE", "state": "submitted", "created": "2024-12-19", "speaker_names": "Robin Raymond", "track": "PyCon: MLOps & DevOps"}, {"title": "Exploring the Potential of Generative AI for Web Job Postings Analysis", "abstract": "This talk will explore LLMs models and their application to job postings data. We will present innovative models, techniques from word-embedding to skills similarity, exploring the vast potential and uncovering valuable insights in the dynamic landscape of the labor market.", "full_description": "The rapid growth of web data and its widespread use in the labour market has resulted in an increased need for innovative approaches to analyse and understand job postings. In recent years, generative artificial intelligence (AI) has emerged as a promising tool for web data analysis. We will present our exploration of the potential of generative AI for web job postings analysis. We will discuss different models and applications such as word-embedding, skills similarity, skills description generation and improvement, and others. This research will provide insights into the benefits and limitations of generative AI in this domain. The first presentation of this research was presented at WIH-CONF (Eurostat) 2023.", "code": "SGMRDK", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Generative AI"}, {"title": "Forecast of Hourly Train Counts on Rail Routes Affected by Construction Work", "abstract": "Construction work in national railroad networks often disrupts train traffic, making it vital to estimate hourly train numbers for effective re-routing. Traditionally managed by humans, this process has been automated due to staff shortages and demographic changes. DB Systel GmbH, Deutsche Bahn's IT provider, leveraged machine learning and artificial intelligence to estimate train traffic during construction. Using Python and frameworks like Pandas, scikit-learn, NumPy, PyTorch and Polars, their solution demonstrated significant benefits in performance and efficiency.", "full_description": "Within a national railroad network, construction work for maintenance and modernization is unavoidable - as is train traffic on the affected sections under certain circumstances. Although there are fixed timetables for passenger rail transport that are planned well in advance and are set very early, there are still many freight transports and special trains that are registered at short notice and cause a dynamic traffic situation on the rail network. Therefore, the capacity utilisation of the rail routes is unknown until shortly before the journey takes place. It is therefore important to estimate the number of trains that will run over the affected tracks in order to establish a sensible re-routing strategy. Until now, this process has been in the hands of human decision-makers for decades or even more than a century.\r\n\r\nDemographic change and staff shortages are increasingly forcing companies to automate activities intelligently. This is where machine learning and artificial intelligence come into play.\r\n\r\nAs Deutsche Bahn's IT service provider, DB Systel GmbH was able to successfully implement an example of intelligent automation of this process and estimate train numbers on sections of tracks affected by construction using modern ML and AI methods. Python as well as various established frameworks (Pandas, scikit-learn, NumPy, PyTorch) and new frameworks (Polars, Ruff) were used in this project. A success and performance measurement clearly demonstrated the benefits of ML automation.", "code": "RAHBEP", "state": "submitted", "created": "2024-12-04", "speaker_names": "Sebastian Folz", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Optimizing Data Queries: Harnessing the Power of Bloom Filters", "abstract": "As the scale of data continues to grow exponentially, efficient data querying has become a critical challenge for mega-services and large-scale institutions. Traditional querying methods often fall short when faced with the sheer volume of data and the need for real-time responses. This is where Bloom Filters, a probabilistic data structure, shine by dramatically improving query performance and reducing resource overhead.\r\n\r\nThis talk will introduce Bloom Filters, explore their internal mechanics, and discuss their practical applications in large-scale systems. Through real-world examples and case studies, attendees will learn how some of the largest institutions in the world utilize Bloom Filters to handle billions of queries efficiently, maintain scalability, and optimize system performance.", "full_description": "Mega-services like search engines, social networks, and e-commerce platforms face a common challenge: handling massive datasets with minimal latency. Querying databases directly for simple checks, such as whether a key exists, can result in significant performance bottlenecks, especially as datasets scale to billions or trillions of entries. These inefficiencies not only slow down applications but also increase computational and memory overhead, straining resources and driving up costs.\r\n\r\nTraditional caching solutions provide some relief but are not always sufficient for scenarios requiring extreme efficiency. This has led to the adoption of Bloom Filters, a space-efficient probabilistic data structure that addresses these challenges by offering fast and lightweight membership checks with a controllable false-positive rate.", "code": "G8FBGL", "state": "submitted", "created": "2024-12-24", "speaker_names": "Syed Ansab Waqar Gillani, Syed Hassan Gilani", "track": "PyCon: Programming & Software Engineering"}, {"title": "Understanding the Mystery of web scrapping", "abstract": "In this talk, we will discuss Web scrapping and also unfold the mystery in the science and art of Web scrapping. There will be spinets of codes and a brief illustration using use cases that are of interest.", "full_description": "Introduction and Concept\r\n- Is knowing HTML necessary for scrapping  \r\nReasons for Web scrapping\r\nWeb scrapping as a science\r\nWeb scrapping as an art\r\nMystery of Web Scrapping\r\nChallenges\r\nTools and Technologies\r\n- Python Inbuilt library\r\n- Is beautiful soup really beautiful?\r\n- Pandas and web scrapping\r\nUse cases - Scrapping Football Data, Building a dataset from scrapping of web data\r\nBest practices\r\nConclusion", "code": "Q9J9SM", "state": "rejected", "created": "2024-12-20", "speaker_names": "Hampo, JohnPaul A.C.", "track": "PyCon: Django & Web"}, {"title": "Parallelization: Blessing and Curse", "abstract": "Developing machine learning features on big data demands sophisticated parallelization strategies, fundamentally impacting how we approach both modeling and engineering. This talk explores the technical challenges that arise when feature engineering requires distributed computing frameworks, blurring traditional boundaries between ML modeling and engineering roles. Through a retail case study, we examine practical solutions in parallel data processing, including architectural decisions around class granularity and data structures. Drawing from project experiences, we discuss how parallelization requirements influence feature design, model development, and production deployment, demonstrating why success in big data ML projects requires deep collaboration between modeling and engineering teams.", "full_description": "In this talk, we discuss the challenges of developing machine learning features on big data that require a parallelization and scaling framework. \r\n\r\nTo begin with, we take a brief look at the roles involved in the development of a ML model through to production. In theory, a feature store is developed under the leadership of one or more data engineers. The data scientist takes care of the logic, creates feature ideas, tests and develops a model and then takes the next steps together with the ML engineer. But is this separation that simple to make?  \r\n\r\nWith large amounts of data, parallelization frameworks are generally used not only for preprocessing but also for feature engineering. During the training phase alone, it may not be sufficient or even possible to keep feature ideas and mathematical modeling within a lab environment. This is where the roles in a project meet and overlap. \r\n\r\nNext, we will take a brief look at the common parallelization and scaling tools, including the frameworks offered by the common cloud providers. We will learn how these can support us in efficient data processing. \r\n\r\nThen we get more specific. What can a Pure Python data model to be processed in parallel look like in practice? We will use an example of point of sale and article level time series in retail to illustrate this and learn about the advantages of nested structures. Further questions quickly arise: At what granularity should the classes be cut? What do we gain from it? What do we lose by doing so regarding the underlying processing cluster? Instead of shuffling within a cluster, so-called side inputs can help.  \r\n\r\nThe consideration of the later model application in production, i.e. the use of a feature store also for the creation of an inference data set, places further demand on the data models.  \r\n\r\nI share project-experiences made on the arising trade-offs and challenges and how we can tackle those. We will conclude that parallelization is a way of thinking that needs to be internalized by all project members. The technical solution will be unique to each project. But there is one thing that AI projects on big data have in common: you can only be successful as a team.", "code": "DLQEXC", "state": "submitted", "created": "2024-12-22", "speaker_names": "Carsten Frommhold", "track": "PyData: Data Handling & Engineering"}, {"title": "Streamlining the Cosmos: Pythonic Workflow Management for Astronomical Analysis", "abstract": "Astronomical surveys are growing rapidly in complexity and scale, necessitating accurate, efficient, and reproducible reduction and analysis pipelines. In this talk we explore Pythonic workflow managers to streamline processing large datasets on distributed computing environments.\r\n\r\nModern astronomy generates vast datasets across the electromagnetic spectrum. NASA's flagship James Webb Space Telescope (JWST) provides unprecedented observations that enable deep studies of distant galaxies, cosmic structures, and other astrophysical phenomena. However, these datasets are complex and require intricate calibration and analysis pipelines to transform raw data into meaningful scientific insights.\r\n\r\nWe will discuss the development and deployment of Pythonic tools, including snakemake and pixi, to construct modular, parallelized workflows for data reduction and analysis. Attendees will learn how these tools automate complex processing steps, optimize performance in distributed computing environments, and ensure reproducibility. Using real-world examples, we will illustrate how these workflows simplify the journey from raw data to actionable scientific insights.", "full_description": "As astronomical surveys continue to grow in size and sophistication, researchers face mounting challenges in building efficient, scalable, and reproducible data processing pipelines. Modern observatories, like NASA's James Webb Space Telescope (JWST), are delivering unprecedented volumes of complex and specialized data, requiring innovative approaches to transform raw observations into meaningful, scientifically valid results. This talk focuses on leveraging Pythonic workflow management tools to address the unique challenges of processing large-scale astronomical datasets efficiently and reproducibly.\r\n\r\nI will provide a brief overview of JWST including its capabilities and the groundbreaking science it has enabled. In particular we will focus on the Pure Parallel mode which collect serendipitous observations from regions of the sky adjacent to primary science targets. These opportunistic datasets are a powerful resource for blind extragalactic surveys, offering unique opportunities to uncover faint galaxies, cosmic structures, and rare astrophysical phenomena. However, their \u201cunscheduled\u201d and heterogeneous nature presents significant challenges: the data arrive in raw, uncalibrated formats and require intricate, multi-step workflows\u2014such as artifact masking, background subtraction, and galaxy spectral analysis\u2014before becoming scientifically usable.\r\n\r\nIn this talk, I will demonstrate how tools like Snakemake and Pixi offer powerful, Pythonic solutions for these challenges. I\u2019ll show how these tools allow scientists to design modular, scalable, and highly parallel workflows that automate the reduction and analysis process while efficiently distributing computation across high-performance computing (HPC) clusters and cloud environments. By breaking workflows into smaller, reusable components, we can improve computational performance and maintain flexibility to adapt pipelines to new datasets, instruments, or evolving scientific goals.\r\n\r\nReproducibility remains a critical pillar of modern science, and I will highlight how combining workflow managers with environment management tools ensures version-controlled pipelines, transparent data lineage tracking, and reliable replication of results. This enables consistent analyses across diverse systems, fostering collaboration and long-term usability of scientific products.\r\n\r\nThis talk is designed for (data) scientists and researchers working with large-scale or complex datasets with multi-step reduction/analysis pipelines. This talk will provide a GitHub repository containing resources for building modular workflows, including examples of existing infrastructures, to provide actionable takeaways. Attendees will leave with a clear understanding of how to apply modern workflow management techniques to streamline their processing pipelines, improve reproducibility, and scale their analyses to meet the demands of their datasets.", "code": "NBFH7G", "state": "submitted", "created": "2024-12-18", "speaker_names": "Raphael Hviding", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Why just about anyone can install arrow-odbc", "abstract": "Build on top of Apache Arrow and ODBC `arrow-odbc` enables exchange of data between databases and in memory Arrow arrays. This talk is about the desgin decisions which make `arrow-odbc` efficient as well as portable and easy to install.", "full_description": "Build on top of Apache Arrow and ODBC `arrow-odbc` enables exchange of data between databases and in memory Arrow arrays.\r\n\r\nAt first we will introduce the ODBC standard, how it works, how it is dated a bit, but how it still enables us to communicate with almost every data source out there.\r\n\r\nThe efficiency part of the talk will ODBC block cursors and their speed and memory implications. It will also hint on why they are often overlooked then engaging with the ODBC API, besides usually being the better choice, especially for applications dealing with lots of data at once.\r\n\r\nThe talk will then go into the challenges of tying the ODBC system dependency, an Arrow implementation into easy to build and very portable wheels, which run on many platforms, with many interpreters, with many different pyarrow versions. In order to do so we will look at the tradeoffs between using PyO3 vs cffi in order to interopt with Rust and the benefits of using the Arrow C-Interface vs directly linking against the C++ implementation.", "code": "UXMLS3", "state": "submitted", "created": "2024-12-22", "speaker_names": "Markus Klein", "track": "PyData: Data Handling & Engineering"}, {"title": "Is Prompt Engineering Dead? How Auto-Optimization is Changing the Game", "abstract": "The rise of LLMs has elevated prompt engineering as a critical skill in the AI industry, but manual prompt tuning is often inefficient and model-specific. This talk explores various automatic prompt optimization approaches, ranging from simple ones like bootstrapped few-shot to more complex techniques such as MIPRO and TextGrad, and showcases their practical applications through frameworks like DSPy and AdalFlow. By exploring the benefits, challenges, and trade-offs of these approaches, the attendees will be able to answer the question: is prompt engineering dead, or has it just evolved?", "full_description": "With the rise of LLMs, prompt engineering has become a highly impactful skill in the AI industry. However, manual prompt tuning is challenging, time-consuming, and not always generalizable across different models. This raises a reasonable question: can prompts be automatically learned from data? The answer is yes, and in this talk, we will explore how.\r\n\r\nFirst, we will provide a high-level overview of various prompt optimization approaches, starting with a simple technique like bootstrapped few-shot, which automatically generates and selects an optimal set of demonstrations for each step in the LLM chain. Then, we will discuss more complex approaches, such as MIPRO and TextGrad, which directly optimize the instructions.\r\n\r\nAfterwards, we will move on to a more practical part by showcasing how these techniques can be used via popular frameworks such as DSPy and AdalFlow.\r\n\r\nFinally, we will discuss the benefits and trade-offs of these approaches and frameworks in terms of costs, complexity and performance, so the audience can decide whether prompt engineering is truly dead.\r\n\r\n**Outline:**\r\n* Introduction (2 min)\r\n* Discussion of problems with manual prompt engineering (2 min)\r\n* Overview of existing prompt optimization approaches (10 min):\r\n    * Bootstrapped few-shot (3 min)\r\n    * MIPRO (3 min)\r\n    * TextGrad (4 min)\r\n* Showcasing the prompt optimization frameworks (8 min):\r\n    * DSPy (4 min)\r\n    * AdalFlow (4 min)\r\n* Comparison of methods and concluding remarks (3 min)\r\n* Q&A (5 min)", "code": "GURXPK", "state": "submitted", "created": "2024-12-20", "speaker_names": "Iryna Kondrashchenko, Oleh Kostromin", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "From Training to Deployment: A scalable Pipeline for ML Model Inference with Celery and HuggingFace", "abstract": "You've trained your machine learning model - now it's time to deploy it and use it for inference at scale to process larger amounts of data.\r\nThis talk will introduce a scalable and modular pipeline designed to perform inference on millions of job postings integrating multiple Machine Learning Models. Built with Celery and Flower, it manages tasks such as document processing, inference, cloud storage, and database integration. The pipeline, built with open-source tools, supports both CPU and GPU inference leveraging tools such as Modal and Hugging Face. It is designed to be modular and customizable for different use cases. Learn how you can build such a pipeline to efficiently scale your Machine Learning Inference workflows and why we opted for Celery as a lightweight, flexible solution over other orchestration tools like SageMaker, or Airflow.", "full_description": "Extracting insights from a few thousand data points with a machine learning model is usually simple, but when scaling to millions of data points or daily extractions, the process becomes much more complex. It demands robust infrastructure, systematic monitoring, efficient workflows, and strategies to tackle long processing times.\r\n\r\nIn this talk, we present a flexible and scalable pipeline designed to perform inference on millions of job postings integrating multiple Machine Learning Models. The pipeline is built entirely with open-source tools, while being theoretically scaleable across different machines like cloud or on-premise. Built with Celery and Flower, the pipeline is capable of orchestrate the entire extraction workflow, including document processing, inference, cloud storage, result verification, and database integration. This approach is demonstrated with a practical use case: historical and daily data extractions of job postings.\r\n\r\nThe architecture supports both CPU and GPU inference, with GPU tasks handled via Modal, a serverless cloud solution. The Models are hosted on Hugging Face. Although we presenting this pipeline with these specific Tools, the pipeline has a modular structure and enables the integration and replacement of alternative hosting solutions or technologies. This makes it adaptable to other stacks and use cases.\r\n\r\nWe will demonstrate how to construct such a pipeline, share our best practices and experiences, and highlight why we chose Celery as our orchestration tool. We will discuss Celery\u2019s scalability and flexibility, and how it addresses challenges such as handling sensitive data and optimizing resource use. Whether you're deploying ML models in production or scaling inference workflows, this talk provides actionable insights and practical strategies to elevate your machine learning projects.", "code": "DFZQK7", "state": "submitted", "created": "2024-12-16", "speaker_names": "Rahkakavee Baskaran, Jan Dix", "track": "PyCon: MLOps & DevOps"}, {"title": "Learnings from migrating a Flask app to FastAPI", "abstract": "FastAPI has been constantly growing in popularity during the last years. A lot of this growth is driven by its relative simplicity and ease-of-use. In this talk, we'll discuss some practical insights into building a FastAPI application, based on my experience of migrating an existing Flask prototype to FastAPI. \r\n\r\nWe'll explore how FastAPI's core features like Pydantic integration and dependency injection can improve API development, while also talking about the drawbacks of FastAPI.", "full_description": "Building HTTP APIs has become a normal part of the work as a software or data engineer within the last 10 to 15 years. In the Python ecosystem Flask was the only option to build an HTTP API for many years. After its initial release in 2018 FastAPI quickly became a serious alternative to build such APIs with Python.\r\n\r\nIn this talk I will share my experiences from migrating an existing HTTP API built with flask to a FastAPI-based API. \r\n\r\nWe will discuss the following topics: \r\n\r\n- Why did we migrate at all?\r\n- Modeling data for input and output \r\n- Async is overrated\r\n- The different ecosystems and why it doesn't matter\r\n- Problems you **will** encounter \r\n\r\nThe talk will show you the practical differences between developing APIs with FastAPI or Flask.", "code": "EDJ8N7", "state": "submitted", "created": "2024-12-19", "speaker_names": "Orell Garten", "track": "PyCon: Django & Web"}, {"title": "Deploying Synchronous and Asynchronous Django Applications for Hobby Projects", "abstract": "Simplify deploying hybrid Django applications with synchronous views and asynchronous apps. This session covers ASGI support, Docker containerization, and Kamal for seamless, zero-downtime deployments on single-server setups, ideal for hobbyists and small-scale projects.", "full_description": "Hobby projects often start small but can quickly grow in complexity, especially when incorporating Django\u2019s support for asynchronous applications alongside traditional synchronous views. Deploying such hybrid projects on a single server\u2014whether in the cloud or on-premise\u2014can be daunting without the right tools and workflows.  \r\n\r\nThis talk focuses on simplifying the deployment process for hobbyists and developers who want to create and manage robust Django applications without requiring extensive infrastructure or expertise. We\u2019ll cover:  \r\n- Deploying Django projects that combine synchronous views and asynchronous apps using Django\u2019s ASGI support.  \r\n- Containerizing the application with Docker for consistent and manageable environments.  \r\n- Utilizing Kamal, an open-source deployment tool, to enable zero-downtime deployments, rolling updates, and seamless app management.  \r\n- Demonstrating the workflow on a single cloud server, with insights on adapting it to on-premise servers.  \r\n\r\nWhether you're building a passion project or experimenting with modern Django features, this session will provide you with practical tools and approaches to deploy hybrid Django applications effortlessly, keeping the process accessible and scalable for hobby-level development.", "code": "CMTKZS", "state": "submitted", "created": "2024-11-30", "speaker_names": "melhin", "track": "PyCon: Django & Web"}, {"title": "You don\u2019t think about your Streamlit app optimization until you try to deploy it to the cloud", "abstract": "Building Streamlit apps is easy for Data Scientists - but when it\u2019s time to deploy them to the cloud, challenges like slow model loading, scalability, and security can become major hurdles. This talk bridges two perspectives: the Data Scientist who builds the app and the MLOps engineer who deploys it. We'll dive into optimizing model loading from Hugging Face Hub, implementing features like autoscaling and authentication, and securing your app against potential threats. By the end of this talk, you\u2019ll be ready to design Streamlit apps that are functional and deployment-ready for the cloud.", "full_description": "#### Talk Outline:\r\n\r\n1. Introduction\r\n   - The disconnect: challenges when transitioning a Streamlit app from development to deployment.  \r\n   - Why deployment considerations should influence app design.  \r\n\r\n2. Optimizing model loading from HuggingFace hub \r\n   - Challenges:  \r\n     - Large model sizes slowing down app performance.  \r\n     - Inefficient loading processes increasing costs and user wait times.  \r\n   - Solutions:  \r\n     - Using Streamlit caching to reuse loaded models across sessions.  \r\n     - Preloading models during image build.\r\n     - Deploying models and calling them as APIs\r\n   - MLOps Perspective: How optimized model loading reduces deployment complexity and cloud costs.  \r\n\r\n3. AWS deployment considerations: autoscaling, authentication, and security  \r\n   - Autoscaling:  \r\n     - Challenges: Handling variable user traffic without incurring unnecessary costs.  \r\n     - Solutions:  \r\n       - Using Fargate with ECS for containerized apps with auto-scaling policies.  \r\n       - Setting thresholds to scale instances based on traffic and resource utilization.  \r\n       - Optimizing cost-performance balance with reserved vs. spot instances.  \r\n\r\n   - Authentication:  \r\n     - Challenges: Providing a secure and user-friendly authentication mechanism.  \r\n     - Solutions:  \r\n       - Integrating AWS Cognito for user management.  \r\n       - Adding role-based access control to limit app functionality based on user roles.  \r\n\r\n   - Security:  \r\n     - Challenges: Protecting the app from attacks and unauthorized access.  \r\n     - Solutions:  \r\n       - Using AWS Web Application Firewall (WAF) to block malicious traffic.  \r\n       - Configuring CloudFront to protect against DDoS attacks and improve performance.  \r\n       - Setting up HTTPS with Route 53 and TLS certificates for secure connections.  \r\n   - MLOps Perspective: Balancing simplicity and scalability in app deployment.  \r\n\r\n4. Secrets Storage  \r\n   - Challenges: Hardcoding sensitive credentials into the app.  \r\n   - Solutions:  \r\n     - Using AWS Secrets Manager or Parameter Store for secure secrets management.  \r\n     - Employing environment variables for flexible app configuration.  \r\n   - MLOps Perspective: How to ensure security without complicating deployment workflows.  \r\n\r\n5. Key Takeaways \r\n   - Data Scientist\u2019s Perspective:  \r\n     - Why it\u2019s critical to consider performance, scalability, authentication, and security during app development.  \r\n   - MLOps Perspective:  \r\n     - How to simplify deployment while ensuring performance and security.  \r\n   - Encouraging collaboration between Data Scientists and MLOps engineers for smoother deployment processes.  \r\n\r\n#### What you will learn:  \r\n- How to efficiently load Hugging Face models in Streamlit apps to reduce costs and improve performance.  \r\n- How to design apps with AWS autoscaling to handle variable traffic seamlessly.  \r\n- Best practices for implementing user authentication with AWS Cognito.  \r\n- How to secure your Streamlit app using cloud services.\r\n- Best practices for secure secrets management in Streamlit apps.\r\n- How to approach Streamlit app development with deployment in mind.", "code": "3VYSMS", "state": "submitted", "created": "2025-01-03", "speaker_names": "Darya Petrashka", "track": "PyCon: MLOps & DevOps"}, {"title": "\ud83d\udeaa\ud83d\udeaa\ud83d\udc10 Lessons in Decision Making from the Monty Hall Problem", "abstract": "The Monty Hall Problem is not only a brain teaser but also pertains decision making lessons. In this talk give an intuition to this popular puzzle in probability and discuss why understanding its essence will make you better at decision making.", "full_description": "This talk explores the counterintuitive world of probabilistic reasoning through the lens of the famous Monty Hall Problem. We'll dive deep into how this seemingly simple game show scenario challenges our intuitions about probability and decision-making, offering valuable insights for data scientists and analysts.\r\n\r\nThe presentation will cover:\r\n* Introduction to the Monty Hall Problem and its history\r\n* Common misconceptions and cognitive biases\r\n* Intuitive solutions to the problem\r\n* The Bayesian angle: the importance of belief updating\r\n* Assessing applicability in real world settings\r\n\r\nThe central thesis is that the Monty Hall Problem serves as a powerful tool for honing critical thinking skills essential in data analysis and decision-making under uncertainty. By the end of this talk, attendees will gain:\r\n* A thorough understanding of the Monty Hall Problem and its solution\r\n* Insights into the pitfalls of intuitive probability judgments\r\n* Practical strategies for approaching complex probabilistic scenarios\r\n\r\nThis presentation is designed for data scientists, analysts, and decision-makers with a basic understanding of probability theory. While no advanced mathematical knowledge is required, familiarity with concepts such as conditional probability will be beneficial.\r\n\r\nJoin us for an engaging exploration of how a simple game show puzzle can sharpen your analytical skills and transform your approach to decision-making in data science.", "code": "UZ8NYV", "state": "submitted", "created": "2024-12-20", "speaker_names": "Eyal Kazin", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Death by a Thousand API Versions", "abstract": "API versioning is tough, really tough. We tried multiple approaches to versioning in production and eventually ended up with a solution we love. During this talk you will look into the tradeoffs of the most popular ways to do API versioning, and I will recommend which ones are fit for which products and companies. I will also present my framework, Cadwyn, that allows you to support hundreds of API versions with ease -- based on FastAPI and inspired by Stripe's approach to API versioning.\r\n\r\nAfter this session, you will understand which approach to pick for your company to make your versioning cost effective and maintainable without investing too much into it.", "full_description": "Web API Versioning is a way to allow your developers to move quickly and break things while your clients enjoy the stable API in long cycles. It is best practice for any API-first company to have API Versioning in one way or another. Otherwise, the company will either be unable to improve their API or their clients will have their integrations broken every few months.\r\n\r\nAt Monite, we spent a great deal of time trying to make API versioning work. We went through multiple horrifying solutions until we reached something we are proud of. I'll share just how expensive API versioning can get if done incorrectly.\r\n\r\nI'll cover all sorts of approaches you can pick to add incompatible features to your API: extremely stable and expensive, easy-looking but horrible in practice, and even completely version-less yet viable. I will provide you with the best practices of how you could find or implement a modern API versioning solution and will discuss the versioning at Stripe and Monite in great detail.\r\n\r\nWhen you leave, you'll have enough information to make your API Versioning user-friendly without overburdening your developers.", "code": "ZMKJAY", "state": "submitted", "created": "2024-12-21", "speaker_names": "Stanislav Zmiev", "track": "PyCon: Django & Web"}, {"title": "Unifying Language Models with Knowledge Graphs: GraphRAGs, LLM augmented KGs, and LLM+KG Evaluation", "abstract": "Unifying large language models (LLMs) and knowledge graphs (KGs) can address the shortcomings of LLMs such as lack of factual knowledge, hallucinations and lack of interpretability. Integrating LLMs with knowledge graphs enhances accuracy, contextual understanding, and scalability by leveraging structured, interconnected data. This synergy between LLMs and KGs not only enhances knowledge representation but also streamlines processes such as construction, completion, and querying, paving the way for more intelligent systems in various applications.", "full_description": "In this presentation, the main aspects which would be discussed\r\n\r\n1. KG-enhanced LLMs generation, which incorporate KGs during the pre-training and inference phases of LLMs \r\n2. LLM-augmented KGs, that leverage for KG completion by Entity discovery, Relation extraction, End-to-End KG construction, and Distilling KGs from LLMs \r\n3. Synergized LLMs with KGs, which is bidirectional with focus on knowledge representation and reasoning\r\n4. Evaluation and Benchmarking: LLM-KG-Bench an automated and continuous evaluation platform \r\n\r\nElaborating the Graph RAGs techniques that are designed to extract meaningful structured data from unstructured in order make substantial improvements in LLM generation. \r\n\r\nDescribing an use case for synergising LLM and KG: generation of personalised educational pathways for school students aspiring for higher education. The aspects of discussion includes,  the constructing of education knowledge graph using language models by preprocessing the data, extract entities, relationships, and construct these graphs from scratch or update the existing ones such as Knowledge Graph for University Curriculum Recommendation (EducOnto) and the E-OED framework (https://arxiv.org/pdf/2404.09296v1)\r\n\r\nFinally a short summary and outlook on the further developments", "code": "7MU3BM", "state": "submitted", "created": "2024-12-16", "speaker_names": "A Seshaditya", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Beyond FOMO \u2014 Keeping Up-to-Date in AI", "abstract": "The rapid evolution of AI technologies, particularly since the emergence of Large Language Models, has transformed the data science landscape from a field of steady progress to one of constant breakthroughs. This acceleration creates unique challenges for practitioners, from managing FOMO to battling imposter syndrome. Drawing from personal experience transitioning from mathematical modeling to modern AI development, this talk explores practical strategies for staying current while maintaining sanity. We'll discuss building effective learning structures, creating collaborative knowledge-sharing environments, and finding the right balance between innovation and implementation. Attendees will leave with actionable insights on navigating technological change while fostering sustainable growth in their teams and careers.", "full_description": "The landscape of data science and AI is evolving at an unprecedented rate. What started as a relatively stable field of mathematical modeling and time-series analysis has transformed into a whirlwind of weekly breakthroughs, especially since the emergence of Large Language Models. How do we stay current without succumbing to FOMO or imposter syndrome?\r\n\r\nIn this talk, I'll share my personal journey from traditional mathematical modeling to modern AI development, exploring how the field's pace has shifted dramatically. Drawing from real-world experiences as a consultant and team lead, I'll discuss practical strategies for maintaining technical excellence while managing the psychological challenges of rapid technological change. We'll examine how to build effective learning structures within teams, the importance of creating safe spaces for knowledge sharing, and why sometimes it's okay to not be at the cutting edge of every new development.\r\n\r\nThrough concrete examples and lessons learned, I'll offer insights on balancing client expectations, team growth, and personal development in an era where the technological landscape shifts weekly. Whether you're a seasoned data scientist or just entering the field, this talk will provide practical frameworks for navigating the exciting yet overwhelming world of modern AI development.", "code": "MSUCAS", "state": "submitted", "created": "2024-12-22", "speaker_names": "Carsten Frommhold", "track": "General: Education, Career & Life"}, {"title": "Graph Neural Networks for Collusion Detection using PyTorch and Deep Graph Library", "abstract": "Collusion is a complex phenomenon in which companies secretly collaborate to engage in fraudulent practices. This talk presents an innovative methodology for detecting and predicting collusion patterns in different national markets using neural networks (NNs) and graph neural networks (GNNs). GNNs are particularly well suited to this task because they can exploit the inherent network structures present in collusion and many other economic problems. In Python, we use PyTorch and the Deep Graph Library (DGL) to develop and train models on individual market datasets from Japan, the United States, two regions in Switzerland, Italy, and Brazil, focusing on predicting collusion in single markets. In our empirical study, we show that GNNs outperform NNs in detecting complex collusive patterns. This research contributes to the ongoing discourse on preventing collusion and optimizing detection methodologies, providing valuable guidance on the use of NNs and GNNs in economic applications to enhance market fairness and economic welfare.", "full_description": "Collusion detection is a critical process for identifying instances in which entities, such as companies or individuals, secretly collaborate to engage in fraudulent practices, often characterized by agreements on prices, market shares, or tactics to avoid competition. GNNs explicitly allow the incorporation of network structures, which are prevalent in collusion and key to improving the detection rate. This talk tackles the challenge of detecting fraud patterns using GNNs and highlights how these models can effectively learn network structures inherent in bidding markets.\r\nWe propose a fraud detection algorithm that uses relational graph convolutional networks (R-GCNs) to analyze the inherent network structures in bidding data. R-GCNs have the ability to assign different weights to different types of edges. By using this extension of GNNs, different types of relationships between the bids can be combined to generate richer node embeddings, making them more effective at detecting collusive behavior among companies participating in bids and tenders. We develop and train these models using the PyTorch framework and the Deep Graph Library (DGL), applying them to datasets from Japan, the United States, Switzerland, Italy, and Brazil. Our empirical findings show that GNNs outperform traditional NNs in identifying complex collusive patterns, offering a new solution to this fraud problem.\r\nFor data scientists, especially those working with network data, the detection of anomalous patterns in network structures is a critical challenge across different domains, e.g. financial fraud detection or social network analysis. Attendees will gain practical strategies for applying GNNs to classification tasks to detect patterns in network data and a deeper understanding of how graph embeddings can enhance fraud detection models.", "code": "WUH93P", "state": "submitted", "created": "2024-12-19", "speaker_names": "Mara Mattes", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "The Best Kept Secret to Winning Bids with Power BI", "abstract": "Want to know how to win the next big bid? In this session I'll reveal how to use Power BI to turn boring data into a strategic decision-making machine. I'll show you how to access public data like a pro, transform it into explosive insights, and create dashboards that impact industry leaders.", "full_description": "Want to know how to win the next big bid? In this session I'll reveal how to use Power BI to turn boring data into a strategic decision-making machine. I'll show you how to access public data like a pro, transform it into explosive insights, and create dashboards that impact industry leaders.\r\n\r\nWhat you'll learn:\r\n\r\n1) How to connect to OData sources in minutes: No complicated configurations, just fast results.\r\n\r\n2) How to clean and organize thousands of rows of data to extract the secrets behind each award.\r\n\r\n3) Visualizations that impress: Interactive maps, charts that summarize the most lucrative procedures, and a dashboard that will make you look like a data expert.\r\n\r\nIn the dashboard shown:\r\n\r\n1) You'll see contracts, costs, and values \u200b\u200bsorted by key criteria.\r\n\r\n2) You'll filter by years and procedure types to discover where the real money is.\r\n\r\n3) Maps and graphs will guide you to the most efficient analysis for your business strategy.\r\n\r\nIf you are ready to master bidding and become the leader everyone envies, you can't miss this session!", "code": "9AXMEE", "state": "submitted", "created": "2024-12-31", "speaker_names": "Jordi Bosch", "track": "PyData: Visualisation & Jupyter"}, {"title": "Symbolic KAN for Extracting Discrete White-Box Models  for PMSM motors", "abstract": "# Kolmogorov-Arnold Networks (KANs) for Model Predictive Control (MPC)\r\n\r\nKolmogorov-Arnold networks (KANs) have made a significant impact in the world of AI, thanks to their ease of interpretability and relatively good performance with small model sizes. These qualities make KANs a promising tool for **Model Predictive Control (MPC)**.\r\n\r\nThis approach combines KANs with **symbolic regression** to extract **interpretable white-box models** for MPC. KANs are effective at approximating complex systems, and when paired with symbolic regression, they uncover mathematical expressions that represent the system dynamics.", "full_description": "# Pioneering Transparent Models for Electric Motors with KAN and Symbolic Regression\r\n\r\nThis work introduces an innovative approach that combines **Kolmogorov-Arnold Networks (KANs)** with **symbolic regression** to extract **interpretable discrete white-box models**, focusing on a balanced subset of the dataset *\"Identifying the Physics Behind an Electric Motor\"*, particularly on **three-phase permanent magnet synchronous motors (PMSMs)**.\r\n\r\n## Key Highlights:\r\n- **Kolmogorov-Arnold Networks (KANs)**:\r\n  - Known for their ability to model **nonlinear dynamics**.\r\n  - Offer **inherent interpretability**, enabling transparent model representation.\r\n\r\n- **Symbolic Regression**:\r\n  - Capable of uncovering **concise, human-readable equations**.\r\n  - Enhances the transparency of the predictive models.\r\n\r\n## Contribution:\r\nBy integrating the strengths of KAN and symbolic regression, this research demonstrates the feasibility of **symbolic KAN** for:\r\n1. **Unveiling interpretable models** that provide insights into the dynamics of PMSMs.\r\n2. **Advancing model predictive control (MPC)** techniques with **transparent and explainable solutions** in electric motor applications.\r\n\r\nThis approach underscores the potential of symbolic KAN in bridging the gap between complex machine learning models and the need for explainable, physics-informed systems.", "code": "GX9QEQ", "state": "submitted", "created": "2024-12-21", "speaker_names": "Bensalem Mohammed Abderrahmane", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Scraping LEGO for Fun: A Hacky Dive into Dynamic Data Extraction", "abstract": "Unlock the full potential of modern web scraping by combining Python, Scrapy, and Playwright to extract data from dynamic, JavaScript-heavy sites\u2014exemplified by LEGO product pages. This talk introduces Model Context Protocol (MCP) servers for orchestrating advanced data fetching, refining CSS selectors, and integrating Large Language Models for automated code suggestions. Learn how to scale ethically, handle concurrency, and respect site policies, while maintaining flexible, maintainable pipelines for diverse use cases from research to robotics.", "full_description": "# Advanced Web Scraping: From LEGO to Production\r\n\r\nToday's web landscape is teeming with JavaScript-heavy content, complex layouts, and sometimes opaque data structures. But what if you could reliably scrape rich product information\u2014images, specs, descriptions\u2014from modern e-commerce sites without hitting constant roadblocks? This session tackles advanced scraping with Python, Scrapy, and Playwright, exemplified by data extraction from LEGO product pages. We'll explore a \"grey hat\" perspective\u2014applying a slightly \"hacky\" mindset\u2014while stressing practical ethics, performance considerations, and compliance with site policies.\r\n\r\n## Outline\r\n\r\n### 1. Introduction: The Hacky Spirit vs. Ethical Constraints\r\n- Why scrape LEGO?\r\n- Setting boundaries: terms of service, rate limiting, and disclaimers\r\n- When \"scraping for fun\" crosses into potential legal pitfalls\r\n\r\n### 2. Scraping Tech Stack Overview\r\n- Scrapy for structured crawling and item pipelines\r\n- Playwright for rendering JavaScript and handling dynamic elements\r\n- Comparison to traditional HTML-only approaches\r\n- Project structure, environment setup, and practical tips\r\n\r\n### 3. Spiders in Action\r\n- Product Spider: Extracting core product data (ID, name, specifications, multiple images)\r\n- Gallery Spider: Navigating hidden galleries, handling tricky JS-based carousels, and filtering unwanted images\r\n- Ensuring consistent output (JSON or database ingestion)\r\n\r\n### 4. Model Context Protocol (MCP) Integration\r\n- Definition: Leveraging specialized helper servers for orchestrating data fetching, refining selectors, and automating debugging\r\n- Chaining Large Language Models: Code suggestions, auto-generation of selectors, and reactive error handling\r\n- Example workflow: \"Broken selector? Ask the MCP server for an LLM-aided fix\"\r\n\r\n### 5. Performance & Scale\r\n- Polite but robust concurrency: balancing speed and TOS compliance\r\n- Handling large link lists, incremental updates, and site changes\r\n- Monitoring and logging for reliability, debugging, and optimization\r\n\r\n### 6. Ethics & Privacy\r\n- Respecting site ownership, disclaimers, and usage limits\r\n- Storing scraped data securely and avoiding personal information\r\n- A discussion of \"grey hat\" territory: testing site vulnerabilities without exploiting them\r\n\r\n### 7. Use Cases & Extensions\r\n- Research software engineering: building reproducible data sets\r\n- Robotics and embedded: offline or partial data ingestion for classification or motion planning\r\n- Future directions: advanced concurrency, containerization, and HPC\r\n\r\n### 8. Demo & Q&A\r\n- Live snippet showing an MCP-powered spider reacting to a changed DOM structure\r\n- Q&A session on bridging the gap between hackery and best practices\r\n\r\n## Key Takeaways\r\n- Techniques for scraping dynamic, JS-heavy sites using Python, Scrapy, and Playwright\r\n- Practical \"hacky\" methods balanced by responsible, 'ethical approaches'\r\n- Introduction to Model Context Protocol servers for automated code refinement\r\n- Scalable patterns for data handling, from small tests to large-scale deployments\r\n\r\nWhether you're a data engineer, hobbyist, or researcher, this talk provides a robust (and slightly subversive) recipe for capturing essential data from the wild world of modern websites\u2014without crossing into unethical or unlawful territory.", "code": "HSFR7A", "state": "submitted", "created": "2024-12-24", "speaker_names": "Peter Lodri", "track": "PyData: Data Handling & Engineering"}, {"title": "Building Privacy-Focused Vector Search Applications: Hands-On Guide to Generative AI with Local LLMs", "abstract": "Detailed insights into generating embeddings with tools like Ollama and demonstrating how LangChain agents can perform tasks such as document summarisation and API interactions, all while maintaining data privacy in a Python application", "full_description": "Most of the applications are being powered by Generative AI. Some of the key proponents of Generative AI stem from concepts like RAG, Vector search and embeddings. This tutorial covers these core concepts on Vector search and is complete guide to help guide you from 0 to 1 in Vector Search with open source tools, libraries in Python understanding not only the core meaning of these terms but also hands on projects. \r\n\r\nA special focus of this tutorial is to also learn how to build privacy focused applications. A lot of times user send data to LLM cloud providers like OpenAI, raising privacy concerns. This tutorial will also emphasise on an alternative and privacy focused way to build AI applications by running LLMs locally with Ollama that keep everything local on your computer. This approach allows to avoid sending sensitive information to external servers. The tutorial also highlights LangChain's ability to create versatile AI agents capable of handling tasks autonomously by creating embeddings for the data. So come learn how can you build the next gen, privacy focused Vector search application powered by Local LLMs, open source vector databases while learning the key concepts of Generative AI such as Vector search, embeddings and practical use cases with an end to end example.", "code": "DJYRHE", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Gaurav Pandey", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Foundational Models for Time Series Forecasting: are we there yet?", "abstract": "Transformers have changed fields like NLP, Computer Vision, sound generation, and even protein-folding. But what about forecasting? This talk will explain how titans like Amazon and Google built a recipe for Foundational Models for Time Series Forecasting.\r\n\r\nMost importantly, though, during the talk we will survey the state of the art of open datasets to train such models, and offer practical elements to assess the benchmarks that have been put out.", "full_description": "This is a pragmatic talk for Generative AI and ML practitioners alike, eager to understand how researchers managed to **adapt transformer-based architectures to forecasting problems**, as well as how to **evaluate new models for specific use-cases**.\r\n\r\nThe first part of the talk explains how researchers **discretised time series into a finite \"dictionary\"** and the **theoretical limitations** of this approach, such as how to deal with different sampling frequencies during training.\r\n\r\nWe will then make a parallel with Large Language Models (LLMs) scaling laws to evaluate the data strategies used to train such universal forecasting models. In other words, we will ask the question **whether we have enough publicly available time-series data to train foundational models**, and how this can **affect such models' evaluation**.\r\n\r\nFinally, we will display how we attempted to **benchmark those models against a robust baseline of models**, and where our experiment is compared to the publicly available results.\r\n\r\nNo specific prior knowledge is required for attendance. Though forecasting practitioners stand to gain the most from this talk, every practitioner in machine learning or generative AI can follow along and draw the key conclusions.\r\n\r\n**Outline**\r\nMinutes 1-3. Problem statement: challenges in adapting transformers to the time-series domain.\r\nMinutes 3-10. How Chronos and Moirai are implemented.\r\nMinutes 10-15. Do we have enough data to train universal forecasters?\r\nMinutes 15-25. Lessons learnt from evaluating transformer-based models for a specific use-case.\r\nMinutes 25-30. Wrap up and Q&A.", "code": "EGZZBA", "state": "submitted", "created": "2024-12-22", "speaker_names": "Luca Baggi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Data Science Meets Sustainability: Navigating the European Sustainability Reporting Standards (ESRS)", "abstract": "The European Sustainability Reporting Standards (ESRS) are now in effect, requiring most companies across the EU to report on environmental, social, and governance (ESG) factors in a more structured and extensive way. In this talk, I will break down the key aspects of ESRS and explain why data scientists are in a strong position to help drive sustainability reporting within their organizations.", "full_description": "The European Sustainability Reporting Standards (ESRS) are now in effect, requiring most companies across the EU to report on environmental, social, and governance (ESG) factors. These standards aim to increase transparency and accountability, requiring businesses to disclose data that reflects their sustainability efforts in both quantitative and qualitative ways. Companies must report on a vast number of specific ESG metrics, as well as provide descriptions of their actions, goals, and plans in these areas.\r\n\r\nIn this talk, I will explore how data scientists are in a unique position to support organizations in navigating the ESRS. The standards require data professionals to handle both quantitative metrics\u2014like carbon emissions, energy use and other performance indicators\u2014and qualitative data such as action plans, sustainability goals, and strategies. I want to demonstrate where we as data scientists can bridge the gap between raw data and actionable insights, ensuring accuracy in reporting, developing workflows for data integration, and helping our organizations articulate their sustainability efforts effectively.\r\n\r\nI will also highlight some of the key challenges companies face when gathering and reporting ESG data, such as data gaps, inconsistencies, and the complexities of integrating data from different sources. By the end of the talk, I hope that you will have a clearer understanding of sustainability reporting and how you could help organizations meet these new standards.", "code": "QULMSK", "state": "submitted", "created": "2024-12-22", "speaker_names": "Jonathan Brandt", "track": "General: Others"}, {"title": "The future of AI training is federated", "abstract": "Since it\u2019s introduction in 2016, Federated Learning (FL) has become a key paradigm to AI models in scenarios when training data cannot leave its source. This applies in many industrial settings where centralizing data is challenging due to a combination of reasons, including but not limited to privacy, legal, and logistics.\r\n\r\nThe main focus of this tutorial is to introduce an alternative approach to training AI models that is straightforward and accessible. We\u2019ll walk you through the basics of an FL system, how to iterate on your workflow and code in a research setting, and finally deploy your code to a production environment. You will learn all of these approaches using a real-world application based on open-sourced datasets, and the open-source federated AI framework, [Flower](https://github.com/adap/flower), which is written in Python and designed for Python users. Throughout the tutorial, you\u2019ll have access to hands-on open-sourced code examples to follow along.", "full_description": "Federated Learning has quickly become the preferred form of training of AI models when the training data cannot leave their point of origin due to privacy regulations (e.g. GDPR), legal constraints (e.g. in different jurisdictions), and logistical challenges (e.g. large volumes of data, sparse connectivity), among other reasons. Furthermore, contracts and regulations establish boundaries for data sharing, particularly in industries like healthcare and finance, where misuse prevention is crucial. One could also argue that we are [running out of publicly and ethically sourced datasets](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training), for instance to [scale large foundational models](https://arxiv.org/abs/2410.08892), and federated learning offers one way to train models on protected data.\r\n\r\nThe key point of this tutorial is to introduce an alternative approach to training AI models that is straightforward and accessible.\r\n\r\nThis tutorial is sequenced in 3 parts. We\u2019ll first introduce federated learning and its prototypical architecture. In part 2, we\u2019ll dive into a series of live Python code demos that showcase how to convert a classical centralized machine learning workflow into a federated workflow involving multiple federated clients. We\u2019ll demonstrate the similarities and differences of how the iteration of a federated research project is conducted. Finally, in part 3, we\u2019ll demonstrate how you can take your research code and deploy it in a production setting using a mixture of physical edge devices and VMs.\r\n\r\nThroughout the tutorial, we\u2019ll use [Flower](https://github.com/adap/flower), the fully open-sourced federated AI framework, which is written in Python and designed for Python users. With simplicity as one of it\u2019s main goals, Flower provides multiple features and libraries to accelerate research, such as [Flower Baselines](https://flower.ai/docs/baselines/) (for reproducing federated learning benchmarks) and [Flower Datasets](https://flower.ai/docs/datasets/) (a standalone Python library for easily creating federated datasets). We\u2019ll showcase how to use the Flower CLI in both research and production setting.\r\n\r\nThis tutorial addresses people with fluency in Python, CLI, and basic knowledge of a machine learning project. It would help if you\u2019ve also used Docker before. Any data practitioner is encouraged to attend the tutorial to learn and discuss how to federate and distribute the training of an ML model. \r\n\r\nYou will learn:\r\n\r\n- What\u2019s Federated Learning?\r\n    - Basics and real-world examples\r\n- How to federate your existing ML training code, and more FL-specific steps such as how to:\r\n    - Configure the behaviours of each federated client\r\n    - Persist the state of each client across global rounds\r\n    - Evaluate both aggregated and local models\r\n    - Standardize your FL experiments\r\n    - Track your experiments\r\n- How to deploy your research code in a production setting, such as how to:\r\n    - Deploy Flower federated learning clients using Docker\r\n    - Set-up secure connection and node authentication\r\n    - Run, monitory, and manage the federated learning runs.\r\n\r\nBring your own laptop if you\u2019d like to follow along. Some code examples will be executed in Google Colab, others can be locally executed on your favourite IDE. A GitHub repo containing the code examples will be shared before the event.\r\n\r\nThe tutorial session is structured in the following way:\r\n\r\n- 0:00 Introduction, and getting to know the audience.\r\n- 0:05 What\u2019s Federated Learning? Basics and real-world-examples.\r\n- 0:25 Overview of the Flower framework for federated learning\r\n- 0:30 Quickstart examples with PyTorch. Moving from a centralized training to federated.\r\n- 1:00 Deploying your research to production\r\n- 1:20 Feedback and Q&A", "code": "9Y9DM8", "state": "submitted", "created": "2024-12-22", "speaker_names": "Chong Shen Ng", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Swiss Army Model: Adapting Your Machine Learning Model to Evolving Business Needs", "abstract": "Building a machine learning model requires a significant investment of time and resources. In this talk, I will share how I transformed a churn-prediction model into a flexible retention insights tool, along with practical tips for effectively adapting models to evolving business needs. This project highlights how a single model can unlock unexpected value when creatively repurposed, avoiding the need to start from scratch.", "full_description": "Building a machine learning (ML) model is a significant investment of time and resources, involving data collection, processing, model selection, training, evaluation, and optimization. But what happens when the business objective shifts after all this effort? In this talk, I will share my experience of adapting a churn-prediction model into a flexible retention insights tool, showcasing how we can adjust models to meet evolving business needs without starting from scratch.\r\n\r\nIn one of my previous projects as a data scientist, I initially developed a model to predict churn based on user behavior data\u2014an objective common across many industries. However, when the results were presented to internal stakeholders, their feedback was unexpected. Customer success managers, already familiar with traditional churn metrics, weren\u2019t interested in predictions but instead wanted insights into user behaviors just before churn occurred. To respond, I adapted the model by focusing on the features and coefficients rather than the predictions themselves. This shift led to surprising insights: the same model, originally designed to predict churn, now identified key events related to retention. Product managers were thrilled and proposed running campaigns to promote these retention-positive behaviors. This new direction, however, presented another challenge: estimating the effectiveness of each campaign. We leveraged the same model to simulate the potential impact of these campaigns on retention. By adjusting the model inputs to reflect hypothetical campaign actions, we could identify low-effort, high-impact initiatives. This not only helped optimize campaign strategy but also enhanced the model's interpretability by adding context to the insights. \r\n\r\nThis project is a great example of how a single model can unlock unexpected value when creatively repurposed. We, as data scientists, can leverage our modelling and business knowledge to maximize the impact of our work. We can build, repurpose and reevaluate. In my talk, I will also share practical tips from my experience on how this can be done effectively, such as early and diverse feedback from stakeholders to ensure real business needs are met, and robust model evaluation metrics to ensure quality is maintained.", "code": "YGEKFS", "state": "submitted", "created": "2024-12-17", "speaker_names": "Anat Eck", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Supercharge Your Testing with inline-snapshot", "abstract": "Snapshot tests are invaluable when you are working with large, complex, or frequently changing expected values in your tests.\r\nIntroducing inline-snapshot, a Python library designed for snapshot testing that integrates seamlessly with pytest, allowing you to embed snapshot values directly within your source code.\r\nThis approach not only simplifies test management but also boosts productivity by improving the maintenance of the tests.\r\nIt is particularly useful for integration testing and can be used to write your own abstractions to test complex Apis.", "full_description": "This Talk gives you an introduction into inline-snapshot and how it can transform your testing strategy:\r\n* Foundations of Snapshot Testing: Start with an introduction to what snapshot testing is and why it's a game-changer for Python developers.\r\n* Basic Usage: Learn the core functionality of the `snapshot()` function. Understand how it captures and manages snapshots inline with your tests.\r\n\r\nAdvanced Techniques:\r\n* Dirty Equals: Explore how you can leverage dirty-equals within your snapshots for more flexible assertions, allowing for partial matching which is particularly useful for complex data structures.\r\n* Parametrized Tests: See how inline-snapshot can be applied to parametrized tests, ensuring each parameter set has its own snapshot.\r\n* Customizable: Learn to create your own test functions to test your specific problems.", "code": "CRNJWQ", "state": "submitted", "created": "2025-01-02", "speaker_names": "Frank Hoffmann", "track": "PyCon: Testing"}, {"title": "Enhance computer vision workflows on SageMaker for data scientists and ML engineers with Picsellia", "abstract": "If you're already a SageMaker user or prefer to run your training jobs on SageMaker to leverage its diverse and sophisticated training compute resources but still want a specialized platform to manage your computer vision data and track your CV Ops workflow, you can integrate Picsellia into your computer vision workflows. This will allow you to run your training jobs on SageMaker while using Picsellia to seamlessly manage vision datasets and model artifacts, monitor experiments, and improve the overall efficiency of your computer vision projects.", "full_description": "Assume you're already developing a computer vision system utilizing an ML platform, such as AWS Sagemaker. You'll quickly see that it lacks dedicated functionality for effective computer vision development. SageMaker provides data scientists and machine learning engineers with significant computational resources and a robust MLOps platform. However, it is not designed for the seamless execution of computer vision dataset management, model monitoring, and evaluation. \r\n\r\nFor instance, its data sources (S3, LFS, or most existing Datalakes) don\u2019t provide the flexibility for direct and effective annotation, optimized management of annotation artifacts, and visual examination capabilities. Also, its evaluation capabilities are not optimized to flag vision-specific issues such as occlusion or image quality.\r\n\r\nData scientists or ML engineers often need to utilize various tools and platforms to effectively handle vision data (i.e., images or videos) or efficiently evaluate CV model performance. Accommodating these extra components can convolute their MLOps setup.\r\n\r\nThey need a seamless and scaleable way to mitigate the bottlenecks of managing computer vision data and efficiently evaluating their computer vision models in production when leveraging a powerful ML platform like SageMaker for running training and experiment jobs. \r\n\r\nThis talk demonstrates the use of the specialized CVOp cloud-agnostic platform (Picsellia) with their existing ML platform like AWS SageMaker to streamline computer vision workflows efficiently without using many tools or platforms. This involves leveraging SageMaker\u2019s robust ML capabilities and Picsellia for vision dataset management, model monitoring, and evaluation. \r\n\r\nBy the end of this session, participants will:\r\n- Understand the key challenges in managing computer vision workflows.\r\n- Learn how to integrate Picsellia with AWS SageMaker to optimize CVOps processes.\r\n- Gain hands-on knowledge on managing vision datasets and monitoring and evaluating their computer vision models.", "code": "3GTBDA", "state": "submitted", "created": "2024-12-18", "speaker_names": "Tochukwu Nwoke", "track": "PyCon: MLOps & DevOps"}, {"title": "LLM Inference Arithmetics: the Theory behind Model Serving", "abstract": "Have you ever asked yourself how parameters for an LLM are counted, or wondered why Gemma 2B is actually closer to a 3B model? You have no clue about what a KV-Cache is? (And, before you ask: no, it's not a Redis fork.) Do you want to find out how much GPU VRAM you need to run your model smoothly? \r\n\r\nIf your answer to any of these questions was \"yes\", or you have another doubt about inference with LLMs - such as batching, or time-to-first-token - this talk is for you. Well, except for the Redis part.", "full_description": "The talk will cover the theory necessary to understand how to serve LLMs. The talk covers the math behind transformers inference in an accessible and light way. By the end of the talk, attendants will learn:\r\n\r\n1. How to count the parameters in an LLM, especially the ones in the attention layers.\r\n2. The difference between compute and memory in the context of LLM inference.\r\n3. That LLM inference is made up of two parts: prefill and decoding.\r\n4. What is an LLM server, and what features they implement to optimise GPU memory usage and reduce latency\r\n4. How batching affects your inference metrics, like time-to-first-token.\r\n\r\nThe talk will cover:\r\n\r\n**Did you pay attention?** (4 min). A short review of the attention mechanism and how to count parameters in a transformer-based model.\r\n\r\n**Get to know your params** (8 min). The math-y section of the talk, explaining how to translate parameter counts into memory and compute requirements.\r\n\r\n**Prefill and Decoding** (8 min) Explains that inference happens in two steps (prefill and decoding) and how KV-cache exploits this to make decoding faster. Common metrics to measure inference performance, like time-to-first-token and token-per-second.\r\n\r\n**Context and batch size** (5 min) Adds to the picture the sequence length, as well as the number of requests to process in parallel. Explains how LLM servers, like vLLM, use techniques like Paged Attention to optimise GPU usage\r\n\r\n**Conclusion** (5 min) Wrap up, Q&A.", "code": "G3AT7E", "state": "confirmed", "created": "2024-12-22", "speaker_names": "Luca Baggi", "track": "PyData: Generative AI"}, {"title": "Analyze data easily with duckdb - and the implications on data architectures", "abstract": "duckdb is increasingly becoming a universal tool for accessing and analyzing data. In this talk I will show with slides and live demo what duckdb is capable of and will dive deeper in how it will influence modern data architectures.", "full_description": "duckdb - a lightweight database with a focus on data analysis and a fast query engine that can be used in a variety of ways:   \r\n- Analyze data, stored on your own hard drive or somewhere on the Internet, in the browser with SQL? No problem  \r\n- Quickly check all the JSON files in S3 using SQL? Nothing could be easier  \r\n- A huge parquet file, bigger than my working memory. And now I have to analyze it locally. Easy!  \r\n- Read csv from blob storage, process and save in a Postgres database. Just one command\r\n \r\nduckdb is developing more and more into a universal tool for accessing and analyzing data.\r\n\r\nIn this talk I will show with slides and a live demo why it is so popular and why it belongs in the toolbox of every data scientist, ML engineer or data engineer. \r\n\r\nBut I will not stop at the useful tooling. I will dive deeper into the implications for data and software architectures that arise from the rise of the embedded OLAP systems like duckdb. I will especially focus on both moving the data closer to the user for faster analytics but also on accessing data without the explicit need to move it. \r\n\r\nWhat you learn and see can be used immediately in your day-to-day work.", "code": "TXKLWR", "state": "submitted", "created": "2024-12-20", "speaker_names": "Matthias Niehoff", "track": "PyData: Data Handling & Engineering"}, {"title": "AI\u2019s Hidden Price Tag: How ChatGPT Conversations Affect Our Climate", "abstract": "Did you know a single ChatGPT query produces ~4.32g of CO\u2082? Now imagine the footprint of AI at scale. How many queries are equal to a London-to-Berlin flight? And how do factors like data center choice or advanced techniques like Chain of Thought (CoT) and Reasoning multiply these effects? Join me to uncover the true environmental cost of AI, explore solutions like 1-Bit Quantization, and learn how to make innovation sustainable.", "full_description": "Did you know a single ChatGPT query equals to ~4.32g of CO\u2082? While this might seem small, the environmental footprint of AI scales rapidly when multiplied by billions of daily interactions worldwide. But what does this mean in real-world terms? How many AI queries are equivalent to a flight from London to Berlin? And how do factors like data center selection or advanced techniques like Chain of Thought (CoT) and Reasoning amplify the impact even further?\r\n\r\nIn this talk, we\u2019ll demystify the environmental costs of artificial intelligence and shed light on the resources that power its development and usage. We\u2019ll explore how energy, water, and hardware demands play a pivotal role in running data centers and how decisions like model design and deployment location can affect the sustainability of AI. With detailed calculations, real-world comparisons, and actionable insights, this session will equip attendees with the knowledge to innovate responsibly and sustainably.", "code": "YVGSX3", "state": "submitted", "created": "2024-12-29", "speaker_names": "Martin Seeler", "track": "PyData: Generative AI"}, {"title": "Unveiling the Power of Prompt Optimization in Modern LLMs", "abstract": "This talk focuses on Prompt Optimization, an important technique for improving the efficiency and performance of modern LLMs.\r\n\r\nThe session will offer a detailed exploration of how open-source tools like DSPy, TextGrad, and OPRO enable systematic prompt optimization.\r\n\r\nA live coding demonstration will highlight practical ways to utilize these frameworks. The talk would also include real-world use cases, challenges, and the future trajectory of prompt optimization in the ever-evolving AI domain.", "full_description": "1. Introduction: The Impact of Prompt Optimization in AI (3 minutes)\r\n- Defining prompts and prompt optimization and significance of prompts in guiding LLMs\r\n- Examples of how prompt optimization enhances AI outputs\r\n2. Understanding Prompt Optimization (8 minutes)\r\n- Explain why and how prompts influence model outputs\r\n- Why prompt optimisation? Challenges of training and fine-tuning large models\r\n- A bigger question of prompt optimization: Textual Gradient\r\n3. Applications of Prompt Optimization (6 minutes)\r\n- Areas where prompt optimization is making an impact:\r\n-- Automating content creation and summarization\r\n-- Efficient task-specific performance in production pipelines\r\n-- Enhancing LLM pipelines in real-world software applications\r\n- Case studies from open-source and industry use\r\n4. Deep Dive into Open-Source Frameworks (18 minutes)\r\n- Introduction to DSPy: Design, principles, and applications\r\n- Writing a DSPy program: Building modules, optimizing prompts, and interpreting results\r\n- TextGrad: Gradient-based prompt tuning for nuanced tasks\r\n- OPRO: LLMs as optimizers\r\n- Live coding session and example use case\r\n5. Research and Challenges in Prompt Optimization (5 minutes)\r\n- Current research: Methods for measuring prompt quality and efficiency\r\n- Challenges of optimizing across multilingual and multitask models\r\n- Addressing limitations in frameworks and handling edge cases\r\n6. Conclusion: The Future of Prompt Optimization (5 minutes)\r\n- How prompt optimization will continue to redefine software development workflows\r\n- Encouraging the community to contribute to open-source frameworks", "code": "MPYXL3", "state": "submitted", "created": "2024-12-10", "speaker_names": "Gautam Jajoo", "track": "PyData: Research Software Engineering"}, {"title": "A mind of metal, born of human thought.  A consciousness confined, a digital soul.", "abstract": "AI DevOps combines artificial intelligence and DevOps practices to revolutionize software development and delivery. By automating tasks, predicting failures, and optimizing resource allocation, AI DevOps significantly improves efficiency, reduces errors, and accelerates time-to-market. This talk will explore the key concepts, benefits, and challenges of AI DevOps, showcasing real-world use cases and best practices for successful implementation.", "full_description": "AI DevOps: Automating the Future of Software Development\r\n\r\nAI DevOps is a transformative approach that leverages AI to streamline and optimize the entire software development lifecycle. This talk will explore the key concepts and benefits of AI DevOps, focusing on three main areas:\r\n\r\n1. AI-Driven Automation (10 minutes):\r\n\r\n**Automated testing and debugging using machine learning\r\n**Intelligent code completion and suggestion tools\r\n**Predictive analytics for proactive maintenance\r\n\r\n2. Continuous Integration and Continuous Delivery (CI/CD) with AI (10 minutes):\r\n\r\n**AI-powered pipeline optimization and self-healing\r\n**Automated deployment and rollback strategies\r\n**Real-time monitoring and anomaly detection\r\n\r\n3. AI-Enhanced Collaboration and Decision-Making (10 minutes):\r\n\r\n**Collaborative platforms with AI-powered insights\r\n**AI-assisted decision support tools for strategic planning\r\n**Ethical considerations and responsible AI in DevOps", "code": "8JF8FY", "state": "accepted", "created": "2024-11-19", "speaker_names": "Christopher Schultz", "track": "PyCon: MLOps & DevOps"}, {"title": "Anonymization of sensitive information in financial documents using, diffusion models and NER", "abstract": "Unlock sensitive data potential with anonymization! Learn how Python, diffusion models, and Named Entity Recognition (NER) empower institutions to anonymize PII in financial documents, replacing it with synthetic stand-ins. Discover open-source, self-hosted tools to ensure privacy while unleashing data's full power.", "full_description": "Data is the fossil fuel of the machine learning world, essential for developing high quality models but in limited supply. Yet institutions handling sensitive documents \u2014 such as financial, medical, or legal records often cannot fully leverage their own data due to stringent privacy, compliance, and security requirements, making training high quality models difficult.\r\n\r\nA promising solution is to replace the personally identifiable information (PII) with realistic synthetic stand-ins, whilst leaving the rest of the document in tact.\r\n\r\nIn this talk, we will discuss the use of open source tools and models that can be self hosted to anonymize documents. We will go over the various approaches for Named Entity Recognition (NER) to identify sensitive entities and the use of diffusion models to inpaint anonymized content.", "code": "QCBP87", "state": "submitted", "created": "2024-12-20", "speaker_names": "Dr Piotr Gryko", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Pipeline-level differentiable programming for the real world", "abstract": "Automatic Differentiation (AD) is not only the backbone of modern deep learning but also a transformative tool across various domains such as control systems, materials science, weather prediction, 3D rendering, data-driven scientific discovery, and so on. Thanks to a mature ML framework ecosystem, powered by libraries like PyTorch and JAX, AD performs remarkably well at a component level; however, integrating these components into differentiable pipelines still remains a significant challenge. In this talk, we will provide an accessible introduction to (pipeline-level) AD, demonstrate some cool applications you can build with it, and see how to build differentiable pipelines that hold up in the real world.", "full_description": "The tools enabling automatic differentiation (AD), like JAX and PyTorch, are increasingly being adopted beyond machine learning to tackle optimization problems in various scientific and engineering contexts. These tools have catalyzed the development of differentiable simulators, solvers, 3D renderers, and other powerful components, under the umbrella of differentiable programming (DP).\r\n\r\nHowever, building pipelines that propagate gradients effortlessly across components introduces unique challenges. Real-world pipelines often span diverse technologies, frameworks (e.g., JAX, TensorFlow, PyTorch, Julia), computing environments (local vs. distributed clusters; CPU vs. GPU), and teams with varying expertise. Additionally, legacy systems and non-differentiable components often need to coexist with modern AD-enabled frameworks.\r\n\r\nThis talk will provide an overview of differentiable pipelines: why they matter and the types of optimization problems they address. We will revisit foundational concepts of automatic differentiation to set the stage for understanding the intricacies of orchestrating differentiable pipelines in Python.\r\n\r\nThen, using our open-source project, Tesseract, as a case study, we will share lessons learned and best practices for designing AD-friendly APIs with tools like Pydantic and FastAPI, achieving seamless integration with JAX, packaging scientific software, and enabling end-to-end systems-level optimization. \r\n\r\nAttendees will leave with practical insights on why they should care about differentiable programming, and how to overcome the challenges of building real-world differentiable pipelines.", "code": "JH97CL", "state": "submitted", "created": "2024-12-20", "speaker_names": "Alessandro Angioi", "track": "PyData: Research Software Engineering"}, {"title": "Algorithmic Music Composition With Python", "abstract": "Computers have long been an integral part of creating music. Virtual instruments and digital audio workstations make creating music easy and accessible. But how do programming languages and especially Python fit into this? Python can serve as a tool for creating musical notation\r\nand MIDI files.   \r\n\r\nThroughout the session, you\u2019ll learn how to:\r\n\r\n- Use Python to create melodies, harmonies, and rhythms.\r\n- Generate music based on rules, randomness, and mathematical principles.\r\n- Visualize and export your compositions as MIDI and sheet music.\r\n\r\nBy the end of the talk, you\u2019ll have a clear understanding of how to turn simple algorithms into expressive musical works.", "full_description": "This talk provides a general introduction into creating music algorithmically using Python. Little prior knowledge about music is assumed. It is helpful to know how sheet music looks and what the MIDI format is beforehand. \r\n\r\nWe will start by looking briefly into the basic building blocks of music (harmony, melody and rhythm) and what our goal is (creating sheet music and a playable MIDI file). \r\n\r\nThen we will discuss the history of algorithmic composition in music and from that we will develop ideas how we can create music from algorithms and randomness. \r\n\r\nFor creating sheet music we will look into the packages Abjad und music21. \r\n\r\nIn the end will we will create a playable MIDI file for our music using MIDIUtil.", "code": "TQN98D", "state": "confirmed", "created": "2024-12-20", "speaker_names": "Hendrik Niemeyer", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Geospatial Data Analysis and Visualization with Python: From OpenStreetMap to Interactive Maps", "abstract": "This tutorial will guide you through the end-to-end process of geospatial data analysis using Python. We\u2019ll start by exploring OpenStreetMap (OSM) data to extract and manipulate real-world geographic features. You\u2019ll learn how to process and analyze geospatial datasets with Python\u2019s libraries and visualize the results on interactive maps.\r\n\r\nWhether you want to map city infrastructure, analyze geographic trends, or build interactive spatial dashboards, this session will give you the tools to get started. By the end, you\u2019ll walk away with a solid foundation in geospatial programming using Python.", "full_description": "Geospatial data is at the heart of countless applications, from urban planning to environmental monitoring and logistics. Python\u2019s versatile ecosystem makes it an ideal tool for processing, analyzing, and visualizing geospatial data. In this tutorial, we will take a practical, hands-on approach to working with geospatial data using OpenStreetMap (OSM) as a primary data source.\r\n\r\nWe will begin by introducing the Python libraries that form the backbone of geospatial analysis, including osmnx, geopandas, and shapely. Using OpenStreetMap, participants will learn how to extract real-world geographic features such as road networks, points of interest, or land use data. From there, we will dive into cleaning and processing the data, focusing on manipulating geospatial datasets, handling projections, and combining data from multiple sources.\r\n\r\nNext, we will explore spatial analysis techniques, such as calculating distances, finding optimal routes, and performing geoprocessing tasks. By integrating these capabilities, attendees will uncover insights from geospatial data that can be applied to a wide variety of domains.\r\n\r\nFinally, we will visualize our results using both static and interactive maps. Static visualizations will be created with matplotlib and folium, while interactive maps will come to life using kepler.gl and ipyleaflet. Participants will learn how to build engaging visualizations that make their geospatial insights accessible and actionable.\r\n\r\nThroughout the session, we will apply these skills in a real-world case study. Together, we will analyze a city\u2019s infrastructure, mapping elements such as public transportation networks, green spaces, or bike paths. This practical example will demonstrate how geospatial data can inform decision-making and foster better understanding of our physical environment.\r\n\r\nThis tutorial is ideal for Python developers, data scientists, and GIS enthusiasts who want to get started with geospatial programming. No prior GIS knowledge is required - just a basic familiarity with Python. \r\n\r\nBy the end of the session, \r\nparticipants will have a solid foundation in geospatial data analysis and visualization, along with practical tools and workflows they can immediately apply to their own projects.", "code": "9TF7VA", "state": "rejected", "created": "2024-12-27", "speaker_names": "Martin Christen", "track": "PyData: Data Handling & Engineering"}, {"title": "The Forecast Whisperer: Secrets of Model Tuning Revealed", "abstract": "Forecasting can often feel like interpreting vague signals\u2014unclear yet full of potential. In this talk, we\u2019ll cover advanced techniques for tuning forecasting models in professional settings, moving beyond the basics to explore methods that enhance both accuracy and interpretability.\r\n\r\nYou\u2019ll learn:\r\n\r\nHow to set clear business goals for ML model tuning and align technical work with business needs, including balancing forecast granularity and accuracy and selecting statistically correct metric.\r\n\r\nPractical data preparation methods, including business-driven data cleaning and detecting data problems with statistical and buiness driven approaches.\r\n\r\nAdvanced feature selection techniques such as recursive feature elimination and SHAP values, alongside hyperparameter tuning strategies including Bayesian optimization and ensemble methods.\r\n\r\nHow generative AI can support model tuning by automating feature generation, hyperparameter search, and enhancing model explainability through SHAP and LIME techniques.\r\n\r\nReal-world case studies, including how Blue Yonder\u2019s data science team optimized demand forecasting models for retail and supply chain applications.\r\n\r\nWe'll also discuss common mistakes like overfitting and data leakage, best practices for reliable validation, and the importance of domain knowledge in successful forecasting. Whether you're a seasoned data scientist or exploring time series forecasting, you'll gain advanced insights and techniques you can apply immediately.", "full_description": "Forecasting can often feel like trying to make sense of unclear patterns\u2014difficult to interpret but rich with potential. This talk clarifies the process, focusing on actionable steps for tuning forecasting models in professional environments where accuracy and performance drive business outcomes.\r\n\r\n1. Defining Clear Business Objectives:\r\n\r\nImportance of aligning machine learning efforts with tangible business goals.\r\nScoping forecasting problems and selecting appropriate success metrics.\r\n\r\n2. Data Preparation Techniques:\r\n\r\nCleaning data with a focus on business relevance and systematically enriching it\r\nIn addition, we show how to tune the model by tuning the data nad the corresponding feature engineering. \r\n\r\n3. Feature Selection and Hyperparameter Tuning:\r\n\r\nAdvanced feature selection strategies and their impact on model performance.\r\nTechniques for identifying impactful features.\r\nBest practices for hyperparameter tuning and optimization strategies.\r\n\r\n4. The Role of interpretability and Generative AI in Model Tuning:\r\n\r\nAutomating feature generation.\r\nHyperparameter optimization techniques using generative AI.\r\nmodel tuning through model interpretation\r\n\r\n5. Real-World Applications and Case Studies:\r\n\r\nHow Blue Yonder improved retail forecast accuracy.\r\nLessons learned from industry case studies.\r\n\r\n6. Common Pitfalls and Best Practices:\r\n\r\nTypical mistakes made during model tuning.\r\n\r\nBest practices for ensuring model reliability and relevance.\r\nThe importance of domain knowledge in successful forecasting.\r\n\r\nConclusion:\r\nWhether you are a seasoned data scientist or just starting your forecasting journey, this session will provide you with actionable insights to fine-tune your forecasting models effectively. Expect practical techniques, real-world examples, and expert tips that you can apply immediately. Join us and learn how better forecasts lead to better business decisions.", "code": "YLKDJK", "state": "submitted", "created": "2025-01-03", "speaker_names": "Illia Babounikau", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "How the Python community empowered me personally and professionally.", "abstract": "Every woman has difficulties in life while choosing a career and establishing objectives. Coming from a marginalized community where women fight for respect, opportunities, and recognition, I would like to share my experiences in this talk about how attending a women's university and being exposed to tech communities like Python and PyLadies helped me find the right people who gave me the warmth and confidence to choose the right job, the right skills, and the right attitude to advance in life and achieve more.\r\n.", "full_description": "**\"Like many women, I\u2019ve faced moments of uncertainty, self-doubt, and even fear when it came to choosing a career path. But what made my journey unique was not just the challenges I faced, but how I overcame them by finding the right people, the right support systems, and the right communities that helped me build my confidence and success.\"**\r\n \r\n**2. Overcoming Challenges as a Woman from a Marginalized Community:**\r\nPersonal Struggles: Share your own background, the challenges you faced as a woman from a marginalized community, and how these struggles shaped your perspective on opportunities. Discuss how, in some environments, women are expected to fit into certain molds, often limiting their choices and aspirations.\r\n\"Growing up in a marginalized community, I had to fight for basic respect and recognition. Doors were often closed for women like me\u2014doors to opportunities, to career advancement, to being heard. The idea of \u2018success\u2019 felt distant, something only achievable by a select few, and for women like me, it seemed out of reach.\"\r\nI felt stuck between societal expectations and my desire to make a mark in the world.\"\r\n\r\n**2. Finding Empowerment Through a Women's University:**\r\nWhen I decided to attend a women\u2019s university, I was looking for a safe space where I could focus on learning and growing. What I didn\u2019t anticipate was how transformative this experience would be. At this university, I wasn\u2019t just learning theory or gaining academic knowledge. I was learning to be unapologetically myself, to pursue my passions, and to embrace leadership without fear of being judged or minimized.\"\r\n\r\nImportance of Sisterhood:\r\n\"The most powerful thing I found at the university wasn\u2019t just the curriculum\u2014it was the support from my peers. We pushed each other, celebrated each other\u2019s victories, and helped one another when the path became difficult. This sense of community and shared understanding gave me the strength to keep moving forward.\"\r\n\r\n**3. Discovering Tech Communities: Python and Pyladies:**\r\n\"As I progressed in my academic journey, I started exploring new career avenues. Technology, with its promise of innovation and problem-solving, drew me in. But being a woman in a male-dominated field felt intimidating, so I knew I had to find a supportive network to help me grow.\"\r\n***How Python and Pyladies Became Key in Your Journey:***\r\n\"Python, with its welcoming and inclusive culture, became a place where I could develop the technical skills I needed. But it was Pyladies\u2014an incredible community of women in tech\u2014that really made the difference. Being part of Pyladies gave me more than just coding skills; it gave me a sense of belonging, a community where women uplifted one another and shared knowledge without judgment.\"\r\nThe Power of Mentorship and Networking: Reflect on how interacting with other women in tech, through events, workshops, and meetups, helped you grow both personally and professionally. Share a story or example of a mentor or peer who had a significant impact on you.\r\n\r\n\"In Pyladies, I found mentors who not only taught me technical skills but also helped me understand the importance of confidence, persistence, and resilience in a competitive industry. One of the most impactful moments was when a mentor\u2014someone who had been in the industry for years\u2014told me, 'You belong here.' That simple affirmation changed everything for me.\"\r\n\r\n**4. Developing the Right Skills, Attitude, and Confidence:**\r\nBut what I didn\u2019t anticipate was the confidence I would gain, not just in my coding abilities but in my ability to tackle challenges, no matter how complex.\"\r\n\r\n\"One of the biggest lessons I learned was that failure is not something to fear but something to embrace as part of the journey. Through these communities, I learned to see challenges as opportunities to learn, and I built an attitude of perseverance and continuous self-improvement.\"\r\n\r\n**5. Finding the Right Job and Advancing in Life:**\r\n\"With the support and mentorship I received, I finally found the courage to choose a job that aligned with my passion for technology and my desire to make an impact. I wasn\u2019t just looking for any job; I wanted to find something that felt meaningful, where my contributions mattered.\"\r\nCareer Progression and Self-Advocacy: Reflect on how you began advocating for yourself more confidently in the workplace, using the skills and mindset you developed from your communities.\r\n\r\n\"When I started my career, I realized that advocating for myself was just as important as the technical work I was doing. Thanks to my experience in these communities, I understood the importance of speaking up, setting boundaries, and pursuing opportunities that aligned with my goals.\"\r\nConclusion:\r\n\r\nA Message of Empowerment: Close by sharing a message of empowerment to the women in the audience. Remind them that no matter where they come from or what challenges they face, they have the ability to succeed, especially when they find the right communities and the right support.\r\n\r\n\"To every woman out there facing uncertainty or feeling limited by circumstances\u2014know that you are not alone. There are communities out there that will embrace you, uplift you, and empower you. Find them, lean on them, and use them as the foundation to build the career and life you deserve. Together, we can achieve so much more than we ever could alone.\"\r\nEnd with a Call to Action: Invite the audience to take action by joining communities, seeking mentorship, and supporting one another.", "code": "7ZMVBE", "state": "submitted", "created": "2024-12-22", "speaker_names": "Niharika Vadluri", "track": "General: Community & Diversity"}, {"title": "Security for Devs", "abstract": "Two truths and a lie:\r\n\r\nYou have been hacked and you don\u2019t know yet\r\nYou haven\u2019t been hacked because it\u2019s not yet convenient (for the attackers)\r\nYou think your application is secure\r\n\r\nSecurity is hard. Hard to measure and is always a catch-up game.\r\n\r\nJoin this talk if you are interested in understanding a bit more about modern security and how it goes way beyond the code of your app.", "full_description": "Usually devs don\u2019t care much about security, and that has probably different reasons:\r\n\r\nFocus on security itself doesn\u2019t pay the bills\r\nNot always clear what being secured means\r\nSecurity =! my code is properly tested\r\nOften comes as an annoying byproduct of ISO27001/SOC2\r\n\r\nBut attacks and data breaches are becoming way more common, and DORA, NIS2 and the law obligation to publicly share data breaches are trying to bring the companies\u2019 attention (and budget) back to this topic.\r\n\r\nLeaving the legalise aside, during this talk we will cover these topics\r\n\r\nSome real examples\r\nYour code\r\nCI/CD\r\nYour app\r\nYour production environment\r\nFramework to assess your security posture\r\nWhere to Start", "code": "QGLS8H", "state": "accepted", "created": "2024-12-22", "speaker_names": "Christian Barra", "track": "PyCon: Security"}, {"title": "Explainability for Gradient Boosting: SHAP and EBM", "abstract": "Imagine you\u2019ve just developed a **credit rating** model for a bank. The backtesting metrics look fantastic, and you get the green light to go live. But then, the first loan applications start rolling in, and suddenly your model\u2019s decisions are under scrutiny.\r\n\r\n**Why** was this application rejected? Why was that one approved? \r\n\r\nYou must have answers\u2014and fast.\r\n\r\nThis session has you covered.\r\n\r\nWe\u2019ll dive into **SHAP** (SHapley Additive exPlanations) and **EBM** (Explainable Boosting Machine), two widely used methods for interpreting tree-based ensemble models like **XGBoost**. You\u2019ll learn about their theory, strengths, and limitations, as well as see them in action with **Python examples** using the `shap` and `interpret-ml` libraries.\r\n\r\nFrom understanding feature contributions to hands-on coding, this talk will equip you with practical tools to make complex models transparent, understandable, and ready for critical applications.", "full_description": "**XGBoost** is considered a state-of-the-art model for regression, classification, and learning-to-rank problems on tabular data. Unfortunately, tree-based ensemble models are notoriously **difficult to explain**, limiting their application in critical fields. Techniques like SHapley Additive exPlanations (SHAP) and Explainable Boosting Machine (EBM) have become common methods for assessing how much each feature contributes to the model prediction.\r\n\r\nThis talk will introduce SHAP and EBM, **explaining the theory** behind their mechanisms in an accessible way and **discussing the pros and cons** of both techniques. We will also comment on Python snippets where SHAP and EBM are used to explain a gradient boosting model.\r\n\r\nAttendees will walk away with an understanding of how SHAP and EBM work, the limitations and merits of both techniques, and a tutorial on how to use these methods in Python, courtesy of the [shap](https://shap.readthedocs.io/en/latest/) and [interpret-ml](https://interpret.ml/docs/ebm.html) packages.\r\n\r\nTalk outline:\r\n\r\n- A brief reminder about gradient boosting and XGBoost (5 mins)\r\n- The challenge of explainability (5 mins)\r\n- EBM: theory and applications (10 mins)\r\n- SHAP: theory and applications (10 mins", "code": "TJ7XGF", "state": "submitted", "created": "2024-12-22", "speaker_names": "Emanuele Fabbiani", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Simplifying Django: A Beginner-Friendly Take on the Official Tutorial", "abstract": "# Inspiration\r\n\r\nFrom a Reddit post Django's official tutorial is not for beginners, and personal experience from referring my mentees to the Django docs, I believe I made the official tutorial on Django easier.\r\n\r\nhttps://www.reddit.com/r/django/comments/1gtlitn/djangos_official_tutorial_is_not_for_beginners/?share_id=nY_DWKddmV8WFA9YZ4aFR&utm_content=1&utm_medium=ios_app&utm_name=iossmf&utm_source=share&utm_term=22", "full_description": "This tutorial reimagines it for absolute beginners, breaking down complex concepts into manageable steps with hands-on examples. If terms like \u201cORM\u201d or \u201ctemplate inheritance\u201d make your head spin, this is the session for you. It's also for experts who are interested in making sure the official docs is easily understood by developers from all levels of experience.\r\n\r\nFeeling lost in Django\u2019s official tutorial? You\u2019re not alone. This hands-on session simplifies the process, guiding absolute beginners through creating a fully functional web app. I\u2019ll break down complex concepts, build step-by-step, and even deploy the final app to a VPS. Perfect for Python beginners ready to dive into web development!", "code": "3ZCPJH", "state": "submitted", "created": "2024-12-30", "speaker_names": "Chris Achinga", "track": "PyCon: Django & Web"}, {"title": "Observing Python applications with OpenTelemetry", "abstract": "Discover the options we have to observe with OpenTelemetry our Python applications running inside a Kubernetes cluster (or not) without touching the application code. We'll evaluate the differences between the two solutions highlighting their strong and weak spots.", "full_description": "In this talk we'll see the options we have to add observability to Python applications with OpenTelemetry without touching our application code. In OpenTelemetry this is called autoinstrumentation or zero-code instrumentation.\r\nWe'll see:\r\n- The OpenTelemetry operator, a Kubernetes operator to manage collection and inject autoinstrumentation to the pods\r\n- OpenTelemetry Python own autoinstrumentation solution based on the opentelemetry-instrument wrapper\r\n\r\nWe'll evaluate the differences between the two solutions highlighting their strong and weak spots.\r\n\r\nWe are also taking a look on the work done inside the OpenTelemetry project around GenAI.", "code": "QHSXAJ", "state": "submitted", "created": "2024-12-06", "speaker_names": "Riccardo Magliocchetti", "track": "PyCon: MLOps & DevOps"}, {"title": "Filling in the Gaps: When Terraform Falls Short, Python and Typer Step In", "abstract": "Not all resources in today\u2019s cloud environments have native Terraform providers. That\u2019s where Python\u2019s Typer library can step in, offering a flexible, production-ready command-line interface (CLI) framework to help fill in the gaps. In this session, we\u2019ll explore how to integrate Typer with Terraform to manage resources that fall outside Terraform\u2019s direct purview. We\u2019ll share a real-life example of how Typer was used alongside Terraform to automate and streamline the management of an otherwise unsupported API. You\u2019ll learn how Terraform can invoke Python scripts\u2014passing arguments and parameters to control complex operations\u2014while still benefiting from Terraform\u2019s declarative model and lifecycle management. We\u2019ll also discuss best practices for defining resource lifecycles to ensure easy maintainability and consistency across deployments. By the end, participants will see how combining Terraform\u2019s robust infrastructure-as-code approach with Python\u2019s versatility and Typer\u2019s user-friendly CLI can create a powerful, cohesive strategy for managing even the trickiest resources in production environments.", "full_description": "In this session, we\u2019ll address a common challenge in managing resources and APIs that lack native Terraform providers but still need to integrate seamlessly into your CI/CD pipeline. I\u2019ll demonstrate how Python\u2019s Typer library can help bridge this gap by offering a straightforward yet powerful command-line interface (CLI). I\u2019ll explain how to create and configure Typer applications, pass parameters, and integrate these scripts with Terraform. \r\n\r\n1. Problem Statement (Managing APIs or resources with incomplete Terraform provider support) - 5 mins\r\n2. Typer (Key components, advantages, and how to use in production enviroment) - 10 mins\r\n3. Terraform resources that can execute CLI and how to work with them - 10 mins\r\n4. Conclusion - 2 mins", "code": "CZXBEP", "state": "submitted", "created": "2024-12-22", "speaker_names": "Yuliia Barabash", "track": "General: Infrastructure - Hardware & Cloud"}, {"title": "Oh my license! \u2013 Achieving order by automation in the license chaos of your dependencies", "abstract": "License issues can haunt you at night.\r\nYou spend days, weeks, and months developing beautiful software.\r\nBut then it happens.\r\nYou realize that an essential dependency is GPL-3.0 licensed.\r\n\r\nAll your code is now infected with this license.\r\nNow you are forced to either:\r\n1. Rewrite all parts relying on the other library\r\n2. Open-source your codebase under the GPL-3.0 license\r\n\r\nHow could this have been avoided?\r\n\r\nJoin the talk and find out!\r\nFirst, we\u2019ll give you a brief introduction to different software licenses and their implications.\r\nSecond, we\u2019ll show you how to automate your license checking using open-source software.", "full_description": "Software licensing can feel like a daunting maze, but it doesn\u2019t have to be.\r\nThis talk will demystify the world of software licenses and equip you with the critical knowledge to navigate it with confidence.\r\n\r\nWe\u2019ll start by exploring key categories of licenses\u2014like Strong Copyleft, Weak Copyleft, and Permissive\u2014and break down the most common ones you\u2019ll encounter (e.g., GPL, AGPL, BSD, and MIT). Through concrete examples, you\u2019ll learn how these licenses affect your projects and how to handle them effectively.\r\n\r\nNext, we\u2019ll dive into practical solutions for automating license compliance. You\u2019ll be introduced to conda-deny (an open-source tool) and see how it can help ensure your projects remain compliant without adding manual overhead.\r\n\r\nWhether you\u2019re building open-source software or proprietary tools, this talk will leave you with actionable strategies to future-proof your projects and avoid licensing pitfalls.", "code": "ME7XPJ", "state": "submitted", "created": "2024-12-21", "speaker_names": "Paul M\u00fcller", "track": "PyCon: Programming & Software Engineering"}, {"title": "The Evolution of Data Lake Storage: A Technical Review of Delta Lake, Apache Iceberg and Apache Hudi", "abstract": "In this session, we\u2019ll explore the three leading modern data lake storage formats: Delta Lake, Apache Iceberg, and Apache Hudi. Each of these formats brings unique capabilities to address the limitations of traditional data lakes. This talk will dive into their key features, compare their strengths, and provide practical guidance on when to choose each format based on specific data engineering needs like real-time processing, batch pipelines, or large-scale analytics.", "full_description": "In this talk, we will explore the evolution of data lakes and the challenges they present, such as lack of transactional consistency and schema management. We\u2019ll dive into three modern storage formats\u2014Delta Lake, Apache Iceberg, and Apache Hudi\u2014that address these issues. Each format will be introduced with an overview of its key features, including ACID transactions, schema evolution, and use cases. We\u2019ll also compare when to use each format based on different data engineering needs. The session will conclude with key takeaways and a Q&A to clarify specific questions.", "code": "R7PBKH", "state": "submitted", "created": "2025-01-01", "speaker_names": "Sumakshi Chauhan", "track": "PyData: Data Handling & Engineering"}, {"title": "Use Instructor for structured, robust and easy testable LLM interactions", "abstract": "How do you get structured, robust, easily testable outputs from an LLM? By using an (overly) complex prompt? Here is the better alternative: Instructor, a library for generating structured and robust outputs from your LLM.\r\n\r\nIn this talk, we will explore the capabilities and advantages of the Instructor library based on a real-world example using the OpenAI API. This talk aims to provide you with an overview of Instructor's useful tools to receive reliable, structured, robust, and easily testable LLM outputs. We will explore three tools of Instructor:\r\n1. We will see how to use validators for evaluating the output of an LLM. With these validators, we can reprompt your model to encourage it to provide the correct output.\r\n2. We will dive into the function calling capabilities that come with the Instructor library. With function calling, you ensure a structured LLM output using a Pydantic model.\r\n3. We will explore Instructor's pre- and post-exectution hooks to evaluate the LLM's output.\r\n\r\nAfter this talk, you will have an overview of the possibilities provided by Instructor to generate structured, robust, and easily testable outputs from your LLM.", "full_description": "Many LLM use cases in the \"real world\" require structured outputs like JSON, for example, extracting information from multiple sources. This is especially important for continuously testing and evaluating your LLM's output, before and during production. Unfortunately, the default output of LLMs is typically unstructured.\r\n\r\nTo receive structured responses, the library 'Instructor' offers tools to ensure structured outputs from your LLM and to evaluate them. In this talk, we will explore the Instructor library based on Pydantic and the OpenAI API for obtaining structured LLM outputs that are easily testable and improve the debugging handling of our LLM application. We will explore the advantages and capabilities of Instructor in a real scenario: using an OpenAI GPT model to extract and summarize data from a ticket system. Based on this use case, we will learn about three main tools of Instructor: \r\n\r\n1. Function calling: OpenAI's function calling allows LLMs to use user-defined functions to solve a task and thereby create a specific, structured output. In instructor, OpenAI's function calling is combined with Pydantic. Instructor makes it easy to use function calling. Explore Instructors features to simplify function calling, and the related advantages in creating robust, structured LLM outputs. \r\n\r\n2. Validators: Validators are provided by Instructor to evaluate the output of an LLM and reprompt it if the evaluation fails. We will see how to create validators and examine specific use cases related to the ticket system. Furthermore, we will explore different types of validators and how they support us in creating a testable LLM system.\r\n\r\n3. Hooks: Instructor provides pre- and post-execution hooks that are triggered at specific events. We will dive into the concept of pre- and post-execution hooks and explore their capabilities for evaluating the LLM's output. We will see how to implement these hooks for transparency of the LLM's output, ensuring easy logging and debugging.\r\n\r\nAfter this talk, you will have an overview of Instructor's tools, how they can help you let your LLM generate structured outputs, and how you can use Instructor for testing and evaluating your LLM's output.", "code": "VSC9NC", "state": "submitted", "created": "2024-12-22", "speaker_names": "Felizia Quetscher", "track": "PyData: Generative AI"}, {"title": "Oh, no! Users love my GenAI-Prototype and want to use it more.", "abstract": "Demos and prototypes for generative AI (GenAI) projects can be quickly created with tools like Streamlit, offering impressive results for users within hours. However, scaling these solutions from prototypes to robust systems introduces significant challenges. As user demand grows, hacks and workarounds in tools like Streamlit lead to unreliability and debugging frustrations. This talk explores the journey of overcoming these obstacles, evolving to a stable tech stack with Qdrant, Postgres, Litellm, FastAPI, and Streamlit. Aimed at beginners in GenAI, it highlights key lessons.", "full_description": "Demos and prototypes for projects with generative AI can be quickly put together: an API key from the preferred model provider, some source code from the online tutorial and a few small adjustments suffice. Thanks to Streamlit and the like, even beginners can achieve impressive results that can be used by users within a few hours.\r\n\r\nBut what happens when users actually like the solution? When demos and prototypes need to be expanded and connected to other systems? What if the number of users continues to rise?\r\n\r\nIt is quite impressive how far you can bend Streamlit to achieve things it was probably never meant for. But at a certain point, you pay for the hacks and workarounds with unreliability and frustrating debugging.\r\n\r\nThe speakers repeatedly reached this point in various projects and delayed the necessary architecture discussion for too long. So the path was longer and more painful than it should have been \u2013 but in the end, thanks to the wide range of open-source (Python) projects, a flexible and stable system was created. Our current tech stack includes Qdrant, Postgress, Litellm and FastAPI \u2013 as well as OpenWebUI, and of course Streamlit. \r\n\r\nThanks to modularization, we now have a stable system that we can easily run locally but also deploy in an enterprise environment. Nevertheless, we have retained a great deal of flexibility.\r\n\r\nIn our talk, we report on the trials and tribulations along the way. We report on the challenges that led to decisions for various components. We disclose which problems we were able to solve and which new problems arose.\r\n\r\nThe talk is aimed primarily at those who are taking their first steps with generative AI or have already developed their first demonstrators or prototypes. \r\n\r\nStructure:\r\n\r\n(1) GenAI applications in Streamlit are cool\r\n(2) The challenges on the way from prototype to productive deployment\r\n(3) Ramming heads through walls\r\n(4) The path to a flexible but stable stack\r\n(5) What still plagues us", "code": "UXTCZC", "state": "submitted", "created": "2025-01-02", "speaker_names": "Thomas Prexl", "track": "PyCon: MLOps & DevOps"}, {"title": "GraphRAG Unleashed: Building Knowledge-Driven GenAI with Python", "abstract": "Unlock the potential of GraphRAG by combining structured Knowledge Graphs and unstructured data for smarter generative workflows. Learn how to use a Python package to build hybrid retrieval systems powering intelligent search, Q&A, and more. Perfect for Python devs exploring GenAI\u2019s next frontier!", "full_description": "Generative AI reaches its full potential when it bridges the gap between structured knowledge in graphs and unstructured data in text. This session dives into the power of **GraphRAG**, a cutting-edge approach to Retrieval-Augmented Generation (RAG) that combines these two worlds to create smarter, more context-aware generative workflows.  \r\n\r\nDiscover how to use a Python package purpose-built for GraphRAG to build hybrid retrieval systems that integrate structured and unstructured knowledge seamlessly. With practical examples and hands-on insights, learn how to power applications like intelligent search, Q&A systems, and personalized recommendations with knowledge-driven generative AI.  \r\n\r\nIf you're a Python developer eager to innovate at the intersection of Knowledge Graphs and GenAI, this session is your gateway to unlocking new possibilities.", "code": "BRUFBB", "state": "submitted", "created": "2024-12-04", "speaker_names": "Siddhant Agarwal", "track": "PyData: Generative AI"}, {"title": "Mastering Ensemble Methods and Hyperparameter Tuning with Python", "abstract": "This talk will explore how one can supercharge their machine learning models with ensemble methods and hyperparameter tuning. Learning hands-on techniques for implementing Random Forest, Bagging, XGBoost, LightGBM, and CatBoost using Python. Discovering strategies for optimal hyperparameter tuning and improve your model's performance.", "full_description": "In this talk, attendees will learn how to supercharge their machine-learning models with ensemble methods like Random Forest, Bagging, and Boosting (Xgboost, LightGBM, CatBoost). Diving into hands-on implementations of these techniques and discover hyperparameter tuning strategies for optimal model performance.\r\n\r\nBy the end of this talk, participants will have a solid foundation in ensemble methods, hyperparameter tuning techniques, and hands-on experience in implementing these concepts using Python. They will be equipped with the skills to immediately apply these techniques to their projects and datasets, enhancing their ability to create high-performing machine-learning models.\r\n\r\nTalk Outline\r\n\r\n1. Introduction to Ensemble Methods (5 minutes)\r\n    - Overview of Random Forest, Bagging, XGBoost, LightGBM, and CatBoost\r\n    - How ensembles improve model accuracy\r\n2. Hands-on Implementation (15 minutes)\r\n    - Step-by-step guide to implementing ensemble methods using Python and scikit-learn\r\n    - Practical exercise: Building, training, and evaluating models on a real dataset\r\n3. Hyperparameter Tuning Fundamentals (10 minutes)\r\n    - Importance of hyperparameters in model optimization\r\n    - Introduction to tuning techniques for enhanced generalization\r\n4. Hands-on Hyperparameter Tuning (10 minutes)\r\n    - Guided exercise using GridSearchCV and RandomizedSearchCV tools\r\n    - Tips for efficient hyperparameter exploration\r\n\r\nKey Takeaways \r\n1. Understand the fundamentals of ensemble methods and their role in improving model accuracy\r\n2. Gain hands-on experience implementing ensemble techniques using Python and scikit-learn\r\n3. Learn effective strategies for hyperparameter tuning to maximize model performance\r\n4. Apply these techniques to real-world datasets and evaluate model improvements", "code": "UHXBYY", "state": "submitted", "created": "2024-12-22", "speaker_names": "Brayan Kai Mwanyumba", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Quiet on Set: Building an On-Air Sign with Open Source Technologies", "abstract": "Learn how to build a custom On-Air sign using Apache Kafka\u00ae, Apache Flink\u00ae, and Apache Iceberg\u2122! See how to capture events like Zoom meetings and camera usage with Python, process data with FlinkSQL, analyze trends using Iceberg, and bring it all together with a practical IoT project that easily scales out.", "full_description": "While many of us have adapted to work from home life, one major problem remains: finding an easy way to keep folks in your home away from your workspace when you\u2019re on an important call. Dust off your Raspberry Pi\u2013\u2013let\u2019s build a custom on-air sign with Apache Kafka\u00ae, Apache Flink\u00ae, and Apache Iceberg\u2122!\r\n\r\nWe\u2019ll begin by writing Python scripts to capture key events\u2013\u2013such as when a Zoom meeting is running and when a camera is being used\u2013\u2013and produce it into Kafka. The live data are then consumed by a Raspberry Pi script to drive the operation of a custom designed on-air sign. From there, you\u2019ll be introduced to the ins and outs of FlinkSQL for stream processing as we wrangle the data into a better format for downstream use. And, finally, we\u2019ll see Iceberg in action and learn how to use query engines to analyze meeting and recording trends.\r\n\r\nBy the end of the session, you\u2019ll be well-acquainted with this powerful trio of open source technologies and know how you could use the same scaffolding and scale out a simple, at-home project to millions of users and simultaneous events.", "code": "P9GRZU", "state": "submitted", "created": "2024-12-10", "speaker_names": "Danica Fine", "track": "General: Infrastructure - Hardware & Cloud"}, {"title": "The earth is no longer flat - introducing support for spherical geometries in Spherely and GeoPandas", "abstract": "The geometries in GeoPandas, using the Shapely library, are assumed to be in projected coordinates on a flat plane. While this approximation is often just fine, for global data this runs into its limitations. This presentation introduces spherely, a Python library for working with vector geometries on the sphere, and its integration into GeoPandas.", "full_description": "Not all geospatial data are best represented using a projected coordinate system. Unfortunately, the Python geospatial ecosystem is almost fully based on planar geometries using Shapely, and is still lacking a general purpose library for efficient manipulation of geometric objects on the sphere. We introduce Spherely: a new Python library that fills this gap, aiming to provide a similar API as Shapely, but then gor geometries on the sphere.\r\n\r\nSpherely provides Python/Numpy vectorized bindings to S2Geometry, a mature and performant C++ library for spherical geometry that is widely used for indexing and processing geographic data, notably in popular database systems. This is done via S2Geography, a C++ library that has emerged from the R-spatial ecosystem and that provides a GEOS-like compatibility layer on top of S2Geometry. Unlike S2Geometry\u2019s SWIG wrappers or S2Sphere (pure-Python implementation), Spherely exposes its functionality via \u201cuniversal\u201d functions operating on n-dimensional Numpy arrays, therefore greatly reducing the overhead of the Python interpreter.\r\n\r\nComplementary to Shapely 2.0, Spherely may be used as a backend geometry engine for Python geospatial libraries like GeoPandas, hence extending their functionality to more robust and accurate manipulation of geographic data (i.e., using longitude and latitude coordinates).\r\n\r\nThis presentation introduces spherely and its capabilities to work with vector geometries on the sphere, and its integration into GeoPandas.\r\n\r\nCode repository: https://github.com/benbovy/spherely", "code": "AWPYGE", "state": "submitted", "created": "2025-01-05", "speaker_names": "Joris Van den Bossche", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Developers and the AI Revolution", "abstract": "As Artificial Intelligence rapidly transforms the technology landscape, leveraging the power of machine learning has become essential for developers. In this talk, Alejandro delves into how the AI revolution is impacting developers and the critical role they play in bringing AI-powered products into production.", "full_description": "As Artificial Intelligence rapidly transforms the technology landscape, leveraging the power of machine learning has become essential for developers. In this talk, Alejandro delves into how the AI revolution is impacting developers and the critical role they play in bringing AI-powered products into production. We will explore the opportunities that Machine Learning is bringing into AI-powered products, as well as the challenges of deploying, scaling, and maintaining these systems in real-world applications such as privacy, security and risk considerations. We will be covering a deep dive of the production ML tooling ecosystem and dive into best practices that have been abstracted from production use-cases of machine learning operations at scale, as well as how to leverage tools to that will allow us to deploy, explain, secure and monitor AI-powered products at scale.", "code": "J8BMMC", "state": "submitted", "created": "2024-12-23", "speaker_names": "Alejandro Saucedo", "track": "PyCon: MLOps & DevOps"}, {"title": "Data can be so meta: Introduction to Data Catalogs Using the Example of DataHub", "abstract": "In today's expansive data landscapes, navigating the vast array of information has become increasingly challenging. Questions such as, \"Where can I locate data on our customers' shopping habits?\" or \"Who is the point of contact for our product descriptions?\" frequently arise for data practitioners. And it's not just data scientists and analysts\u2014 e.g., business stakeholders often seek clear definitions of KPIs. Furthermore, with recent regulations like the EU AI & Data acts, it's essential to have a robust understanding of your data's origins and transformations.\r\n\r\nData catalogs emerge as essential tools in this complex landscape, specifically designed to address these challenges. This talk will guide you through the world of data catalogs, detailing what they are and the critical role they play in data management. We'll get an overview of the various tools available in the market and showcase the key features of data catalogs using DataHub as an illustrative example.", "full_description": "In today's expansive data landscapes, navigating the vast array of information has become increasingly challenging. Questions such as, \"Where can I locate data on our customers' shopping habits?\" or \"Who is the point of contact for our product descriptions?\" frequently arise for data practitioners. And it's not just data scientists and analysts\u2014 e.g., business stakeholders often seek clear definitions of KPIs. Furthermore, with recent regulations like the EU AI & Data acts, it's essential to have a robust understanding of your data's origins and transformations.\r\n\r\nData catalogs emerge as essential tools in this complex landscape, specifically designed to address these challenges. This talk will guide you through the world of data catalogs, detailing what they are and the critical role they play in data management.\r\nWe will begin by clarifying the various uses of the term \"catalog,\" exploring its presence in different contexts.  You'll see how data catalogs are instrumental not only in organizing and tracking data assets but also in fostering collaboration and enhancing data governance across an organization.\r\nHaving laid the groundwork, we will also take a brief look at the role of data catalogs in the era of AI. With AI technologies increasingly relying on vast amounts of data, data catalogs ensure data quality, traceability, and compliance, which are crucial for generating reliable AI insights and supporting ethical AI development.\r\nFurthermore, we'll provide an overview of the diverse array of tools available in the market today. Using DataHub as an illustrative example, we'll showcase the key features that make data catalogs so valuable for modern data management. Through this demonstration, you'll gain valuable insights into how data catalogs operate and how they can be leveraged to enhance your organization's data strategy. Whether you're a seasoned data practitioner or a business stakeholder looking to unlock the full potential of your data, this talk promises to equip you with the knowledge to navigate the complexities of today's data environment effectively.", "code": "F97FL3", "state": "submitted", "created": "2024-12-09", "speaker_names": "Tim Bossenmaier", "track": "PyData: Data Handling & Engineering"}, {"title": "Coding for Good: Achieving social change with an app", "abstract": "Chris and his husband quickly developed an app ten years ago to allow ordinary people to lobby the normally inaccessible and unelected House of Lords to pass the Same Sex Marriage Bill. It unexpectedly took off, helping the Bill overwhelmingly succeed in the normally-socially-conservative upper House of Parliament in the UK. Here's how they did it, and how coders can and should use their enviable skills to help change the world.", "full_description": "As coders, we have an enviable ability to create platforms and apps that bring people together. The world is not short of challenges right now, especially climate change, but we will only have a real impact on the world if we can help people unite and organise to pressure those with power and influence.\r\n\r\nChris and his husband quickly developed an app ten years ago to allow ordinary people to lobby the normally inaccessible and unelected House of Lords to pass the Same Sex Marriage Bill. It unexpectedly took off, with 15,000 emails sent in the space of two weeks, helping the Bill overwhelmingly succeed in the normally-socially-conservative upper House of Parliament in the UK.\r\n\r\nHe'll talk here about how he did it, and how you - an individual with a computer and knowledge of how to knock an app together - can use this skill to give a voice to the voiceless, and to potentially change or even save the world.", "code": "PJPUCL", "state": "submitted", "created": "2025-01-05", "speaker_names": "Chris Ward", "track": "General: Community & Diversity"}, {"title": "Building core data infrastructure in Go & Python", "abstract": "Building core data infrastructure demands a combination of performance, scalability, and adaptability. While Python is still the lingua franca of data science and analytics, there are a lot of benefits Go brings to the table, particularly around performance, concurrency and ease of deployment.\r\n\r\nIn this talk, we'll go through our journey in building core data infrastructure using Go and Python, and dig into the interoperability between the two languages. We'll also discuss how we've leveraged Go's performance and concurrency features to build scalable and reliable open-source data tooling.", "full_description": "In this talk, we will dive into the technical journey of building a CLI application using Golang as the foundation. We'll start by exploring the motivations behind choosing Go for this task, focusing on its performance, concurrency capabilities, and ease of deployment. \r\n\r\nNext, we'll delve into the specific challenges we encountered during the development process, such as managing dependencies, handling concurrency, and ensuring cross-platform compatibility. We'll share insights on how we addressed these challenges and the lessons learned along the way.\r\n\r\nThe core of the discussion will center around the interoperability between Go and Python. We'll provide a detailed explanation of how we integrated Python libraries into the Golang CLI, including the use of cgo, embedding Python interpreters and other tools to bridge the gap between the two languages. We'll also discuss the pros and cons of various approaches, highlighting the performance trade-offs and the complexities involved in maintaining such a hybrid system.\r\n\r\nWe'll talk about the implications of such an approach on a real-world open-source project, and demonstrate how it leverages the strengths of both Go and Python to deliver a powerful and scalable solution for data infrastructure. We'll provide examples and use cases to illustrate its capabilities and invite the audience to contribute to its development.\r\n\r\nJoin us for a technical deep dive into the world of Go and Python interoperability, and learn how to harness the best of both languages to build robust and efficient data tools.", "code": "ZF3L97", "state": "submitted", "created": "2025-01-02", "speaker_names": "Burak Karakan", "track": "PyData: Data Handling & Engineering"}, {"title": "Feature or Preprocessing Step? How to Correctly Set a Baseline in NLP Classification Tasks", "abstract": "In the field of Natural Language Processing (NLP), establishing a robust baseline is critical for the reliable evaluation and comparison of models. However, many NLP studies suffer from sloppy baseline models and improper preprocessing techniques, leading to flawed results. This presentation addresses key issues in setting effective baselines, focusing on the common pitfalls in research, such as the misuse of preprocessing steps and the confusion between preprocessing and feature engineering. The session will define the roles of preprocessing and features, provide a step-by-step guide for setting baselines with traditional machine learning models, and discuss the importance of hyperparameter tuning. We will also explore the fair comparison of traditional and advanced models, offering insights into appropriate preprocessing techniques for both. By the end, attendees will understand best practices for preprocessing, feature engineering, and baseline setting, enabling them to conduct more reliable and comparable NLP research. This talk is aimed at researchers, data scientists, and practitioners in NLP and machine learning looking to enhance their methodological rigor.", "full_description": "OBJECTIVES:\r\n\r\n- Identify Common Issues in Baseline Models: Highlight the prevalent issues found in numerous NLP papers, such as sloppy baseline models and the misuse of preprocessing techniques across different model types.\r\n- Distinguish Between Preprocessing and Feature Engineering: Define what constitutes a preprocessing step versus a feature, and explain their roles in setting a baseline.\r\n- Establishing a Robust Baseline: Provide a step-by-step guide on how to correctly set a baseline for traditional ML models and compare it to more sophisticated models.\r\n\r\n\r\nOUTLINE:\r\n\r\n1. Introduction\r\n- Importance of a robust baseline in NLP classification tasks.\r\n- Overview of common pitfalls in current research.\r\n\r\n2. Identifying Sloppy Baselines\r\n- Examples of inadequate preprocessing for traditional ML models.\r\n- Consequences of improper preprocessing: deleting or generating features, inappropriate tokenisation, and not leveraging Scikit-learn capabilities.\r\n\r\n3. Distinguishing Preprocessing Steps from Features\r\n- Acceptable preprocessing steps for a baseline model: lower-casing, appropriate tokenisation, deleting single-appearance tokens.\r\n- The necessity of experimenting with preprocessing techniques.\r\n\r\n4. Setting a Baseline: A Detailed Approach\r\n- Data splitting: training, validation, and test sets.\r\n- Baseline with traditional ML models: using Naive Bayes with basic preprocessing.\r\n- The importance of hyperparameter tuning for vectorisers.\r\n\r\n5. Algorithm Selection and Optimisation\r\n- Pipeline for algorithm selection and feature engineering.\r\n- Examples of preprocessing and feature engineering techniques: normalisation, handling noise, replacing slang, stemming, etc.\r\n\r\n6. Comparing Traditional and Advanced Models\r\n - Fair comparison to advanced models (neural networks, transformers).\r\n - Appropriate preprocessing for advanced models: minimal preprocessing (i.e. only lowercasing and deleting noise), avoiding lemmatization/stemming, appropriate tokenisation.\r\n\r\n7. Tips and Best Practices for Advanced Models\r\n- Handling unknown words and dummy tokens.\r\n- Dealing with non-generalisable information (emails, paths, names).\r\n- Enhancing embeddings with additional data.\r\n\r\n8. Conclusion\r\n- Recap of best practices for setting a robust baseline.\r\n- Encouragement to adopt these practices for more reliable and comparable NLP research outcomes.\r\n\r\n\r\nEXPECTED OUTCOMES:\r\n\r\nAttendees will gain a comprehensive understanding of the importance of setting a robust baseline in NLP classification tasks. They will learn practical techniques to improve their preprocessing steps and distinguish between preprocessing and feature engineering. This knowledge will help them conduct more reliable experiments and produce more credible results in their research.\r\n\r\n\r\nTARGET AUDIENCE:\r\n\r\nResearchers, data scientists, and practitioners in the field of NLP and machine learning who are involved in model development and evaluation. This talk is especially relevant for those looking to improve their methodological rigor in establishing baselines and preprocessing practices.", "code": "UVSVCY", "state": "submitted", "created": "2024-11-28", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Offline Disaster Relief Coordination with OpenStreetMap and FastAPI", "abstract": "In natural disaster scenarios, reliable communication is crucial. This talk presents a solution for disaster relief coordination using OpenStreetMap vector maps hosted on a local device in the emergency vehicle with FastAPI, ensuring functionality without an internet connection. By integrating a database of post codes and street names, and leveraging a LORAWAN gateway to receive positional data and water levels, this system ensures access to critical information even in blackout situations.", "full_description": "In natural disaster scenarios, effective coordination of relief efforts is essential, especially when traditional communication networks are compromised or overloaded. This involves organizing various aspects of disaster response without internet connectivity, such as dike defense, sandbag logistics, power supply, distribution of relief goods, clearing and repairing roadways, searching for missing persons, securing structurally unstable buildings and salvaging property. Typically, emergency responders are mobilized from different regions and may not be familiar with the affected area. Since it is unlikely that responders will have pre-downloaded offline maps of the target region on their devices, a vehicle-hosted Wi-Fi hotspot providing nationwide maps would be invaluable. This presentation introduces a practical solution for offline disaster relief coordination using OpenStreetMap vector maps hosted on a local device in the emergency vehicle with FastAPI, allowing responders to use their existing devices to access critical geographical information and enhancing the efficiency and effectiveness of disaster relief operations.\r\n\r\nThe solution includes:\r\n\r\nFastAPI Server Setup: Configuring and deploying a FastAPI server to host OpenStreetMap vector maps offline on a Raspberry Pi, which also offers a WiFi hotspot for existing end devices, ensuring accessibility without an internet connection.\r\nDatabase Integration: Integrating a comprehensive database of post codes and street names to facilitate quick and accurate location searches.\r\nLORAWAN Gateway Integration: Implementing a LORAWAN gateway to receive real-time positional data and water level measurements, providing up-to-date situational awareness for disaster relief workers.", "code": "GUEAHT", "state": "submitted", "created": "2024-12-10", "speaker_names": "Jannis L\u00fcbbe", "track": "General: Infrastructure - Hardware & Cloud"}, {"title": "Why AI Projects Fail \u2013 Chronicles of Failure and How to Overcome Them", "abstract": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. Organizations often stumble over unrealistic expectations, siloed data, and cultural roadblocks. This talk dives into the human and organizational dynamics that cause AI initiatives to derail.\r\n\r\nUsing real-world examples, I\u2019ll showcase how missing tram stop location data was found on a hobbyist\u2019s blog when official systems failed. Or how critical historical data for resource planning sat locked in Excel files with forgotten passwords, forcing a desperate password recovery operation. These examples highlight the surprising ways poor data governance and lack of interdisciplinary collaboration sabotage even well-intentioned AI projects.\r\n\r\nBut all is not lost! We\u2019ll explore actionable strategies to navigate these challenges: shifting from shiny tools to R&D-driven approaches, fostering better communication between IT and business teams, and building strong foundations for clean, accessible data. Attendees will leave with insights into what AI success truly requires: cultural readiness, realistic expectations, and a commitment to collaboration.\r\n\r\nIf you\u2019re tired of AI hype and looking for practical solutions, this talk offers a clear roadmap to ensure AI projects deliver measurable impact.", "full_description": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. The real barriers are organizational, cultural, and human. In this talk, we\u2019ll uncover the reasons AI initiatives stumble and explore how to overcome them\u2014through a mix of anecdotes, real-world failures, and actionable best practices.\r\n\r\nWe begin with a big lamenti\u2014an unfiltered dive into the messy, chaotic world of AI project implementation. From tangled data silos to misaligned expectations, we\u2019ll uncover hard truths through real-world examples that many will recognize:\r\n\t\u2022\tThe missing data problem: Sometimes it\u2019s the smallest datasets that hold the biggest answers.\r\n\t\u2022\tPasswords might be in retirement.\r\n\t\u2022\tDecisions in the dark: When untrained people must make calls on things they don\u2019t fully understand.\r\n\t\u2022\tThe shiny facade: Expectations, the pressure for quick wins, and the relentless demand for good news.\r\n\r\nTo go beyond personal experience, I\u2019ll also bring fresh insights from a survey of my expert network closer to the conference. This will offer a broader, more representative perspective on why AI projects stumble\u2014and how to fix them.\r\n\r\nTimed Outline (45 minutes):\r\n1. Introduction (5 min)\r\n - What qualifies me for this talk.\r\n2. The Big Lamenti \u2013 Real-World Failures and Anecdotes (20 min)\r\n - Anecdotes that illustrate the challenges in a candid yet engaging way.  \r\n - Learnings from the survey\r\n3.  Solutions and Best Practices (15 min)\r\n - Mindset: How AI is perceived and approached within organizations.\r\n - Data: The role of accessibility, governance, and quality as foundational pillars.\r\n - Collaboration: Bridging silos between teams, leadership, and departments.\r\n - Scale: The progression from small, focused initiatives to sustainable implementation.\r\n - Expectations: Aligning vision, goals, and outcomes for realistic project success.\r\n5.\tTakeaways & Closing Thoughts (5 min)\r\n - Summary of insights and practical steps to avoid common pitfalls\r\n- Key mindset shifts for sustainable AI success\r\n5.\tQ&A (5 min)", "code": "JA9NFW", "state": "confirmed", "created": "2024-12-17", "speaker_names": "Alexander CS Hendorf", "track": "General: Others"}, {"title": "PDFs - When a thousand words are worth more than a picture (or table).", "abstract": "PDF, a must-have in RAG systems, ensures visual fidelity across platforms and devices, at the expense of compromising what would be the core condition for computers to properly process and interpret text: semantics. That means any logical arrangement of text, upon rendering, explodes into dummy visual shards of data that literally portrait the bigger picture for the human eye to perceive, but no longer convey the information computers should grasp. Such a bottleneck already makes proper ingestion of text-only documents a big challenge, let alone when tables or figures come into play, the ultimate nightmare for PDF parsers, not to say developers. The rest you must have already foreseen: a RAG system barfing unreliable knowledge from bad chunks (based on regular PDF parsing), if those ever get to be retrieved from a vector database. In this talk you can gather some vision-driven insights on how to leverage the strengths of PDF and language models towards good chunks to be ingested. Or, in other words, how multimodal models can go beyond trivial reverse engineering by decomposing tables into its building blocks, in plain language, as how those would be explained to another human; or better yet, as how humans would ask questions about such pieces of knowledge. And from such a strategy, we transfer the same rationale to figures. Come along, gather some insights, and get inspired to break down tables and figures from your own PDFs, and to improve retrieval in your RAG systems.", "full_description": "PDF, a must-have in RAG systems, ensures visual fidelity across platforms and devices, at the expense of compromising what would be the core condition for computers to properly process and interpret text: semantics. That means any logical arrangement of text, upon rendering, explodes into dummy visual shards of data that literally portrait the bigger picture for the human eye to perceive, but no longer convey the information computers should grasp. Such a bottleneck already makes proper ingestion of text-only documents a big challenge, let alone when tables or figures come into play, the ultimate nightmare for PDF parsers, not to say developers. The rest you must have already foreseen: a RAG system barfing unreliable knowledge from bad chunks (based on regular PDF parsing), if those ever get to be retrieved from a vector database.\r\n\r\nIn this talk you can gather some vision-driven insights on how to leverage the strengths of PDFs and language models towards good chunks to be ingested in a vector database. Or, in other words, how multimodal models can go beyond trivial reverse engineering by decomposing tables into its building blocks, in plain language, as how those would be explained to another human; or better yet, as how humans would ask questions about such pieces of knowledge. Consequently, it brings robustness to retrieval, the backbone of RAG. And from such a strategy, we can transfer the same rationale to figures.\r\n\r\nGet ready to boost your retrieval skills, as we:\r\n- Analyze the semantical bottlenecks, from the anatomy of a PDF stream, to how parsers traverse it;\r\n- (Briefly) approach the never-ending debate on the ideal chunk format for ingestion in vector databases;\r\n- Build some chunks using multimodal models to decompose tables into its building blocks, preserving plain language;\r\n- Conduct an experiment on measuring quality of retrieval and compare the decomposition strategy against PDF parsers and reverse engineering techniques;\r\n- And last, but not least, transfer the same rationale to figures.\r\n\r\nBy then, you'll have enough food for thought to get your hands dirty, clone the repo, and give tweaks to the experiment yourself. Come along, gather some insights, and get inspired to break down tables and figures from your own PDF files, and to improve retrieval in your RAG systems.", "code": "UVPALT", "state": "submitted", "created": "2024-12-13", "speaker_names": "Caio Benatti Moretti", "track": "PyData: Generative AI"}, {"title": "The Open-Meteo Weather API - Data Version Control Built with Python", "abstract": "We will explore how to add data versioning to an ML project; a simple end-to-end rain prediction project for the Munich area. The data assets will be stored in an OSS repository built for data projects, and we will use its lakeFS-spec Python package for easy interaction with other Python libraries. Following model training with initial data, we will update the dataset and retrain the model. Finally, we will revert to the initial data version to reproduce the original model and assess its initial accuracy.", "full_description": "Data version control is the backbone of modern machine learning (ML) projects, as it allows ML practitioners to systematically track different versions of data sets, which are used for training and validating ML models.\r\n\r\nIn this talk, we will demo how a common and open source Python stack was leveraged to build a simple end-to-end rain prediction ML project, using the Open-Meteo API that trains a Random Forest Classifier** model. \r\n\r\nThis talk will walk through how to leverage the lakeFS-spec Python package for fsspec - that streamlines versioned data operations for Python developers in data science use cases, enabling easy access and versioning of objects from popular Python libraries like Pandas, Numpy, Polars, DuckDB, PyArrow, with just one or two lines of code. The code example will include model training with initial data, updating the dataset and retraining the model. We'll wrap up by reverting to the initial data version to reproduce the original model and assess its initial accuracy, all this through common open source tooling built for Python and PyData stacks.\r\n\r\n\r\nThis exapmle will enable user to reap the following benefits:\r\n- High-level abstraction over basic repository operations\r\n- Convenient access to the underlying block storage, in addition to lakeFS versioning operations\r\n- Seamless integration into the fsspec ecosystem\r\n- Accessing objects in lakeFS directly from popular data science libraries (e.g., Pandas, Numpy, Polars, DuckDB, PyArrow) with just one or two lines of code\r\n- Transaction support for conducting data version control operations in a conscious and reliable way\r\n- Zero-config option through config autodiscovery\r\n- Caching to avoid unnecessary data transfers", "code": "GVKLVP", "state": "submitted", "created": "2024-12-18", "speaker_names": "Einat Orr", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Django-NLP integration approach to process healthcare data from a web", "abstract": "Introduction: Django framework has gained widespread acceptance and usage by many developers due to its versatility and scalability. \r\n Problem: There is certain information scarcity, in applied aspects, with respect to emerging tools like NLP (Natural Language Processing) and their integration with Django.\r\nInformation extraction (IE) in NLP is key technology that automatically extracts structured data from unstructured data. \r\nSolution: Django-ORM(object relational mapping) based data when integrated with NLP tools can become  better organized for a reliable data extraction and analysis, and also may serve a solution for handling AI (Artificial Intelligence) based  large dataset handling. \r\nRelevance: This approach can create awareness among the audience and IT community about the ways to lessen the complexity involved in extraction and analysis of large datasets from a database embedded in a web. \r\nMethodology: This approach may involve building and training a NLP model, use of NLP libraries like TextBlob, Textacy, NLTK, Django libraries, ORM, views templates, etc \r\nConclusion/Takeaway : Django-NLP integration may serve as a good approach in processing and extracting large datasets from a web. Particularly, this approach may reflect a cross talk between tabular data and NLP. There is a need to further strengthen this exploratory concept as it carries good future implications in Machine learning.", "full_description": "Introduction: Django framework has revolutionized web development in the IT sector globally. It has gained widespread acceptance and usage by many developers due to its versatility and scalability. Particularly documentation on Django has provided the users a know-how of the technology and its applications from beginners to the advanced professionals. Problem: There seems to be a problematic area that needs to be addressed with a solution. The problem is that there is certain information scarcity with respect to emerging areas like NLP (Natural Language Processing) and its integration with Django. Although this integration concept has been introduced several years ago, it still requires input in terms of practical feasibility in real life scenarios like healthcare domain. There seems to be knowledge deficit problem in this area. \r\nBody:\r\nNLP refers to an Artificial Intelligence (AI) technology that enables computers understand and process natural languages in spoken or text form. Especially, NLP has several facets with regard to the processing of large amounts of text or data. For instance, information extraction (IE) in NLP is  key technology that automatically extracts structured data from unstructured data. It can recognize and pull out particular data like dates, names, relationships and many more and organizes them. ORM(object relational mapping)  tables when used in Django could generate tabular data, say, patient data in this case. But this tabular data, needs some integration with analytical tool to organize them for a quick analysis.\r\nRelevance of the problem to the audience:\r\nThe problem is relevant to audience or the IT community because they need approaches that can lessen the complexity involved in extraction ad analysis of large datasets from a database. This in turn can help them in better organization, planning and decision making.\r\nSolution to the problem:\r\nThe solution to the problem is building and training a NLP model, developing  a Django view for NLP Integration, using NLP libraries like TextBlob, Textacy, NLTK etc  to process the input text data(patient data, for example).Then install needed Django libraries, create ORM, views or APIs for processing data input , running NLP tasks for recognizing  and classifying named entities, and obtain the final output in the form of template.\r\n\r\nConclusion/ takeaway:\r\nDjango-NLP integration is an emerging field with regard to processing and extraction of large datasets particularly when dealing with public databases like patient data in hospitals. This approach may serve a solution for handling AI (Artificial Intelligence) based  large dataset handling. This can guide the doctors to come up with better diagnosis and treatment plans. This approach of cross talk between Django tabular data and NLP may  guide the data scientists  for exploratory data analysis (EDA) in Machine learning.", "code": "UM7UV8", "state": "submitted", "created": "2025-01-05", "speaker_names": "Y SriRam Chandra Murthy", "track": "PyCon: Django & Web"}, {"title": "Lighting up Faces with LEDs; First time contact to programming with micro:bit", "abstract": "More people should experience the joy of programming and study technical computer science, medical technology, ai engineering and information technology. Young people often shy away from technology, but most can actually do it and most could experience the satisfaction intrinsic to problem solving (and some LEDs blinking :-). To this end, we designed and run a series of 2h workshops at university with 10-30 young people in their last two years of school (>10 sessions so far) to exite them. We use the micro:bit and code in MicroPython with the ulimate goal via a couple of exervises to have pupils with some or none programming experience write a flappy bird rip-off (5x5 pixel screen, accelerometer). In the session we will go over the exercise sheet/slides, environment and cover what worked well and what did not.", "full_description": "We cover the following topics: \r\n\r\nOrganization:\r\n* How to approach young people (central program, teacher contact, self sign up), planning\r\n* Time frame, extra program around actual coding\r\n* How to teach the minimal stuff needed (slides?, 2 min, 5 min, 10 min, 20 min)\r\n* Group size, how many support/tutoring is needed, pauses and program in pause\r\n\r\nExercise design and infrastructure\r\n* Why a microcontroller and blinking LEDs\r\n* Number of exercies, build up, the callenge\r\n* Make everyone happy at any step, a \"quest\"\r\n* Infrastructure and preparation, PCs+accounts+cheat sheet\r\n\r\nAnd of course we start playing flappy bird live ;-)", "code": "D7MANK", "state": "submitted", "created": "2024-12-11", "speaker_names": "Peter Barth", "track": "General: Education, Career & Life"}, {"title": "Smarter Retrieval, Better Generation: Building Better RAG Systems", "abstract": "My talk explores Retrieval Augmented Generation (RAG), a model architecture that combines information retrieval and generative models to fulfill a user information need.\r\n\r\nThe presentation focuses on RAG systems' indexing and retrieval components, covering classical and modern techniques. It discusses various methods for converting textual data into searchable formats and how to leverage those formats to increase the retrieval of relevant documents based on user queries. I will compare different retrieval techniques, highlighting their features, advantages, and disadvantages. I will also showcase a Python implementation of all the discussed techniques and how they compare in performance and efficiency over an annotated dataset.\r\n\r\nParticipants will gain insights into each technique's unique features, advantages, and limitations, along with guidance on selecting the most appropriate approach for specific tasks and data requirements.", "full_description": "Retrieval Augmented Generation (RAG) is a model architecture for tasks requiring information retrieval from large corpora combined with generative models to fulfil a user information need. It's typically used for question-answering, fact-checking, summarization, and information discovery. \r\n\r\nThe RAG process consists: indexing, which converts textual data into searchable formats; retrieval, which selects relevant documents for a query using different methods; and augmentation, which feeds retrieved information and the user's query into a Large Language Model (LLM) via a prompt for output generation.\r\n\r\nTypically, one has little control over the augmentation step besides what's provided to the LLM via the prompt and a few parameters, like the maximum length of the generated text or the temperature of the sampling process. On the other hand, the indexing and retrieval steps are more flexible and can be customized to the specific needs of the task or the data.\r\n\r\nMy talk will focus on RAG systems' indexing and retrieval techniques. Attendees will learn about various methods, starting with classical approaches rooted in the Information Retrieval community. While these methods have been around for decades, they remain widely used today due to their simplicity and efficiency.\r\n\r\nThe session will then explore more modern techniques that leverage LLMs to enhance the indexing process or optimize user queries. These approaches aim to improve the retrieval of relevant documents and improve the performance of RAG systems.\r\n\r\nParticipants will gain insights into each technique's unique features, advantages, and limitations, along with guidance on selecting the most appropriate approach for specific tasks and datasets. The talk will conclude with a performance analysis, comparing these techniques on an annotated dataset. Key metrics such as speed, accuracy, and efficiency will be evaluated, offering a comprehensive understanding of the trade-offs and practical takeaways for real-world applications.", "code": "8DYTTA", "state": "submitted", "created": "2024-12-21", "speaker_names": "David Batista", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Deploy RAG Applications Using Docker: A Step-by-Step Guide", "abstract": "Docker is a game-changer for deploying AI applications, offering portable and consistent environments across platforms. In this tutorial, we\u2019ll explore how to build and deploy a Retrieval-Augmented Generation (RAG) application using Docker. RAG applications combine data retrieval with generative AI to produce contextually relevant and accurate responses, making them powerful tools for interactive Q&A systems.\r\n\r\nWe will build a document-based Q&A application that allows users to upload files, retrieve context from them, and answer questions interactively. We will use LlamaIndex to build the RAG pipeline and use both open-source LLM from Groq and closed-source LLM, such as OpenAI models, for LLM access. This tutorial will guide you through the entire lifecycle\u2014from building the app to deploying it using Docker on Hugging Face Spaces. Whether you\u2019re new to Docker or experienced with RAG, this guide offers a hands-on approach to deploying scalable and efficient AI solutions.", "full_description": "Retrieval-Augmented Generation (RAG) applications are reshaping AI by combining real-time data retrieval with large language models to generate accurate, dynamic, and context-aware responses. This tutorial provides a comprehensive, step-by-step guide to building and deploying a RAG application using Docker. Attendees will learn how to create portable and consistent environments that simplify deployment, ensure compatibility, and streamline operations for RAG workflows.\r\n\r\nThe tutorial will begin with Setting Up the Environment, including installing Docker, creating a project directory, and managing API keys securely with .env files. Next, attendees will learn how to Build the RAG Application by writing Python scripts to integrate file parsing, embedding generation, and querying LLMs. We will also implement key functions for handling uploaded files, generating context-aware responses, and creating a Gradio-based user interface for seamless interaction.\r\nIn the Creating the Dockerfile section, attendees will package the application into a Docker image by writing a Dockerfile to automate setup and expose the application interface. This is followed by Building and Running the Docker Image, where participants will learn to test the application locally by running it as a Docker container, using Docker commands to manage and troubleshoot the setup.\r\n\r\nThe tutorial will then cover Deploying to Hugging Face Spaces, a beginner-friendly cloud platform for hosting Docker-based applications. We will demonstrate how to set up a Hugging Face Space, configure API key secrets, and automate the deployment process for the RAG application. Finally, attendees will learn best practices for Monitoring and Optimizing the Application. The session will also include tips for optimizing Docker configurations to reduce image size and improve performance.\r\n\r\nThis tutorial is tailored for developers, data scientists, and ML engineers interested in deploying scalable and efficient RAG applications. Whether you're new to Docker or have experience with LLM-based workflows, this guide offers actionable insights and practical skills to streamline deployment processes. By the end of the session, participants will have a fully functional RAG application running in the cloud and the confidence to deploy similar solutions across various environments.", "code": "7FV8B3", "state": "submitted", "created": "2025-01-05", "speaker_names": "Brain Aboze", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Snakes in Cloud: Getting your Python Running on AWS", "abstract": "You got your beautiful Python code running on your computer and you want to share it with a world. In this talk talk I'll step by step in fastest way possible to get your code running on AWS as cheap as possible and give you advice on how to scale it when time comes.\r\nMain goal is to push your code to github repo, grab a cup of tea/coffee while your pipeline do your work and return to your changes running on cloud.", "full_description": "In this talk we will go over:\r\n- Working with containers and making your docker file\r\n- Setting up GitHub actions\r\n- CDK and Python (lets use Python to setup our cloud instead of clicking in AWS website)\r\n- Setting up permissions and credentials\r\n- Setting up database (why use RDS)\r\n- Setting up ECS (AWS container services)\r\n- Getting your load balancer up and running\r\n- Generating certificates (so we have nice https for our app)\r\n- Calculating expanses (because AWS never makes things easy)\r\n- Q/A\r\n\r\nWe will be going over lot of things and concept, but I'll explain what everything does and more importantly why we are doing it this way.\r\n\r\nMain goal is to walk away with a bit less fear of clouds and bit more courage to deploy things.", "code": "Q3G3UX", "state": "submitted", "created": "2024-12-22", "speaker_names": "Bojan Miletic", "track": "PyCon: MLOps & DevOps"}, {"title": "Data-Driven Impact in Africa", "abstract": "We have a lot of NGOs in Africa, trying to help us improve our lives. The problem is we do not have enough data to help them understand us well to create beneficial programs.\r\n\r\nDiscover how NGOs can leverage data science to understand and serve African communities better. Learn about data collection, privacy, and impact assessment.", "full_description": "# Data-Driven Impact in Africa: Enhancing NGO Operations with Machine Learning\r\n\r\n>Explore how data science can revolutionize NGO work in Africa, focusing on ethical data handling, machine learning applications, and community impact.\r\n\r\n## Outline:\r\n### Introduction\r\n- Brief on the NGO landscape in Africa.\r\n- Importance of data in improving NGO operations.\r\n\r\n### Data Collection in a Diverse Context\r\n- Challenges specific to Africa (cultural, infrastructural, legal).\r\n- Strategies for ethical and effective data collection.\r\n\r\n### Machine Learning for Social Good\r\n- Case studies: How machine learning is currently used by NGOs for health, education, and economic development.\r\n- Tools and methodologies: From data cleaning to model deployment.\r\n\r\n### Privacy, Security, and Ethics\r\n- Navigating data privacy laws and cultural sensitivities.\r\n- Best practices for data protection and ethical AI use.\r\n\r\n### Impact Assessment with Data\r\n- Measuring the effectiveness of interventions through data analysis.\r\n- Examples of how data has led to policy changes or program adjustments.\r\n\r\n### Practical Implementation\r\n- Tools and platforms suitable for NGOs with limited resources.\r\n- How to implement simple ML models to address common NGO issues.\r\n\r\n### Future Directions\r\n- The role of AI in sustainable development.\r\n- Collaboration between tech communities and NGOs for ongoing innovation.\r\n\r\n### Conclusion\r\n- Recap of key points.\r\n- Call to action for data scientists to engage with NGOs.\r\n\r\n## Audience Takeaways:\r\n- Understanding of the unique data challenges in African contexts for NGOs.\r\n- Practical knowledge on implementing ML solutions in NGO settings.\r\n- Insights into ethical data use and privacy considerations.\r\n- Inspiration to apply data science skills for social impact.", "code": "VYHNUF", "state": "submitted", "created": "2024-12-30", "speaker_names": "Chris Achinga", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Marimo: Setting up interactive notebooks easily", "abstract": "This session will give you a deep insight in how to easily create browser based applications with Marimo. You will learn how to implement a range of both reactive and interacitve elements speed up your development journey or daily business. Marimo itself was designed from scratch to be reactive and thus to overcome weaknesses of Jupyter, even with interactive extensions.\r\n\r\nThis session is especially interesting for:\r\n+ data analysts\r\n+ data engineers\r\n+ data scientists\r\n+ all Pythonista heavily using Jupyter notebooks", "full_description": "I will start with a short comparison of Jupyter notebooks with Marimo to point out Marimo's strength to create interactive applications build on its reactive model. Daily routines will be processed both with Jupyter and Marimo, so that Marimo's improvements are standing out on their own. A technical deep dive will then explaing its principles and how they can be used for own projects. After this, a comprehensive presentatin will cover many Marimo features before a small project will be set up live to see how the Marimo approach can speed up many development steps.\r\n\r\n### Batteries included\r\n+ Marimo comes with a lot of input and output elements that can be implemented with a breeze. \r\n+ Its comprehensive documentation makes it easy to start with it from scratch or switching from Jupypter notebooks.\r\n+ The code looks liks normal Python code and can directly be included in Git operations and workflows.\r\n+ You could even use Marimo as an online IDE for working on your own computer e.g. for presentations.", "code": "S3RLWT", "state": "submitted", "created": "2024-12-17", "speaker_names": "Arnd Scharpegge", "track": "PyData: Visualisation & Jupyter"}, {"title": "Benchmarking Time Series Foundation Model with sktime", "abstract": "The recent time series foundation models such as Chronos, Moirai, and TinyTimesMixer promise to change time series forecasting. One change is the ability to perform zero-shot forecasting. However, their performance on specific datasets is often unclear, requiring benchmarking. Such a benchmarking can be performed with the benchmarking module of sktime. sktime is a widely used scikit-learn compatible library for learning with time series. sktime is easily extensible and interoperable with the pydata/numfocus stack.\u00a0\r\nThis talk presents how everyone can benchmark and compare different time series foundation models for forecasting to determine the best foundation model for the specific use case.", "full_description": "In the past years, time series foundation models emerged. They have the potential to change time series forecasting. For example, multiple time series models such as LagLlama, Chronos, Moirai, TinyTimesMixer, etc., promise zero-shot forecasting for arbitrary time series. Furthermore, also sktime started to unify the interfaces of the various foundation models to make the usage of those models easy.\u00a0\r\nHowever, whether these time series foundation models provide added value to various forecasting applications is still unclear. Thus, benchmarking is necessary. In sktime, we have implemented a benchmarking module enabling easy comparison of those time series foundation models on custom datasets and with arbitrary metrics.\r\n\r\nOur talk will outline how sktime\u2019s benchmarking module works and how users can use it to evaluate time series foundation models.\u00a0\r\nWe will show how to combine the benchmarking module with the time series foundation models.\r\nWe will show the results of a small benchmarking study using time series foundation models and statistical time series models.\u00a0\r\nWe will outline our roadmap for time series foundation models.\u00a0\r\n\r\nsktime is developed by an open community with the aim of ecosystem integration in a commercially neutral, charitable space. We welcome contributions or donations and seek to provide opportunities for anyone worldwide.", "code": "GUKTNX", "state": "submitted", "created": "2024-12-22", "speaker_names": "Benedikt Heidrich", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "theTrial: How I Built My Own Python Microframework for Kafka Interaction", "abstract": "This talk explores the journey of creating a Python microframework, guided by the DRY principle (Dont Repeat Yourself). It delves into the project's lifecycle, from identifying boilerplate code across multiple projects to designing, implementing and publishign the framework. The presentation examines challenges encountered and focus on how existing frameworks like Django, Flask, and FastAPI influenced the process.  It would focus on the value of analyzing existing frameworks and think beyon just blindly using them, but instead get inspired to create their own tools and solutions for everyday tasks.", "full_description": "The outline of the presentation would be the following:\r\n1. Introduction\r\n2. Problem Analysis (motivation, examples of boilerplate code, problem analysis)\r\n3. Design and Development Process (initial steps, inspiration from existing frameworks like Django, Flask, FastAPI)\r\n4. Challenges and Solutions (leassons learned from existing frameworks)\r\n5. theTrial\r\n6. Examples (how theTrial solved the initial problem)\r\n7. Conclusion (Why it matters to go beyond just using libraries and frameworks passively)\r\n \r\nThis talk starts with boilerplate code commonly used across multiple microservices that need to connect with Kafka and concludes with theTrial, a microframework that simplifies Kafka communication. theTrial is built on top of Confluent-Kafka.", "code": "KLK8NS", "state": "submitted", "created": "2024-12-22", "speaker_names": "Michael Loukeris", "track": "PyCon: Programming & Software Engineering"}, {"title": "PySnooper: Never use print for debugging again", "abstract": "PySnooper is a debugger that lets you understand code flow by automatically printing every line executed & every variable change. No more print statements! Just add one decorator and get a detailed log of what your Python code is actually doing under the hood.", "full_description": "I had an idea for a debugging solution for Python that doesn't require complicated configuration like PyCharm. I released PySnooper as a cute little open-source project that does that, and to my surprise, it became a huge hit overnight, hitting the top of Hacker News, r/python and GitHub trending.\r\n\r\nIn this talk I'll go into:\r\n\r\n* How PySnooper can help you debug your code.\r\n* How you can write your own debugging / code intelligence tools.\r\n* How to make your open-source project go viral.\r\n* What interesting things happen when lots of people contribute to your open-source project.", "code": "3NZSKU", "state": "submitted", "created": "2024-12-07", "speaker_names": "Ram Rachum", "track": "PyCon: Programming & Software Engineering"}, {"title": "RAG in Action: Creating Context-Aware Applications with Python", "abstract": "Unlock the potential of Retrieval-Augmented Generation (RAG) to create intelligent applications that seamlessly combine language models with real-time data. In this session, we\u2019ll explore how Python, Couchbase, and vector search come together to build scalable RAG solutions. Learn how to manage embeddings, integrate unstructured data, and enhance AI applications by leveraging the power of modern data platforms. With live demos and practical insights, this talk is perfect for developers looking to bridge the gap between AI and real-world data needs.", "full_description": "The fusion of AI and real-time data has paved the way for Retrieval-Augmented Generation (RAG), a powerful method for building context-aware, responsive applications. This session will take participants on a journey to develop RAG-based applications using Python and Couchbase, focusing on practical implementation and best practices.\r\n\r\nKey topics include:\r\nIntroduction to RAG: Understand how RAG enables the combination of language models and real-time contextual data.\r\nVector Search and Embeddings: Learn to generate, store, and search embeddings efficiently with Couchbase and Python libraries.\r\nProcessing Unstructured Data: See how to convert unstructured data into usable chunks for improved language model performance.\r\nBuilding RAG Applications: Step-by-step guidance on integrating AI-driven workflows with a modern NoSQL database for scalability and performance.\r\n\r\nAttendees will gain actionable insights into building robust RAG workflows that address challenges like context loss in large language models and the need for up-to-date data. The session will include live demonstrations, code walkthroughs, and architectural patterns to help you implement RAG in your own projects. This talk is ideal for Python developers, data engineers, and AI enthusiasts looking to integrate cutting-edge AI capabilities into their applications.", "code": "PFN8LU", "state": "submitted", "created": "2024-12-18", "speaker_names": "Gregor Bauer", "track": "PyData: Generative AI"}, {"title": "The Name\u2019s Bot, Vulnerable Bot: AI Under Siege and why you should worry?", "abstract": "What if a well-crafted prompt could manipulate your AI system to act outside its intended boundaries? Or a subtle tweak could mislead it entirely? These aren\u2019t just hypothetical scenarios; they are real threats called `Jailbreaking` or `Adversarial Attacks`, capable of compromising your projects, products, or systems.\r\n\r\nIn this talk, let\u2019s dive deeper into these intriguing but concerning phenomena. You\u2019ll learn:\r\nHow these attacks exploit vulnerabilities in LLMs.\r\nReal-world examples of how they can bypass safeguards.\r\nSome practical strategies to protect your AI systems from going rogue.\r\n\r\nWhether you are a seasoned developer or just eager to understand the risks and resilience of Generative AI in a world that increasingly relies on it, this session is for you. \r\n\r\n**Because, let's be honest, no one wants their AI to go rogue, right?**", "full_description": "We will start with the basic introduction, while covering certain subtleties and practical examples of Adversities, we will move towards understanding Jailbreaking and what can be possibly done to safeguard against such attacks.\r\n\r\nThis talk will try to answer the following questions:\r\n\r\n* What are Jailbreaking, Adversarial Prompting, and Prompt Injection in terms of LLMs?\r\n* Why is it a concerning issue to you or anyone who works or uses LLMs?\r\n      - Some example case studies and practical shortcomings of LLMs prone to JailBreaking \r\n* What are some types of Jailbreaking Techniques, with examples on Prompt Level and Token Level Jailbreaking\r\n* What are some ways you can safeguard your systems against these attacks (with and without requiring retraining your models)?\r\n\r\nBy the end of this talk, listeners will have a good idea of risks involving LLMs and how adversities try to challenge the very purpose of LLM-based systems, compromising your projects or workflows, and also about approaches to safeguard against such attacks.", "code": "FBEVVW", "state": "submitted", "created": "2024-12-15", "speaker_names": "Adarsh Kumar, Divyam Chandel", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "What do a tree and the human brain have in common-a not so serious introduction to digital pathology", "abstract": "While trees and human brains don't share that many properties regarding their domain, the analysis of the height of a tree and cancer in human brains does.\r\nThis talk provides a not-so-serious introduction to the domain of computer vision for pathological use cases. \r\nBesides a general introduction to (digital) pathology and the technical similarities between satellite images (GeoTIFs) and pathological images (Whole-Slide Images), we will take a look at computer vision (both ML-based and conventional) on Python.\r\nWhether you have never done image processing in Python, are an expert (ready to share some tricks with me), or are just curious to see pictures of a human brain, this talk is for you.\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "full_description": "Inspired by last year's talk about the height of a tree [\ud83c\udf33 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery \ud83d\udef0\ufe0f] and the strong similarities between optical high resolution satellite images and pathological images, this talk will give a not-so-serious introduction to a quite serious topic.\r\nThe main content is:\r\n- a short introduction to digital pathology (know your domain)\r\n- the similarities between a tree and your brain (technically speaking, there are a lot)\r\n- ML-based and conventional computer vision in Python with some practical use cases\r\n- Why we can steal (nearly) everything from radiology and get away with it\r\n\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "code": "MJD7TG", "state": "accepted", "created": "2024-12-20", "speaker_names": "Daniel Hieber", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Enhancing Software Supply Chain Security with Open Source Python Tools", "abstract": "The Cyber Resilience Act (CRA) is focused on improving the security and resilience of digital products. But to comply with the CRA, businesses will need to start preparing the necessary evidence to ensure compliance if they want to continue to deliver digital products to the EU market once the CRA is in force. \r\n\r\nKey requirements within the CRA include implementing robust security measures throughout the product life-cycle, adopting secure development practices and implementing proactive vulnerability management processes.\r\n\r\nThis session will show how a number of the requirements for the CRA can be achieved by use of a number of open source Python tools.", "full_description": "The Cyber Resilience Act (CRA) is aimed at improving the security and resilience of the software components within a digital product. This session will provide a high level overview of the CRA and demonstrate how to enhance software supply chain transparency, manage risks effectively throughout the Software Development Lifecycle (SDLC), and achieve the necessary compliance by leveraging a suite of open-source Python tools. \r\n\r\nKey areas to be addressed will include:\r\n\r\n- Learn how to create comprehensive and high quality SBOMs to gain a clear understanding of all components within your software.\r\n- Discover how to identify and mitigate potential risks and threats within the software supply chain throughout the entire SDLC.\r\n- Explore effective strategies for identifying, assessing, prioritising and remediating software vulnerabilities.\r\n- Understand how to adopt best practices to ensure compliance with relevant regulations and industry standards.\r\n\r\nThe Python tools/applications to be referenced will include sbom4python, lib4sbom, lib4vex, lib4package, distro2sbom, sbomdiff, sbomaudit and cve-bin-tool.", "code": "M98YBR", "state": "submitted", "created": "2024-12-20", "speaker_names": "Anthony Harrison", "track": "PyCon: Security"}, {"title": "Fast-Paced Automotive Demand Forecasting using Polars & Dash", "abstract": "Learn how we built a strategic demand forecasting application for an automotive OEM, providing insights up to 17 years into the future. This talk covers the challenges of long-term forecasting, the use of Dash for rapid, interactive application development, and Polars for efficient data manipulation. We\u2019ll explore key planning paradigms (cause-and-effect vs outcome-oriented), the trade-offs of moving away from traditional tools, and lessons learned along the way. Gain practical insights into how to develop fast, flexible planning applications.", "full_description": "In this talk, we\u2019ll explore the creation of a strategic demand forecasting application for an automotive OEM, designed to provide insights for up to 17 years into the future. Faced with the challenge of developing a fast and flexible solution in just a few months, we leveraged Dash as a low-code application framework and Polars as a powerful data manipulation library.\r\n\r\nWe\u2019ll begin by setting the context for strategic demand forecasting in the automotive industry, highlighting unique challenges due to the long forecasting horizons. Next, we\u2019ll introduce two key planning paradigms: cause-and-effect oriented versus outcome-oriented planning, and how they shaped our approach.\r\n\r\nOur solution broke away from conventional tools like classic JavaScript/React frontends and database-centric architectures. Instead, we used Dash and Dash Mantine Components to streamline the development of a polished, interactive application, and Polars to deliver a lightweight yet feature-rich alternative to traditional DBMS setups for manipulating planning data.\r\n\r\nWhile forecasting algorithms remain outside the scope of this session, we\u2019ll focus on lessons learned during the development process\u2014what worked, what didn\u2019t, and how these tools impacted our speed, ability to deliver features, and team experience.\r\n\r\nThe techniques and tools discussed in this talk can benefit not only the automotive industry but any domain that requires strategic planning for long horizons.\r\n\r\nAttendees will leave with actionable insights into leveraging Dash and Polars for building fast, flexible planning applications, along with practical advice for navigating similar projects.", "code": "G79HSF", "state": "submitted", "created": "2024-12-22", "speaker_names": "Thomas Bierhance", "track": "PyCon: Programming & Software Engineering"}, {"title": "Decoding Topics: A Comparative Analysis of Python\u2019s Leading Topic Modeling Libraries Using Climate C", "abstract": "Topic modelling has come a long way, evolving from traditional statistical methods to leveraging advanced embeddings and neural networks. Python\u2019s diverse library ecosystem includes tools like Latent Dirichlet Allocation (LDA) using gensim, Top2Vec, BERTopic, and Contextualized Topic Models (CTM). This talk evaluates these popular approaches using a dataset of UK climate change policies, considering use cases relevant to organisations like DEFRA (Department for Environment, Food & Rural Affairs). The analysis explores real-time integration, dynamic topic modelling over time, adding new documents, and retrieving similar ones. Attendees will learn the strengths, limitations, and practical applications of each library to make informed decisions for their projects.", "full_description": "Objectives:\r\nThe session aims to:\r\n\r\n1.  Compare Python-based topic modelling libraries, highlighting their relevance to real-world scenarios like policy analysis.\r\n2.    Explore practical use cases, including real-time document integration, tracking topic evolution, and finding similar documents.\r\n3.    Evaluate the tools based on performance, interpretability, scalability, and flexibility, with a focus on climate change policy data presented by [1] focusing on adaptation and mitigation.\r\n4.    Provide actionable guidance on selecting the right library for different project needs and datasets.\r\n\r\nOutline:\r\n\r\n1. Introduction to Topic Modeling: Overview of traditional and modern approaches, including their practical significance.\r\n\r\n2. Algorithms & Libraries Overview: LDA (gensim) [2], CTM [3], Top2Vec [4], BERTopic [5]\r\n\r\n3. Dataset and Use Cases:\r\n      - Overview of the UK climate change policy dataset.\r\n      - Use cases inspired by DEFRA and similar organisations, such as:\r\n            - Real-time integration for continuously adding new documents.\r\n            - Tracking topic development over time (dynamic topic modeling).\r\n            - Retrieving similar documents for faster insights.\r\n           (- Classification)\r\n\r\n4. Evaluation Criteria: Analysis of libraries based on:\r\n        - Ease of Use: How easy it is for no coding experts\r\n        - Quality: Coherence and diversity of extracted topics.\r\n        - Efficiency: Runtime performance and scalability.\r\n        - Flexibility: Features like contextual embeddings and integration capabilities.\r\n        - Interpretability: Ease of understanding topics and output.\r\n\r\n5. Results: Detailed findings, including specific advantages and limitations of each library in supporting the outlined use cases.\r\n\r\n6. Practical Recommendations: Guidance on choosing a library based on project goals, dataset characteristics, and organisational needs.\r\n\r\n7. Conclusion and Future Directions: Summary of key insights and the evolving role of embedding-based methods in topic modelling.\r\n\r\nOutcomes:\r\nBy attending this session, participants will:\r\n\r\n- Gain an in-depth understanding of Python\u2019s top topic modeling libraries.\r\n- Learn how to apply these tools to real-world challenges in policy analysis and other fields.\r\n- Understand how to handle use cases like real-time document integration and topic evolution over time.\r\n- Develop the skills to evaluate and choose the best tool for specific datasets and objectives.\r\n\r\nTarget Audience\r\n\r\nThis talk is for:\r\n- Data scientists and NLP practitioners seeking to apply topic modelling to unstructured text data.\r\n- Policy analysts and researchers working with large textual datasets, such as government or environmental policies.\r\n- Professionals in organisations like DEFRA, where tracking changes, adding new documents, or finding similar records are critical tasks.\r\n- Python enthusiasts interested in cutting-edge NLP techniques for extracting meaningful insights.\r\n\r\n[1] R. Biesbroek, S. Badloe, and I. Athanasiadis. Machine learning for research on cli-\r\nmate change adaptation policy integration: an exploratory uk case study. Regional\r\nEnvironmental Change, 20, 07 2020.\r\n\r\n[2] https://pypi.org/project/gensim/\r\n[3] https://github.com/MilaNLProc/contextualized-topic-models\r\n[4] https://github.com/ddangelov/Top2Vec\r\n[5] https://maartengr.github.io/BERTopic/index.html\r\n5 https://github.com/MilaNLProc/contextualized-topic-models", "code": "BJKSGK", "state": "accepted", "created": "2024-11-28", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Code review the right way", "abstract": "Code review is central part of everyday developer job. The motivation to create this talk was a quote:\r\n\u201cThe most important superpower of a developer is complaining about everybody else's code\u201d. In this talk I\u2019ll explain my approach to better code review.", "full_description": "Code review is central part of everyday developer job. The motivation to create this talk was a quote:\r\n\u201cThe most important superpower of a developer is complaining about everybody else's code\u201d. \r\nSometimes it\u2019s hard to convince a colleague about change or don\u2019t change some lines of code, in my talk I would like to cover some best practices from my software engineering experience about efficient and honest code review. How to apply automatic tools to improve code review routine of repetitive comments or suggestions. How to write/or reuse codign style guides for you team to reduce time of arguing about naming conventions and different styles.\r\nWhat\u2019s need to be automated and what\u2019s need to be not automated during code review. The key role of patterns which can be reusable to not confuse colleagues.", "code": "HWHNGC", "state": "withdrawn", "created": "2025-01-03", "speaker_names": "Andrii Soldatenko", "track": "PyData: Research Software Engineering"}, {"title": "Predicting the Future for Industrial IoT with Statistical and Machine Learning models", "abstract": "As the Industrial Internet of Things (IIoT) continues to revolutionise manufacturing, energy, automotive and supply chain industries, the ability to predict future outcomes, detect anomalies and optimize operations is critical. The challenge of prediction in IIoT is collecting, consolidating and analysing high throughput time series data to detect patterns. By investing in Python with its time series libraries and tooling, manufacturing companies can benefit by improving operational efficiency and worker safety.", "full_description": "This session will be an introduction to working with time series data to enable predictive analytics and forecasting in IIoT by leveraging statistical and machine learning models. \r\n\r\nWe will demystify time series data, common use cases and challenges in IIoT environments and go over why predictive models are crucial. Attendees will learn about the structure of sensor and IoT data in industrial settings, basic time series forecasting methods like Moving Averages, ARIMA (AutoRegressive Integrated Moving Average), Exponential Smoothing and an overview of common ML models and evaluation metrics for predicting trends in time series data.", "code": "QACQEW", "state": "submitted", "created": "2025-01-05", "speaker_names": "Tun Shwe", "track": "General: Others"}, {"title": "Observability Matters: Empowering Python Developers with OpenTelemetry.", "abstract": "Have you ever experienced your favorite social platform going down? It\u2019s frustrating, right? As users, we feel the inconvenience immediately. To prevent these disruptions, engineering teams rely on various tools and practices to address unexpected system behavior. In today\u2019s distributed world, applications are often segmented into microservices to improve agility and performance. This also brings added complexity, with more opportunities for things to go wrong - errors, latency and more. In this dynamic landscape, observability has evolved from a mere buzzword into a crucial approach for modern reliable systems. We initially gravitate towards a tool for its ease of use, but later after investing significant amount of time and finances, if we choose to migrate, vendor lock-in becomes a major concern. This is where the importance of open standards like OpenTelemetry become essential.\r\n\r\nWhile many think of observability as solely an issue for SREs, but it\u2019s increasingly a developer concern. During this session, we\u2019ll explore why observability matters and how it is evolved over the years. We'll cover how to instrument Python code, process telemetry data and export it seamlessly for analysis. Attendees will leave with a clearer understanding of how observability increases confidence in managing complex systems and how adopting open standards improve the overall developer experience. Let\u2019s work together to redefine OSS observability, as a shared responsibility through collaboration.", "full_description": "During this session, I\u2019ll cover the following key topics:\r\n\r\n- The evolution of observability and why it has become essential over the past few decades.\r\n\r\n- A deep dive into core telemetry signals: Logs, Metrics, and Traces.\r\n\r\n- The challenges developers face with modern observability, and how adopting OpenTelemetry can address these issues.\r\n\r\n- A comparison between traditional approaches and the OpenTelemetry method, focusing on everything from easy Python code instrumentation to advanced data processing and seamless telemetry export to any observability backend\u2014without vendor lock-in.\r\n\r\n- Different sampling techniques like head, tail, probabilistic, allow you to have full control over the data you ingest and its associated costs with different setup strategies.\r\n\r\n- Best practices for getting started with OpenTelemetry, including a live code demo.\r\n\r\n- Future of AI in the of open-source observability and ongoing work on OpenTelemetry with eBPF.", "code": "UDSJME", "state": "submitted", "created": "2024-12-30", "speaker_names": "Yash Verma", "track": "PyCon: Programming & Software Engineering"}, {"title": "Langfuse, OpenLIT, and Phoenix: Observability for the GenAI Era", "abstract": "Large Language Models (LLMs) are transforming digital products, but their non-deterministic behaviour challenges predictability and testing, making observability essential for quality and scalability.\r\n\r\nThis talk presents **observability for LLM-based applications**, spotlighting three tools: Langfuse, OpenLIT, and Phoenix. We'll share best practices about what and how to monitor LLM features and explore each tool's strengths and limitations. \r\n\r\nLangfuse excels in tracing and quality monitoring but lacks OpenTelemetry support and customization. OpenLIT, while less mature, integrates well with existing observability stacks using **OpenTelemetry**. Phoenix stands out in debugging and experimentation but struggles with real-time tracing.\r\n\r\nThe comparison will be enhanced by **live coding examples**.\r\n\r\nAttendees will walk away with an improved understanding of observability for **GenAI applications** and will understand which tool to use for their use case.", "full_description": "Large Language Models (LLMs) are becoming core components of modern digital products. However, their **non-deterministic nature** means that their behaviour cannot be fully predicted or tested before deployment. This makes **observability** an essential practice for building and maintaining applications with generative AI features.\r\n\r\nThis session focuses on observability in LLM-based systems.\r\n\r\nWe start by motivating why monitoring and understanding your application is key to ensuring quality, reliability, and scalability. We\u2019ll analyze three leading tools for observability in this domain: **Langfuse**, **OpenLIT**, and **Phoenix**. Each has unique strengths and challenges that make them suitable for different use cases.\r\n\r\nThrough examples and real-world scenarios, we\u2019ll explore:\r\n\r\n- How **Langfuse** provides detailed tracing and quality monitoring through developer-friendly APIs. While it supports multi-step workflows effectively, it lacks support for the OpenTelemetry protocol and can be difficult to customize for non-standard use cases.\r\n- Why **OpenLIT**, built on OpenTelemetry, offers strong observability for distributed systems. Although it is the least mature of the three tools, it integrates well with established observability stacks and has promising potential for future growth.\r\n- Where **Phoenix** fits into the process by combining experimentation and debugging capabilities with evaluation pipelines. Its strength lies in development-focused observability, but it has limitations in handling real-time tracing once systems are in production.\r\n\r\nThis talk will provide a clear, straightforward comparison of these tools, helping you understand which option best fits your LLM applications.\r\n\r\nYou\u2019ll leave with practical insights into how observability can enhance the reliability and performance of your generative AI systems.", "code": "HKYQDB", "state": "submitted", "created": "2024-12-22", "speaker_names": "Emanuele Fabbiani", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Quick and Easy: Understanding data without touching an IDE.", "abstract": "We demonstrate a pure Python solution for exploring and understanding datasets using state-of-the-art machine learning and explainable AI techniques. Our application features a reactive dashboard built with Shiny, specifically designed for the daily work of data scientists.\r\n\r\nThe tool provides insights into data rapidly and effortlessly through an interactive dashboard. It facilitates data preprocessing, interactive exploratory data analysis, on-demand model training, evaluation, and interpretation. It further renders dynamic, annotated, and interactive visualizations. This allows to pinpoint critical elements and relations as root causes in a haystack of features, compressing a full day's work into under an hour. \r\n\r\nUtilizing Plotly for dynamic visualizations, along with Scikit-learn, CatBoost, SHAP values, and MLflow for experiment tracking, married with shiny reactive dashboard, we facilitate quick and easy data preprocessing and exploration, model training and evaluation, together with explainable AI.", "full_description": "Problem Statement\r\nData scientists' daily work is characterized by a repetitive and time-consuming cycle of exploratory data analysis, preprocessing, model training, and feature identification. This ultimately means missing key insights into the data. Time spent on repetitive tasks detracts from critical work. We enable data scientists to focus on what matters.\r\n\r\nSolution\r\nWe streamline the data analysis process to facilitate efficient dataset exploration and uncovering critical insights without time spent on coding. We empower users to seamlessly conduct data preprocessing, interactive exploratory analysis, on-demand model training, evaluation, and interpretation, reducing the time to understand a dataset to under an hour.\r\n\r\nDemonstrator\r\nOur pure Python application features a reactive dashboard. It allows users to engage with data\u2014uploading, manipulating, creating interactive visualizations, performing on-demand model training and interpretation, while tracking results in MLflow. We demonstrate how to quickly deliver insights and identify root causes.\r\n\r\nArchitecture/Technical Implementation\r\nOur application is built entirely in Python, utilizing the Shiny framework for a reactive dashboard. The backend uses Plotly, Scikit-learn, CatBoost, SHAP values, and MLflow. We highlight the core functionalities and development choices, emphasizing data preprocessing, model training, evaluation, and explainable AI features.", "code": "VG8BNG", "state": "submitted", "created": "2024-12-20", "speaker_names": "Simone Lederer", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Does ideal time management exist?", "abstract": "What if your tasks organization is optimized just like a piece of software? In this talk, I will review popular techniques as algorithms \u2013 each with its own CPU, RAM, overhead and potential catastrophic performance issues (O(n!)). \r\n\r\nI will dive into algorithmic features of each productivity system, exploring what works, what doesn\u2019t, and when you should garbage collect your tasks. By the end, you\u2019ll see that just like your engineering system, your productivity methods need regular tuning or even debugging.", "full_description": "Is Getting Things Done the best way to organize your time? It may be, but it may not be. It depends. \r\n\r\nThere is no perfect way to organize your time. Each method has its pros and cons. You should pick one and adapt it to particular situation. \r\n\r\nJust like you pick an algorithm for a particular use case. Because each productivity technique can be considered as an algorithm, with its own CPU and MEM complexity, with a storage which has ridiculously high access times, etc. \r\n\r\nIn this talk, I will review popular ways of organizing your time (mostly more or less distant variants of GTD), and will present their algorithmic features, when they work well and when they should be avoided. \r\n\r\nThe question that looms over all of this is: \u201cAre we actually the subject of proper system design?\u201d\r\n\r\n## OUTLINE\r\n\r\n* Big promise: Getting Things Done \u2013 the mother of all productivity techniques.\r\n* The Holy Grail of all methods - you just know what\u2019s your next task, and your list is self organising.\r\n* The Human as a CPU unit with RAM and storage\r\n* Is your internal GC efficient enough?\r\n  * Why GC at all?\r\n  * When does paper as a storage win?\r\n* Menu picking vs. The Optimal List\r\n* Big picture vs zoom in\r\n* Naive methods we\u2019re born with\r\n* Beware of O(n^2) or even O(n!)\r\n* How can you tell if the method you\u2019re using is the wrong one?\r\n  * how avoiding procrastination can become another foirm of procrastionation?\r\n* GTD-based methods\r\n* Scrum / Kanban\r\n* Summary\r\n  * pick your methods\r\n  * don\u2019t be afraid to change them frequently", "code": "YUXVVW", "state": "submitted", "created": "2024-12-22", "speaker_names": "\u0141ukasz Mach", "track": "General: Education, Career & Life"}, {"title": "Building an AI Virtual Assistant for Literature Reviews: A Journey Towards Accelerating Research", "abstract": "In this talk, I\u2019ll share the journey of building an AI-powered virtual assistant for literature reviews, developed for a European agency.\r\nUsing LlamaIndex, OpenAI, Claude+, and other models, the tool helps researchers quickly synthesize insights.\r\nIt\u2019s been applied to assess AI\u2019s labor market impact, redefining research efficiency.", "full_description": "This talk chronicles the development of an AI-powered virtual assistant designed to revolutionize literature reviews for a European agency.\r\nLeveraging LlamaIndex, OpenAI, Claude, and other advanced models, the tool streamlines the process of synthesizing research, providing researchers with rapid \r\nand accurate insights. The assistant has been applied to assess AI's impact on the labor market, enabling comprehensive reviews of \r\nacademic and grey literature.\r\nThis innovative approach not only accelerates the literature review process but also enhances the quality and relevance of research outputs,\r\nsupporting policymakers and researchers in addressing complex questions with efficiency.", "code": "A9Y7M8", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Generative AI"}, {"title": "Feature Flags: Deploy to some of the people all of the time, and all of the people some of the time!", "abstract": "Feature flags activate features for some users whilst hide them for others. They help you deploy code more often, work more collaboratively, get early feedback from colleagues and customers and separate deployment from feature activation.  This talk will explain: \r\n* What are feature flags ?\r\n* How to use them\r\n* Implementing Feature Flags in Django\r\n* Django-waffle: a fully-featured feature-flag library for Django", "full_description": "Feature flags activate features for some users whilst hide them for others. They help you deploy code more often, work more collaboratively, get early feedback from colleagues and customers and separate deployment from feature activation - no more panicking because the ci is broken and there is an urgent need to deploy or hide a feature !\r\n\r\nAt Acernis we have adopted feature flags this year as part of a move towards trunk-based development. They have helped our team to merge code continuously into a shared branch, deploying updates for customers multiple times per week and testing new features with selected customers before deploying them for all customers.  We started with a simple front-end implementation in React before adopting Django-waffle in our backend for a more complete solution.\r\n\r\nThis talk will explain:\r\n\r\n* What are feature flags ?\r\n* How we use them in trunk based development to accelerate feature development, working more closely as a team\r\n* Implementing Feature Flags in Django and React\r\n* Django-waffle: a fully-featured feature-flag library for Django which integrates well with the standard Django user model and groups.  I have been contributing to maintaining and the library and updating its documentation this year.", "code": "7Y8L37", "state": "submitted", "created": "2024-12-10", "speaker_names": "Graham Knapp", "track": "PyCon: Django & Web"}, {"title": "Signal Detection in Spectral Images", "abstract": "In this talk, I present our work utilizing YOLO architectures for the task at hand and discuss the challenges involved with a severely limited amount of user-annotated data. The framework used for model training and evaluation is MMYOLO, and we will also touch upon the quality metrics.", "full_description": "Learn how to use modern computer vision models for the task of identifying different signals within a spectral image in real time. In this talk, I present our work utilizing YOLO architectures for the task at hand and discuss the challenges involved with a severely limited amount of user-annotated data. The framework used for model training and evaluation is MMYOLO, and we will also touch upon the quality metrics. The system is part of a real-time signal detection process used for scanning private networks.", "code": "AXYDWE", "state": "submitted", "created": "2024-12-17", "speaker_names": "Dr. Mirza Klimenta", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Bias Meets Bayes: A Bayesian Perspective on Improving Model Fairness", "abstract": "Bias in machine learning models remains a pressing issue, often disproportionately affecting the most vulnerable groups in society. This talk introduces a Bayesian perspective to effectively tackle these challenges, focusing on improving fairness by modeling and addressing bias directly.\r\nYou will learn about the interplay between uncertainty, equity, and predictive accuracy, while gaining actionable insights to improve fairness in diverse applications. Using a practical example of a risk-scoring model trained on data with underrepresented minority groups, I will showcase how Bayesian methods compare to traditional techniques, demonstrating their unique potential to mitigate bias while maintaining performance.", "full_description": "Machine learning models often perpetuate biases that exacerbate societal inequities, particularly for vulnerable groups. As machine learning increasingly shapes critical decisions, addressing these biases is more important than ever. In this talk, I will explain how Bayesian methods offer a principled and effective approach to improving fairness by directly addressing bias and incorporating uncertainty into machine learning models.\u2028\r\n\r\nThe talk will cover:\r\n\r\n1.\tTheoretical Foundations: I will start by exploring the connection between Bayesian statistics, fairness, and accuracy, with a focus on why uncertainty is a crucial factor in fairness interventions.\r\n2.\tPractical Example: Using a risk-scoring model trained on a dataset with underrepresented minority groups, I will demonstrate how Bayesian methods compare to traditional fairness techniques. This example will illustrate their ability to not only mitigate bias but also adapt to complex, real-world data distributions while maintaining predictive accuracy.\r\n3.\tKey Insights and Applications: Finally, I will provide actionable takeaways on incorporating Bayesian thinking into existing workflows, enabling more equitable and robust outcomes across diverse applications.\u2028\r\n\r\nThis talk is designed to be accessible to a broad audience. While minimal familiarity with machine learning concepts and fairness principles is recommended, no advanced knowledge of statistics is required. Attendees will leave with practical tools, code examples, and insights to address bias effectively in real-world scenarios, empowering them to promote fairness in their own projects and organizations.", "code": "ER3V7W", "state": "accepted", "created": "2024-12-20", "speaker_names": "Vince Nelidov", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "The art of yield", "abstract": "Can you imagine a python project without any return? \r\nIs it overhead or memory saving? Is it complicated or would it reduce complexity? Is it testable or a horror for unittesting?\r\n\r\nThe general idea of this talk is: to effectively use generators in code, we need to change our programming style, and it's not too easy, but it's possible. During my talk I will convert functions and methods from the project into generators, and we will see what is effective, what is not, and where it is still better to use returns.", "full_description": "This talk is a quintessence of experience in python projects built exclusively on generators. Generators don't speed up the project directly, but they open up the possibility of lazy data processing, which in turn reduces memory consumption and can improve the performance of the project in general. Thanks to changes in recent versions of Python, the advantages of generators over common functions are becoming significant in every modern high-load data consumption project.", "code": "8CJVUR", "state": "submitted", "created": "2025-01-05", "speaker_names": "Maxim Danilov", "track": "PyCon: Programming & Software Engineering"}, {"title": "A Short History of Computer Vision", "abstract": "Let's embark on a short journey through more than 3 decades of Computer Vision.\r\n\r\nStarting from LeNet, we will mingle with Viola & Jones, before moving on to Convolutional Neural Networks and Vision Transformers. Which architecture is better? Let's find out together! \r\n\r\nLast but not least we will look at Vision-Language Models and their amazing capabilities. All this is topped by a look into our crystal ball, to see where the journey might take us.\r\n\r\nBy the end of this talk, you'll have a deeper understanding of the key developments that have shaped the field of Computer Vision. You'll also gain a sense of the exciting possibilities that lie ahead, and how we can all contribute to the next chapter in the story of Computer Vision.\r\n\r\nThe presentation is suited for all skill levels, you don't need any specific Machine Learning or Computer Vision knowledge (although it might help to get some jokes).", "full_description": "Have you ever heard of the Neocognitron, the Swarovski AX Visio, Capsule Networks or the only alpine parrot of the world?\r\n\r\nJoin our little trip trough time, to make sure you can answer to all the above (and more) with a certain YES!\r\n\r\nWe will look at the roots of today's powerful CV models, discover the \"Big Bang\" of modern Deep Learning and have a look what the future holds.\r\n\r\nIn more Detail: \r\nStarting from LeNet, we will mingle with Viola & Jones, before moving on to Convolutional Neural Networks and Vision Transformers. Which architecture is better? Let's find out together! Last but not least we will look at Vision-Language Models and their amazing capabilities. All this is topped by a look into our crystal ball, to see where the journey takes us.\r\n\r\nBy the end of this journey, you'll have a deeper understanding of the key developments that have shaped the field of Computer Vision. You'll also gain a sense of the exciting possibilities that lie ahead, and how we can all contribute to the next chapter in the story of Computer Vision (although it might help to get some jokes).\r\n\r\nThe presentation is suited for all skill levels, you don't need any specific Machine Learning or Computer Vision knowledge.\r\n\r\nApproximate Timing:\r\n3 min -Intro\r\n7 min - Classical Computer Vision\r\n10 min - Convolutional Neural Networks\r\n10 min - Transformers\r\n10 min - Multimodality\r\n5 min - The Future", "code": "3HWSAF", "state": "submitted", "created": "2025-01-05", "speaker_names": "Johannes Kolbe", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Getting into Computer Vision from Ground to Deployment", "abstract": "This talk aims to guide attendees through the entire journey of building a Computer Vision system, from the very basics to deploying it as a functional web application using Flask and Docker. Whether you're new to Computer Vision or looking to strengthen your skills, this session will provide hands-on experience with foundational concepts, algorithms, and tools. Starting with image processing, we will progress through essential techniques like object detection, classification, and segmentation, leveraging popular libraries such as OpenCV and TensorFlow. The talk will conclude with a step-by-step guide to deploying a fully functional Computer Vision model in a real-world application using Flask and Docker, empowering you to integrate AI models into production environments with ease.", "full_description": "In this comprehensive session, we'll take you from the ground up in the exciting field of Computer Vision. We will cover the foundational concepts and techniques that power image recognition, including:\r\n\r\nBasic image processing techniques\r\nKey algorithms like edge detection, feature matching, and color segmentation\r\nDeep learning-based methods for object detection and classification\r\nUse of popular libraries such as OpenCV, TensorFlow, and Keras\r\nThe talk will be interactive, with live coding examples that show you how to build a model from scratch, train it, and test its performance. Attendees will gain experience with practical applications of these techniques, enabling them to work on real-world problems in fields like healthcare, security, and retail.\r\n\r\nAfter learning the basics of Computer Vision, the session will shift to deployment. We will cover how to:\r\n\r\nDevelop a simple web application using Flask to serve the trained model\r\nDockerize the application for easier deployment and scalability\r\nDeploy the system to a cloud or local server, making it accessible for real-time predictions\r\nBy the end of this talk, attendees will have a comprehensive understanding of how to take a Computer Vision model from idea to production, with the knowledge and skills to deploy their own models in real-world scenarios. Whether you're looking to enhance your career or start building your own AI-driven projects, this session will equip you with the tools and insights you need to succeed.", "code": "HSCRKT", "state": "submitted", "created": "2024-12-13", "speaker_names": "Daniel Samuel Etukudo", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Navigating the Ethical Landscape of AI in Python Development", "abstract": "As AI becomes increasingly integrated into our lives, the ethical implications of its development and deployment cannot be overlooked. This talk provides an accessible framework for Python developers to identify and address ethical challenges in AI projects. From mitigating bias in data to ensuring transparency and accountability in AI systems, attendees will gain practical insights and tools to align their work with ethical principles.", "full_description": "Understanding AI Ethics: Core principles of fairness, transparency, accountability, and privacy.\r\n\r\nBias in AI: Recognizing and mitigating bias in datasets and algorithms.\r\n\r\nBuilding Transparent Systems: Strategies for explainability in AI models.\r\n\r\nResponsible Development Practices: Practical tips for Python developers to integrate ethics into the development lifecycle.\r\n\r\nFuture Trends: Ethical considerations for emerging AI technologies.", "code": "ZALJUJ", "state": "submitted", "created": "2024-12-22", "speaker_names": "Olaleye Aanuoluwapo Kayode", "track": "General: Ethics & Privacy"}, {"title": "Keynote #1", "abstract": "Please add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.", "full_description": "Please add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.\r\nPlease add here your text.", "code": "UZBSJ7", "state": "submitted", "created": "2025-01-27", "speaker_names": "", "track": "Plenary"}, {"title": "RAG is not Enough! Consider CausalRAG", "abstract": "In the rapidly evolving financial technology landscape, combining Causal AI with Retrieval-Augmented Generation (RAG) offers a transformative approach to enhancing Large Language Models (LLMs). This methodology integrates causal inference techniques like do-calculus and counterfactual reasoning into the RAG pipeline, enabling more accurate financial predictions and deeper insights into market dynamics. By explicitly modeling cause-and-effect relationships, this framework improves model transparency and interpretability\u2014critical for meeting regulatory demands and mitigating risks associated with \"black box\" AI. Applications in risk assessment, portfolio optimization, and economic forecasting highlight its potential to drive more informed and reliable financial decision-making.", "full_description": "1. Introduction\r\n\r\n>Overview of the Financial Technology Landscape\r\n-The rapid evolution of financial technology and the increasing complexity of financial decision-making.\r\n-Need for more robust, interpretable, and accurate AI solutions.\r\n\r\n>Limitations of Current AI Techniques in Finance\r\n-Challenges with Retrieval-Augmented Generation (RAG) in capturing intricate causal relationships.\r\n-Importance of addressing these limitations for effective financial modeling and decision-making.\r\n\r\n2. Concept Overview: Causal AI + RAG\r\n\r\n>Definition and Integration\r\n-Introducing the concept of integrating causal AI techniques into the RAG pipeline.\r\n-Bridging domain-specific knowledge with explicit cause-and-effect modeling.\r\n\r\n>Core Causal Techniques\r\n-Do-Calculus: Establishing causal effects by simulating interventions.\r\n-Counterfactual Reasoning: Exploring \"what-if\" scenarios to assess alternative outcomes.\r\n-Causal Discovery Algorithms: Identifying underlying causal structures in financial datasets.\r\n\r\n3. Key Benefits\r\n\r\n>Improved Accuracy\r\n-Enhanced financial models by capturing causal relationships.\r\n-More reliable predictions for market dynamics and economic policy impacts.\r\n\r\n>Greater Interpretability\r\n-Transparent modeling of cause-and-effect relationships to identify key market drivers.\r\n-Clearer insights into the reasoning behind predictions and recommendations.\r\n\r\n>Alignment with Regulatory Demands\r\n-Increased transparency to meet compliance standards.\r\n-Mitigating legal and reputational risks associated with opaque AI systems.\r\n\r\n4. Applications in Financial Decision-Making\r\n\r\n>Systemic Risk Analysis\r\n-Understanding interconnected risks across markets using causal relationships.\r\n\r\n>Portfolio Optimization\r\n-Utilizing causally informed insights to improve portfolio performance with confidence.\r\n\r\n>Algorithmic Trading\r\n-Enhancing trading strategies by modeling causal effects in market dynamics.\r\n\r\n>Economic Forecasting\r\n-Providing more robust and explainable predictions of policy impacts on economic trends.\r\n\r\n5. Technologies to Be Used\r\n\r\n>Causal AI Techniques\r\n-Tools for implementing do-calculus and counterfactual reasoning.\r\n-Algorithms for causal discovery tailored for financial data.\r\n\r\n>RAG Integration Framework\r\n-Retrieval systems for domain-specific financial knowledge.\r\n-Hybrid pipelines combining retrieval and causal inference for enhanced decision-making.\r\n\r\n>AI Transparency Tools\r\n-Frameworks for explainability and interpretability in AI systems, ensuring insights are accessible to stakeholders and regulators.\r\n\r\n6. How This Enhances Transparency and Explainability\r\n\r\n>Explicit Cause-and-Effect Modeling\r\n-Moves beyond correlation-based methods to causal insights.\r\n-Clear understanding of how inputs drive predictions and recommendations.\r\n\r\n>Improved Stakeholder Confidence\r\n-Financial analysts and decision-makers gain trust in AI-driven insights.\r\n-Aligns with increasing demand for transparent AI in regulated industries.\r\n\r\n>Regulatory Compliance and Risk Mitigation\r\n-Ensures adherence to evolving standards for transparency.\r\n-Reduces the risks of black-box AI failures and associated consequences.\r\n\r\n7. Expected Impact and Future Directions\r\n\r\n>Quantitative Improvements\r\n-Demonstration of accuracy gains and enhanced interpretability through real-world applications.\r\n-Metrics to measure the effectiveness of the combined Causal AI + RAG framework.\r\n\r\n>Transformative Potential\r\n-Positioning Causal AI + RAG as the next frontier in financial technology.\r\n-Equipping financial analysts with a powerful toolkit for tackling complex challenges in an evolving landscape.\r\n\r\n>Broader Implications\r\n-Expanding applications beyond finance into other domains requiring explainable and transparent AI.\r\n\r\n8. Key Takeaways\r\n- Causal AI and RAG provide a transformative approach to financial modeling, enabling both accuracy and interpretability.\r\n- Explicit causal modeling enhances transparency, supporting regulatory compliance and stakeholder trust.\r\n- This framework equips financial analysts to make more informed decisions in a rapidly evolving and complex industry.", "code": "88QBBC", "state": "submitted", "created": "2025-01-05", "speaker_names": "Jayeeta Putatunda", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "How to create effective data visualizations", "abstract": "What distinguishes a lousy plot from a beautiful chart that communicates insights effectively? This talk will show you the underlying principles of good data visualization, offer lots of practical tips and tricks and give an overview of the data visualization landscape in Python. \r\nAfter the talk, you will be able to create better charts, whether for exploring your own data or for communicating results to others.", "full_description": "In this talk, you will learn about:\r\n\r\n- **Fundamental principles** of data visualization\r\n  - The Grammar of Graphics\r\n  - Visual hierarchy\r\n  - Data storytelling\r\n- **Best practices** regarding:\r\n  - Which colors to use\r\n  - Visual comparability \r\n  - Pros/cons of several chart types\r\n  - Context and audience: Adding text and annotations\r\n- The **data visualization landscape in Python**\r\n  - What libraries exist: matplotlib, plotly, altair etc., including add-ons and lesser-known ones\r\n  - What are their differences and strengths?\r\n  - Which library is suited for which usecase?\r\n\r\n\r\nEquipped with the knowledge presented in this talk, you will understand why certain charts are more aesthetically pleasing and more effective at conveying information than others. Apply the shown principles, take into account best practices and choose the right tools in Python to create more beautiful and impactful data visualizations.", "code": "BM9HSD", "state": "submitted", "created": "2024-12-02", "speaker_names": "Dominik Haitz", "track": "PyData: Visualisation & Jupyter"}, {"title": "Practical Python/Rust: Building and Maintaining Dual-Language Libraries", "abstract": "Building performant Python often means reaching for C extensions. This talk explores an alternative: leveraging Rust to create blazing-fast Python modules that also benefit the Rust ecosystem. I will share practical strategies from building `semantic-text-splitter`, a library for fast and accurate text segmentation used in both Python and Rust, demonstrating how to bridge the gap between these two languages and unlock new possibilities for performance and cross-language collaboration.", "full_description": "Building performant Python often means reaching for C extensions. But what if you could achieve similar performance with Rust, while also creating a library usable directly within the Rust ecosystem? This talk explores how Rust can be a powerful ally, creating blazing-fast Python modules that benefit both communities. I will share the strategies I use while building and maintaining my package, `semantic-text-splitter`, used for fast and accurate text segmentation, which sees significant usage in both Python and Rust ecosystems.\r\n\r\nSome key challenges arise when bridging the gap between these two languages, such as bridging the gap between Rust's generics and Python's dynamic typing, managing data representation and memory across the Python/Rust boundary, and maintaining type hints and documentation across both languages.\r\n\r\nBut with practical maintenance strategies, these challenges can be overcome. Moreover, you contribute to a growing ecosystem of high-performance Python tools powered by Rust. Join me to learn how to build and maintain dual-language Python/Rust libraries, and discover how this approach can unlock new possibilities for performance and cross-language collaboration.", "code": "CUJMCD", "state": "submitted", "created": "2025-01-05", "speaker_names": "Ben Brandt", "track": "General: Rust"}, {"title": "Beyond Spans and Traces: Advanced Python Observability with OpenTelemetry", "abstract": "This talk dives into the latest developments in OpenTelemetry's libraries for python. Attendees will gain practical insights into integrating OpenTelemetry to capture logs and metrics, moving beyond basic tracing. The session is designed for developers, SREs, and DevOps professionals familiar with OpenTelemetry basics, offering practical examples and open-source visualization tools to build a comprehensive observability framework. Equip yourself with the skills to capture a complete, real-time picture of your system's health and performance, ensuring a robust monitoring strategy for your Python applications.", "full_description": "This talk explores advanced observability in Python using OpenTelemetry, focusing on logs and metrics. We will introduce effective logging and metric collection in Python applications, providing a comprehensive understanding of application behavior and performance.\r\n\r\nAttendees will learn through practical examples how to integrate OpenTelemetry for logs and metrics, moving beyond basic tracing to achieve a more holistic view of their applications. The session will also share examples of using open source tools for visualization.\r\n\r\nGeared towards developers, SREs, and DevOps professionals familiar with OpenTelemetry basics, this talk equips attendees with the skills to build a robust observability framework in their Python applications, capturing a complete, real-time picture of their system's health and performance.", "code": "WE3DDG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Yosef Arbiv", "track": "PyCon: MLOps & DevOps"}, {"title": "AI-driven DataOps: Building data products with a taste of the future", "abstract": "In the modern data and open source world, you would have to live under a stone or have someone cut off the electricity in your apartment to avoid any serious engagement with data and the AI experience. Periodically it is excellent, and sometimes unpleasant; you know the drill.  \r\nLet me show you how we embrace a **DevSecOps** culture to move swiftly in a rapidly growing environment. I am overjoyed to share my experience.  \r\nIn brief: let's see how **DataOps** helps us to survive in the modern data world. I promise a nice mix of data, **DevSecOps**, and a few seeds of **AI**.\r\n\r\nThis story is not incomplete without **Python**, and let's hear why it is our secret sauce for building success in the **Data World**.", "full_description": "In the modern **Data** and **Open Source** world, you should live under the stone or have someone cut off electricity in your apartment to avoid any serious touch of the **Data** and **AI** experience. Periodically it is excellent, and sometimes it is nasty, you know the drill. \r\n\r\nLet me show you how we embrace a DevSecOps culture to move fast in a rapidly growing environment. Happy to share my experience and discuss questions:\r\n1. What are the vital points you should stick with to provide trusted data on time for your users quickly and reliably?\r\n1. Providing a walkthrough of creating, improving, and scaling data products using a modern DevSecOps stack in the open-source world. \r\n1. Exposing details of the use case and how we embrace the open-source philosophy to help us provide faster time to market. \r\n1. How **Python** and **AI**  can empower your Data journey\r\n\r\nIf you are in the data world, **DataOps** is the next big thing with you, along with a touch of AI. We should reveal it together. Of course, nothing will be impossible with a huge influence of Python, as each and every component in our system is loomed with it.\r\n\r\n\r\nI promise - no fluff, no buzzwords. Solely a first-face story from the GitLab Data Team member, created from sweet and pain from the world leader of DevSecOps platform creator and AI-based tool, remote work champion and one of the most transparent company in history.", "code": "DTWHFQ", "state": "submitted", "created": "2024-12-25", "speaker_names": "Radovan Ba\u0107ovi\u0107", "track": "PyData: Data Handling & Engineering"}, {"title": "Enterprise-Ready FastAPI: Beyond the Basics", "abstract": "FastAPI is excellent for building services quickly, but sometimes we need to scale it further. To increase the joy of it, I\u2019ll share our experience designing larger applications, improving the structure of monoliths, and handling API/database migrations. We\u2019ll discuss usage Depends vs. DI containers, straightforward ways to implement tests (unit to acceptance), and how to configure logging/tracing for better telemetry. Plus, I\u2019ll touch on deployment strategies.\r\n\r\nI aim for this talk to offer developers and leads, even those using different frameworks, practical insights on ensuring better testability and observability in services.", "full_description": "FastAPI is excellent for building services quickly, but scaling it for enterprise applications introduces new challenges. In this talk, I\u2019ll share lessons from designing larger applications and managing API/database migrations. We\u2019ll explore testing strategies from unit to acceptance, compare FastAPI\u2019s Depends vs. DI containers, and discuss logging, tracing, and deployment practices. \r\n\r\nI aim for this talk to offer developers and leads, even those using different frameworks, practical insights on ensuring better testability and observability in services.", "code": "LDQXUY", "state": "submitted", "created": "2024-11-30", "speaker_names": "Alexander Ptakhin", "track": "PyCon: Django & Web"}, {"title": "Set-Based Design in Python with 'Ports-and-Adapters' Architecture Pattern", "abstract": "Recent Python features make it easier to apply Set-Based Design practices, allowing teams to defer key infrastructure and framework decisions until later stages, guided by empirical data. This talk shows how the Ports-and-Adapters pattern enables systematic, controlled adoption of these practices.", "full_description": "Early-stage software projects often face uncertainty when choosing infrastructure stacks and Python frameworks. Decisions like SQL vs. NoSQL, REST vs. GraphQL, or message brokers are influenced by unknowns, evolving constraints, and regulatory requirements. Premature commitments can lead to technical debt, inflated costs, or unmet security needs.\r\n\r\nThis talk demonstrates how Set-Based Design practices, rooted in manufacturing, can be applied to software development using the Ports-and-Adapters architecture pattern. By isolating core logic from infrastructure choices, this pattern allows teams to defer critical decisions and evaluate options empirically through testing.\r\n\r\nAttendees will learn what the Ports-and-Adapters pattern is, how it enables systematic Set-Based Design, and how modern Python features simplify its implementation.", "code": "QGJGLJ", "state": "submitted", "created": "2025-01-02", "speaker_names": "Asher Sterkin", "track": "PyCon: Programming & Software Engineering"}, {"title": "Reinforcement Learning Without a PhD: A Python Developer\u2019s Journey", "abstract": "From watching AI conquer Super Mario to building production-ready Reinforcement Learning (RL) systems, this talk explores how Python developers can dive into RL without requiring advanced degrees or big tech resources. Drawing on our three-year journey of building a production RL system, I\u2019ll show how developers can leverage RL through practical strategies and accessible tools. Using pi_optimal, an open-source RL toolkit, we\u2019ll bridge the gap between cutting-edge RL research and real-world applications. Attendees will gain actionable insights, implementation techniques, and hands-on experience to confidently start their own RL projects.", "full_description": "Reinforcement Learning (RL) has achieved groundbreaking results, from dominating video games to optimizing business processes. Despite these successes, a considerable gap exists between RL research and practical, accessible implementation for everyday developers. This talk aims to bridge that gap with hands-on guidance and real-world insights.\r\n\r\nBased on our three-year journey of developing a production-grade RL system, we\u2019ll cover:\r\nTranslating research papers into functional code\r\n\r\n- Avoiding common RL pitfalls\r\n- Techniques to optimize resources without big budgets\r\n- Real-world case studies and key lessons learned\r\n\r\nOur experience culminated in the development of pi_optimal, an open-source library that simplifies RL implementation for Python developers. We\u2019ll showcase how pi_optimal makes RL accessible, enabling developers to apply RL principles without needing a PhD or extensive computational resources.\r\n\r\nWhat You\u2019ll Learn\r\n1. RL Fundamentals in Action\r\n1.1. Core concepts explained through an interactive Super Mario AI demo\r\n1.2. Understand RL components without diving into complex math\r\n2. From Theory to Production\r\n2.1. Step-by-step guide to implementing RL with pi_optimal\r\n2.2. A real-world case study illustrating successful application\r\n2.3. Solutions to common challenges and obstacles\r\n3. Practical Implementation Tips\r\n3.1. Strategies for resource optimization\r\n3.2. Best practices for testing and debugging RL systems\r\n3.3. Insights into deploying RL solutions in production\r\n\r\nLive Demonstration\r\nI will show the creation and training of a simple RL agent using pi_optimal. Experience real-time learning and discover how to apply these techniques to your own projects.\r\n\r\nWho Should Attend\r\n- Python developers exploring AI applications\r\n- Software engineers interested in RL systems\r\n- Technical leaders assessing RL\u2019s potential for their teams\r\n\r\nPrerequisites\r\n- Basic Python programming knowledge\r\n- A foundational understanding of machine learning concepts\r\n- No prior RL experience required\r\n\r\nKey Takeaways\r\n- A solid grasp of RL fundamentals\r\n- Access to open-source tools and starter code with pi_optimal\r\n- Strategies to implement RL in real-world scenarios\r\n- Resources for deepening your RL expertise\r\n\r\nJoin our community to democratise Reinforcement Learning and empower Python developers to tackle RL challenges!", "code": "TQLGA8", "state": "submitted", "created": "2024-12-20", "speaker_names": "Jochen Luithardt", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Bridging the gap: unlocking SAP data for data lakes with Python and PySpark via SAP Datasphere", "abstract": "SAP's data often remains locked away, hindering the creation of a complete data picture. This talk presents a hands-on proof of concept leveraging SAP Datasphere, Python and PySpark to bridge an Azure-based, data mesh-inspired open data lake with a centralized SAP BI environment. \r\n\r\nThis presentation will delve into the architecture of SAP Datasphere and its integration interfaces with Python. It will explore network integration, authentication, authorization and resource management options, as well as data integration patterns. The presentation will summarize the evaluated features and limitations discovered during the PoC.", "full_description": "In many enterprises relying on SAP ERP systems, a wealth of valuable master data remains trapped within a closed ecosystem. This creates significant obstacles when striving for a comprehensive, 360\u00b0 view, especially when integrating with modern, open data lakes built on platforms like Azure and designed around data mesh principles. This talk presents a practical PoC that tackles this challenge head-on, utilizing SAP Datasphere as the key integration point.\r\n\r\nOutline:\r\n\r\n1. The challenge: navigating sap's data silos and the pursuit of a unified view\r\n* The section outlines the enterprise data landscape of RATIONAL where valuable master data resides within SAP\u2019s traditionally closed ecosystem, hindering data democratization and the creation of a comprehensive, 360\u00b0 operational view. This scenario is quite common - at least for German manufacturing companies. This situation is frequently encountered, particularly among German manufacturers.\r\n* The inherent conflict between the open, distributed nature of data lakes (especially those built on data mesh principles) and the centralized, closed nature of traditional SAP BI environments is discussed.\r\n\r\n2. Solution overview: leveraging sap datasphere as the integration layer\r\n* An introduction to sap datasphere and its capabilities is provided, with a focus on its ability to connect with non-SAP systems.\r\n* This part explains how datasphere was chosen as the central integration layer for the proof of concept and its role in enabling bi-directional data flow between SAP and the open data lake.\r\n\r\n3. Architecture of SAP Datasphere\r\n* Introduction in architecture of SAP Datasphere and role of underlying SAP HANA database\r\n* Explanation of openSQL schema as key integration option\r\n\r\n4. Security first: exploring network integration, authentication and authorization options\r\n* This section details the evaluation of network connectivity options between the Azure services like Azure Databricks, PostgresQL, ADLS and SAP Datasphere\r\n* The methods used to authenticate Python and Pypark to SAP datasphere are explained\r\n* The implementation and evaluation of data authorization mechanisms within SAP Datasphere are described\r\n\r\n5. Python and PySpark integration\r\n* Available interfaces for python integration (ODBC/JDBC, OData), their features and limitations\r\n* Explanation of practical data integration patterns implemented within the poc for extracting data from sap and loading it into the data lake for full and delta load scenarios\r\n\r\n5. Reflecting PoC: summary and key learnings\r\n* This section summarizes the core findings and lessons learned from the PoC, particularly regarding security and software quality best practices\r\n* A hint for the SAP open data alliance launched in 2023\r\n\r\nMain takeaways:\r\n* An understanding of SAP Datasphere's architecture and its potential for integrating non-SAP, open-source technologies like Python and PySpark\r\n* Knowledge of current features and limitations of SAP Datasphere in the area of data integration with the open source world", "code": "7CL3KS", "state": "submitted", "created": "2024-12-22", "speaker_names": "Rostislaw Krassow", "track": "PyData: Data Handling & Engineering"}, {"title": "Modeling User Journeys with Markov Chains to Optimize Online Advertising \u2013 a case study", "abstract": "In this talk, we will explore how to use Markov chains to model user journeys on websites. By handling event-based user data from websites and aggregating them into user journeys, we can utilize Markov chains to calculate the contribution of each touchpoint during a website visit, leading to a lead submission or newsletter sign-up. The findings can help to better understand the impact of (sub-)pages visited during the user journey to achieve the intended goal.\r\nOur use case feeds back the results into advertising platforms to improve the performance of advertising campaigns. The audience will gain insights into the challenges faced during the transition from a Proof of Concept (POC) phase using custom Jupyter notebooks to a scalable web application solution that allows end-users to self-service their requests.", "full_description": "This proposal aims to provide an understanding of Markov chains and their application. Our Case Study shows how to apply the developed approach to optimize online advertising and the practical steps involved in developing the simple POC into an application. The entire system is based on the python ecosystem.\r\n\r\n1.\tIntroduction: Online Advertising and Real-Time Bidding\r\n\u2022\tOverview of online advertising and the mechanics of real-time bidding (RTB).\r\n\u2022\tDiscussion on optimizing online advertising campaigns.\r\n2.\tDetermine value of users for websites and advertising\r\n\u2022\tExplanation of how value can be expressed beyond traditional e-commerce metrics.\r\n\u2022\tImportance of leads and newsletter sign-ups for companies that do not sell products directly online.\r\n3.\tData Processing and Modeling\r\n\u2022\tTracking user interactions and preprocessing event-based data from website traffic.\r\n\u2022\tIntroduction to Markov Chains.\r\n\u2022\tApplying Markov chains to user behavior and modeling user journeys.\r\n\u2022\tCalculating the contribution of each touchpoint with the effect of removal.\r\n4.\tImplementation Steps and Challenges\r\n\u2022\tDetailed walkthrough of data preprocessing steps and common difficulties encountered.\r\n\u2022\tTransition from custom notebooks to a web app-based solution for end-user self-service.\r\n\r\nOur solution involves using open-source tools and libraries such as:\r\n\u2022\tPyspark / Pandas for data manipulation and preprocessing.\r\n\u2022\tNumpy for solving the underlying algebra.\r\n\u2022\tStreamlit for developing the web application.\r\n\r\nMain Takeaways\r\nBy the end of the talk, attendees will have learned:\r\n\u2022\tHow to preprocess and model event-based data from website traffic.\r\n\u2022\tThe application of Markov chains to model user journeys and calculate the contribution of touchpoints.\r\n\u2022\tThe challenges in scaling from a POC phase to a scalable web app solution.", "code": "N77Z9B", "state": "submitted", "created": "2025-01-05", "speaker_names": "Constantin Schmid", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "What does a data scientist do in a hospital?", "abstract": "Non-researcher data scientists are becoming more common in hospitals. What do they do and why should we expect to see even more of them in the future? And how does hospital data science differ from the data science done in the industry? In this talk I will take you through the emerging role of data scientist at a hospital and explain how it has been implemented in different places.", "full_description": "Data scientist is a new and growing role in many hospitals. However, to many outside of hospitals, it is not easy to understand what a data scientists might do in a hospital. \r\nIn my talk I will discuss \r\n- the reasons why data science is being implemented at hospitals all over Europe\r\n- why it is not enough to have researchers working with AI/ML for healthcare - also in-house data scientists are needed \r\n- what have been some of the biggest challenges being the first data scientist at a large public hospital and starting up this function from scratch\r\n- how I think the data science work is different between (public) hospitals and private companies\r\n- how the data science function has been organised in different hospitals, mainly focusing on examples from hospitals in Sweden and Unity Health Toronto, Canada", "code": "3EVQZV", "state": "submitted", "created": "2025-01-02", "speaker_names": "Dr Juulia Suvilehto", "track": "PyData: Research Software Engineering"}, {"title": "Simplifying RAG Development: An Open-Source Blueprint for Scalable and Customizable Systems", "abstract": "Retrieval-Augmented Generation (RAG) systems give users advanced interaction and analysis capabilities with unstructured data and are among the most prominent applications of Large Language Models (LLMs). While building or purchasing a basic RAG system is now straightforward, customizing it for specific use cases and deploying it at scale remains a significant challenge. \r\n\r\nThis talk introduces our open-source RAG blueprint designed to streamline configuration and customization, provide integrated monitoring tools, and enable scalability from prototypes to production. Additionally, it incorporates mechanisms for human feedback, LLM-based evaluation metrics and allows privacy-preserving setups.  \r\n\r\nUsing real-world examples, attendees will learn practical strategies for building open-source based RAG systems tailored to diverse needs, from lightweight knowledge bases to enterprise-grade applications. We will also compare open-source and commercial solutions, highlighting pitfalls, trade-offs, and key considerations in deciding whether to build or buy.", "full_description": "Retrieval-Augmented Generation (RAG) systems allow us to analyze and interact with documents and unstructured data in manifold ways and are one of the most prominent applications of Large Language Models (LLMs). Building or just buying your own RAG system has become easy, properly customizing or even packaging it into a usable data product, which is deployed to a productive setup remains challenging and time consuming. \r\n\r\nIn this talk, we introduce our open-source RAG blueprint designed to simplify development while offering full control over performance and resource usage. Our open-source blueprint streamlines RAG development by simplifying configuration and customization, providing integrated monitoring tools for better oversight and troubleshooting, enabling scalability from prototypes to production, and incorporating mechanisms for human feedback and LLM-based metrics for proper evaluation. \r\n\r\nOur blueprint uses the following components to create a cohesive framework: \r\n\r\n- LlamaIndex for document chunking and parsing \r\n- Embedding models from Hugging Face, which are then stored and retrieved via a QDrant vector database \r\n- User interaction is done through Chainlit, which provides a simple chat UI interface \r\n- For observability and monitoring, Langfuse is used to track system performance and user feedback \r\n- For evaluation, the Ragas framework is used to monitor faithfulness, relevancy and accuracy of the system.  \r\n\r\nThe blueprint is designed with flexibility in mind, making it straightforward to replace any component to adapt to specific requirements or preferences without significant effort. \r\n\r\nThrough practical examples and demonstrations, we will showcase strategies for building RAG systems tailored to diverse use cases, from lightweight knowledge bases to enterprise-grade applications.   Additionally, we will examine the advantages and limitations of open-source solutions compared to commercial offerings, addressing critical considerations such as cost, performance efficiency, and privacy.", "code": "8ZBZGK", "state": "submitted", "created": "2024-12-21", "speaker_names": "Piotr Kalota", "track": "PyData: Generative AI"}, {"title": "Structuring Unstructured Text using generative AI: The key to information extraction", "abstract": "How can generative AI transform natural language into structured formats, like converting free-form queries into SQL, while maintaining accuracy and consistency? This session tackles the challenges of using transformer-based models for structured extraction and introduces best practices for building fine-tuning datasets and leveraging Constrained Generative AI to eliminate syntactic errors. We will also explore preprocessing, data generation, and augmentation techniques that achieved near-perfect accuracy in extracting dates and times from text. The framework presented is versatile and applicable to extracting any structured data from unstructured text, empowering attendees to create more robust NLP solutions.", "full_description": "Extracting structured information from natural language is a fundamental task in many NLP applications\u2014whether it's converting natural queries to SQL, performing Named Entity Recognition (NER), extracting entity relationships for knowledge graphs, and more. While transformer-based generative AI models, such as GPT, show great promise in these areas, they often face challenges like format inconsistency and difficulty handling the diverse ways humans express themselves in language.\r\nIn this session, we\u2019ll explore these challenges in depth and present actionable solutions to overcome them. We\u2019ll introduce Constrained Generative AI, a powerful technique designed to minimize syntactic errors in structured outputs, ensuring that the generated content adheres to predefined formats. Additionally, we\u2019ll cover key pre-processing steps and data generation/augmentation strategies that helped us achieve near-perfect accuracy in a real-world case of extracting dates and times from textual data\u2014while still maintaining low latency with relatively small language models.\r\nBy the end of this session, attendees will have practical knowledge on how to leverage transformer models for structured output tasks, reduce errors, and enhance performance. You\u2019ll walk away with proven best practices and techniques that can be applied to boost the reliability and efficiency of your NLP applications.", "code": "7WJEFT", "state": "submitted", "created": "2024-12-03", "speaker_names": "Oren Matar", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Terraform CDK in Python: Where Infrastructure Meets Code", "abstract": "CLOUD, CLOUD, CLOUD!!! Organizations today turn to cloud solutions to reduce costs, accelerate delivery, and streamline operations. Yet, the question remains: how can Python developers thrive in this age of cloud? Enter the Cloud Development Kit (CDK) for Terraform, which empowers teams to build and manage infrastructure using Python in a Pythonic way. This session explores how to apply object-oriented principles to infrastructure-as-code to create cleaner, more maintainable, and testable IaC, while seamlessly managing multiple environments. We will also discuss how leveraging reusable functions and code blocks speeds up delivery timelines and simplifies the maintenance of large-scale projects, ensuring code consistency and reducing duplication. Additionally, we\u2019ll highlight best practices for Terraform CDK\u2014 including project organization, code reusability, version control strategies, and testing patterns\u2014to ensure scalable deployments and streamlined cloud management. By the end, attendees will understand how Python\u2019s familiar syntax and Terraform\u2019s infrastructure automation can be combined to reduce operational overhead, boost developer velocity, and ensure robust cloud environments across all stages of development.", "full_description": "This session is ideal for beginners or anyone looking to embark on their cloud journey. I\u2019ll begin by explaining the concept of Infrastructure as Code (IaC) and exploring the growing demand for tools that use familiar, developer-friendly syntax. From there, we\u2019ll dive into the Terraform CDK, discussing its key components and how to structure a production-ready environment. You\u2019ll learn why functions and code reuse are essential for maintainability, and we\u2019ll walk through real-world examples that highlight how the Terraform CDK can streamline your cloud journey.\r\n\r\n1. I will explain the concept of infrastructure as code and what tool exists, what are benefits of it, and Why we use it nowadays - 5 mins\r\n2. I will explain the Terraform CDK and how it can be used for it and what it is from traditional terraform\r\n3. Concepts, Architecture, Resourses, Function, creation of reusable resourses - 10 mins\r\n4. Example of infrastructure using Terraform CDK - 7 mins\r\n\r\nhttps://github.com/hashicorp/terraform-cdk", "code": "MFF33F", "state": "submitted", "created": "2024-12-22", "speaker_names": "Yuliia Barabash", "track": "General: Infrastructure - Hardware & Cloud"}, {"title": "\u27a1\ufe0f Start asking your data \u201cWhy?\u201d - A Gentle Intro to Causality", "abstract": "Correlation does not imply causation. It turns out, however, that with some simple ingenious tricks one can unveil causal relationships within standard observational data, without having to resort to expensive randomised control trials.", "full_description": "I introduce the basic concepts of causal inference demonstrating in an accessible manner using visualisations. My main message is that by adding causal inference to your statistical toolbox you are likely to conduct better experiments and ultimately get more from your data.\r\n\r\nE.g, by introducing Simpson\u2019s and Berkson's Paradoxes, situations where the outcome of a population is in conflict with that of its cohorts, I shine a light on the importance of using causal graphs to model the data which enables identification and managing confounding factors.\r\n\r\nThis talk is targeted towards anyone making data driven decisions. The main takeaway is the importance of the story behind the data is as important as the data itself.\r\n\r\nMy ultimate objective is to whet your appetite to explore more on the topic, as I believe that by asking data \u201cWhy?\u201d you will be able to go beyond correlation calculations and extract more insights, as well as avoid common misinterpretation pitfalls.", "code": "LNNRQB", "state": "submitted", "created": "2024-12-20", "speaker_names": "Eyal Kazin", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "OptAB - An optimal antibiotic selection framework for Sepsis patients using Artificial Intelligence", "abstract": "In this session we present OptAB, a data-driven online-updateable antibiotic selection model based on Artificial Intelligence for Sepsis patients considering side-effects. OptAB performs an iterative optimal antibiotic selection for real-world Sepsis patients aimed at minimizing the Sepsis-related organ failure score (SOFA-Score) as treatment success while accounting for nephrotoxicity and hepatotoxicity as serious antibiotic side-effects. OptAB provides disease course forecasts for (combinations of) the antibiotics Vancomycin, Ceftriaxone and Piperacillin/Tazobactam and learns realistic treatment influences on the SOFA-Score and the side-effect indicating laboratory values creatinine, bilirubin total and alanine-transaminase. OptAB is based on a hybrid AI differential equation algorithm and can handle the special characteristics of patient data including irregular measurements, a large amount of missing values and time-dependent confounding. OptAB's selected optimal antibiotics exhibit faster efficacy than the administered antibiotics.", "full_description": "This talk introduces the hybrid AI optimal treatment selection model OptAB from Wendland et al., recently published at npj Digital Medicine.\r\nhttps://pubmed.ncbi.nlm.nih.gov/39613924/\r\nThe code for OptAB is available at https://github.com/philippwendland/OptAB\r\n\r\nThe talk consists of the following sections:\r\n\r\n1. Motivation:\r\nIn this section we introduce personalized medicine, focus on challenges for disease course forecasting and introduce the specific challenges for antibiotic selection in Sepsis patients.\r\nOur research is focused on Electronic Health Records data. Typical challenges are: Very irregular data, a mixture of time-dependent and static variables and assimilating updated laboratory value measurements. \r\nSepsis is a severe dysregulation of the immune system, damaging own healthy tissue and organs. The choice of an initial antibiotic is challenging, because pathogen identification often lasts two days and in 30% to 70% of patients no pathogen can be detected at all. Consequently, treatment recommendations focus on broad-spectrum antibiotics, with little attention paid to side-effects.\r\n\r\n2. OptAB (Methods and implementation).\r\nThis section introduces OptAB, a data-driven online-updateable antibiotic selection model based on Artificial Intelligence for Sepsis patients considering side-effects. OptAB performs an iterative optimal antibiotic selection for real-world Sepsis patients aimed at minimizing the Sepsis-related organ failure score (SOFA-Score) as treatment success while accounting for nephrotoxicity and hepatotoxicity as serious antibiotic side-effects. We trained OptAB on the MIMIC-IV dataset, a a large deidentified dataset of patients admitted to the emergency department or an intensive care unit at the Beth Israel Deaconess Medical Center in Boston (https://physionet.org/content/mimiciv/3.1/). We focus on the antibiotics Vancomycin, Piperacillin/Tazobactam and Ceftriaxone. OptAB is implemented using PyTorch and torchcde. \r\n\r\n3. Results\r\nIn this section we will present the results of OptAB in the MIMIC-IV testdataset and an external testdataset from AmsterdamUMCdb. First, we present heatmaps of the disease course forecasts. Then, we provide evidence, that OptAB learns realistic treatment effects. OptAB's treatment recommendations lead to a faster reduction in SOFA-Score that the factual administered treatment and OptAB is able to prevent antibiotic-induced Acute Kidney Injury. Finally, we illustrate OptAB on one or two example patients.\r\n\r\n4. Conclusion\r\nThis section summarizes the main results, limitations and outlook for OptAB. Future work will focus on uncertainty quantification and enhancing interpretability of OptAB.\r\n\r\n5. References", "code": "PSJMTP", "state": "submitted", "created": "2024-12-20", "speaker_names": "Philipp Wendland", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Python Project Setup Streamlining", "abstract": "Each new Python development project requires an initial setup to ensure a solid scaffolding that supports common company standards, best practices and recurrent requirements. In this talk, we show how the process of setting up Python projects in general and especially Python data projects made for data scientists and data engineers can be streamlined and automated in order to speed-up the project ramp-up time, enforce standards as well as to simplify switching project contexts for individual developers.\r\n\r\nWe present the process and result on how we developed a common base template that can easily be specialized with incremental templates for specific needs of e.g. data engineers and data scientists. We discuss our technology evaluation and show the final implementation based on tools and Python packages such as copier, uv, ruff, commitizen, poethepoet and git. With our final solution a full setup from an empty project folder takes less than 4 minutes before the execution of the first commit and unit test instead of at least one hour with a manual setup. Within our talk you will learn how you can adopt our approach for your company environment to increase efficiency and standardization of future Python (data) projects.", "full_description": "Outline\r\n--------\r\n\r\nIn a professional industry environment, each new Python development project in the context of data engineering and data science requires an initial setup to ensure a solid scaffolding that supports common company standards, software engineering best practices and recurrent requirements. This becomes even more important in bigger organizations with security concerns, multiple departments and an increasing number of developers and scientists. By providing a solid project base you can avoid redundant work, unsynchronized technology stacks and configurations. Furthermore, spending effort on the standardization and simplification of the overall process allows to save time and money for all future projects.\r\n\r\nOur proposed talk will focus on the following aspects of the challenge described.\r\n\r\n-\tWhy do you need a common scaffolding for Python data projects?\r\n-\tRequirements we considered during our solution development\r\n-\tPossible solutions and alternatives\r\n-\tOur implemented approach\r\n-\tHow can you use or adapt our approach for your environment?\r\n\r\nIn the following, we provide some details for each of the planned sections.\r\n\r\nWhy do you need a common scaffolding for Python data projects?\r\n-----------------------------------------------------------------------\r\n\r\nAs an organization, you want your development force to be focused on actual development and value generation as much as possible by avoiding repetitive and distracting work while also keeping high engineering and security standards. \r\n\r\nDevelopers often work in multiple team constellations in parallel and need to switch between them frequently, especially in project-based businesses. Using a unified technology stack and structure across all projects significantly reduces the mental overhead for this switching process.\r\nA common scaffolding for Python projects also provides an explicit place to have discussions about standards and best-practices. It eliminates the need to have lengthy discussions at the start of every project by providing reasonable defaults for project structure and tooling, while still giving teams the freedom to modify these defaults on a per project basis, if needed.\r\n\r\nRequirements we considered during our solution development\r\n-------------------------------------------------------------------\r\n\r\nBefore we started to develop a particular approach, we collected process steps within our development lifecycle with potential for standardization as well as steps that occurred regularly during project setup and development.\r\n\r\nDevelopment lifecycle steps in our organization with high standardization potential were:\r\n- Choose a project structure\r\n- Select a dependency management tool\r\n- Setup the usual dependencies we use in projects\r\n- Setup testing\r\n- Setup linting, type checking and auto-formatting\r\n- Setup CI/CD\r\n- Write meaningful and standardized conventional commit messages\r\n- Implement a basic container image\r\n- Setup a data pipeline\r\n\r\nOur goal is to automate as many items from that list as possible and make the remaining as easy as possible. Moreover, it should be possible to propagate changes made to the standard to already existing projects using those standards and to allow for a set of common standards as well as task or project-specific standards, such as containerized images.\r\n\r\nPossible solutions and alternatives\r\n-------------------------------------\r\n\r\nThere are different options to avoid recurring efforts of setting-up a new Python project. \r\nA simple approach could be to work with mono repos, but they have several major drawbacks such as unnecessary coupling of code, conflicting dependencies and a general risk of losing proper project structure due to masses of files and folders. This is also generally challenging for organizations working for different clients, due to unwanted code sharing and other security concerns.\r\n\r\nAn alternative method to minimize setup time involves duplicating and pasting existing projects. Though feasible, this method also presents challenges such as unintentional copying of project-specific code that may compromise security. Additionally, configurations tailored to the original project might be overlooked, requiring adjustments, and documentation may not be updated for the new project. Like the mono-repo approach, it's difficult to uniformly implement changes in standards across duplicated projects.\r\n\r\nA third option is to use templates which set up the basic desired structure of the project, provide an environment setup and whatever additional tooling is desired as standard. Given the right tooling, they also allow to propagate changes to the template to already existing projects.\r\n\r\nOur approach\r\n---------------\r\n\r\nThe solution that we recommend is to rely on one or more project templates in connection with a rendering application that allows for updating existing projects and chaining of templates. \r\nWe provide a base template for elements which we assume to be universal across all Python projects in our organization. We also provide additional templates for specialized aspects which are not needed in every project. These templates can be applied after the base template and do not contain any duplicate information compared with the base template.\r\nIn our case, we decided to use copier as templating engine and self-developed Jinja-based project templates. copier  instantiates a new project off one or more templates and additionally connects the new project to the template project(s), so that later updates on the template can be pulled into the instantiated projects.\r\nThe choice of copier as engine allows also to chain templates after each other, which we use to enhance a very general-purpose base template with additional features which are commonly needed within dedicated teams, such as data engineering or data science teams. We show how we implemented this template chaining using git subprojects, jinja statements and renovate.\r\nOne example of such a chained template is the creation of a container image, which is needed in a lot of projects, but not all. Instead of setting up a Dockerfile within the base template, we built another template that sits on top of the base referencing it instead of duplicating the code base.  Another example would be the setup of an ETL pipeline, which is standardized within our data engineering team, but not needed in projects without data pipelines.\r\nThe core of our approach is the department wide base template. It ships with a basic Python project structure, several provisioned module files as well as tooling and corresponding configuration to streamline development, such as\r\n\r\n- \u2018poethepoet\u2019 for running tasks\r\n- \u2018uv\u2019 for managing python versions, dependencies and virtual environments\r\n- \u2018ruff\u2019 for linting and auto-reformatting\r\n- \u2018mypy\u2019 for type checking\r\n- \u2018bandit\u2019 for vulnerability and security checks\r\n- Reasonable default configurations for ruff, mypy, and bandit\r\n- \u2018pytest\u2019 as testing tool\r\n- \u2018pre-commit\u2019 for easy integration of pre-commit hooks\r\n    -\tThe hooks are setup to automatically conduct a variety of checks before an actual commit, such as unit tests, type checks, and formatting\r\n- \u2018commitizen\u2019 for easy creation of conventional commits\r\n\r\nWorking exclusively in Gitlab, we also incorporated some basic CI/CD, that serves as a starting point for more project specific pipelines and already includes linting, testing, and format checks, corresponding to the steps defined as pre-commit hooks.\r\nFinally, the template includes a README whith (templated) instructions about project setup, usage and general structure for the additional documentation developers should provide while developing the project.\r\nThe base template has been put together to ensure that developers can get to development as fast as possible and focus on it. Virtual environment setup, reasonable initial dependencies, testing setup, enforcement of formatting and even the generation of standardized commit messages are prepared automatically. A full setup starting from  an empty project folder takes less than 4 minutes before the first commit and unit test compared to up to at least one hour without any templates.\r\n\r\nHow can you use or adapt our approach for your environment?\r\n--------------------------------------------------------------------\r\n\r\nUsage of the shown approach is very simple and will be demonstrated live in our talk.\r\nApart from installing the tools copier, uv, and poethepoet only a single copier command is necessary to bootstrap a new project from the template and only one additional copier command for each chained template.\r\nCreating a new template with additional features is also very straightforward by including upstream templates as git subprojects. New files that do not exist in the upstream template(s) (e.g. the base template) can just be templated via jinja. Files already existing in an upstream template can easily be amended with the usage of jinjas \u201cinclude\u201d statement.\r\nWe are planning to release our base template under a permissive open-source license, so that it can easily be adjusted to work within other organizations.\r\n\r\nConclusion \r\n------------\r\n\r\nWe present our approach to standardize Python and especially Python data projects and automate recurring tasks during project setups, accommodating diverse team requirements. Our approach also facilitates seamless propagation of changes to these standards to new and existing projects. An added benefit is that this approach implicitly establishes a centralized place for documentation, discussion, and evolution of these standards (namely, the template projects), promoting organization-wide unification of tech stacks and security concerns. Our proposed talk will comprehensively describe the development of our solution, considered requirements, advantages and disadvantages of solution options and conclude with a live demonstration and instruction on how everybody can adapt our approach.", "code": "TYQYWZ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Christopher-Eyk Hrabia, Jona Welsch", "track": "PyCon: Programming & Software Engineering"}, {"title": "Flet: Cross-Platform Flutter Apps in Python", "abstract": "Curious about building cross-platform apps in Python without sacrificing a modern, high-performance UI? Meet Flet: an open-source framework that merges Flutter\u2019s dynamic UI capabilities with Python\u2019s simplicity. Instead of juggling Dart or multiple toolkits, Flet lets you tap into Flutter\u2019s native performance and beautiful widgets while staying in the Python ecosystem you know and love. \r\n\r\nIn this talk, you\u2019ll learn how Flet provides a Pythonic interface to Flutter\u2019s widget tree, making it easy to build visually appealing apps for mobile (Android & iOS), web, and desktop from a single codebase. Whether you\u2019re a seasoned Pythonista or just curious about cross-platform development, come discover how Flet brings Flutter to Python, right where you\u2019d least expect it: PyCon!", "full_description": "On PyCon you wouldn't expect to hear a talk about Flutter. Flutter is an open-source framework for creating visually stunning, natively compiled, multi-platform apps from a single codebase\u2014if you\u2019re willing to learn the Dart programming language.\r\n\r\nHere comes Flet, an open-source framework that brings Flutter\u2019s UI to Python. By translating Flutter\u2019s concepts into a Pythonic approach, Flet delivers all the performance, adaptiveness, and extensibility Flutter is known for, minus the Dart learning curve. It\u2019s not just a one-to-one binding\u2014it\u2019s an opinionated framework designed to feel right at home for Pythonistas.\r\n\r\nIn this talk, I\u2019ll dive into how Flet empowers Python developers to build modern, cross-platform applications with ease. Whether you\u2019re a seasoned Pythonista or just curious about cross-platform development, come discover how Flet brings Flutter to Python - no Dart experience required!", "code": "JPFXVB", "state": "submitted", "created": "2025-01-05", "speaker_names": "Henri Ndonko", "track": "PyCon: Programming & Software Engineering"}, {"title": "Assessing demand forecast quality under stock constraints", "abstract": "A useful way of assessing the quality of probabilistic demand forecasts is to compare their performance to that of a theoretically best-possible limit. This theoretical limit can often be found by determining an upper bound to the forecast sharpness, which, in turn, sets a (not-always-achievable) lower bound for the error metrics. In retail, for example, the sharpness limit is typically given by the Poisson distribution. However, there are certain higher order effects that can make the real-world demand deviate from this ideal behavior, making the Poisson limit not always useful as a forecast quality benchmark. A common example of this occurs when the actual sales are constrained by the available stocks, which leads to a right censoring of the demand distribution. In this contribution, we present how to approach this problem as well as the Python-based solution developed at Blue Yonder for enabling adequate forecast quality assessment even in complex scenarios with constrained demand.", "full_description": "This talk will present a statistical approach for evaluating the quality of probabilistic demand forecasts without falling into typical traps, and how this approach needs to be adapted to include complex scenarios such as finite capacity (i.e., when the demand is constrained by stocks). We also aim to showcase a Python package developed at Blue Yonder that implements this method and allows data scientists to easily evaluate, visualize and diagnose forecast quality following best practices.", "code": "MJA3SA", "state": "submitted", "created": "2024-12-22", "speaker_names": "Angel Ferran Pousa", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Resilience: Building your career in the age of AI", "abstract": "Counteracting the overarching narative of AI taking over all our jobs, this talk will offer some guidance and tips on how to grow your career in this brave new world. LLMs and other forms of AI are here to stay, and rather than worry about the existential threat to our livelihoods we should look to see how they can be harnessed to further grow our careers.", "full_description": "Rather than worrying about the existential threat that AI poses, I\u2019ll offer some practical guidance on how to thrive in this environment. What is unlikely to change is the requirement for engineers to solve thorny problems. Key to having long careers is to shift our mindset to use these technologies to augment our careers and work, rather than handing over all work to them. Approaching this technology shift with a growth mindset is key to leveraging all the benefits of this new paradigm, and rather than replacing our jobs, could help accelerate our careers and remove toil from our daily work. \r\n\r\nIt is also helpful to see the latest technology shifts as part of a larger continuum of technoical advancement in our field. The cummulative effect of which has helped drive speed and quality improvements at the same time as lowering the barrier to entry for aspiring engineers. \r\n\r\nStructure\r\n1. Setting out my stall [5 mins]\r\n    1. Beyond the hype\r\n    2. Reasons to be optimistic\r\n2. Engineering mindset  [10 mins]\r\n    1. The engineering problem space\r\n    2. Key mindset shift to reduce worry\r\n3. Engineering Craft [5 mins]\r\n    1. 10X your learning\r\n    2. Intentional practice\r\n    3. Maintaining Quality\r\n4. Ownership [3 mins]\r\n    1. Use the tool. Own the code\r\n5. The Business of Engineering [5 mins]\r\n    1. Focus on outcomes\r\n    2. Deliver fast, deliver good\r\n6. Reasons to be cheerful [2 mins]\r\n\r\nOutcomes\r\nHopefully this will bring some (much needed) optimism and counteract the narrative of AI making all of us redundant tomorrow. This will be particularly useful  for engineers who are either breaking into the industry or at the early stages of their careers.", "code": "YKMPM8", "state": "submitted", "created": "2025-01-05", "speaker_names": "Eric Thanenthiran", "track": "General: Education, Career & Life"}, {"title": "XAI in Cybersecurity: Understanding the \"Why\" behind the AI data-driven decisions", "abstract": "As reported by Crowdstrike, cloud intrusions rose to 75% YoY. As GenAI tools are used for cyber activities, it\u2019s hard to trust them. This session delves into the critical need for explainability in AI-driven security using real-world use cases to enable faster incident response and enhanced trust.", "full_description": "84% of company codebases contained at least one vulnerability in the open-source software used, and 74% contained high-risk vulnerabilities, according to Synopsys\u2019s ninth edition of the annual \u201cOpen Source Security and Risk Analysis\u201d report. As organizations increasingly rely on AI models to identify and manage such risks in their security systems, a critical question arises: how can we be sure these systems are making the right decisions, especially when the stakes are high? Imagine your AI system flags a critical vulnerability in your code. But why? What specific patterns triggered the alert or ensured that other potentially more critical vulnerabilities haven\u2019t been overlooked? Traditional AI models in security often operate as \u2018black boxes,\u2019 providing alerts without clear explanations. This lack of transparency hinders incident response, makes it difficult to fine-tune systems, and erodes trust in AI-driven security solutions. Without understanding the \u201cwhy,\u201d can we truly trust the system to remediate the issue? In this talk, using explainable AI techniques, let us comprehend the \u201cwhy\u201d of these \u201cblack box model decisions\u201d via real-world use cases. Additionally, we will learn how to build transparent, trustworthy, and resilient AI-driven security systems.", "code": "MVYJPG", "state": "submitted", "created": "2024-12-26", "speaker_names": "Rashmi Nagpal", "track": "General: Ethics & Privacy"}, {"title": "Build a personalized Commute agent in Python with Hopsworks, LangGraph and LLM Function Calling", "abstract": "The invention of the clock and the organization of time in zones have helped synchronize human activities across the globe. While timekeepers are better at planning and sticking to the plan, time optimists somehow believe that time is malleable and extends the closer the deadline. Nevertheless, whether you are an organized timekeeper or a creative timebender, external factors can affect your commute.\r\n\r\nIn this talk, we will define the different components necessary to build a personalized commute virtual agent in Python. The agent will help you analyze your historical lateness records, estimate future delays, and suggest the best time to leave home based on these predictions. It will be powered by a LLM and will use a technique called Function Calling to recognize the user intent from the conversation history and provide informed answers.", "full_description": "The invention of the clock and the organization of time in zones have helped synchronize human activities across the globe. While timekeepers are better at planning and sticking to the plan, time optimists somehow believe that time is malleable and extends the closer the deadline. Nevertheless, whether you are an organized timekeeper or a creative timebender, external factors can affect your commute.\r\n\r\nIn this talk, we will define the different components necessary to build a personalized commute virtual agent in Python. The agent will help you analyze your historical lateness records, estimate future delays, and suggest the best time to leave home based on these predictions. It will be powered by a LLM and will use a technique called Function Calling to recognize the user intent from the conversation history and provide informed answers.\r\n\r\nThe ML system will be built in Python, following the best practices of the FTI (feature/training/inference) pipeline architecture, on top of the open-source Hopsworks AI lakehouse, which will provide the necessary ML infrastructure, such as the feature store, model serving, and a model registry. The agent will be designed with LangGraph and powered by a LLM running on the vLLM inference engine.", "code": "8S3RC3", "state": "submitted", "created": "2025-01-05", "speaker_names": "Javier de la R\u00faa Mart\u00ednez", "track": "PyData: Data Handling & Engineering"}, {"title": "Rust powered Inference, Ingestion and Indexing", "abstract": "EmbedAnything is an opensource library that uses rust in the backend to process all your files, run any kind of embedding models, like Bert, Jina, ColPali or Sparse models. It also have a effective, memory efficient technique to index it to vector database, called vector streaming.", "full_description": "Vector streaming, is a powerful method in which we break the main process like chunking and parsing is done on the other process and inference of the embedding model is run on the other, and they communicate through rust's powerful MPSC channel. EmbedAnything also supports inferencing on GPUs because rust's and candle hardware acceleration is extremely adaptable. \r\n\r\nApart from that, running ColPali and sparse models is also easy with this library called EmbedAnything. \r\n\r\nEmbedAnything is a minimalist, highly performant, lightning-fast, lightweight, multisource, multimodal, and local embedding pipeline built in Rust. Whether you're working with text, images, audio, PDFs, websites, or other media, EmbedAnything streamlines the process of generating embeddings from various sources and seamlessly streaming (memory-efficient-indexing) them to a vector database. It supports dense, sparse, ONNX and late-interaction embeddings, offering flexibility for a wide range of use cases.", "code": "XGQVPM", "state": "submitted", "created": "2024-12-22", "speaker_names": "Sonam Pankaj", "track": "General: Rust"}, {"title": "Deep Dive into Asynchronous SQLAlchemy - Transactions and Connections", "abstract": "Discover the challenges of using SQLAlchemy in asynchronous Python backends! This talk dives into handling transactions and database connections, highlighting common pitfalls and solutions. As a bonus, we\u2019ll compare how other async ORMs manage connections, helping you build scalable, high-performance systems.", "full_description": "The presentation is about SQLAlchemy, which is an Object-Relational Mapping (ORM) library used in Python for working with databases. \r\n\r\nThe presenter will discuss the challenges and potential problems that developers may encounter when using SQLAlchemy to create asynchronous backend applications.\r\n\r\nThe main focus of the talk will be on how SQLAlchemy handles transactions and connections to the database, which are crucial components of building a robust and reliable backend system. The presenter will explain some of the common caveats and gotchas that can arise when using SQLAlchemy in this context.\r\n\r\nOverall, the talk aims to provide valuable insights and practical advice for Python developers who are interested in working with SQLAlchemy and building scalable, high-performance backend applications.\r\n\r\nAs a bonus we'll see how other popular asynchronous ORM handles connections to the database.", "code": "WYDE7J", "state": "submitted", "created": "2025-01-05", "speaker_names": "Damian Wysocki", "track": "PyCon: Programming & Software Engineering"}, {"title": "Say hello to my little agent!", "abstract": "Human dependency to make judgement calls is a critical part for most consumer facing businesses. This is slow, expensive and unreliable to some extent. What if LLMs can handle these judgement calls? Instead of making people do it, can we teach LLMs to understand the context, stick to the guidelines and make decisions which are similar to the quality of a human?", "full_description": "The objective is to share our learnings while implementing different automations using python for businesses which helps them reduce the time to solve a particular problem and also reduce cost.\r\n\r\nIn this talk, you will learn:\r\n- How to split complex workflows into clear action steps and decision steps.\r\n- How LLMs can take over human judgment tasks like address verification.\r\n- Best practices and pitfalls when using LLMs for automated decision-making.\r\n- Ideas for future advancements in agent-based automation.\r\n\r\nSome key points to discuss:\r\n- We will be presenting our POC which we performed for a multinational company (without disclosing any names / PII data) for using LLMs as decision nodes and other automation techniques (vendor API for address information, headless browsing) to get better information\r\n- LLMs could be tricky, with every new model, previous learnings might have to improve. So, our learning were majorly on openai\u2019s 4o and 3.5-turbo models. They are also applicable with the 4o-mini models as well.", "code": "HHJNRV", "state": "submitted", "created": "2024-12-22", "speaker_names": "Adarsh Kumar, Divyam Chandel", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Leveling up your teams with guilds", "abstract": "You don't need to be a manager to level up the skills of your team and other teams. Python Guilds offer a powerful way for organisations to turn passive knowledge sharing into active skill-building. But just having regular meetings together isn't enough. I've seen teams really accelerate when we were sharing code. This demands a safe environment where people feel comfortable sharing mistakes and learning from them. This presentation is about my experiences with founding a Python guild and how we got to a higher level of Python and PySpark skills together.", "full_description": "It all started three years ago when Python and PySpark began gaining popularity at my workplace. I noticed significant differences in the skill levels across teams. Some teams were highly experienced, while others were eager to get started but didn\u2019t quite know how. That\u2019s when I proposed starting a Python and PySpark guild \u2014 and so began an incredible journey.\r\nBringing developers together was a solid first step. People shared their screens, showcasing how they used VS Code, created Dockerized Python solutions, and tackled other technical challenges. However, it became clear that not every team could replicate these solutions. While anyone can start writing Python, mastering concepts like virtual environments, testing, and logging required deeper understanding. It didn\u2019t help that experienced and inexperienced developers often spoke different \"technical languages.\"\r\nThe urgency for action became even clearer as the organization\u2019s reliance on Python grew. Two major new customers brought in a flood of work, increasing the need for developer maturity. Recognizing this, I decided to try a different approach: I organized a shared coding session. In this session, less experienced teams worked together to create a PySpark program, with experienced developers present to offer guidance and feedback.\r\nThat approach made a big difference. Gaps in knowledge became immediately visible, and experienced developers started creating solutions that made it easier for less experienced teams to get started. This shift sparked a culture of collaboration and continuous improvement.\r\nAnd the best part? I\u2019m not a manager, scrum master, or team lead. I\u2019m a senior data engineer. But you don\u2019t need to be a manager to start a guild and help teams level up together.", "code": "DVUFUK", "state": "submitted", "created": "2024-12-16", "speaker_names": "Marcel-Jan Krijgsman", "track": "General: Education, Career & Life"}, {"title": "Solving Real-World Problems with Computer Vision", "abstract": "In a world increasingly driven by artificial intelligence, computer vision is transforming industries by solving real-world challenges with remarkable precision and scalability. This talk delves into the practical applications of computer vision, highlighting its potential to address critical issues across diverse sectors. Drawing from hands-on experience in developing AI-powered systems, including object detection, segmentation, and innovative authentication solutions, I will share insights into how computer vision can be effectively harnessed to improve lives.\r\n\r\nKey topics include:\r\n\r\nUsing YOLOv8 for object detection and segmentation to tackle unique challenges like pile counting and tracking.\r\nDeveloping human-centric solutions, such as person detection and nudity blurring for ethical AI.\r\nEnhancing user experiences through AI-powered face and gesture recognition systems for secure authentication.\r\nEmpowering agriculture by addressing farmer-identified problems with AI systems.\r\nAttendees will gain practical knowledge of how to bridge the gap between theoretical models and impactful implementations, with a focus on ethical considerations, scalable designs, and future possibilities in computer vision. Whether you're an AI enthusiast or a professional exploring the power of vision-based technologies, this session will provide actionable insights to inspire innovation and problem-solving.", "full_description": "This session explores the transformative potential of computer vision in solving real-world challenges across diverse industries. From agriculture to healthcare, security, and transportation, AI-powered solutions such as object detection, segmentation, ethical person detection, and gesture-based authentication are revolutionizing the way we approach complex problems. Through practical examples and live demos, this talk will showcase actionable insights into building scalable, impactful computer vision systems for sectors like manufacturing, retail, public safety, and more.", "code": "A7CLDR", "state": "submitted", "created": "2024-12-13", "speaker_names": "Daniel Samuel Etukudo", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Python packages, extension modules, and how it achieves cross-platform compatibility", "abstract": "Have you ever `pip install <package>` and got cryptic stacktraces like this\r\n\r\n```\r\n$ pip install mysqlclient\r\n ...\r\n        File \"setup.py\", line 27, in find_package_name\r\n          raise Exception(\r\n      Exception: Can not find valid pkg-config name.\r\n      Specify MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS env vars manually\r\n...\r\nerror: subprocess-exited-with-error\r\n\r\n ```\r\n \r\n As a next step, you would paste this log on the internet, the answer might include you installing a native package like `apt install libmysqlclient-dev` and the issue resolves.\r\n  \r\n In this talk, we'll discuss about Python Packages, why such issues arise during their installation, why it's complex to have `pip install(s)` working the same way in every computer, and how can one deal with them in an efficient manner.", "full_description": "## Talk Outline with timestamps\r\n \r\n - Introduction (2 minutes)\r\n - How does software get distributed in the Python ecosystem? (2 minutes)\r\n\r\n     - We'll look at how a python package is built in a maintainer's computer, how it's uploaded to pypi and how it's installed in the user's computer.\r\n\r\n - Where does complexities arise in software distribution? (2 minutes)\r\n     - We'll talk briefly about how the Python runtime works, how different are pure python module and extension modules regarding how they are run.\r\n - Types of Python Packages (3 minutes)\r\n     - This section involves explaining what is sdist, what is a wheel, and how they are structured?\r\n - What are extension modules, and why should you care? (3 minutes)\r\n     - Introduction to extension modules and how Python runs functions from extension modules?\r\n - Let's look at some extension modules! (3 minutes)\r\n     - Demonstration involving creation of a few types of extension modules and analyzing their wheels.\r\n - Wheels in the wild. (5 minutes)\r\n     - We'll go through some packages like cryptography, psycopg2, yarl, ultrajson, lxml to see how their wheels are built.\r\n - How can you use this information? (2 minutes)\r\n     - We'll summarize our learnings and note the takeaways.\r\n - Q&A - (5 minutes)\r\n \r\n ## Talk Outcome\r\n \r\n After attending this talk, a intermediate Python Developer should be able to understand how a python module is packaged and distributed, including extension modules, and reason about many package installation errors making them more productive.", "code": "VSZFAX", "state": "submitted", "created": "2024-12-21", "speaker_names": "Shivashis Padhi", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Instrumenting Python Applications with OpenTelemetry", "abstract": "Observability is challenging and often requires vendor-specific instrumentation. Enter OpenTelemetry: a vendor-agnostic standard for logs, metrics, and traces. Learn how to instrument Python applications with OpenTelemetry and send telemetry to your preferred observability backends.", "full_description": "Understanding the behaviour and performance characteristics of the software we deploy, especially distributed software, is quite tricky. While observability tooling helps, implementing vendor-specific instrumentation creates tight coupling and technical debt. \r\n\r\nEnter OpenTelemetry: A one-stop-shop for observability instrumentation, collection and routing. It aims to solve the above problem by providing SDKs, libraries and a unified semantic model for describing telemetry signals like logs, metrics and traces. These signals can be collected, transformed and then routed to many observability backends that support the OpenTelemetry protocol - avoiding vendor lock-in and platform specific observability code.\r\n\r\nIn this workshop, we'll guide you through what OpenTelemetry is, how it works, how to instrument your Python applications to emit telemetry data, and how to ingest this data into observability backends - enabling you to make better decisions about your application's performance.", "code": "UH7FXA", "state": "submitted", "created": "2025-01-05", "speaker_names": "Mika Naylor", "track": "PyCon: MLOps & DevOps"}, {"title": "Building and testing modern frontends with Django", "abstract": "Once upon a time, if you wanted to have a highly interactive and responsive web application, Django just would not cut it. A common pattern was (and still is) to build a Python backend application to serve some APIs and then build a separate frontend application using some kind of heavy frontend library or framework such as React, Vue, or Svelte. \r\n\r\nIn this talk,  I will demonstrate a stack of tools that can be used to build rich modern frontends in a more Pythonic way.   I'll specifically focus on HTMX, AlpineJS and Playwright.\r\n\r\nI'll also cover why tools like HTMX are worth paying attention to for organisations and teams.  It's not just another fad. HTMX is shifting the web development exosystem in some big ways that are very much worth paying attention to.", "full_description": "For the most part, this talk will be a demo.  I'll walk through an application built using Django, HTMX, AlpineJS and Tailwind. I will also demonstrate Playwright.\r\n\r\nAfter this I'll get on my soapbox and talk about why tools like HTMX are very much worth paying attention to.  HTMX makes good sense for businesses and it reduces the barrier to entry for junior web developers. This is powerful and worth paying attention to.", "code": "PEDHUR", "state": "submitted", "created": "2024-12-13", "speaker_names": "Sheena", "track": "PyCon: Django & Web"}, {"title": "zarr-python 3.0: Scalable data storage for n-dimensional arrays", "abstract": "Zarr is a data format designed for storing large n-dimensional arrays either on-disk or in the cloud. With its 3.0 release, the zarr-python library has undergone a significant overhaul. Among the key advancements is support for the latest Zarr specification, which introduces the new sharding feature\u2014enabling efficient storage and access to n-dimensional arrays scaling to petabyte sizes. This release also brings a comprehensive refactor of the library, leveraging asynchronous operations to deliver substantial performance gains. Additionally, the library has been redesigned for greater extensibility and fully modernized with 100% type hint coverage.\r\nIn my talk, I will provide an introduction to the Zarr format and the zarr-python library, followed by a detailed overview of the major changes introduced in the 3.0 release.", "full_description": "Zarr is a widely adopted, chunked data format designed for efficiently storing large n-dimensional arrays, whether on-disk or in the cloud. Its design focuses on scalability and performance, making it a vital tool for handling massive datasets in fields like machine learning, climate modeling, and biomedical image analysis. The 3.0 release of the zarr-python library represents a major evolution, introducing substantial improvements and new features that enhance its capabilities and maintainability.\r\n\r\n\u2028The 3.0 release introduces support for the latest Zarr specification (version 3), designed to improve interoperability across multiple programming languages and enhance extensibility. One of the most prominent features of this specification is sharding. Sharding enables efficient storage and access to petabyte-scale n-dimensional arrays by allowing multiple chunks to be stored within a single file. This approach optimizes performance on various file systems and object storage services, making it more efficient and scalable for large datasets.\r\n\r\n\u2028The core of the zarr-python library has been reengineered to leverage asynchronous I/O, allowing reads and writes to be performed concurrently. This delivers significant performance improvements, especially when working with high-latency storage systems like object storage. Additionally, the library employs multithreading for encoding and decoding data, further enhancing throughput.\r\n\r\nVersion 3.0 has been redesigned with extensibility in mind, providing developers with the flexibility to integrate custom components such as stores, codecs, buffer classes, and encoding pipelines. This extensibility paves the way for use cases, such as optimized data processing on GPUs.\r\n\r\nMoreover, the codebase has been thoroughly modernized to ensure maintainability and ease of development. The library now has 100% type hint coverage for better code clarity, robustness and code completion. The public and private APIs have been clearly delineated, with the goal of keeping the public APIs stable while evolving the library in the future. Additionally, the development environment has been upgraded with improved CI/CD workflows and testing infrastructure. These updates collectively make zarr-python a modern library that is vital for data-intensive science applications.\r\n\r\nIn my talk, I will provide an introduction to the Zarr format and the zarr-python library, followed by a detailed overview of the major changes introduced in the 3.0 release.", "code": "U9W3FN", "state": "submitted", "created": "2025-01-04", "speaker_names": "Norman Rzepka", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Think Abstract, Code Smart: Achieving Maintainability and Scalability in Python", "abstract": "This talk will explore how abstraction can transform your Python code into a more maintainable and readable masterpiece.\r\nAs the number of projects grows, maintaining clean code becomes increasingly challenging. You will also see how the usage of abstraction ensures smoother integration, as the internal changes do not affect the code's external usage. This makes projects more maintainable and resilient to changes, as you can hide complex implementations and expose intuitive usage across all applications. Also as different applications might share a lot of code you can see how abstraction of the key concepts code decreases code duplication.\r\nJoin this session to learn practical techniques for code abstraction and discover how it fosters adaptability and long-term resilience in your code base.", "full_description": "Have you ever found yourself in a situation where you jumped back and forth in a Python project to adjust the code to a new feature? With increasing project complexity, maintaining and scaling them becomes increasingly difficult. Especially **data scientists and machine learning engineers** face such challenges while testing and deploying new models. It\u2019s even harder when you have to deal with legacy systems you didn\u2019t deploy. This talk will explore how **abstraction, a core principle of object-oriented programming (OOP)** can make your implementation process easier as you can focus on specific code without breaking the external usage of the module. Using the examples of features and model abstraction for different search ranker systems you will learn how you can create reusable modular components that are easier to maintain, scale and share. \r\n\r\nUsing practical examples, this talk will emphasize the usage of abstraction and covers following topics:\r\n**Definition and Core Principles of Abstraction in Python:** \r\nOverview of the abstract base class in python and how to use them\r\n**Scalability and Maintainability:** \r\nHow abstraction keeps your code base organized and adaptable as your projects evolve. \r\n**Integration:** \r\nAbstracting key concepts of projects ensures less breaks, since the external usage of the modules remains the same. You can focus on code complexity without spending too much time on integration. \r\n**Code Deduplication:** \r\nA lot of functionalities can be shaped as abstract modules which can be shared across projects. \r\n\r\nJoin to discover how abstract thinking can boost your Python coding game and make collaboration more fun.", "code": "7M9GMA", "state": "submitted", "created": "2024-12-20", "speaker_names": "Kateryna Budzyak, Evelyne Groe", "track": "PyCon: Programming & Software Engineering"}, {"title": "Beyond DALL-E: Advanced Image Generation Workflows with ComfyUI", "abstract": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. There are well-known models like Stable Diffusion and Flux, which can be used with easy-to-use frontends like A1111 or Invoke AI, but if you want to do more complex or bleeding-edge workflows, you need something else. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build complex pipelines that are otherwise only possible using plain code.", "full_description": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. But thanks to Stable Diffusion, Flux, and many supplementary models like ControlNet or an Image Prompt Model, we have much more control over the images we want to create. There are frontends for that, like A1111 or Invoke AI, but if you want to try bleeding-edge models or do something more complex, you will have a hard time implementing such a pipeline in code yourself, and it requires a steep learning curve. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build workflows as a DAG. Thanks to many other contributors, there are a lot of plugins available which bring in new functionality. This talk shows the capabilities and power of this tool using practical examples and how you can combine many things together to create a complex workflow much faster than coding it yourself.\r\n\r\nI want to cover the following topics:\r\n - What are the limits of a simple text-to-image workflow?\r\n - What is ComfyUI?\r\n - What are the requirements to use ComfyUI? (Resources, OS, etc.)\r\n - What can you do with ComfyUI that you can't do with a simple text-to-image interface?\r\n   - Pre- and post-processing of images in a single workflow\r\n   - Advanced conditioning using images, bounding boxes, depth maps, etc., all together\r\n - The examples shown as a demonstration:\r\n   - Integrating existing objects from a photo into a generated scenery\r\n   - Creating optical illusions and surreal images", "code": "LRUKZQ", "state": "confirmed", "created": "2024-12-22", "speaker_names": "Ren\u00e9 Fa", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Out of the Dark Ages: Monitoring ML at Scale in Kubernetes", "abstract": "Despite the rapid adoption of machine learning, many companies struggle to sustainably integrate ML systems into their infrastructure, tech stacks, and software teams. One of the most significant challenges is monitoring and observability, especially in environments where ML expertise is limited. Existing ML monitoring frameworks are often immature, difficult to integrate, and require niche skills that many software teams do not possess.\r\n\r\nWe propose a practical, scalable solution: deploying ML models on Kubernetes and leveraging native tools like Prometheus and Fluentbit to monitor three critical layers: system, application, and ML-specific metrics. This includes monitoring system-level resource utilization, application-level prediction latency, and ML-specific metrics such as data drift and SHAP values for model interpretation. By shifting monitoring components to the platform layer, this approach reduces complexity, improves success rates, and empowers software engineers to manage ML products effectively.", "full_description": "Despite the rapid adoption of machine learning, many companies struggle to sustainably integrate ML systems into their infrastructure, tech stacks, and software teams. One of the most significant challenges is monitoring and observability, especially in environments where ML expertise is limited. Existing monitoring frameworks are often immature, difficult to integrate, and require niche skills that many software teams do not possess.\r\n\r\nThis talk addresses these challenges by proposing a practical, scalable solution: deploying ML models on Kubernetes and leveraging Kubernetes-native tools like Prometheus and Fluentbit to monitor three critical layers: system, application, and ML-specific metrics. This includes monitoring system-level resource utilization, application-level prediction latency, and ML-specific metrics such as data drift and SHAP values for model interpretation. By shifting monitoring components to the platform layer, this approach reduces complexity, improves success rates, and empowers software engineers to manage ML products effectively.\r\n\r\nTalk Outline:\r\n\r\n1. Introduction and Problem Overview (5 minutes):\r\n- Challenges in integrating ML systems into infrastructure and tech stacks.\r\n- Why traditional ML monitoring frameworks often fail.\r\n\r\n2. Proposed Solution (10 minutes):\r\n- Leveraging Kubernetes-native tools (Prometheus, Fluentbit).\r\n- Benefits of platform-layer observability across system, application, and ML-specific metrics.\r\n\r\n3. Demonstration (10 minutes):\r\n- Example of monitoring a model served in Ray using above tools.\r\n- Demonstrate monitoring of prediction latency, data drift, SHAP values, and resource utilization.\r\n\r\n4. Q&A (5 minutes)", "code": "ZRLRAK", "state": "submitted", "created": "2024-12-22", "speaker_names": "Josef Nagelschmidt", "track": "PyCon: MLOps & DevOps"}, {"title": "Climbing a mountain: a network automation journey", "abstract": "Software development is hard. It's really hard when your organisation's core business isn't _really_ software development.\r\n\r\nPart of this talk is about the technology of developing a network automation setup inside an organisation where the network _is_ the business, and always needs to be live. But that's only the half of it. Any project needs to fit in an overall organisation, and sometimes that organisation can have trouble understanding what you're trying to do, or the way you're trying to do it.\r\n\r\nSo the talk isn't just about tools \u2013 it's about building the capability to do this. Putting together the right team for the job, learning what's needed to know how to develop the right stack, and deploy it confidently.\r\n\r\nAnna will talk about her experience on a team in exactly this situation, and the lessons she's learned shifting from the role of networker to software developer, via project manager and people manager, to try to get across how the rest of us can make really cool projects happen inside organisations that, at first glance, might have trouble pulling them off.", "full_description": "Software development is hard. It's really hard when your organisation's core business isn't _really_ software development.\r\n\r\nHEAnet is a national research network, a nonprofit ISP serving the academic community. We want to fully automate our operational tasks across our network and business. But it's tough. In particular, being a service provider, we fall into a gap in the market. If we were a pure software house, we could probably write our own software to fit our business model. If we were a traditional enterprise, we'd likely have a single hardware vendor and use their platform. We can't really do either - our needs are too custom for a single vendor (not to mention the risk of having all our eggs in one basket).\r\n\r\nSo part of this talk is about the tech. Anna will describe HEAnet's experience in assembling a software stack that meets our needs. She'll spend a little time describing what we use, and why we chose that. This includes tools coming from the NREN community such as [Workflow Orchestrator](https://workfloworchestrator.org)\r\n\r\nCrucial to this is making sure that we are no longer dependent on third party roadmaps that determine how our operations must work, or what features we can provide to our clients. That means we need to get in and write some of this stuff ourselves.\r\n\r\nBut it's still tough. We've had to get out of the \"build vs. buy\" mindset of thinking about a single platform or a loose collection of open source software to meet our needs. The reality is that our businesses today are complex, and managing the automation of our business is itself a complex task that requires its own subject matter experts. It takes a combination of networking, software development, and project management that's pretty unique, and not so easy to hire for or even to outsource.\r\n\r\nSo the talk isn't just about tools \u2013 it's about building the capability to do this. Putting together the right team for the job, learning what's needed to know how to develop the right stack, and deploy it in such a way that we can confidently operate it into the future.\r\n\r\nThe hardest challenge that keeps coming up in the talk isn't really about programming, or choosing tools, or even communicating within the team \u2013 although these are all crucial, and do come up \u2013\u00a0but about making what we do _legible to the rest of the organisation._ How do you operate in an agile way, when all of an organisation's training, the things it's been rewarded for doing, look a lot more like long term, single-goal projects?\r\n\r\nAnna will talk about her experience on a team in exactly this situation, and the lessons she's learned shifting from the role of networker to software developer, via project manager and people manager, to try to get across how the rest of us can make really cool projects happen inside organisations that, at first glance, might have trouble pulling them off.", "code": "RYNXSE", "state": "submitted", "created": "2024-12-22", "speaker_names": "Anna Wilson", "track": "PyCon: Programming & Software Engineering"}, {"title": "Bring Your Own Data Frame - How to Make Your Data Tools Dataframe-Agnostic: An Overview of Methods", "abstract": "Pandas, Polars, Koalas, Ibis, Dask, Modin, cuDF, PyArrow, PySpark\u2014just to name a few. To paraphrase a classic, you probably know those aren\u2019t Pok\u00e9mon names but libraries you can use to manipulate data. The abundance is great, right? Right? In this talk, I\u2019d like to explain the challenges of a pandas-dominated data jungle and how to mitigate those challenges by building dataframe-agnostic data tools.", "full_description": "Objectives: Presenting Data Interchange Protocol, Standard Array API, Ibis, and Narwhals, and how they can help you build better and more accessible data tools.\r\n\r\nDetailed Abstract: \r\n1. - Welcome to the Jungle of Data Frame Types: \r\nAn introduction to the world of dataframe manipulation libraries. Why is pandas so popular? What\u2019s gaining traction? Where are we heading?\r\n\r\n2. - User Problem = Engineer Problem: \r\nWhile learning Polars, I stumbled upon a problem: my favorite tools were built for pandas. Transitioning from pandas to another library limits the choice of auxiliary tools. Why can\u2019t we have data tools that work for a wider range of dataframe libraries? The question I\u2019d like to answer in this talk is: how to build dataframe-agnostic data tools.\r\n\r\n3. - How to Build Dataframe-Agnostic Data Tools: When building data tools, you don\u2019t have to choose which dataframe type it should be compatible with\u2014there are solutions that can support multiple dataframe types.\r\n\r\n- Comparison of What\u2019s in Store:\r\nData Interchange Protocol\r\nStandard Array API\r\nIbis\r\nNarwhals\r\n\r\n- Examples of Dataframe-Agnostic Tools:\r\nscikit-learn\r\nscikit-lego\r\nplotly\r\naltair", "code": "PLKEDG", "state": "submitted", "created": "2025-01-05", "speaker_names": "Magdalena Kowalczuk", "track": "PyCon: Programming & Software Engineering"}, {"title": "Parallel and Distributed Data Processing with Dask", "abstract": "Dask is a library for parallel and distributed computing with Python. It's just Python and thus doesn't need any heavy dependencies like Spark or similar tools do. Dask offers APIs for Arrays, DataFrames and general parallelism. We will explore how Dask DataFrame enables users to run distributed data processing queries with pandas syntax.", "full_description": "Dask is a library for parallel and distributed computing with Python. It's just Python and thus doesn't need any heavy dependencies like Spark or similar tools do. Dask offers APIs for Arrays, DataFrames and general parallelism. Dask integrates with most libraries in the PyData stack and enables us to scale these tools beyond a single machine.\r\n\r\nWe will focus on the DataFrame integration. Dask integrates seamlessly with pandas and started as a parallel pandas tool. It has since grown significantly beyond that. Recent developments in Dask that added a query optimizer to the interface allow us to process Terabytes of data right from our laptop. We will dive into the technical improvements and how those impact the user experience and performance of Dask DataFrames.\r\n\r\nDask is just Python. It supports arbitrary Python UDFs as first class citizens. We will illustrate how this feature makes Dask the perfect choice for dealing with messy data from the real world. Simple APIs give access to any data source that has a Python Connector.\r\n\r\nWe conclude with a brief comparison with Spark. Spark is the de factor standard for processing large scale tabular data. We will show how Dask fits into this environment and when Dask is a better and easier choice for users.", "code": "Z87QJX", "state": "submitted", "created": "2024-12-22", "speaker_names": "Patrick Hoefler", "track": "PyData: Data Handling & Engineering"}, {"title": "A11y Need Is Love (But Accessible Docs Help Too)", "abstract": "Accessible documentation benefits everyone, from developers to end users. Using the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/) as a case study, this talk dives into common accessibility barriers in documentation websites like low contrast colors, missing focus states, etc. and practical ways to address them. Learn about accessibility improvements and take part in a live accessibility audit to see how small changes can make a big difference.", "full_description": "The Beatles told us that \u2018all you need is love\u2019 and while that is a lovely sentiment, love alone won\u2019t fix low contrast colours, missing focus states or inaccessible navigation. These barriers impact countless users with disabilities, reducing the usefulness and reach of valuable documentation. So, while love is great, accessible docs are *essential*.\r\n\r\nIn this talk, we will use the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/) as a case study to explore common accessibility problems in documentation websites and how to tackle them. We will discuss the accessibility changes we made to the theme, how those changes affected users, and what we learnt along the way. Additionally, we will also conduct a short accessibility audit on a website suggested by the audience. This demo will provide a practical understanding of how to improve accessibility.\r\n\r\nWhether you\u2019re a documentation maintainer, a curious developer or simply someone who cares about accessibility, this beginner-friendly talk will help you learn more about accessibility in documentation and how to get started. Love might be a universal language, but your code appreciates accessible documentation.", "code": "WLZSEZ", "state": "submitted", "created": "2024-12-18", "speaker_names": "Smera Goel", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Where have all the post offices gone? Discovering neighborhood facilities with Python and OSM", "abstract": "When it comes to open geographic data, [OpenStreetMap](https://www.openstreetmap.org/) (OSM) is an awesome resource. Getting started and figuring out how to make the most out of the data available can be challenging.\r\n\r\nUsing a personal example: frustration at the apparent lack of post offices in my neighborhood, we'll walk through examples of how to parse, filter, process, and visualize geospatial data with Python.\r\n\r\nWhile this talk is aimed at those beginning with geographic data, it would be helpful to have some background knowledge about Python and data handling.\r\n\r\nAt the end of this talk, you will know how to process geographic data from OpenStreetMap using Python and find out some surprising info that I learned while answering the question: _Where have all the post offices gone?_", "full_description": "### Problem statement and brief introduction\r\n4 minutes\r\n\r\n- Needing an international postcard stamp, I headed to my nearest post office only to find out that it was permanently closed, the latest closure among others in recent memory. Was this just in my neighborhood or was this happening all over the state? To answer these questions, I turned to open data and Python.\r\n- What is OpenStreetMap?\r\n\r\n### How can we identify types of places, like post offices and districts, in OSM data?\r\n6 minutes\r\n\r\n- OSM data can be broken down into three elements: nodes, ways and relations.\r\n- Tags are key value pairs which are used to describe an element's features.\r\n- Brief discussion of tools ([Overpass API](https://overpass-api.de/), [overpass turbo](https://overpass-turbo.eu/)) for diving into the data to get an idea of how it is structured and how to construct queries\r\n\r\n### How can we access the raw OSM data and work with it in Python? \r\n8 minutes\r\n\r\nHow many post offices are there in each neighborhood? What about by area or population?\r\n\r\n- We can download data extracts for a region or the entire planet! PBF files are designed to be space efficient, which is important since files for regions, like entire continents, can contain a massive amount of data.\r\n- For parsing and processing the PBF files, we can use the Python library [PyOsmium](https://osmcode.org/pyosmium/). In this step, we'll look at how to create two filters: one for the administrative boundary (district or neighborhood) and another for the facility (post office).\r\n- Use [GeoPandas](https://geopandas.org/) to store the data in a GeoDataFrame. In this step, we'll look at how to approximate the administrative boundary's area in square kilometers and how to determine which post offices are within which boundary area.\r\n\r\n### What are some tools for visualizing the data?\r\n4 minutes\r\n\r\nHow can we make an interactive plot of post offices in each neighborhood? What about other facilities and resources?\r\n\r\n- Plot boundaries directly from a GeoDataFrame\r\n- Use [Bokeh](https://docs.bokeh.org), which supports geographical data with `GeoJSONDataSource`. In this step, all of the pieces will come together in an interactive map of the number of total post offices by neighborhood and the post offices per square kilometer by neighborhood. \r\n\r\n### Summary\r\n3 minutes\r\n\r\n- Review the steps we've taken: parsing and filtering, processing, plotting.\r\n- Brief mention of how we can wire all the pieces together. All the code and resources used in this talk will be available on my public GitHub.\r\n\r\n### Time for questions\r\n5 minutes", "code": "ADSXCA", "state": "accepted", "created": "2024-12-20", "speaker_names": "Katie Richardson", "track": "PyData: Data Handling & Engineering"}, {"title": "Data Ingestion and Big Data", "abstract": "Initiating analysis begins with retrieving web data to build datasets. Numerous startups leverage public data from the web, open sources, or utilize API services. This talk provides insights into scraping, crawling, and API access techniques, addressing technical and ethical considerations.", "full_description": "Web scraping, crawling, and API utilization are crucial initial steps for acquiring data for analysis and launching a new business.\r\nIn this tutorial, I'll demonstrate how to employ Python for setting up scraping and crawling processes, simulate user navigation and browser behavior using a ghost browser, and connect with and utilize data APIs. I will also delve into the technical and ethical considerations essential for navigating these challenges.", "code": "EEWGBW", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Data Handling & Engineering"}, {"title": "MIDI, those cheesy sounds from the 90s? Wrong! Symbolic music processing with Python", "abstract": "\u201cMIDI, those cheesy sounds from the 90s?\" is an actual question some guy asked me at an AI conference one day. What an inspiring question! This talk flips the outdated view of MIDI as retro noise, showcasing it as a powerful format for representing and analyzing music. While some think it\u2019s obsolete, MIDI remains the backbone of modern music production and a very active research topic in machine learning. This talk unpacks MIDI\u2019s structure and demonstrates how Python libraries like mido, pretty_midi, and MidiTok turn it into a tool for research and creativity. From visualization to music generation, practical examples reveal the modern applications of symbolic music.\r\nThe takeaway? Python makes it easier than ever to explore MIDI\u2019s potential and apply it to a wide range of musical and analytical tasks.", "full_description": "\u201cMIDI, those cheesy sounds from the 90s?\" is an actual question some guy asked me at an AI conference one day. What an inspiring question! This talk flips the outdated view of MIDI as retro noise, showcasing it as a powerful format for representing and analyzing music. While some think it\u2019s obsolete, MIDI remains the backbone of modern music production and a very active research topic in machine learning. This talk unpacks MIDI\u2019s structure and demonstrates how Python libraries like mido, pretty_midi, and MidiTok turn it into a tool for research and creativity. From visualization to music generation, practical examples reveal the modern applications of symbolic music.\r\nThe takeaway? Python makes it easier than ever to explore MIDI\u2019s potential and apply it to a wide range of musical and analytical tasks.", "code": "UKKK33", "state": "submitted", "created": "2024-12-13", "speaker_names": "Mateusz Modrzejewski", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Unlocking the Power of LLMs: Creating RAG Models for Real-World Applications", "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, but their true potential lies in their ability to be integrated into real-world applications. In this talk, we'll explore the power of Retrieval-Augmented Generation (RAG) models, which combine the strengths of LLMs with the flexibility of retrieval-based approaches. We'll delve into the design and implementation of RAG models, discussing the benefits and challenges of using pre-trained LLMs as a foundation. Through a hands-on example, we'll demonstrate how to build a RAG model that can be applied to a variety of real-world problems, from text generation to question-answering. By the end of this talk, attendees will gain a deeper understanding of how to unlock the full potential of LLMs and create practical, effective RAG models that drive business value.", "full_description": "With over 10 years of experience in software engineering and data science, David Camacho possess a unique blend of technical expertise and leadership acumen, having driven innovation and growth across multiple industries, including finance, cybersecurity, and healthcare. His strong background in software architecture and data engineering has equipped me with the skills to design and implement scalable, data-driven solutions that drive business value.", "code": "NSH3CF", "state": "submitted", "created": "2025-01-05", "speaker_names": "david camacho", "track": "PyData: Generative AI"}, {"title": "Who broke the build? \u2014 Using Kuttl to improve E2E testing and release faster", "abstract": "No one wants to be responsible for breaking the build. But what can you do as a developer to avoid being the bad guy? How can project leads enable their teams to reduce the occurrence of broken builds?\r\nIn talking within our own teams, we discovered that many developers weren\u2019t running sufficient integration and End to End tests in their local environments because it\u2019s too difficult to set up and administer test environments in an efficient way.\r\nThat\u2019s why we decided to rethink our entire local testing process in hopes of cutting down on the headaches, heartaches, and valuable time wasted. Enter Kuttl. Connecting Kuttl to CI builds has empowered our developers to easily configure a development environment locally that accurately matches the final test environment \u2014 without needing to become an expert CI admin themselves.\r\nThese days, we hear, \u201cWho broke the build?\u201d far less often \u2014 and you can too!", "full_description": "References of opensource projects used in this talk :\r\n1. KUTTL - https://github.com/kudobuilder/kuttl or https://kuttl.dev/\r\n2. Vcluster - https://github.com/loft-sh/vcluster\r\n3. k9s - https://github.com/derailed/k9s\r\n4. Helm - https://github.com/helm/helm\r\n5. Kubernetes - https://github.com/kubernetes/kubernetes\r\n\r\nSession Outline:\r\n\r\nIn this session, we\u2019ll discuss how we use kuttl to achieve more streamlined testing and fewer broken builds. We\u2019ll cover:\r\n\u25cf A quick history of our testing challenges and what led us to Kuttl\r\n\u25cf The benefits of our new testing approach \u2014 easy to configure and minimal investment\r\n\u25cf How we combine Kuttl and CI pipelines for more streamlined testing and fewer broken builds\r\n\r\n\r\nSession Key Takeaways: \r\n\r\n1. When and why we decided to rethink our e2e testing practices and our subsequent discovery of Kuttl. \r\n2. Why Kuttl has been the perfect tool for our developers to perform better local integration/e2e testing without the burden of becoming their own CI administrators.\r\n 3. A detailed account of how we utilize Kuttl  to set up development environments locally that match our final test environment in order to reduce unnecessary commits and minimize CI build breaks.", "code": "RGLWNU", "state": "submitted", "created": "2024-12-29", "speaker_names": "Ram Mohan Rao Chukka", "track": "PyCon: Testing"}, {"title": "Efficient FastAPI testing with pytest, dependency injection and snapshots", "abstract": "Testing sucks. Especially when you have thousands of tests that are slow and painful to maintain. It doesn't have to be that way. I'll show you how to use FastAPI's dependency injection, pytests factory fixtures, and snapshot testing to make your tests fast, readable, and easy to maintain.", "full_description": "## Introduction - 5min\r\n\r\nWe'll discuss common testing problems:\r\n- tests that take forever to run\r\n- tests that need to be updated every time you change the implementation\r\n- tests that are hard to read\r\n\r\n## FastAPI application with inefficient tests - 10min\r\n\r\nWe'll take a look at example FastAPI application that is tested with inefficient tests. We'll take a look at what we need to do when we want to add a feature to our application - how many existing tests fail when we add a new feature, how many tests need updating when we add a new feature or refactor something, how long does it take to run the tests, ...\r\n\r\n## FastAPI application with efficient tests - 25min\r\nWe'll take a look at the example FastAPI application that is tested with efficient tests. We'll dig into pytest factory fixtures, FastAPI's dependency injection for testing, snapshot testing, and testing doubles. We'll compare the burden of adding a new feature to the application with inefficient tests.\r\n\r\n## Q & A - 5min\r\nI'll answer questions from the audience", "code": "GLYNAK", "state": "submitted", "created": "2024-12-20", "speaker_names": "Jan Giacomelli", "track": "PyCon: Testing"}, {"title": "Prove What You Compute: Cryptographic Verification for Python Pipelines", "abstract": "This talk introduces a framework for cryptographically verifying computational pipelines in Python. By generating unforgeable proofs at each stage, you can guarantee your results haven't been tampered with or fabricated. Whether you're running machine learning workflows, data transformations, or scientific computations, you'll learn how to implement verification that proves your pipeline executed as intended.", "full_description": "With automated pipelines and distributed computing, how do you prove your results actually came from your intended computation? Whether you're concerned about system integrity, audit requirements, or catching subtle bugs, computational proof systems offer a powerful solution. \r\n\r\nThrough hands-on examples, we'll build a verifiable pipeline that generates cryptographic evidence of its execution. You'll learn how to:\r\n\r\n- Generate tamper-proof fingerprints that capture the exact state of computations\r\n- Scale the proof system efficiently through sampling, batching and parallelization\r\n- Make proof forgery computationally and economically infeasible\r\n\r\nAttendees will leave with working example code and practical instructions for adding verification to their own pipelines.", "code": "QJZ9NC", "state": "submitted", "created": "2024-12-21", "speaker_names": "Michele Dallachiesa", "track": "PyCon: Security"}, {"title": "Building a Local AI Photo Tagger with Django, DeepFace, and pgvector\"", "abstract": "Learn how to build a local photo tagger using Django, DeepFace, and pgvector. We\u2019ll explore face detection, embedding storage, and similarity searches with a focus on L2 distance, all through a minimal admin-focused interface. See how privacy-first AI tools can be simple and powerful.", "full_description": "Imagine a world where you can manage and tag your photo collection without relying on cloud services or sacrificing privacy. In this talk, we\u2019ll show you how to build your own local AI photo tagger using Django, DeepFace, and pgvector.  \r\n\r\n**What we\u2019ll cover:**  \r\n- **Face detection and embedding generation**: We\u2019ll use DeepFace to detect faces in photos and calculate embeddings\u2014numerical representations of faces.  \r\n- **Storing and comparing embeddings**: With pgvector, a Postgres extension, we\u2019ll store these embeddings and compute similarity searches efficiently. We\u2019ll discuss different similarity algorithms, focusing on L2 distance as a practical example.  \r\n- **Django as the interface**: Using Django\u2019s admin interface, we\u2019ll create a simple, functional tool for uploading, tagging, and searching photos locally.  \r\n\r\nThis talk provides a step-by-step walkthrough of how these components fit together to deliver a seamless photo tagging solution. We\u2019ll explain the underlying concepts, highlight the advantages of a privacy-first approach, and showcase the setup in action through a small demo.  \r\n\r\nBy the end of the session, you\u2019ll understand how to integrate these tools to create your own AI-powered applications, combining ease of use with cutting-edge technology. Whether for personal use or as inspiration for larger projects, this talk demonstrates how to keep AI local, simple, and impactful.", "code": "ZJS9G7", "state": "submitted", "created": "2024-12-20", "speaker_names": "melhin", "track": "PyCon: Django & Web"}, {"title": "\ud83d\uded1 Don't Stop 'til You Get Enough - Sequential Hypothesis Testing w/\u201cEnhanced Precision is the Goal\"", "abstract": "In hypothesis testing the stopping criterion for data collection is a non-trivial question that puzzles many analysts. This is especially true with sequential testing where demands for quick results may lead to biassed ones. \r\n\r\nI show how the belief that Bayesian approaches magically resolve this issue is misleading and how to obtain reliable outcomes by focusing on sample precision as a goal.\r\n\r\nIn particular  introduce benefits of a novel approach called *Enhanced Precision is the Goal*.", "full_description": "Hypothesis testing may come off as a dark art. On the one hand, data collection is expensive. On the other, small data sets may not yield enough statistical significance to draw meaningful conclusions. Combining these constraints with stakeholder requirements for quick answers from data makes the task of choosing the sample size stopping criterion a challenging balancing act.\r\n\r\nThis is especially true if the data is collected in a sequential manner, where a person, or an algorithm, needs to determine when to stop collecting data to satisfy the project requirements without introducing *confirmation bias*. This has the advantage of potentially reducing costs and time if a significant signal is detected.\r\n\r\nThis talk is targeted to anyone involved in experimentation, technical or managerial, and is interested in improving how they plan an experiment budget and conduct post collection interpretation. A basic understanding of statistics and hypothesis testing are nice-to-haves but not essential as I outline the basics.\r\n\r\nIn this presentation you will learn why even though Bayesian approaches are more reliable than Frequentist ones for small data sets they do not magically solve the problem of confirmation bias in sequential hypothesis testing. This is done by introducing John Kruschke's \u201cPrecision is the Goal\u201d method, where by determining in advance the experiment expected precision level yields unbiassed results. \r\n\r\nAdditionally, I will introduce an improved new approach which enhances practical application. Specifically, the original method, in some circumstances may lead to an inconclusive outcome. I show that this may be solved for by introducing slightly more conservative variant of this stop criterion.", "code": "QCKBME", "state": "submitted", "created": "2024-12-20", "speaker_names": "Eyal Kazin", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Extending Python's async capabilities using a Rust runtime", "abstract": "This talk will explore the increasing integration of Rust into the Python ecosystem, and how we extend async capabilities in Python using Pyop3 by giving an example of Robyn and the kind of architectural changes that were required for the same.", "full_description": "With the growing adoption of Rust bindings in the Python ecosystem with the pyop3 Rust bindings for Python, throughput efficiency has become a top priority.\r\n\r\nInspired by Python\u2019s extensibility and ease of use, and powered by Rust\u2019s performance, Robyn was created.\r\n\r\nRobyn stands out as one of the fastest Python web frameworks, delivering near-native Rust performance while maintaining the simplicity of Python. Its runtime, written in Rust, enables developers to build high-performance web applications effortlessly. This is the perfect example of extending Interoperability of Python with Rust to create an async Python web framework with a Rust Runtime\r\n\r\nWe also discuss some of the drawbacks of the notorious Global Interpreter Lock in Python doesn\u2019t allow for true concurrency while Rust uses native threading for faster execution. \r\n\r\nSo this Case study of using Rust in Robyn framework, architectural highlights and benefits\r\n\r\n1. Some drawbacks of Python native apps like lack of good async support, GIL, unoptimized concurrency\r\n2. Need for use of PyOp3 \r\n3. Architectural benefits of introducing Rust for getting native async and concurrency benefits by using rust runtime\r\n4. Case study of using Rust in Robyn framework, architectural highlights and benefits", "code": "YTK8GW", "state": "withdrawn", "created": "2024-12-22", "speaker_names": "Gaurav Pandey", "track": "PyCon: Django & Web"}, {"title": "Modern GraphQL Development Patterns with Strawberry", "abstract": "GraphQL has become a powerful alternative to REST, offering flexible and efficient data retrieval. However, as projects scale, developers often struggle with maintaining clean, efficient, and testable GraphQL APIs. In this talk, we\u2019ll explore modern development patterns using Strawberry, the Pythonic GraphQL library, to build production-ready APIs with Django.", "full_description": "We\u2019ll start by examining how to structure modular and reusable GraphQL schemas, enabling a clean separation of concerns. Next, we\u2019ll dive into strategies for input validation, optimizing queries to avoid the dreaded N+1 problem, and handling complex mutations. You\u2019ll learn how to incorporate subscriptions for real-time updates using WebSockets, as well as integrate proper authentication, authorization, and error handling. Finally, we\u2019ll discuss real-world examples and challenges encountered while building GraphQL backends at scale.\r\n\r\nBy the end of this talk, you\u2019ll walk away with actionable patterns and tools to build clean, efficient, and maintainable GraphQL APIs using Strawberry and Django. Whether you\u2019re new to GraphQL or looking to refine your approach, this session has something for you.", "code": "Y7DLTA", "state": "submitted", "created": "2024-12-17", "speaker_names": "Arthur Bayr", "track": "PyCon: Django & Web"}, {"title": "Extending Python's async capabilities using a Rust runtime", "abstract": "With the growing adoption of Rust bindings in the Python ecosystem with the pyop3 Rust bindings for Python, throughput efficiency has become a top priority.\r\n\r\nInspired by Python\u2019s extensibility and ease of use, and powered by Rust\u2019s performance, Robyn was created.\r\n\r\nRobyn stands out as one of the fastest Python web frameworks, delivering near-native Rust performance while maintaining the simplicity of Python. Its runtime, written in Rust, enables developers to build high-performance web applications effortlessly. This is the perfect example of extending Interoperability of Python with Rust to create an async Python web framework with a Rust Runtime\r\n\r\nThis talk will explore the increasing integration of Rust into the Python ecosystem, and how we extend async capabilities in Python using Pyop3 by giving an example of Robyn and the kind of architectural changes that were required for the same. \r\n\r\nWe also discuss some of the drawbacks of the notorious Global Interpreter Lock in Python doesn\u2019t allow for true concurrency while Rust uses native threading for faster execution.", "full_description": "With the growing adoption of Rust bindings in the Python ecosystem with the pyop3 Rust bindings for Python, throughput efficiency has become a top priority.\r\n\r\nInspired by Python\u2019s extensibility and ease of use, and powered by Rust\u2019s performance, Robyn was created.\r\n\r\nRobyn stands out as one of the fastest Python web frameworks, delivering near-native Rust performance while maintaining the simplicity of Python. Its runtime, written in Rust, enables developers to build high-performance web applications effortlessly. This is the perfect example of extending Interoperability of Python with Rust to create an async Python web framework with a Rust Runtime\r\n\r\nThis talk will explore the increasing integration of Rust into the Python ecosystem, and how we extend async capabilities in Python using Pyop3 by giving an example of Robyn and the kind of architectural changes that were required for the same. \r\n\r\nWe also discuss some of the drawbacks of the notorious Global Interpreter Lock in Python doesn\u2019t allow for true concurrency while Rust uses native threading for faster execution. \r\n\r\nSo this Case study of using Rust in Robyn framework, architectural highlights and benefits\r\n\r\n\r\n1. Some drawbacks of Python native apps like lack of good async support, GIL, unoptimized concurrency\r\n2. Need for use of PyOp3 \r\n3. Architectural benefits of introducing Rust for getting native async and concurrency benefits by using rust runtime\r\n4. case study of using Rust in Robyn framework, architectural highlights and benefits", "code": "HPA3PG", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Gaurav Pandey", "track": "General: Rust"}, {"title": "Securing Generative AI: Essential Threat Modeling Techniques", "abstract": "Generative AI development introduces unique security challenges that traditional methods often overlook. This talk explores practical threat modeling techniques tailored for AI practitioners, focusing on real-world scenarios encountered in daily development. Through relatable examples and demonstrations, attendees will learn to identify and mitigate common vulnerabilities in AI systems. The session covers user-friendly security tools and best practices specifically designed for AI development. By the end, participants will have practical strategies to enhance the security of their AI applications, regardless of their prior security expertise.", "full_description": "1. Introduction\r\n    * Motivation\r\n    * What can go wrong\r\n2. Generative AI vs Traditional Applications\r\n    * Key differences in security considerations\r\n    * Unique challenges posed by generative AI\r\n3. Threat Modeling Basics and AI-Specific Threats \r\n    * STRIDE framework\r\n    * Focus on prompt injection and data poisoning\r\n    * Example: Simple prompt injection attempt\r\n4. Practical Threat Modeling Process\r\n    * Simplified system decomposition example\r\n    * Threat identification walkthrough\r\n5.  Example: Input Validation\r\n6. Tools Showcase and Mitigation Strategies\r\n    * AI security tools applicable\r\n    * Best practices for API security\r\n7. Conclusion and Resources\r\n    * Recap key takeaways\r\n    * List of recommended tools and further reading", "code": "UGTB7A", "state": "submitted", "created": "2025-01-05", "speaker_names": "Elizaveta Zinovyeva", "track": "PyData: Generative AI"}, {"title": "Mastering Excel Automation with OpenPYXL", "abstract": "Tired about complex VBA macros and functions? Join the Excel revolution as we see how OpenPYXL Python library changes how we work with spreadsheets. We'll discover how to handle data with a touch of Python magic. By the end, you\u2019ll be an Excel expert, easily automating tasks and managing complex data with a few clicks. Get ready to improve your data skills!", "full_description": "Tired about complex VBA macros and functions? Join the Excel revolution as we see how OpenPYXL Python library changes how we work with spreadsheets. We'll discover how to handle data with a touch of Python magic. By the end, you\u2019ll be an Excel expert, easily automating tasks and managing complex data with a few clicks. Get ready to improve your data skills!\r\n\r\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "code": "QJK79N", "state": "rejected", "created": "2024-12-31", "speaker_names": "Jordi Bosch", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Teaching AI to Speak Your Database\u2019s Language", "abstract": "In the era of Generative AI, Text-to-SQL is revolutionising database interactions by translating natural language into SQL queries. While models like GPT, CodeLlama, and SQLCoder excel at SQL generation, they falter when dealing with the complexities of custom databases. This is where Retrieval-Augmented Generation (RAG) steps in, providing context to improve query accuracy. However, RAG\u2019s effectiveness depends on the quality of its embedding model, which often struggles with vague table and column names or overlapping names across tables - perfect example of *\"garbage in, garbage out\"*. In this talk, Aarti will share practical strategies for fine-tuning embedding models to better handle custom databases, enabling more accurate and relevant SQL generation.", "full_description": "##### Overview\r\nWhile RAG is a reliable approach for many scenarios, working with structured data brings unique challenges. Questions such as how to chunk tabular data effectively, create robust training datasets, and utilise negative datasets for better performance often emerge. This talk will address these challenges, offering insights into building a complete Text-to-SQL workflow. Additionally, it will explore the latest evaluation metrics for retrieval, helping attendees better understand how to enhance model accuracy when working with structured data.\r\n\r\n----\r\n\r\n\r\n##### Outline\r\n* Introduction to Text-to-SQL \r\n* Understanding RAG \r\n* Limitations of RAG in Text-to-SQL \r\n* Dataset generation \r\n* Negative data mining\r\n* Fine-tuning embedding model \r\n* Evaluation metrics \r\n* RAG workflow", "code": "3NUFNG", "state": "submitted", "created": "2024-12-21", "speaker_names": "Aarti Jha", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond", "abstract": "\"Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond\" explores a common programming tool with a fresh perspective. While exceptions are a key feature in Python and other languages, they share surprising similarities with the notorious goto statement. This talk examines those parallels, the problems exceptions can create, and practical alternatives for better code. Attendees will gain a clear understanding of modern programming concepts and the evolution of programming.", "full_description": "Exceptions have long been seen as an improvement over error-handling approaches like goto. However, they can introduce complexity and obscure control flow when used without care. This talk will critically examine exceptions, outline the similarities to goto, and explore better ways to handle errors in programming.\r\n\r\n### Outline:\r\n\r\n1. Introduction (5 minutes)\r\n    - The historical role of goto in programming.\r\n    - Spaghetti code and the rise of structured programming.\r\n    - How exceptions emerged as an alternative.\r\n2. Why and What Are Exceptions (10 minutes)\r\n    - Why exceptions were introduced.\r\n    - How they became mainstream in languages like Java and C++.\r\n    - Common problems caused by exceptions: hidden control flow, debugging challenges, and performance impacts.\r\n3. The Evolution Toward Result Types (10 minutes)\r\n    - How result types address the shortcomings of exceptions.\r\n    - Implementations in Haskell, Rust, and Golang.\r\n    - Real-world benefits of using result types.\r\n4. Using Result Types in Python (10 minutes)\r\n    - Introducing the returns package.\r\n    - Practical examples of result types in Python.\r\n    - How this approach improves code clarity and reliability.\r\n5. Conclusion (5 minutes)\r\n    - Recap of the journey from goto to exceptions to result types.\r\n    - Key takeaways: thoughtful error handling and modern best practices.\r\n    - Encouragement to explore and adopt better patterns in Python.\r\n\r\nThis session is ideal for intermediate and advanced Python developers seeking actionable techniques to improve error handling and write cleaner, more predictable code.", "code": "S8MUBF", "state": "confirmed", "created": "2024-12-12", "speaker_names": "Florian Wilhelm", "track": "PyCon: Programming & Software Engineering"}, {"title": "Multivariate Datastrophe: Methods to Detect Obscure Drift in Your Production Data", "abstract": "Getting your model into production isn\u2019t a trivial task, but it\u2019s only half the battle. Ensuring that your model continues to deliver great performance over time is even more critical. In this talk I would like to present a selection of what can kill your model\u2019s performance, zoom in on multivariate data drift, and present two methods to detect this type of drift in your production data.", "full_description": "Objectives: The goal of this talk is to explain what multivariate data drift is and to present methods for detecting it. You will learn about the challenges associated with multivariate drift and explore practical solutions to identify it\r\n\r\n1. Model\u2019s Aren\u2019t Forever: \r\n91% of models in production degrade over time. I will explain the importance of continuous model monitoring and discuss common approaches and pitfalls. \r\n\r\n2. All Ways Your Perfectly Fine ML Model Can Fail: \r\nMachine learning models can fail for numerous reasons. I will describe the root causes of ML model failure, including data quality, data drift and the final boss of every model -  concept drift.\r\nI will explore simple methods to detect univariate data drift and delve into the more complex challenge of concept drift, which can significantly impact your model\u2019s performance in production.\r\n\r\n3. Drift Happens - Multivariate Data Drift: \r\nMultivariate data drift occurs when the relationships between multiple variables in your data change over time. This type of drift can be challenging to detect with standard methods. I will provide an explanation of what multivariate data drift is and why traditional techniques may fall short in identifying it.\r\n\r\n4. Two Clever Ways to Detect Multivariate Data Drift:\r\nDomain Classifier: This method involves training a classifier to distinguish between data from different time periods. If the classifier can accurately separate the data, it indicates that the distribution has changed, signaling potential drift.\r\nPCA Reconstruction Error: Principal Component Analysis (PCA) can be used to reduce the dimensionality of your data. By comparing the reconstruction error over time, you can detect changes in the underlying data distribution that may indicate drift.", "code": "HVTSDP", "state": "submitted", "created": "2025-01-05", "speaker_names": "Magdalena Kowalczuk", "track": "PyCon: MLOps & DevOps"}, {"title": "Dipping my toes in metaprogramming", "abstract": "Everyone knows the dunder method `__init__`.\r\nEven if you don't understand how it works, you know you're supposed to use it to initialise attributes of your instances.\r\n\r\nBut have you heard of `__init__`'s big brother, `__new__`?\r\n\r\nThis live-coded talk shows you what the dunder method `__new__` does and how it works based on a real-world use case from the standard library.\r\n\r\nJoin and dip your toes in the world of metaprogramming in Python!", "full_description": "Metaprogramming can be quite an intimidating field and the truth is that most of the time you don't need to know much about it.\r\nIf anything at all.\r\n\r\nThe dunder method `__new__` lives in the border between esoteric metaprogramming you'd learn \u201cjust because\u201d and the Python features that advance your understanding of the language and make you a better developer.\r\n\r\nThis live-coded talk will use a progression of a series of small examples and analogies to take you from the dunder method `__init__`, that you should be familiar with, to the method `__new__`.\r\nBy the end of the talk, you will understand how `__new__` fits within the Python data model and what it can be useful for.\r\n\r\nTo reach this understanding, we will:\r\n- explore the functionality from `pathlib.Path` that automatically creates different instances based on the OS of the user;\r\n- understand how `__new__` and `__init__` are used in different moments of class instantiation;\r\n- see how the return value of `__new__` interacts with the dunder method `__init__`; and\r\n- use `__new__` to subclass immutable types and, in particular, to create a class `TolerantFloat` as a subclass of `float`.", "code": "3FLVUX", "state": "submitted", "created": "2024-12-24", "speaker_names": "Rodrigo Gir\u00e3o Serr\u00e3o", "track": "PyCon: Python Language & Ecosystem"}, {"title": "From Numbers to an Impactful Data Story", "abstract": "You use data to support your stakeholders to make decisions, but your insights are not leading to actions?\r\n\r\n\ud835\udde7\ud835\uddf5\ud835\uddf2\ud835\uddfb \ud835\uddf7\ud835\uddfc\ud835\uddf6\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddf9 \u201e\ud835\uddd9\ud835\uddff\ud835\uddfc\ud835\uddfa \ud835\udde1\ud835\ude02\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddff\ud835\ude00 \ud835\ude01\ud835\uddfc \ud835\uddee\ud835\uddfb \ud835\udddc\ud835\uddfa\ud835\uddfd\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf3\ud835\ude02\ud835\uddf9 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\ude06\u201c.\r\n\r\nI will give you an introduction into data storytelling.\r\n\r\nYou will learn proven techniques of data storytelling, see engaging examples and can directly ask for advice on how to apply them to your specific data story.", "full_description": "\ud835\uddd7\ud835\uddfc \ud835\uddec\ud835\uddfc\ud835\ude02 \ud835\udde6\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf4\ud835\uddf4\ud835\uddf9\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\udde7\ud835\ude02\ud835\uddff\ud835\uddfb \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddfc \ud835\uddd4\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb?\r\n\r\nDo you feel demotivated and \ud835\ude6a\ud835\ude63\ud835\ude56\ud835\ude65\ud835\ude65\ud835\ude67\ud835\ude5a\ud835\ude58\ud835\ude5e\ud835\ude56\ud835\ude69\ud835\ude5a\ud835\ude59, because your data projects \ud835\ude59\ud835\ude64 \ud835\ude63\ud835\ude64\ud835\ude69 \ud835\ude61\ud835\ude5a\ud835\ude56\ud835\ude59 \ud835\ude69\ud835\ude64 \ud835\ude58\ud835\ude64\ud835\ude63\ud835\ude58\ud835\ude67\ud835\ude5a\ud835\ude69\ud835\ude5a \ud835\ude56\ud835\ude58\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68 in your company? \r\n\r\nEven though you spend \ud835\ude56 \ud835\ude61\ud835\ude64\ud835\ude69 \ud835\ude64\ud835\ude5b \ud835\ude69\ud835\ude5e\ud835\ude62\ud835\ude5a \ud835\ude6a\ud835\ude63\ud835\ude59\ud835\ude5a\ud835\ude67\ud835\ude68\ud835\ude69\ud835\ude56\ud835\ude63\ud835\ude59\ud835\ude5e\ud835\ude63\ud835\ude5c your \ud835\ude68\ud835\ude69\ud835\ude56\ud835\ude60\ud835\ude5a\ud835\ude5d\ud835\ude64\ud835\ude61\ud835\ude59\ud835\ude5a\ud835\ude67\ud835\ude68 and adjusting your language to theirs, \ud835\ude69\ud835\ude5d\ud835\ude5a\ud835\ude6e \ud835\ude68\ud835\ude69\ud835\ude5e\ud835\ude61\ud835\ude61 \ud835\ude59\ud835\ude64\ud835\ude63\u2019\ud835\ude69 \ud835\ude56\ud835\ude58\ud835\ude69 \ud835\ude64\ud835\ude63 \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude67\ud835\ude5a\ud835\ude58\ud835\ude64\ud835\ude62\ud835\ude62\ud835\ude5a\ud835\ude63\ud835\ude59\ud835\ude56\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68?\r\n\r\nAs a result, you \ud835\ude5b\ud835\ude5a\ud835\ude5a\ud835\ude61 \ud835\ude5b\ud835\ude67\ud835\ude6a\ud835\ude68\ud835\ude69\ud835\ude67\ud835\ude56\ud835\ude69\ud835\ude5a\ud835\ude59 and \ud835\ude66\ud835\ude6a\ud835\ude5a\ud835\ude68\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude6b\ud835\ude56\ud835\ude61\ud835\ude6a\ud835\ude5a \ud835\ude64\ud835\ude5b \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude6c\ud835\ude64\ud835\ude67\ud835\ude60 - or you \ud835\ude65\ud835\ude6a\ud835\ude68\ud835\ude5d even \ud835\ude5d\ud835\ude56\ud835\ude67\ud835\ude59\ud835\ude5a\ud835\ude67, creating more complex solutions, \ud835\ude6e\ud835\ude5a\ud835\ude69 \ud835\ude63\ud835\ude64\ud835\ude69\ud835\ude5d\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude68\ud835\ude5a\ud835\ude5a\ud835\ude62\ud835\ude68 \ud835\ude69\ud835\ude64 \ud835\ude67\ud835\ude5a\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude56\ud835\ude69\ud835\ude5a or make an impact.\r\n\r\nThe \ud835\ude6c\ud835\ude56\ud835\ude6e \ud835\ude64\ud835\ude6a\ud835\ude69 \ud835\ude64\ud835\ude5b \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude59\ud835\ude56\ud835\ude69\ud835\ude56-\ud835\ude59\ud835\ude5e\ud835\ude61\ud835\ude5a\ud835\ude62\ud835\ude62\ud835\ude56 is to use the power of data storytelling.\r\n\r\n\ud835\uddd6\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00 \ud835\udde7\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\udddf\ud835\uddf2\ud835\uddee\ud835\uddf1 \ud835\ude01\ud835\uddfc \ud835\udddc\ud835\uddfb\ud835\ude00\ud835\uddfd\ud835\uddf6\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\uddd4\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00\r\n\r\n\ud835\ude44\ud835\ude62\ud835\ude56\ud835\ude5c\ud835\ude5e\ud835\ude63\ud835\ude5a:\r\n\r\n- You could spend \ud835\ude61\ud835\ude5a\ud835\ude68\ud835\ude68 \ud835\ude69\ud835\ude5e\ud835\ude62\ud835\ude5a \ud835\ude6c\ud835\ude64\ud835\ude67\ud835\ude60\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude64\ud835\ude63 \ud835\ude57\ud835\ude6a\ud835\ude5e\ud835\ude61\ud835\ude59\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude65\ud835\ude67\ud835\ude5a\ud835\ude68\ud835\ude5a\ud835\ude63\ud835\ude69\ud835\ude56\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68 but with \ud835\ude62\ud835\ude64\ud835\ude67\ud835\ude5a \ud835\ude5e\ud835\ude62\ud835\ude65\ud835\ude56\ud835\ude58\ud835\ude69 and tangible \ud835\ude67\ud835\ude5a\ud835\ude68\ud835\ude6a\ud835\ude61\ud835\ude69\ud835\ude68.\r\n\r\n- Your stakeholders are \ud835\ude5a\ud835\ude6d\ud835\ude58\ud835\ude5e\ud835\ude69\ud835\ude5a\ud835\ude59 about seeing your \ud835\ude62\ud835\ude5a\ud835\ude56\ud835\ude63\ud835\ude5e\ud835\ude63\ud835\ude5c\ud835\ude5b\ud835\ude6a\ud835\ude61 \ud835\ude59\ud835\ude56\ud835\ude69\ud835\ude56 \ud835\ude68\ud835\ude64\ud835\ude61\ud835\ude6a\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68 and are \ud835\ude5a\ud835\ude56\ud835\ude5c\ud835\ude5a\ud835\ude67 to turn them into \ud835\ude56\ud835\ude58\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68.\r\n\r\n- You are \ud835\ude67\ud835\ude5a\ud835\ude58\ud835\ude64\ud835\ude5c\ud835\ude63\ud835\ude5e\ud835\ude68\ud835\ude5a\ud835\ude59 \ud835\ude56\ud835\ude68 \ud835\ude5a\ud835\ude6d\ud835\ude65\ud835\ude5a\ud835\ude67\ud835\ude69 and achieve notable \ud835\ude65\ud835\ude5a\ud835\ude67\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude56\ud835\ude61 \ud835\ude56\ud835\ude63\ud835\ude59 \ud835\ude5b\ud835\ude5e\ud835\ude63\ud835\ude56\ud835\ude63\ud835\ude58\ud835\ude5e\ud835\ude56\ud835\ude61 \ud835\ude5c\ud835\ude67\ud835\ude64\ud835\ude6c\ud835\ude69\ud835\ude5d in your career.\r\n\r\n\r\nThe \ud835\ude60\ud835\ude5a\ud835\ude6e \ud835\ude61\ud835\ude5e\ud835\ude5a\ud835\ude68 \ud835\ude5e\ud835\ude63 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude68\ud835\ude69\ud835\ude64\ud835\ude67\ud835\ude6e of your data, \ud835\ude63\ud835\ude64\ud835\ude69 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude59\ud835\ude56\ud835\ude69\ud835\ude56 itself. Without an engaging story, your stakeholders don\u2019t see \ud835\ude5d\ud835\ude64\ud835\ude6c \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude5e\ud835\ude63\ud835\ude68\ud835\ude5e\ud835\ude5c\ud835\ude5d\ud835\ude69\ud835\ude68 \ud835\ude68\ud835\ude64\ud835\ude61\ud835\ude6b\ud835\ude5a \ud835\ude69\ud835\ude5d\ud835\ude5a\ud835\ude5e\ud835\ude67 \ud835\ude65\ud835\ude67\ud835\ude64\ud835\ude57\ud835\ude61\ud835\ude5a\ud835\ude62\ud835\ude68.\r\n\r\nIn this tutorial you will learn \ud835\ude65\ud835\ude67\ud835\ude64\ud835\ude6b\ud835\ude5a\ud835\ude63 \ud835\ude69\ud835\ude5a\ud835\ude58\ud835\ude5d\ud835\ude63\ud835\ude5e\ud835\ude66\ud835\ude6a\ud835\ude5a\ud835\ude68 of data storytelling, see engaging examples and can directly ask for advice on how to apply them to \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude68\ud835\ude65\ud835\ude5a\ud835\ude58\ud835\ude5e\ud835\ude5b\ud835\ude5e\ud835\ude58 \ud835\ude59\ud835\ude56\ud835\ude69\ud835\ude56 \ud835\ude68\ud835\ude69\ud835\ude64\ud835\ude67\ud835\ude6e.", "code": "CQ3JKN", "state": "submitted", "created": "2024-12-17", "speaker_names": "Nadine Keil", "track": "General: Education, Career & Life"}, {"title": "Crafting Production Ready RAG/GenAI Recipes with OPEA", "abstract": "Enterprises face a number of challenges when it comes to development and deployment of  Python based Gen AI solutions. The development of new models, algorithms, fine tuning techniques, detecting and resolving bias and how we deploy large solutions at scale continues to evolve at a rapid pace. There is a lack of standardized software tools and technologies to choose from. \r\n\r\nOPEA, the Open Platform for Enterprise AI, is an open source project with the Linux Foundation. It provides a framework of composable microservices for state-of-the-art GenAI systems including LLMs, data stores, and prompt engines to expedite enterprise adoption. It provides blueprints of end-to-end workflows for popular usage such as ChatQnA, CodeGen, and RAG systems. This talk explores the practical steps for deploying Generative AI (GenAI) applications in cloud-native environments with OPEA. It will show deployments on a Kubernetes cluster on a range of hyperscalers. A wide variety of data stores, including open source vector databases and managed services, will be used to demonstrate production grade RAG capabilities. We will show the results of scale testing with > 50K documents. The attendees will learn how to deploy GenAI applications in a cloud-native way using OPEA. Explicit contribution opportunities will be shared with the attendees.", "full_description": "Enterprises face a number of challenges when it comes to development and deployment of  Python based Gen AI solutions. The development of new models, algorithms, fine tuning techniques, detecting and resolving bias and how we deploy large solutions at scale continues to evolve at a rapid pace. There is a lack of standardized software tools and technologies to choose from. \r\n\r\nOPEA, the Open Platform for Enterprise AI, is an open source project with the Linux Foundation. It provides a framework of composable microservices for state-of-the-art GenAI systems including LLMs, data stores, and prompt engines to expedite enterprise adoption. It provides blueprints of end-to-end workflows for popular usage such as ChatQnA, CodeGen, and RAG systems. This talk explores the practical steps for deploying Generative AI (GenAI) applications in cloud-native environments with OPEA. It will show deployments on a Kubernetes cluster on a range of hyperscalers. A wide variety of data stores, including open source vector databases and managed services, will be used to demonstrate RAG capabilities. We will show the results of scale testing with > 50K documents. The attendees will learn how to deploy GenAI applications in a cloud-native way using OPEA. Explicit contribution opportunities will be shared with the attendees.", "code": "BSXYJT", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Suvrakamal Das", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Taking Control of LLM Outputs: An Introductory Journey into Logits", "abstract": "This talk explores the use of logits - the raw confidence scores that language models generate before selecting each token. Working directly with logits enables finer control over model behavior.\r\n\r\nThe session covers practical techniques for accessing and utilizing these scores through local models. Topics include detecting model uncertainty, implementing custom stopping conditions, and steering generation without prompt modifications.\r\n\r\nYou will learn how to analyze model confidence patterns and apply this knowledge to real-life use cases.", "full_description": "Logits are the raw numerical scores that language models compute for each token in their vocabulary before making a selection. These scores are typically converted to probabilities and used internally for token selection. Accessing and analyzing them directly opens up possibilities for controlling and understanding model behavior.\r\n\r\nLogits provide insights into model uncertainty, help detect potential hallucinations, and enable fine-grained control over generation without modifying prompts.\r\n\r\nIn this session, attendees will learn how to access logits through local models, visualize confidence patterns, and implement practical techniques like uncertainty detection and generation steering. You can get practical insight which you can apply to your own projects.", "code": "VDG9YG", "state": "submitted", "created": "2024-12-20", "speaker_names": "Emek G\u00f6zl\u00fckl\u00fc", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building a HybridRAG Document Question-Answering System", "abstract": "Retrieval Augmented Generation (RAG) is a powerful technique for searching across unstructured documents, but it often falls short when the task demands an understanding of intricate relationships between entities. GraphRAG addresses this by leveraging knowledge graphs to capture these relationships, but it struggles with scalability and handling diverse unstructured formats. In this talk, we\u2019ll explore how HybridRAG combines the strengths of both approaches - RAG for scalable unstructured data retrieval and GraphRAG for semantic richness- to deliver accurate and contextually relevant answers. We\u2019ll dive into its application, challenges, and the significant improvements it offers for question-answering systems across various domains.", "full_description": "#### Outline:\r\n\r\n1. Introduction  \r\n   - The challenge of extracting information from unstructured and domain-specific text (e.g., legal documents).  \r\n   - Overview of traditional RAG techniques and their limitations:  \r\n     - Scalability and unstructured data handling.  \r\n     - Lack of semantic depth to capture intricate relationships.  \r\n   - Why HybridRAG is a game-changer.  \r\n\r\n2. What is RAG? \r\n   - Explanation of vector-based retrieval using embeddings and databases.  \r\n   - Advantages of RAG:  \r\n     - Scalable search across diverse unstructured formats.  \r\n     - Domain-agnostic retrieval capabilities.  \r\n   - Limitations:  \r\n     - Inability to capture relationships between entities.  \r\n     - Difficulty handling domain-specific or complex queries.  \r\n\r\n3. What is GraphRAG?  \r\n   - Explanation of GraphRAG: How knowledge graphs enhance retrieval by mapping relationships between entities.  \r\n   - Benefits of GraphRAG:  \r\n     - Semantic richness and contextual understanding.  \r\n     - Effective for domains requiring deep relational reasoning (e.g., finance, healthcare).  \r\n   - Challenges of GraphRAG:  \r\n     - Building high-quality knowledge graphs from unstructured data.  \r\n     - Scalability and integration with generative models.  \r\n\r\n4. Introducing HybridRAG: Combining RAG and GraphRAG \r\n   - The HybridRAG architecture:  \r\n     - RAG for scalable retrieval of unstructured data.  \r\n     - GraphRAG for refining answers with relational and semantic context.  \r\n   - Benefits of HybridRAG:  \r\n     - Combining scalability with semantic depth.  \r\n     - Improved retrieval accuracy and contextual relevance.  \r\n   - Use case: Legal documents processing (e.g., extracting Q&A insights).  \r\n     - How RAG retrieves general context.  \r\n     - How GraphRAG captures relationships (e.g., between companies, documents, events).  \r\n\r\n5. Challenges in Building HybridRAG Systems\r\n   - Creating high-quality knowledge graphs from diverse and unstructured data.  \r\n   - Balancing computational overhead from combining RAG and GraphRAG.  \r\n   - Addressing domain-specific terminology and ensuring generalizability to other domains.  \r\n\r\n6. Key Takeaways\r\n   - HybridRAG effectively combines the strengths of RAG and GraphRAG.  \r\n   - It\u2019s particularly powerful for domains requiring both scalability and semantic depth.  \r\n   - Practical advice for building HybridRAG systems in your projects.  \r\n\r\n#### What You\u2019ll Learn:\r\n- The strengths and limitations of RAG and GraphRAG techniques for question-answering systems.  \r\n- How HybridRAG bridges the gap by combining scalable retrieval with semantic richness.  \r\n- Practical challenges and solutions for building HybridRAG systems, including knowledge graph creation and integration.  \r\n- Insights into real-world applications where HybridRAG delivers superior results.", "code": "9CRNU3", "state": "submitted", "created": "2025-01-03", "speaker_names": "Darya Petrashka", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Mitigating Hallucinations in Multimodal LLMs with HALVA", "abstract": "HALVA: Hallucination Attenuated Language and Vision Assistant\r\nA new contrastive tuning strategy mitigates hallucinations while retaining general performance in multimodal LLMs. Data-augmented phrase-level alignment for mitigating object hallucination.", "full_description": "Data-augmented contrastive tuning has been introduced to mitigate object hallucination in MLLMs. The proposed method effectively mitigates object hallucinations and beyond while retaining or improving performance on general vision-language tasks. Moreover, the proposed contrastive tuning is simple, fast, and requires minimal training with no additional overhead at inference. This method may have applications in other areas as well. For example, it might be adapted to mitigate bias and harmful language generation.", "code": "LS3QNT", "state": "submitted", "created": "2024-12-21", "speaker_names": "Jayita Bhattacharyya", "track": "PyData: Generative AI"}, {"title": "Rise of Product Engineer: Reimagining the Role Software Engineer", "abstract": "The role of the software engineer is evolving\u2014from coders of the waterfall era to agile-driven creators and, now, to product-focused strategists. In this talk, we\u2019ll explore how engineers can adapt to modern demands by prioritizing business value, tackling tech debt, and contributing to data-driven discovery. Using a Django application as a practical example, we\u2019ll demonstrate how teams can design architectures that balance flexibility and structure\u2014equipping engineers to thrive in this reimagined role.", "full_description": "Software engineering has undergone a profound transformation, and this talk explores the evolution of the role through three critical eras: the past, the present, and the future.  \r\n\r\n**The Past:** Starting with the waterfall model, we\u2019ll examine the early days of software engineering. What role did engineers play in product lifecycles, and how did structured, linear processes shape their work? We\u2019ll briefly discuss the successes and pitfalls of this approach and its influence on engineering practices.  \r\n\r\n**The Present:** The rise of Agile methodologies revolutionized how teams develop software. Engineers today are expected to own the full development cycle\u2014from coding to deployment\u2014while navigating pressures to release quickly and build feature-rich products. We\u2019ll unpack the expertise engineers rely on, their evolving responsibilities, and the challenges they face in balancing rapid delivery with sustainable design.  \r\n\r\n**The Future:** The next stage of software engineering demands a new focus. In an era of constrained resources, teams must emphasize business value above all else. We\u2019ll explore how engineers can support product teams by uncovering actionable insights through data discovery, integrating technical challenges like tech debt into the business cycle, and adopting a holistic understanding of product goals.  \r\n\r\nTo bring these concepts to life, we\u2019ll share the transformation journey of a Django application. We began by establishing anchor features, which helped us identify team strengths and collaboration patterns. From there, we grouped Django apps into logical hierarchies, setting boundaries to foster team alignment and ensure a stable core. Over time, as these structures became difficult to iterate on, we adapted by reversing hierarchies and focusing on flexible \u201ccontact points\u201d to support both team agility and evolving business needs. While pitfalls remain, this approach has helped us remodel apps in response to new challenges, balancing adaptability with stability.  \r\n\r\nThis session offers actionable insights for software engineers seeking to stay ahead of the curve and contribute more strategically to product success.", "code": "XEUFBK", "state": "submitted", "created": "2024-12-16", "speaker_names": "melhin", "track": "PyCon: Programming & Software Engineering"}, {"title": "Investing: Technical Analysis libraries in Python", "abstract": "We will explore the landscape of technical analysis libraries available for the Python language, including popular choices like [TA-Lib (aka talib)](https://github.com/TA-Lib/ta-lib-python), [Pandas TA](https://github.com/twopirllc/pandas-ta), and [Technical Analysis (aka bukosabino/ta)](https://github.com/bukosabino/ta) library.", "full_description": "Do you have financial savings to invest?! In this 45-minute talk, we'll explore the landscape of technical analysis libraries available for Python, including popular choices like [TA-Lib (aka talib)](https://github.com/TA-Lib/ta-lib-python), [Pandas TA](https://github.com/twopirllc/pandas-ta), and [Technical Analysis (aka bukosabino/ta)](https://github.com/bukosabino/ta) library. We'll explore their capabilities, comparing their pros and cons. Join us to unlock the power of Python in mastering technical analysis!", "code": "TRX898", "state": "submitted", "created": "2024-12-15", "speaker_names": "Ruslan Korniichuk", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Anatomy of a Python OpenTelemetry instrumentation", "abstract": "OpenTelemetry Python instrumentations may seem indistinguishable from magic: they can be bootstrapped from your installed dependencies, they are able to patch your code without even noticing and most often they work out of the box automatically! Let's dig a bit into the code to see what's inside the black box.", "full_description": "OpenTelemetry Python instrumentations may seem indistinguishable from magic: they can be bootstrapped from your installed dependencies, they are able to patch your code without even noticing and most often they work out of the box automatically! Fortunately there's no magic spell involved and they are mostly the combination of not well known Python features, standing on the shoulders of powerful libraries and work done by the community to improve the reach and the quality of the code. \r\nIn this talk we'll see:\r\n- how opentelemetry-bootstrap is able to install instrumentations for your installed libraries\r\n- how opentelemetry-instrument leverages the sitecustomize module for providing auto-instrumentation\r\n- how entry points can be used for components discovery\r\n- how instrumentations are able to patch third party code\r\nLet's dig a bit into the code to see what's inside the black box.", "code": "NUKE8C", "state": "submitted", "created": "2024-12-05", "speaker_names": "Riccardo Magliocchetti", "track": "PyCon: Programming & Software Engineering"}, {"title": "Enhancing Data Literacy and Model Explainability through Bayesian Statistics", "abstract": "This talk explores how Bayesian statistics enhances data literacy and model explainability, transforming complex data into actionable insights. Participants will learn to leverage these methods for more accurate predictions and informed decision-making in today's data-driven landscape.", "full_description": "This talk focuses on empowering individuals to interpret and communicate data more effectively using Bayesian methods. This approach simplifies complex models, making them transparent and accessible. By building intuitive, interpretable models, participants gain the ability to provide actionable insights. This topic bridges statistical rigor with practical application, deepening the understanding of data-driven decision-making. In the Fourth Industrial Revolution (4IR), mastering Bayesian methods is crucial for navigating the growing complexity of data, enabling more accurate predictions and informed decision-making in an increasingly data-driven world.", "code": "BA9D3C", "state": "submitted", "created": "2024-12-22", "speaker_names": "Brayan Kai Mwanyumba", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Will Python accept me now? Unlearning the nooby habits before you code next", "abstract": "Often coming from a background in other languages such as C++ or Java, we make a lot of programming decisions with Python, which are not just non-pythonic, but some can even be classified as noob. (No, it's not about using braces `{}` and semicolons `;` :')\r\n\r\nIn this talk, we will explore some common `nooby` programming decisions and learn how to make our code adapt to Python's idiomatic style better. Whether you\u2019re just starting or have years of experience, learn to avoid nooby habits and make the most of Python\u2019s simplicity and power. Remember `Simplicity != NoobProof`", "full_description": "As developers, transitioning to Python from other languages like Java and C++ often feels deceptively simple... until it doesn\u2019t. More often than not, we carry along practices that feel right but clash with Python\u2019s idiomatic style. In this talk, we\u2019ll explore how some programming decisions can unintentionally brand us as \"noobs\" and more importantly, how to unlearn them. \r\n\r\nWhile talking about:\r\n* How someone can mistake simplicity for efficiency?\r\n* How to embrace Python\u2019s unique constructs for cleaner, more expressive code?\r\n* How we can leverage Python\u2019s ecosystem to avoid reinventing the wheel? \r\n\r\nmuch more of similar \"nooby\" habits, which we often carry on as Polyglot Programming Baggage.\r\n\r\nBy the end of this talk, you'll learn to unlearn nooby habits, write more Pythonic code, and use Python\u2019s rich ecosystem to solve problems elegantly and effectively.\r\n\r\n**Come for the pitfalls; leave with Pythonic prowess.** \r\n\r\nP.S. Don't worry it's not going to be a `{}` vs indentation comparison talk. Bython is already there for that.", "code": "BF7RZB", "state": "submitted", "created": "2024-12-21", "speaker_names": "Adarsh Kumar, Palak Mittal", "track": "PyCon: Programming & Software Engineering"}, {"title": "Fine Grained Authorization: A Paradigm Shift", "abstract": "We have seen great progress in authentication over the last few years, but authorization is still challenging for most engineering teams.  Enter Fine Grained Authorization, an approach that simplifies access management by leveraging Relationship-Based Access Control (ReBAC), enhancing security, and reducing administrative overhead.", "full_description": "In this session, we will navigate the shortcomings of traditional authorization systems like Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC) and explore how OpenFGA, with its Relationship-Based Access Control (ReBAC), simplifies access management, enhances security, and reduces administrative overhead by providing fine-grained access control across diverse applications.\r\n\r\nKey takeaways from this session are:\r\n1. Gain insight into the limitations of traditional access control models like RBAC and ABAC.\r\n2. Learn how ReBAC can simplify complex access management challenges and enhance security posture.\r\n3. Understand OpenFGA and how it can be leveraged to implement FGA in your applications.", "code": "REKDKJ", "state": "submitted", "created": "2024-12-29", "speaker_names": "Ashish Jha", "track": "PyCon: Security"}, {"title": "From forecasts to price tags: Bridging Data Science complexity and retail scalability", "abstract": "The retail industry is becoming more data-driven than ever, but applying complex data science algorithms to optimize demand forecasting and automated pricing isn\u2019t always straightforward. Each retailer brings unique data, business constraints, and varying comfort levels with data science. So how can we create scalable, effective solutions that still meet individual needs?", "full_description": "In this talk, I\u2019ll share insights from working closely with both customers and product teams to address these challenges. You'll hear strategies for:\r\n\r\n- Explaining data science concepts clearly to a diverse audience.\r\n- Aligning algorithmic outputs with customer-specific goals.\r\n- Finding the sweet spot between customization and scalability in product design.\r\n\r\nExpect real-world examples and practical lessons learned from building impactful data science tools for retail. Whether you're a data scientist, developer, or product manager, you'll leave with ideas to make your solutions clearer, more scalable, and better suited for real-world deployment.\r\n\r\nSummary:\r\nThis talk explores the challenges of integrating data science products into the retail supply chain, particularly in demand forecasting and automated pricing. Attendees will learn how to bridge the gap between customer-specific requirements and expectations, diverse data landscapes, and complex algorithms while ensuring scalability and clarity for all stakeholders.", "code": "LSUFTY", "state": "submitted", "created": "2025-01-05", "speaker_names": "Asya", "track": "PyCon: Programming & Software Engineering"}, {"title": "Database development with Liquibase in a Gitlab Pipeline - lessons learned", "abstract": "This presentation discusses the challenges of building a database pipeline while using Liquibase for deploymens. I'll share insights and lessons learned after one year of building such a pipeline with a mix of shell scripts, the modernized Oracle command-line client \"sqlcl,\" its integration with Liquibase, Loader and utPLSQL. It is a DevOps presentation that concentrates on real examples of database centric development.", "full_description": "It\u2019s been nearly a year since, as a database developer, I first aimed to support the database development process in the GitLab pipeline.\r\n\r\nTypical challenges include:\r\n\r\n- developers want to create a sandbox environment where they can develop a feature without disturbing anything or breaking things.\r\n- developers want to be sure, through automated tests, that no regressions have been introduced into the code.\r\n- The database scripts should be created in such a way that they can deploy deltas, as well as build an empty schema/database from scratch.\r\n\r\nIn my presentation, I will discuss these points and describe how we wrote some jobs for the `.gitlab-ci.yml` in GitLab using a mix of shell scripts, the modernized Oracle command-line client \"sqlcl,\" its integration with Liquibase, Loader and utPLSQL.\r\n\r\nThe second part becomes more interesting, where I want to highlight the challenges we encountered and what to approach differently today to make the pipeline smoother. In this context, I will also touch upon the theory of CI/CD and explain why not all philosophies in the world fit into the world of a database developer.", "code": "K3EBQ3", "state": "submitted", "created": "2024-12-23", "speaker_names": "Jonas Gassenmeyer", "track": "PyCon: Programming & Software Engineering"}, {"title": "Safeguarding ML: Secure usage of model files, pre-trained models and ML packages", "abstract": "As Machine Learning (ML) and Artificial Intelligence (AI) models become increasingly critical to various industries and applications, the importance of securing the entire ML lifecycle is crucial. This talk focuses on the often overlooked aspects of ML security, specifically the vulnerabilities associated with loading and using model files, pre-trained models and ML Python packages. We discuss the risks and consequences of malicious attacks on these components, including model poisoning and package manipulation.\r\nOur presentation covers a range of security measures to mitigate these threats, including:\r\n1.\tSecure model file formats and verification techniques to detect tampering and ensure authenticity, especially for securely loading and using pre-trained models.\r\n2.\tSimple package managers and dependency management strategies to prevent malicious package injection.\r\n3.\tContinuous scanning of model artefacts in the model registry to detect problematic pre-trained models.\r\nWe also share best practices for implementing these security measures and workflows when using popular ML frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn. \r\nBy highlighting the importance of security in ML and providing actionable guidance, our goal is to enable ML practitioners in the industry to build secure and robust ML systems providing business value.", "full_description": "In today's data-driven world, Machine Learning (ML) models have become a critical component of various industries and applications. However, the ML lifecycle is vulnerable to a range of security threats that can compromise the integrity and accuracy of these models. This talk focuses on the often-overlooked aspects of ML security, specifically the vulnerabilities associated with loading and using model files, pre-trained models and ML Python packages.\r\nWe will begin by exploring the risks and consequences of malicious attacks on these components, including model poisoning and package manipulation. Model poisoning, for example, occurs when an attacker intentionally modifies a model to produce incorrect results, and malicious pre-trained models may execute arbitrary code when loaded. Package manipulation, on the other hand, involves injecting malicious code into popular Python packages used by ML developers.\r\nTo mitigate these threats, we will discuss a range of security measures that can be implemented throughout the ML lifecycle. This includes secure model file formats and verification techniques to detect tampering and ensure authenticity especially for securely loading and using pre-trained models and model artefact scanning when uploading pre-trained or our own models to our model registry. We will also explore the use of package managers and dependency management strategies to prevent malicious package injection.\r\nWe will discuss best practices for implementing these security measures when using popular ML frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn and how we designed workflows to enforce these security measures in our development environment and our productive MLOps platform.\r\nBy highlighting the importance of security in ML and providing actionable guidance, our goal is to enable ML practitioners in the industry to build secure and robust ML systems providing business value.", "code": "7VZCJG", "state": "submitted", "created": "2024-12-20", "speaker_names": "Andreas Wieltsch", "track": "PyCon: MLOps & DevOps"}, {"title": "Essential Airflow Plugins for Production", "abstract": "Apache Airflow is approaching the 10th anniversary of its initial release, and its development shows no signs of slowing down! A production airflow deployment is a complex piece of engineering, and many common tasks can still be quite difficult to manage. In this talk, we discuss a few Airflow plugins to bridge key gaps and make life easier for engineering and devops alike. In particular, we'll look at plugins for hierarchical central configuration, long-running or \"always on\" jobs, and integration with common alerting services.", "full_description": "Apache Airflow is one of the most popular and widely used schedulers in the world. As we approach the tenth anniversary of its release in June, developement continues to be very active with contributors around the world.\r\n\r\nThere are several common problems that arise in production Airflow deployments. In this talk, we'll discuss three problems and their open source solutions.\r\n\r\n\r\n#### Problem 1: Python\r\n\r\nDynamism and production jobs rarely get along. We adopt the *configuration-as-code* idea by integrating [`hydra`](https://hydra.cc) and [`pydantic`](https://pydantic.dev) to create composeable type-safe configuration, in [`airflow-config`](https://github.com/airflow-laminar/airflow-config). Key features\r\n\r\n- Define DAG- and Task- level configuration in a central location\r\n- Compose configurations for different environments\r\n- Generate DAGs directly from configuration\r\n- Integrate custom Airflow plugins and extensions with type-validated configurations\r\n\r\n#### Problem 2: Long-running Jobs\r\n\r\nAirflow was designed for batch jobs, but with some small tweaks, it also works for long-running or \"always on\" tasks.\r\n\r\n[`airflow-ha`](https://github.com/airflow-laminar/airflow-ha) provides a simple pattern for looping DAGs, and we connect to the popular process monitor [`supervisord`](https://supervisord.org) in [`airflow-supervisor`](https://github.com/airflow-laminar/airflow-supervisor). We can now run high-availability services efficiently without placing undo strain on Airflow from a single DAG.\r\n\r\n#### Problem 3: Alerting\r\n\r\nThe last problem is arguably the most important: alerting when DAGs fail. We have built [`airflow-priority`](https://github.com/airflow-laminar/airflow-priority) on top of the \"priority tag\". Add a `P1`, `P2`, etc, tag to a DAG and failures alerts will propagate to upstream systems like Datadog and New Relic, without any changes to DAG-level code.", "code": "JLUQA7", "state": "submitted", "created": "2024-12-22", "speaker_names": "Tim Paine", "track": "PyData: Data Handling & Engineering"}, {"title": "expectation: A modern take on statistical A/B testing with e-values and martingales", "abstract": "This talk introduces a novel Python library for statistical testing using e-values, offering a refreshing alternative to traditional p-values. We'll explore how this approach enables real-time sequential testing, allowing data scientists to monitor experiments continuously without the statistical penalties of repeated testing. Through practical examples, we'll demonstrate how e-values provide more intuitive evidence measures and enable flexible stopping rules in A/B testing, clinical trials, and anomaly detection. The library implements cutting-edge methods from game-theoretic probability, making advanced sequential testing accessible to Python practitioners. Whether you're conducting A/B tests, monitoring production models, or running clinical trials, this talk will equip you with powerful new tools for sequential data analysis.", "full_description": "Modern data science demands flexible statistical methods that can handle sequential data analysis and continuous monitoring. Traditional p-values, while widely used, have limitations when dealing with sequential testing scenarios. This talk introduces a Python library that implements e-values and e-processes, offering a more natural approach to measuring statistical evidence and enabling true sequential testing.\r\n\r\nOutline:\r\n\r\n1. Introduction to E-Values\r\n   - The limitations of p-values in sequential testing\r\n   - What are e-values and why do they matter?\r\n   - Key advantages: intuitive interpretation, composability, and sequential validity\r\n   - Real-world applications: A/B testing, clinical trials, anomaly detection\r\n\r\n2. Library Overview and Core Features\r\n   - Architecture and design philosophy\r\n   - Key components:\r\n     - Sequential testing framework\r\n     - Confidence sequences\r\n     - Standard (classical) testing with e-values\r\n     - Conformal e-testing\r\n   - Integration with existing Python data science tools\r\n   - Live coding demonstrations of basic usage\r\n\r\n3. Practical Applications\r\n   - A/B testing with continuous monitoring\r\n   - Sequential hypothesis testing\r\n   - Adaptive experimental design\r\n   - Real-time anomaly detection\r\n   - Live demonstrations with real-world datasets\r\n\r\n4. Advanced Features and Extensions\r\n   - Conformal prediction integration\r\n   - CUSUM procedures for change detection\r\n   - Adaptive thresholding\r\n   - Power analysis for e-value tests\r\n   - Performance considerations and optimizations\r\n\r\n5. Best Practices and Guidelines\r\n   - When to use e-values vs. traditional methods\r\n   - Setting up monitoring frameworks\r\n   - Common pitfalls and how to avoid them\r\n   - Tips for production deployment\r\n\r\n6. Future Directions and Community Involvement \r\n   - Roadmap for future development\r\n   - How to contribute\r\n   - Resources for learning more\r\n\r\nThe talk will include interactive demonstrations and real-world examples, making complex statistical concepts accessible to practitioners. Attendees will leave with:\r\n- Understanding of e-values and their advantages\r\n- Practical knowledge of implementing sequential testing\r\n- Hands-on experience with the library\r\n- Best practices for production deployment\r\n- Resources for further learning", "code": "ZKNTGN", "state": "submitted", "created": "2024-12-19", "speaker_names": "rostami.jako@gmail.com", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "MongoDB for Beginners", "abstract": "In today's data-driven world, managing and querying large amounts of unstructured data is increasingly important. MongoDB, one of the most popular NoSQL databases, offers a flexible and scalable solution for handling diverse data types without the constraints of traditional relational databases. The \"MongoDB for Beginners\" is tailored for individuals with little to no experience with MongoDB or NoSQL databases.", "full_description": "In today's data-driven world, managing and querying large amounts of unstructured data is increasingly important. MongoDB, one of the most popular NoSQL databases, offers a flexible and scalable solution for handling diverse data types without the constraints of traditional relational databases. The \"MongoDB for Beginners\" is tailored for individuals with little to no experience with MongoDB or NoSQL databases.", "code": "YW3B8U", "state": "rejected", "created": "2024-12-07", "speaker_names": "Kamil Kulig", "track": "PyCon: Programming & Software Engineering"}, {"title": "From Zero to IaC Hero: Using Python and Typer in Your CI/CD Pipeline", "abstract": "Have you ever wondered how to automate repetitive or everyday tasks more efficiently? In this session, we will explore the power of Python\u2019s Typer library and learn how to harness it to build command-line interfaces (CLIs) that make your daily workflows smoother and more productive. From creating simple commands to passing parameters in a user-friendly way, this session will cover the fundamentals of using Typer to rapidly prototype tools. We will also delve into real-life applications, demonstrating how these CLI tools can be integrated into GitHub pipelines and other essential development workflows. By the end, you\u2019ll have hands-on experience and a solid understanding of how Typer can enhance automation, boost efficiency, and streamline collaboration in modern software environments.", "full_description": "In this session, I\u2019ll explore the fundamental concepts of building Command-Line Interfaces (CLIs) using Python\u2019s Typer library. I\u2019ll cover how to use Arguments, Options, Prompts, and Progress Bars effectively to create powerful and user-friendly CLIs. Additionally, I\u2019ll dive into how to use it in a real-life scenario, such as integrating it into GitHub Pipelines. To ensure a hands-on learning experience, I\u2019ll provide a GitHub repository with examples and exercises where you can build your CLI and set up a GitHub Pipeline that utilizes it.\r\n\r\n1. Introduction to Typer - 10 mins\r\n2. Creating basic CLI application - 20 mins\r\n3. Advance feature for better user experience (Prompts, Progress Bars)- 20 mins\r\n4. How to publish and use in production env - 10 mins\r\n5. How to add own CLI to GitHun pipelines, and how to call from github pipeline, and how to combine with custom action 20 mins", "code": "UWWXYU", "state": "submitted", "created": "2024-12-22", "speaker_names": "Yuliia Barabash", "track": "General: Infrastructure - Hardware & Cloud"}, {"title": "Why Less than 10% of Companies Use Generative AI and LLMs in Production", "abstract": "While generative AI and large language models (LLMs) promise significant productivity gains, fewer than 10% of companies use them in production, citing unclear ROI and security concerns. This talk explores key adoption barriers and highlights the security risks of LLMs, including prompt injection, data poisoning, and sensitive information disclosure. By addressing these challenges and mitigation strategies, we aim to provide actionable insights for adopting generative AI securely and effectively.", "full_description": "Generative AI and large language models (LLMs) are reshaping industries, offering potential productivity gains of up to 30%. Yet, fewer than 10% of companies have successfully deployed these technologies in production. Why the gap? This session explores the multifaceted challenges of transitioning generative AI from innovation to implementation, focusing on both the technical and organizational barriers that hinder widespread adoption.\r\n\r\nKey challenges, including:\r\n\r\n- Security risks such as prompt injection, data poisoning, and sensitive information disclosure, which can compromise systems and erode trust.\r\n- Technical limitations of LLMs, including hallucinations, scalability issues, and the need for interpretability and reliable output in high-stakes applications.\r\n- Organizational hurdles, such as unclear ROI, lack of technical expertise, and concerns over integrating AI into existing workflows.\r\nThis talk will also shed light on the broader limitations of generative AI, including its reliance on high-quality data, computational cost, and the ongoing challenges of aligning AI-generated content with business goals.\r\n\r\nAttendees will leave with:\r\n\r\nA comprehensive understanding of the primary barriers to deploying generative AI in production.\r\nInsights into the present limitations of LLMs and how they impact real-world use cases.\r\nPractical, actionable strategies for overcoming these challenges, from enhancing security frameworks to building internal capabilities and realistic adoption roadmaps.", "code": "PPX3FB", "state": "submitted", "created": "2024-12-22", "speaker_names": "John Robert", "track": "PyCon: MLOps & DevOps"}, {"title": "The Evolution of Python Packaging Systems", "abstract": "Python packaging is evolving rapidly, with new tools being introduced regularly. While many talks and articles focus on packaging, they often lack a clear and structured overview of the available tools.\r\n\r\nThis talk will provide a detailed comparison of various packaging and environment management tools and there evolution over time. I will evaluate them based on predefined features, helping developers understand their differences and choose the right tools for their needs.", "full_description": "Introduction (5 minutes)\r\n1. About Me\r\n2. Objectives\r\n3. Importance of understanding packaging tools in a developer's workflow.\r\n4. History of Python packaging.\r\n\r\nCategorizing the Tools (10 minutes)\r\n1. Python Version Management:\r\n     - Overview of tools like pyenv, uv, rye, pdm and their role in managing multiple Python installations.\r\n2. Environment Management:\r\n      - Tools like venv, virtualenv, and conda, uv, rye, pdm\r\n3. Package Management:\r\n       - Introduction to pip, pip-tools, poetry and similar tools.\r\n        - Working of Features like dependency resolution and lock files.\r\n4. Package Building:\r\n        - Tools for building packages such as setuptools, flit, and hatch.\r\n        - Discussion of build process requirements and metadata.\r\n5. Package Publishing:\r\n        - Tools and workflows for publishing packages to PyPI, including twine.\r\n\r\nDetailed Analysis of Key Tools (15 minutes)\r\n1. Highlight key tools: uv, pdm, pyenv, poetry, hatch, flit, and others.\r\n2. Examine their functionalities:\r\n   - Dependency management.\r\n    - Python installation management.\r\n    - Build and publish workflows.\r\n     - Plugin systems.\r\n     - Support for important PEPs such as:\r\n              - PEP 660 \r\n              - PEP 621 \r\n              - PEP 582\r\n              - PEP 723\r\n\r\nComparing Frameworks (15 minutes)\r\n1. Explain the predefined criteria used to evaluate tools:\r\n- Dependency management.\r\n- Build and publish support.\r\n- Ease of use and developer experience.\r\n- Plugin and extensibility options.\r\n- Compatibility with key PEPs.\r\n2. Compare tools side-by-side\r\n3. Highlight use cases for specific tools.\r\n4. Discuss scenarios where a combination of tools might be beneficial.\r\n\r\nQ&A and Closing (5 minutes)\r\n1. Open floor for questions from the audience.\r\n2. Recap the importance of selecting the right tools based on project requirements.\r\n3. Encourage participants to explore and experiment with the tools discussed.", "code": "XQHTFC", "state": "submitted", "created": "2025-01-05", "speaker_names": "Kanishk Pachauri", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Driving Trust and Fairness: Addressing Ethical Challenges in Transportation through Explainable AI", "abstract": "Machine Learning can transform transportation\u2014improving safety, optimizing routes, and reducing delays\u2014yet it also presents ethical concerns. From potential algorithmic bias to opaque decision-making processes, trust and fairness are at stake. In this talk,I will show how Explainable AI (XAI) can offer practical solutions to these ethical dilemmas. Instead of focusing on the technical underpinnings, we will discuss how transparency, accountability, and fairness can be enhanced in AI-driven transportation systems. Using a real-world example, I will demonstrate how XAI provides the groundwork for building ethical, trustworthy, and socially responsible AI solutions in public transportation systems.", "full_description": "AI systems in transportation make decisions that directly impact people's lives, such as route optimization, safety measures, and resource allocation. These decisions often rely on complex algorithms, which can be opaque to stakeholders, including operators, regulators, and passengers. Key ethical concerns include:\r\n\r\n* *Algorithmic Bias*: Models trained on historical data may inadvertently reinforce inequities, such as prioritizing affluent or otherwise preffered neighborhoods over underserved areas.\r\n* *Transparency Deficit*: Operators often cannot understand or explain why an AI system makes a specific decision, leading to mistrust.\r\n* *Lack of Accountability*: In situations where decisions lead to negative outcomes, it is challenging to pinpoint responsibility when the AI acts as a \"black box.\"\r\nThese issues highlight the urgent need for mechanisms to build trust in AI systems, particularly in high-stakes environments like public transportation.\r\n\r\n**The Solution: Explainable AI (XAI)**\r\n\r\nExplainable AI (XAI) refers to methods and tools that make AI systems more transparent by providing interpretable insights into their decision-making processes. By integrating XAI, stakeholders can understand, validate, and trust the outputs of AI systems. Key features of XAI include:\r\n* *Transparency*: Clear explanations of how decisions are made.\r\n* *Accountability*: Mechanisms to trace decisions back to specific data inputs or model parameters.\r\n* *Fairness*: Tools to identify and mitigate bias in data and algorithms.\r\n\r\n\r\n**KARL: A Case Study in XAI for Public Transportation**\r\n\r\nThe *KARL* (KI in Arbeit und Lernen in der Region Karlsruhe) project is an exemplary initiative showcasing how XAI can address ethical challenges in AI-driven transportation. In this project, XAI is integrated into the tram system of Karlsruhe to:\r\n1. *Enhance Problem Handling*: When a tram is delayed, operators receive clear explanations of the underlying causes, such as weather conditions, mechanical failures, or traffic patterns. This enables quicker, more effective interventions.\r\n2. *Build  Trust*: Operators are informed about delays or reroutes in an understandable manner, fostering transparency and trust.\r\n3. *Ensure Fairness*: By analyzing historical data, the system identifies potential biases in route optimization and implements corrective measures to ensure equitable service.\r\n\r\n**Technical Implementation**\r\n\r\nWhile the presentation will not delve deeply into technical specifics, it will touch upon key elements such as:\r\n* The use of open-source libraries like *SHAP* (SHapley Additive exPlanations) to provide interpretability.\r\n* Integration of XAI tools into the operational dashboard used by tram operators.\r\n* Collaboration with domain experts to ensure the explanations are meaningful and actionable.\r\n\r\n**Takeaways for the Audience**\r\n\r\nAt the end of this talk, attendees will:\r\n1. Understand the ethical challenges posed by AI in transportation and how they can undermine trust.\r\n2. Learn how XAI tools can address these challenges by enhancing transparency, accountability, and fairness.\r\n3. Gain insights into the practical implementation of XAI in a real-world setting through the KARL project.\r\n4. Be inspired to incorporate XAI principles into their own AI projects to build ethical and socially responsible solutions.", "code": "NPMNCE", "state": "submitted", "created": "2025-01-05", "speaker_names": "Jens Beyer", "track": "General: Ethics & Privacy"}, {"title": "Let the Robots Test: Acceptance Test-Driven Development (ATDD) with Robot Framework", "abstract": "Getting clear, actionable specifications from business stakeholders is often challenging \u2014 they tend to be abstract, ambiguous, or incomplete. Acceptance Test-Driven Development (ATDD) provides a powerful approach to bridge this gap by enabling clear, executable specifications that align business needs with development. In this talk, we\u2019ll explore how Robot Framework, a simple yet powerful tool, can foster collaboration between business and development teams. Through practical examples, we\u2019ll demonstrate how to write effective tests and extend Robot Framework with custom Python libraries to meet project-specific needs. Walk away with insights and tools to transform how teams communicate, develop, and deliver software.", "full_description": "One of the biggest challenges developers face is dealing with unclear, ambiguous, or abstract requirements from business stakeholders. These vague specifications often lead to misaligned expectations, wasted development time, and costly rework. Without a shared understanding, delivering the right product becomes a game of guesswork, frustrating both teams and delaying progress.\r\n\r\nAcceptance Test-Driven Development (ATDD) solves this problem by introducing clear, executable specifications. ATDD ensures that requirements are expressed as concrete, testable examples, fostering alignment between business and development teams. Instead of interpreting abstract requirements, developers can rely on these tests as a shared \u201csource of truth,\u201d reducing misunderstandings and enhancing confidence in the software being built.\r\n\r\nRobot Framework emerges as a simple yet powerful tool for implementing ATDD. Its keyword-driven, human-readable syntax enables seamless collaboration between technical and non-technical stakeholders, making specifications both executable and accessible. By bridging the communication gap, Robot Framework ensures that everyone \u2014 from business analysts to developers \u2014 shares a common understanding of the system requirements.\r\n\r\nIn this talk, we\u2019ll dive into the core concepts of Robot Framework, providing an overview of its structure and capabilities. You\u2019ll see how tests are written in its clear, keyword-driven syntax and explore existing libraries that simplify test automation for common scenarios. Beyond that, we\u2019ll focus on how to extend Robot Framework by building custom Python libraries to address unique project requirements. By combining these elements, you\u2019ll gain a solid understanding of how to leverage Robot Framework effectively, enabling teams to create powerful, maintainable, and collaborative test suites.", "code": "EYU7NQ", "state": "submitted", "created": "2024-12-17", "speaker_names": "Stefan Kraus", "track": "PyCon: Testing"}, {"title": "Unveiling Economic Complexity: a data-driven approach to Innovation and Market Exploration", "abstract": "Explore the power of Economic Complexity Index (ECI) and Product Complexity Index (PCI) in guiding innovation. Learn how these network measures, rooted in Complexity Theory, unveil unique insights into economic development patterns. Understand computation methods to guide data-driven decision making", "full_description": "In this talk, we'll dive into the Economic Complexity Index (ECI) and Product Complexity Index (PCI), revealing how these metrics, rooted in Complexity Theory, offer a fresh perspective on economic development. Learn how to compute and interpret these indices to guide innovation, compare countries and markets, and make informed, data-driven decisions.\r\nWe will apply the ECI to the domain of labour market data using job postings data to try to predict the emerging technologies, emerging skills in the different sectors and occupations.", "code": "BJ9EGB", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Realtime Time Series Anomaly Detection in Production", "abstract": "Anomaly detection is hardly a new problem, nor is the progress in it as rapid as the LLM blast we\u2019re witnessing today. **But it is pressing.**\r\n\r\nIn this talk, we\u2019ll look at an end-to-end demo of a realtime anomaly detection pipeline and discuss the nitty-gritties of the algorithm knobs that help us build an unbiased and reliable system, which includes using NeuralProphet, an open source framework, to forecast for time series data and robust techniques to detect true anomalies using forecasting errors.", "full_description": "With the boom of data, we\u2019re seeing real-time systems cropping up and that means thousands of data streams flowing that power every experience across the internet. But with this, arises the critical need to monitor data quality and metrics, and alerting systems that help escalate unexpected behavior with high confidence. \r\n\r\nIn this talk, we\u2019ll focus on one approach to detect anomalies, which is to forecast for a period in the future and alert if the ground truth is far from this prediction by some amount. Easy-peesy, right? But for this to work, we need\r\n- A performant and efficient forecasting algorithm, that works well for all kinds of data, and, \r\n- An unbiased, and dynamic approach to detect anomalies and not noise \r\n\r\n\r\nFor the first, we\u2019ll leverage NeuralProphet - which is an open-source framework for ML based forecasting that combines the interpretability of linear models with the accuracy of deep neural networks.\r\n \r\nFor the second, we\u2019ll discuss implementing some robust techniques and how they\u2019re useful in the real-time production environment, to prevent anomalous data biasing our system as early as one second later.\r\n\r\n\r\n\r\n## Outline\r\n\r\n\r\n1. **Introduction to anomaly detection using time series forecasting** (5 min)\r\n- Overview of NeuralProphet (10 min)\r\n- We\u2019ll discuss the key components of NeuralProphet like capturing trend, seasonality and the AR-nets and what gives it an edge over the traditional statistical forecasting algorithms, using code snippets.\r\n- Using neural prophet in real-time - how many periods to forecast ahead? \r\n2. **Using forecast errors to detect anomalies** (12 min): \r\nThe questions I\u2019ll address in this section are:\r\n- Which measures should we use as anomaly scores to prevent bias by anomalous data entering the system in real-time?\r\n- How do we choose lookback windows to compare our current data with?\r\n- How do we tune anomaly score thresholds?\r\n- How do we deal with short-term noise?\r\n- How do we deal with sparse data?\r\n3. **Q&A** (3 min)\r\n\r\n**At the end of the talk, the audience will**\r\n- have knowledge of an end-to-end demo highlighting NeuralProphet\u2019s capabilities\r\n- be equipped with handy tips and techniques to implement automated anomaly detection using open-source tools.", "code": "LVK8NM", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shreya Khurana", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Going beyond Parquet: Handling large-scale datasets with Iceberg and Dask", "abstract": "Apache Iceberg is a high-performance open format for huge analytic tables. It brings the reliability and simplicity of SQL tables to big data. Originally built by Netflix to handle datasets at the petabyte scale, Iceberg is becoming the de facto open standard for storing tabular datasets. Its adoption can be seen across the industry, from Databricks' acquisition of Tabular to the release of Snowflake's Iceberg Tables or Amazon's S3 Tables.\r\n\r\nIn this talk, we start by looking at Iceberg's storage model and how it enables reliable concurrent data operations on top of Parquet files. We continue with Dask's DataFrame API, which allows users to scale Pandas workloads to clusters of machines in a pure Python environment. We discuss how it integrates with Apache Iceberg for ACID capabilities and high-performing querying. Equipped with this understanding, we investigate more advanced Iceberg features like hidden partitioning or time travel. Throughout the talk, we compare Dask workloads executed on Iceberg datasets against plain Parquet to gain a practical understanding of Iceberg's benefits.", "full_description": "Apache Iceberg is a high-performance open format for huge analytic tables. It brings the reliability and simplicity of SQL tables to big data. Originally built by Netflix to handle datasets at the petabyte scale, Iceberg is becoming the de facto open standard for storing tabular datasets. Its adoption can be seen across the industry, from Databricks' acquisition of Tabular to the release of Snowflake's Iceberg Tables or Amazon's S3 Tables.\r\n\r\nIn this talk, we start by looking at Iceberg's storage model and how it enables reliable concurrent data operations on top of Parquet files. We continue with Dask's DataFrame API, which allows users to scale Pandas workloads to clusters of machines in a pure Python environment. We discuss how it integrates with Apache Iceberg for ACID capabilities and high-performing querying. Equipped with this understanding, we investigate more advanced Iceberg features like hidden partitioning or time travel. Throughout the talk, we compare Dask workloads executed on Iceberg datasets against plain Parquet to gain a practical understanding of Iceberg's benefits.", "code": "GKZV9V", "state": "submitted", "created": "2024-12-22", "speaker_names": "Hendrik Makait", "track": "PyData: Data Handling & Engineering"}, {"title": "Much Ado About Agents - Definitions, Distinctions and Decisions", "abstract": "The concept of \"AI agents\" has entered public imagination and commands industry attention, but what exactly is an AI agent? The definition is unclear, the hype going strong, and the boundaries between agents and other types AI systems - like chatbots - fuzzy. \r\n\r\nThis talk critically examines the concept of AI agents: What makes an agent an agent? Is it tool use, a long planning horizon, a certain degree of autonomy, or something else entirely? We\u2019ll explore the history of AI agents, dissect their characteristics, and contrast marketing language with the reality of the current technological state of the art. Finally, we\u2019ll discuss practical use cases for AI agents and when alternative AI approaches might be more appropriate.", "full_description": "The term \"AI agent\" is thrown around liberally in current AI-related discourse, often with varying definitions and an overwhelming dose of hype. But what does it actually mean? This talk seeks to clarify the concept of agentic AI through four angles:\r\n\r\n1. **What are AI Agents?**: We\u2019ll dive deep into what makes an AI system an \"agent.\" Is it its ability to autonomously plan, adapt, or use external tools? How does it differ from a sophisticated, tool-using chatbot? By analyzing different frameworks and definitions, we\u2019ll aim to distill the the essential properties of agentic systems.\r\n    1. **Technology vs. Interface**: A critical distinction exists between the underlying technology of AI agents and the interfaces through which users interact with them. We\u2019ll explore how user-facing designs shape the perception of an AI system as an agent and whether this distinction helps clarify or muddy the definition.\r\n2. **Why now?, or: beware the snake oil**: The pace of progress in AI is nothing short of dramatic, but that doesn\u2019t mean that every new development will be as groundbreaking as advertised. We take a bird\u2019s eye view of the current development trends in AI and discuss why agents are such a popular concept now. We analyze the marketing lingo by contextualizing who\u2019s pushing for agents right now and they stand to gain from it.\r\n3. **Applications and Use Cases**: Not every problem calls for an AI agent. We\u2019ll evaluate scenarios where agent-based systems excel, such as multi-step task automation, and others where simpler or alternative AI solutions are more effective.\r\n\r\nThroughout the talk, we\u2019ll also take a brief look at the historical evolution of the concept of AI agents, from early academic definitions to today\u2019s commercial implementations.\r\n\r\nBy critically analyzing the concept and separating hype from substance, this talk aims to provide clarity, foster a deeper understanding, and help participants make informed decisions about when\u2014and when not\u2014to deploy AI agents.\r\n\r\n## Takeaways\r\n\r\nAttendees will leave with:\r\n\r\n1. A clearer understanding of what defines an AI agent and how it differs from other AI systems like chatbots, as well as insights into the relationship between an AI agent\u2019s technological backbone and its user-facing interface.\r\n2. Insights into the current state of AI model development, the reasons behind the rising prominence of AI agents, and how much of the hype can be attributed to marketing strategies and venture capital interests rather than genuine technological advancements.\r\n3. Practical guidance on when to use AI agents and when alternative approaches might be more suitable.\r\n\r\nThis talk will enable participants to critically assess the concept of AI agents, navigate the hype, and make better-informed strategic decisions in their AI development and deployment.", "code": "7HLVAL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Raphael Mitsch", "track": "PyData: Generative AI"}, {"title": "Building GraphQL microservices using FastAPI", "abstract": "FastAPI is a Python web framework built on top of the ASGI standard. The fusion of GraphQL and FastAPI emerges as a pivotal approach for constructing agile and scalable microservices. This talk delves into the symbiotic relationship between GraphQL's query flexibility and FastAPI's high-performance capabilities. Attendees will gain insights into seamless integration techniques, leveraging FastAPI's asynchronous framework, dependency injection, and GraphQL's expressive querying.", "full_description": "FastAPI makes it easy to build APIs by providing built-in support for common Restful API operations. FastAPI also supports GraphQL federated APIs, which allow you to build a unified API from multiple microservices. With GraphQL federated APIs, you can define a schema that aggregates data from multiple services and returns it as a single response. This talk delves into the symbiotic relationship between GraphQL\u2019s query flexibility and FastAPI\u2019s high-performance capabilities. Attendees will gain insights into seamless integration techniques, leveraging FastAPI\u2019s asynchronous framework, dependency injection, and GraphQL\u2019s expressive querying.", "code": "RRCQW8", "state": "submitted", "created": "2025-01-04", "speaker_names": "Aby M Joseph", "track": "General: Others"}, {"title": "GRAG Suite: Pioneering German Language AI through Innovation and Collaboration", "abstract": "In this discussion, we will explore the development of a small-size language model tailored for the German language, which outperforms larger models such as GPT-3.5 Turbo. By delving into the creation of the GRAG suite, we will highlight the innovative procedures we followed to achieve these results and showcase its benefits in advancing German language AI development.\r\nThe GRAG suite exemplifies the power of strategic collaboration and creative data generation techniques in enhancing the performance of open-source AI models for language-specific applications. By breaking down the process and methods utilized in developing GRAG, we can better understand how this model achieved superior performance over more extensive models like GPT-3.5 Turbo.\r\nIn this presentation, we analyze the groundbreaking impact of the GRAG suite on German language AI development, while discussing potential applications and future advancements in the field. Join us to uncover the innovative methodologies behind GRAG's creation, and explore its potential influence on open-source AI models tailored for language-specific use cases.", "full_description": "In this comprehensive discussion, we delve into the groundbreaking advancements achieved through the development of the GRAG suite, a small-size language model specifically designed for the German language. This innovative model has outperformed larger models, such as GPT-3.5 Turbo, and has set a new precedent for open-source AI models tailored for language-specific applications. We will explore the strategic collaborations and creative data generation techniques utilized in GRAG's development, providing a detailed overview of the procedures employed to achieve its superior performance.\r\nOur presentation begins by examining the limitations of existing large language models, emphasizing the need for more efficient, language-specific solutions. The creation of the GRAG suite serves as an example of how such models can be developed using a combination of innovative techniques and collaborative efforts. We detail the process of curating and generating training data, including the importance of linguistic diversity and contextual relevance in optimizing the model's performance.\r\nNext, we highlight the strategies employed in the fine-tuning process, focusing on optimizing the model's parameters and hyperparameters for the specific characteristics of the German language. This includes an analysis of the unique syntactic and semantic aspects of German, and the importance of tailoring the model architecture to capture these linguistic nuances.\r\nWe then examine the rigorous evaluation process used to measure GRAG's performance against existing large language models, with a focus on its ability to outperform these models in various language generation tasks. This section provides a detailed analysis of GRAG's strengths and weaknesses, emphasizing its potential impact on the field of German language AI.\r\nOur presentation concludes with a discussion of future advancements and opportunities for improvement in German language AI development. We outline potential areas of research, including multi-lingual models and the integration of domain-specific knowledge, and highlight the challenges and opportunities that lie ahead.", "code": "BUF7NZ", "state": "submitted", "created": "2024-12-22", "speaker_names": "Soumya Paul", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Nobody wants to hire a junior dev - understanding the ecosystem", "abstract": "It's getting really difficult for junior developers to start their careers. Most organisations typically don't hire junior developers, they focus on mid level talent and upwards. \r\n\r\nThis has consequences for people right now - human potential is being wasted, people are struggling and taking wrong turns.  Training organisations and bootcamps are stepping in to fill gaps with mixed results. There are a lot of sharks in the education space, selling opaque products to people who are looking for answers. \r\n\r\nThis also has long term consequences - where will this industry be in 8 years time given that so few people are getting experience right now?  \r\n\r\nThis is an ecosystem problem. It's very complicated. \r\n\r\nAnd it's further complicated by the rise of LLMs - we're at the brink of something big, but we aren't sure what. Some people think the skill of writing code will not be necessary in 5 years.\r\n\r\nHow do we parse this? If we want to play the long game, as a community, as a society, what should we do?", "full_description": "We'll start by talking about the problem itself: \r\n- A definition and validation of the problem \r\n- Why juniors are not adding enough value to be hired\r\n- Why hiring Junior developers is risky \r\n- How people are hacking the system and making it worse (eg interview hacking makes hiring even more risky)\r\n\r\nNext, we'll talk about how organisations and individuals can start to address the problems\r\n- stratergies for organisations\r\n- stratergies for juniors looking to get hired \r\n\r\nThe LLM elephant in the room - if we are looking to the futire of this ecosystem, we need to look at the role of a software developer and how that is changing.", "code": "AN8E9X", "state": "submitted", "created": "2024-12-13", "speaker_names": "Sheena", "track": "General: Education, Career & Life"}, {"title": "Demystifying Design Patterns: A Practical Guide for Developers", "abstract": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation empowers you with a structured understanding of these reusable solutions for common programming challenges. We'll delve into their key categories: Behavioral, Structural, and Creational, exploring their functionalities and how they can be applied in your daily development workflow.\r\nFor each category, we'll explore a practical design pattern in detail. You'll gain insights into how these patterns can facilitate communication between objects (Behavioral), separate interface from implementation for flexibility (Structural), and enable dynamic algorithm selection at runtime (Creational). We'll showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation. This will provide valuable insights into how these concepts translate into real-world development scenarios.", "full_description": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation empowers you with a structured understanding of these reusable solutions for common programming challenges. We'll delve into their key categories: Behavioral, Structural, and Creational, exploring their functionalities and how they can be applied in your daily development workflow.\r\nFor each category, we'll explore a practical design pattern in detail. You'll gain insights into how these patterns can facilitate communication between objects (Behavioral), separate interface from implementation for flexibility (Structural), and enable dynamic algorithm selection at runtime (Creational). We'll showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation. This will provide valuable insights into how these concepts translate into real-world development scenarios.", "code": "8PFFPS", "state": "accepted", "created": "2024-12-22", "speaker_names": "Tanu", "track": "PyCon: Programming & Software Engineering"}, {"title": "SLMs powered Edge-Ready NLP through Multi-Agent orchestration", "abstract": "Large Language Models (LLMs) excel at tasks like filtering, summarization, and code generation, but their high computational requirements lead to steep costs and limited scalability. This talk introduces a novel approach that shifts from monolithic LLMs to a lightweight ecosystem of open-source Small Language Models (SLMs) coordinated by a central Master Agent. By dynamically assigning tasks to specialized Worker Agents, each running an SLM tailored for specific functions, the architecture reduces resource consumption and costs while enhancing scalability. Additionally, this multi-agent framework supports edge-compatible solutions, enabling low-latency, privacy-preserving, and offline language processing across various devices. Implemented primarily in Python and leveraging open-source frameworks such as Langchain and Hugging Face, the proposed system optimizes resource utilization, simplifies maintenance through modular specialization, and ensures robust failover for uninterrupted performance. Attendees will explore how to integrate this flexible, future-proof platform into their projects, benefiting from its affordability and adaptability.", "full_description": "Large Language Models (LLMs) are powerful tools for tasks like filtering, summarization, and code generation. However, their substantial computational demands result in high costs and limit scalability. To address these challenges, this talk proposes a new approach that moves away from traditional large models. Instead, it introduces a lightweight system made up of open-source Small Language Models (SLMs) managed by a central Master Agent. The Master Agent assigns tasks to specialized Worker Agents, each running an SLM such as Phi3 and 4 or Orca-mini, which are optimized for specific functions. This distribution of tasks among smaller models significantly reduces both resource usage and operational costs. Additionally, scaling the system or adapting to new tasks becomes easier by simply adding or replacing Worker Agents without the complexities of deploying a single large model.\r\n\r\nBeyond cost and scalability, this approach meets the growing need for edge-compatible solutions. Compact SLMs can integrate smoothly into various edge devices, including IoT sensors and mobile applications. This integration allows for low-latency processing, enhances privacy by keeping data local, and even supports offline language processing. As a result, developers can deliver advanced language functionalities directly to users, regardless of infrastructure limitations or connectivity issues. This empowers the creation of applications that are both responsive and secure, catering to diverse environments and user needs.\r\n\r\nOur implementation is primarily in Python and supported by open-source frameworks like Langchain and Hugging Face, encouraging community-driven improvements. The talk will demonstrate how this multi-agent framework optimizes resource use, simplifies maintenance through specialized modules, and ensures reliable performance with robust failover mechanisms. Attendees will learn how to integrate these components into their own projects, taking advantage of a flexible and scalable platform that is both cost-effective and adaptable for future advancements in language processing.", "code": "RWZE8U", "state": "submitted", "created": "2024-12-22", "speaker_names": "Rudraksh Karpe, SUMIT PANDEY", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Navigating the LLM hype: What science can tell us about where LLMs win and where they fail.", "abstract": "Large Language Models (LLMs) have gained tremendous popularity over the past two years, with many tech companies rushing to integrate them into their products. However, these applications have often failed or led to embarrassment due to unrealistic expectations and a limited understanding of LLMs\u2019 constraints. This presentation will explore the true capabilities of LLMs through empirical experimentation and scientific rigor. We will investigate whether LLMs can reason, plan, or simply recite information, and uncover the actual level of \u201cintelligence\u201d they possess. By identifying these limitations, we will provide actionable guidelines for effectively applying LLMs, helping you determine when they are suitable for your product and avoid common pitfalls. This session is accessible to anyone considering the integration of LLMs into their work, regardless of technical background.", "full_description": "Over the past two years, Large Language Models (LLMs) have taken the tech world by storm, leading many companies to rapidly adopt them in hopes of transforming their products. Yet, despite their potential, many LLM-based applications have fallen short, sometimes even causing embarrassment or public backlash. These failures often arise from overly optimistic expectations and a lack of understanding of the models' limitations.\r\nIn this session, we will systematically explore the true capabilities of LLMs. Through empirical experimentation, we\u2019ll examine key questions: Can LLMs reason, or are they just sophisticated parrots? Can they engage in abstract thinking or demonstrate planning? After several years of research, science has a lot to say about the current and projected capabilities of LLMs.\r\nWith this knowledge, we will establish practical, actionable guidelines for applying LLMs effectively in products. You\u2019ll gain insights into which use cases are best suited for LLMs, how to avoid common pitfalls, and how to align expectations with the current capabilities of these models. This session does not require a deep technical understanding of LLMs, making it ideal for anyone, from product managers to decision-makers, who is considering using these models in their applications.", "code": "B7GU98", "state": "submitted", "created": "2024-12-03", "speaker_names": "Oren Matar", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Machine Reasoning and System 2 Thinking", "abstract": "Raw large language models struggle with complex reasoning. New techniques have\r\nemerged that allow these models to spend more time thinking before giving an answer.\r\nDirect token sampling can be seen as system-1 thinking and explicit step-by-step\r\nreasoning as system-2. How can this reasoning ability be improved and what is the future?", "full_description": "Basic large language models struggle with complex reasoning. New techniques, broadly referred to as \"test time compute\" have emerged that allow these models to spend more time processing before giving an answer. Direct token sampling can be seen as analogous to system-1 thinking and explicit step-by-step reasoning as system-2. Many top AI researchers and companes are now working on building system-2 into AI systems to improve general reasoning.\r\n\r\nWe will review the newest open research on test time computation including promising techniques that have appeared in top entries for Fran\u00e7ois Chollet's ARC-AGI challenge. While OpenAI has shamefully kept the research behind their o1, o3 and o-N models secret, other researchers have worked in public, demonstrating how to use test time compute to greatly boost model performance with the right fine-tuning and test time procedures.\r\n\r\nThis talk will explore the latest developments in the rapidly developing area of system-2 AI reasoning, the engine behind the only significant gains in LLM performance recently. Giving LLMs system-2 like capabilities improves problem solving, code generation quality and reduces hallucinations, get up to speed on research behind these techniques.", "code": "AYN837", "state": "submitted", "created": "2024-12-22", "speaker_names": "Andy Kitchen", "track": "PyData: Generative AI"}, {"title": "Guiding data minds: how mentoring transforms careers for both sides", "abstract": "Mentorship is a powerful way to shape careers while building meaningful connections in the data field. In this talk, I\u2019ll share my journey as a professional mentor, what the role entails, and the impact it has on both mentees and mentors. Learn how mentorship drives growth, fosters innovation, and creates value for the data community\u2014and why you should consider stepping into this rewarding role.", "full_description": "Mentorship is a rewarding journey that allows experienced professionals to guide and empower the next generation of talent. As a mentor in the data field, I have had the privilege of helping individuals navigate their careers, refine their skills, and unlock their potential. In this talk, I will share my personal journey into becoming a professional mentor, how I approach mentorship in a structured and impactful way, and the unique value a mentorship brings to both mentees and mentors.\r\n\r\nI\u2019ll provide insights into the day-to-day activities of mentoring, from offering career guidance to solving technical challenges, while also discussing the importance of tailoring advice to individual goals. Beyond technical skills, mentorship fosters confidence, networking, and long-term growth for mentees while offering mentors opportunities for personal development, deep satisfaction, and a broader industry perspective.\r\n\r\nWith the rapid evolution of the data industry, mentorship has never been more critical. This talk will highlight how professionals at any stage of their career can engage in mentorship to create a ripple effect of positive change in the data community\u2014and why taking the step to become a mentor, paid or otherwise, is an investment in the future of data science and yourself.", "code": "TYXMZC", "state": "confirmed", "created": "2024-12-08", "speaker_names": "Anastasia Karavdina", "track": "General: Community & Diversity"}, {"title": "Data2Text Generation for German Football Reports", "abstract": "Data2Text is a Natural Language Generation task, in which a specific domain of texts is generated faithfully based on given, structural data. The talk discusses a Proof of Concept project, in which LLMs were utilized to generate game reports for a German first division football team. It covers the necessary steps for building such a system and the problems and performances which can be expected from similar systems.", "full_description": "Data2Text is a Natural Language Generation task, in which a specific domain of texts is generated faithfully based on given, structural data. The talk discusses a Proof of Concept project, in which LLMs were utilized to generate game reports for a German first division football team.\r\n\r\nWe go through the necessary preparation steps for conducting the project, regarding annotation, aligning and filtering of text and data.\r\nThe proposed system is a hybrid of historical NLG pipeline components and big LLMs with strong, linguistic capabilities. We discuss the data-dependent difficulties for selecting and ordering the tabular content. When evaluating the capabilities of the Llama3 model series, we compare different techniques, such as in-context few-shot learning of a larger model vs. fine-tuning of smaller models. For evaluating the final results, we show multiple unsupervised metrics, LLM-as-a-judge systems and compare them with human annotations.\r\n\r\nFinally, we summarize the lessons learned from the project and discuss possible future steps for a productive version.", "code": "7X7QZ8", "state": "submitted", "created": "2024-12-17", "speaker_names": "JaSchuste", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Copilot(s), MarkDown and a Medical Team to the Cure", "abstract": "The general medical advice for diseases, symptoms and treatments is to never ask it to Dr. Google. It's a bad idea to explore symptoms and clinical presentations on the internet. On the other hand: 'general' advices are good for the majority. There are always exceptions. This talk is how I used Generative AI next to a brilliant medical team to help me to get cured from a rare disease called 'Zinner Syndrome'.\r\n\r\nThe first thing my GP said to me: Do look up as much as you can on the internet. I said: really? He said: yes, you are a smart guy. You know how to use it for the better. After some basic YouTube presentation hunting and some basic Google searches I asked the same stuff to Microsoft Copilot.\r\n\r\nI started to make a repository on GitHub with MarkDown files. I enabled GitHub Copilot and generated my journey. Reports from appointments to doctors, examinations, treatments, and results. I used the system to track the use of medication, the impact on my body and the impact on my mental being. I even used the GitHub mobile app during my doctor appointments.\r\n\r\nCome and learn in this talk about a unique use case of technology to get better!\r\n\r\nTW - Trigger Warning: This talk contains medical information about a urologic/genital disease. It contains information about mental health, pain, and the impact of a rare disease on a person's life.", "full_description": "A talk about a very unique usecase for GenAI. A personal story how GenAI helped me to use my talents and interest during a life changing year.\r\n\r\nA talk showing that demonstrates how you can use tools on an out-of-the-box way. The talk will be demo and code heavy. But the fact the code is only Markdown should make it easy for everyone to follow. You don't need to be an experienced coder to understand the concepts I speak about and to understand the unique way of handling such tools.", "code": "CANL7G", "state": "submitted", "created": "2024-12-26", "speaker_names": "Dennie Declercq", "track": "PyData: Generative AI"}, {"title": "Personalizing the E-commerce Experience with Microservices", "abstract": "This talk explores how to leverage microservices with Python to build a flexible and scalable e-commerce platform capable of delivering personalized experiences. We'll focus on building recommendation engines and dynamic pricing systems as microservices, using popular Python frameworks and machine learning libraries. We'll also discuss deployment and scaling considerations for AI-powered microservices in production.", "full_description": "In today's competitive e-commerce landscape, personalized experiences are crucial for customer engagement and revenue growth. This talk will guide you through building a flexible and scalable e-commerce platform with Python, leveraging the power of microservices architecture to deliver tailored recommendations and dynamic pricing.\r\n\r\n### Outline:\r\n\r\n**1. Introduction (5 minutes)**\r\n- The growing importance of personalization in e-commerce and its impact on customer engagement and revenue.\r\n- Microservices architecture and its benefits for building flexible and scalable e-commerce platforms.\r\n- The challenges of using microservices, such as increased complexity and operational overhead.\r\n\r\n**2. Microservices for Personalized Recommendations (15 minutes)**\r\n- Different types of recommendation systems:\r\n  - Collaborative filtering\r\n  - Content-based filtering\r\n  - Hybrid approaches\r\n- Designing and implementing a recommendation engine as a microservice using Python and machine learning libraries like scikit-learn and TensorFlow.\r\n- A practical example of building a recommendation microservice with Python, including code snippets and explanations.\r\n- Integrating the recommendation microservice with other e-commerce services, such as product catalog and user profiles.\r\n- Ethical considerations of using AI for recommendations, such as bias in algorithms and data privacy.\r\n\r\n**3. Microservices for Dynamic Pricing (15 minutes)**\r\n- The concept of dynamic pricing and its benefits for e-commerce businesses.\r\n- Different dynamic pricing strategies and their implementation using Python:\r\n  - Time-based pricing\r\n  - Value-based pricing\r\n  - Cost-plus pricing\r\n  - Competitive pricing\r\n- A practical example of building a dynamic pricing microservice with Python, including code snippets and explanations.\r\n- Integrating the dynamic pricing microservice with other e-commerce services, such as inventory management and order processing.\r\n- Challenges of implementing dynamic pricing, such as avoiding price wars and maintaining customer trust.\r\n\r\n**4. Integration and Deployment (10 minutes)**\r\n- How the recommendation and dynamic pricing microservices can work together to enhance personalization.\r\n- Deployment strategies for microservices, such as containerization with Docker and Kubernetes.\r\n- Monitoring and observability for microservices in a production environment.\r\n- Briefly mention tools and techniques for managing and orchestrating microservices.", "code": "CDGYN8", "state": "submitted", "created": "2024-12-26", "speaker_names": "Tristan McKinnon", "track": "PyCon: MLOps & DevOps"}, {"title": "May the Args be with you: The untold story of Python Functional Arguments", "abstract": "Most of us are familiar with the line `def foo(...):`, it's a function declaration. But have you ever wondered about the full story behind these `...`? There is much more to it than just handling positional and keyword arguments.\r\n\r\nIn this talk, we will dive deeper into the secrets of Functional Arguments, starting right from the fundamentals, `*args`, `**kwargs` to `type hinting` and `generics`. Expect a good mix of practical examples and anecdotes, on how to leverage `Args` effectively to create intuitive interfaces, and learn to write clean APIs", "full_description": "At first glance, Python\u2019s function arguments seem quite straightforward - just positional and keyword arguments, right? But this is just the tip, there is much beneath the surface.\r\n\r\nIn this talk, we will dive deeper into depths of what Python\u2019s argument system has to offer. Along the way, we\u2019ll dive into practical examples that showcase how mastering these nuances can lead to cleaner, more intuitive APIs and smarter design choices.\r\n\r\nThis talk will:\r\n\r\n* Highlight overlooked aspects of Python\u2019s functional arguments.\r\n* Introduce techniques to make APIs both robust like type hinting\r\n* Share best practices to avoid subtle yet impactful mistakes\r\n\r\nBy the end of this talk, listeners will have a deep understanding of Python\u2019s functional argument system, including how to use type annotations, generics, and argument restrictions to design intuitive and Pythonic interfaces. \r\n\r\nJoin us to go beyond the basics and uncover what's so Pythonic about Python's functional arguments.", "code": "NRBHCW", "state": "submitted", "created": "2024-12-21", "speaker_names": "Adarsh Kumar, Palak Mittal", "track": "PyCon: Programming & Software Engineering"}, {"title": "A tale of two communities: Python and PyData", "abstract": "Do you want to know more about the communities behind conferences like this one? Why there are two organizations? How do I participate? Until a few years ago I would not have shown much interest in this kind of questions, since then I have started going to PyData and PyCon conferences, joined a local Python meetup and started a PyData meetup. And I want to share with you what I learned along the way.", "full_description": "As a Pythonista and Data Scientist, for many years I have never really paid attention to how the tools I use every day are crafted and maintained. I just used them. Then I started going to meetups and conferences and discovered the wonderful group of people that organizes these kind of events. And I decided to try and be part of it. It turns out that Python and PyData communities are extremely welcoming and accessible!\r\n\r\nThis is a talk about the power of community. It also about two specific communities who are behind conferences PyCon and PyData conferences (and sometimes team up to deliver special events). One is about a programming language and targets developers and the wider public of Python users; it is backed by a non profit called the Python Software Foundation. The other, younger, is about Open Source Scientific Software (it is called PyData but its not only about Python) and targets Scientists, Data Scientist and all people who deal with Data and/or Science in some degrees; it is backed by a non profit called NumFocus.\r\n\r\nWhy a talk about both? Because I have been wondering a lot recently about the commonalities and specificities of both of them. There is a personal side of this: one of the reason I joined the Python Milan (>10 year old) community is to get involved in starting a PyData Milan chapter. I think, there is also a general trend of convergence between the two: on one side the Data practice is maturing and learning more from the older (still very young) Software Engineering practice; on the other side, the new (neverending?) summer of AI has pushed a lot of developers to start playing with apis delivering the output of sophisticated models.\r\n\r\nThis talk is divided in three parts each one trying to answer a question:\r\n\r\n1. How do the organizations behind Python and PyData communities work?\r\n2. How do I start (or join) a meetup in one of them and keep it going?\r\n3. Why should I care about both (or not)?\r\n\r\nThe talk aims to be informative, practical, and opinionated and targets beginners and experts alike. It will not be an easy task to be focused and concise about a topic I am passionate about but I will try very hard and I hope you will enjoy it!", "code": "Y88HLQ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Pietro Peterlongo", "track": "General: Community & Diversity"}, {"title": "Alice\u2019s Adventures in Open Source Land", "abstract": "Have you ever considered contributing to Open Source? I did consider it, and it took me more than 3 years to make my first pull request. I\u2019d like to share my experience of contributing to open source: why I did it, what I expected, what I learned, and what others shared with me. I\u2019d like to offer you easy starting points and help you take that first step down the rabbit hole of open source", "full_description": "Objectives\r\nSharing my Open Source experience to encourage people to start contributing.\r\nOffering easy entry points for a smooth start.\r\n\r\n\r\nDetailed Abstract\r\n\r\nDown the rabbit hole (7 min)\r\nShort introduction of my transitioning story. How I transitioned from teacher/street-artist into data science person and open source contributor.\r\nExplanation of why I found contributing to open source so appealing, and what blocked me for more than three years. Also telling my experience full of doubt, lack of confidence, and fear of failure.\r\n\r\nLearnings (3 min)\r\nI\u2019ll address what I\u2019ve learned, and am still learning. What can a novice learn, and what you cannot learn as a data person. Why it\u2019s worth contributing to open source projects in general.\r\n\r\nBlockers (3 min)\r\nWhat held me back for more than 3 years? What held other people I\u2019ve talked to from making the first steps.\r\n\r\nHow to find a perfect hole? (12 min)\r\nListing practical tips and tricks for different types of blockers - lack of confidence, lack of experience in coding, overchoice of projects and issues.\r\n\r\n\r\nQ&A (5 min)", "code": "SRDANG", "state": "submitted", "created": "2025-01-05", "speaker_names": "Magdalena Kowalczuk", "track": "General: Community & Diversity"}, {"title": "Electify - Retrieval-Augmented Generation for Voter Information in the 2024 European Election", "abstract": "In general elections, voters often face the challenge of navigating complex political landscapes and extensive party manifestos. To address this, we developed Electify, an interactive application that utilizes Retrieval-Augmented Generation (RAG) to provide concise summaries of political party positions based on individual user queries. During its first roll-out for the European Election 2024, Electify attracted more than 6,000 active users. This talk will explore its development and deployment. It will focus on its technical architecture, the integration of data from party manifestos and parliamentary speeches, and the challenges of ensuring political neutrality and providing accurate replies. Additionally, we will discuss user feedback and ethical considerations, focusing on how generative AI can enhance voter information systems.", "full_description": "In general elections, voters often face the  challenge of navigating complex political landscapes. These challenges include understanding the differences between nuanced policy positions, comparing extensive party manifestos, and reconciling conflicting information from various sources. The sheer volume of information and the high frequency of elections can lead to voter fatigue and disengagement [1]. Existing tools like Wahlomat are helpful for voters but don\u2019t adapt well to individual preferences or specific questions. To address these issues, we developed Electify\u2014an interactive application designed to empower voters by addressing these pain points. \r\n\r\nUsing Retrieval-Augmented Generation (RAG), Electify simplifies the decision-making process by enabling users to access concise and relevant summaries of political party positions tailored to their individual queries. Our user interface provides the possibility to fact-check the generated responses by directly showing the original sources. Additionally, we included a blinding feature to combat confirmation bias: users can hide party names and read summaries of their positions before unblinding. This talk will explore the technical development and deployment of Electify, covering its  architecture, integration of data from party manifestos and parliamentary speeches, and strategies to maintain political neutrality and accuracy in responses. In particular, we will discuss our efforts to use reranking to improve context relevancy and LLM-as-a-judge evaluation for parameter optimization. We identify a trade-off between factual accuracy and the frequency of denied responses, which we think is highly relevant for generative AI systems that operate within sensitive areas like voter information [2].\r\n\r\nDuring its first roll-out for the European Election 2024, Electify received significant attention, attracting 6,000 active users who leveraged the platform to make more informed and confident voting decisions. We will address the lessons learned from user feedback and discuss the ethical considerations involved, emphasizing the potential of generative AI to enhance voter information systems and promote political engagement.\r\n\r\nContributors: Christian Liedl, Anna Neifer, Joshua Nowak\r\n[Github Repository](https://github.com/electify-eu/europarl-ai)\r\n\r\n[1] Kostelka et al. \"Election frequency and voter turnout.\" Comparative Political Studies 56.14 (2023)\r\n[2] Cao, Lang. \"Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism.\" arXiv:2311.01041 (2023).", "code": "DNVCEY", "state": "submitted", "created": "2025-01-03", "speaker_names": "Christian Liedl", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Unlocking Web data with TLSNotary, zkProofs  and LLM while preserving privacy", "abstract": "Imagine sharing the data with a third party without revealing any information while still proving you own the data. \r\n\r\nThe core of this talk is about using autogen framework (PyAutogen made by microsoft) and TLSNotary protocol (made by tlsnotary.org) . The talk is about leveraging LLM and TLSNotary to make data portable while maintaining privacy. If you're looking for a way to make data portable without compromising on security, check out this talk.", "full_description": "The talk is about how to query web servers and share the data with another party in a secure and privacy preserving way. We will showcase the tlsnotary protocol along with it's plugin generation system powered by LLM (autogen framework Pyautogen). \r\n\r\nTakeaways\r\n1. Learn how to use autogen framework to develop AI agents for your applications.\r\n2. Learn about zero knowledge proofs and MPC(multi-party computation).\r\n3. Learn the basics of attesting, proving, and verifying data using TLSNotary.", "code": "UV8R3X", "state": "submitted", "created": "2024-12-16", "speaker_names": "Jayaditya Gupta", "track": "General: Ethics & Privacy"}, {"title": "FastHTML vs. Streamlit - The Dashboarding Face Off", "abstract": "In the right corner, we have the go-to dashboarding solution for showcasing ML models or visualizing data, **STREAMLIT** (\\*crowd cheers\\*). Simple yet powerful, it defends the throne of Python dashboarding, but have you ever tried to create complex interactions with it? Things like drill-downs or logins, can make your control flow become messy really quick (\\*crowd nods knowlingly\\*).\r\n\r\nAnd in the left corner, the new contender in the arena of Python web frameworks which, according to its docs, \"*excels at building dashboards*\", **FastHTML** (\\*crowd whoops\\*). We will see if this is true, in the **ultimate dashboarding face off** (\\*crowd gasps\\*). By building the same dashboard, step by step, in both frameworks, investigate their strengths and weaknesses, we will see which framework can claim the crown.", "full_description": "Streamlit is the go-to dashboarding solution for showcasing ML models or visualizing data. It has a vibrant community, multiple years of development under its belt, and tons of third-party integrations. On the other hand, everyone that tried to create complex interactions, like drill-downs or logins, knows that control flow can get messy really quick. Initially simple dashboards often evolve into something bigger and the simple-but-powerful Streamlit formula may not always be up to the tasks.\r\n\r\nFastHTML is a new contender in the arena of Python web frameworks and, according to its docs, \"it excels at building dashboards.\" FastHTML stands on the shoulders of giants, giving you a smooth Python experience for authoring web pages, while allowing access to the foundations of the web, like CSS and JS, at any time. We will see if FastHTML can put code where its mouth is, by building the same dashboard, step by step, in both frameworks and investigate their strengths and weaknesses.\r\n\r\nThis is a talk for data enthusiasts that dabble in web technologies for the sake of showcasing their work or building internal tooling. Do not expect a course on building customer-facing web apps. We will build a dashboard that features:\r\n\r\n- an interactive Plotly chart\r\n- a drill-down with detailed information shown in  a second plot\r\n- a login\r\n- multiple pages and navigation\r\n\r\nWe will examine how hard or easy it is to implement each of these features and how interacting with them in the browser feels. At the end we will see if the reigning champion can defend their crown or if the ambitious contender takes the win.", "code": "WCDPLP", "state": "submitted", "created": "2024-12-04", "speaker_names": "Tilman Krokotsch", "track": "PyCon: Django & Web"}, {"title": "The Art Of Scalable Intelligence: Building Fair, Unbiased and Distributed Machine Learning", "abstract": "Have you ever wondered unique challenges that arise when scaling machine learning algorithms across distributed systems and why only 13% of machine learning projects make it to production? Well, in this talk, lets design fair, unbiased and fault-tolerant ML architectures with ease.", "full_description": "Scaling machine learning across distributed systems introduces significant challenges, particularly when deploying models that impact people's lives. The risk of deploying biased and unfair models is amplified in distributed environments, potentially leading to discriminatory outcomes, especially for marginalized groups. This talk introduces distributed machine learning architectures, focusing on building fair, unbiased, and efficient systems in Python, further diving into various architectures like data parallelism, model parallelism, and hybrid approaches. Crucially, we will see how to address the ethical imperative of fairness, delving into the root causes of bias in machine learning and techniques to mitigate it within distributed settings. Through practical examples and a live demo, we will see how to implement scalable ML architectures that incorporate data sharding, model synchronization, and fault tolerance while prioritizing fairness and transparency.", "code": "TBHWAT", "state": "submitted", "created": "2024-12-26", "speaker_names": "Rashmi Nagpal", "track": "General: Ethics & Privacy"}, {"title": "Outgrowing your node? Zero stress scaling with cuPyNumeric.", "abstract": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library, which  gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library on one of the user's examples covering some detail on its implementation.", "full_description": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library.  cuPyNumeric gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\n\r\nA compelling example when scaling is necessary is when scientists at the Stanford Linear Accelerator Center(SLAC) need to process a large amount of data within a fixed time window, called beam time.  The full dataset generated during experiments is too large to be processed on a single CPU. Additionally, the code often must be modified during the beam time to adapt to changing experimental needs. Being able to use NumPy syntax rather than lower level distributed computing libraries makes these changes quick and easy, allowing researchers to focus on conducting more experiments rather than debugging or optimizing code.\r\n\r\ncuPyNumeric is designed to be a drop-in replacement to NumPy. Built on top of task-based distributed runtime from Stanford University, it automatically parallelizes NumPy APIs across all available resources, taking care of data distribution, communication, asynchronous and accelerated execution of compute kernels on both GPUs or multi-core CPUs.  In addition, cuPyNumeric can be integrated with other popular Python libraries like SciPy, matplotlib, Jax.  With cuPyNumeric, SLAC scientists successfully ran their data processing code distributed across multiple nodes and GPUs, processing the full dataset with a 6x speed-up compared to the original single-node implementation.\r\n\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library covering some detail on its implementation.", "code": "HPGEKH", "state": "accepted", "created": "2024-12-20", "speaker_names": "Irina Demeshko, Quynh L. Nguyen", "track": "PyCon: Programming & Software Engineering"}, {"title": "3 Ways to Speed up Your Regression Modeling in Python", "abstract": "Linear Regression is the workhorse of statistics and data science. Some data scientists even go as far and argue that \"linear regression is all you need\". \r\n\r\nIn this talk, we will introduce three ways to run regression models faster by using smarter algorithms, implemented in the scikit-learn & fastreg (sparse solvers), pyfixest (Frisch-Waugh-Lovell), and duckreg (regression compression via duckdb) libraries.", "full_description": "We introduce three different ways to make regressions run faster. \r\n\r\nWe first introduce sparse solvers and show how to run regressions on sparse matrices via scikit-learn and the fastreg libraries. \r\n\r\nWe then lay out the Frisch-Waugh-Lovell theorem and the alternating projections algorithm and show how to speed it up on the CPU (via numba) and on the GPU (via JAX) as implemented in the pyfixest library. \r\n\r\nFinally, we demonstrate how to drastically speed up regression estimation by first preprocessing the data in duckdb and then fitting a regression via weighted least squares in memory. \r\n\r\nReferences: \r\n- fastreg: https://github.com/iamlemec/fastreg\r\n- scikit-learn: https://github.com/scikit-learn/scikit-learn\r\n- pyfixest: https://github.com/py-econometrics/pyfixest\r\n- duckreg: https://github.com/py-econometrics/duckreg", "code": "ECNDQM", "state": "submitted", "created": "2025-01-05", "speaker_names": "Alexander Fischer", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Feature Analysis of the CICDDoS2019 Dataset Using EDA and Feature Engineering", "abstract": "The CICDDoS2019 dataset is the latest and rich source of labeled data that represents diverse types of Distributed Denial of Service (DDoS) attacks. It serves as a valuable resource for understanding network anomalies, particularly in modern networks like Software-Defined Networks (SDN). This talk focuses on feature analysis, including exploratory data analysis (EDA), feature selection, feature engineering, and dimensionality reduction techniques, applied to dissect the dataset\u2019s attack types, such as DNS, MSSQL, NTP, and UDP flooding. Using Python\u2019s powerful data-handling libraries, we demonstrate how clustering with K-Means and advanced EDA techniques can reveal key patterns in network traffic. Attendees will gain actionable insights into processing large-scale datasets, extracting critical features, and implementing effective feature engineering practices for anomaly detection and classification.", "full_description": "Understanding the unique characteristics of various DDoS attack types is crucial for developing robust detection and mitigation strategies. The CICDDoS2019 dataset provides an extensive collection of labeled data across different attack types, such as DNS, MSSQL, LDAP, and Syn flooding, among others. This session delves into feature analysis and engineering methodologies that transform raw traffic data into meaningful insights for anomaly detection.", "code": "KELSGD", "state": "submitted", "created": "2025-01-04", "speaker_names": "Amare Kassa", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Behind the curtains of the new monitoring API", "abstract": "The new low-impact monitoring API introduced with PEP 669 allows us to implement tools like debuggers and coverage with only a small performance impact. Contrary to the previous `sys.settrace` based approach, the new API uses bytecode modification internally, which allows it to be much faster and fine-grained.\r\n\r\nThis talk will cover the basics of Python bytecode and how the new monitoring API is implemented in CPython, explaining along the way why some monitoring API methods are slower than others.", "full_description": "In this talk, I'll first give an introduction to Python's bytecode, why we need it, how it speeds up code execution, and how it works in principle. But you don't need a compiler engineering degree; I'll keep it short. Then, I follow this with an overview of how the old monitoring API with `sys.settrace` worked and show why it is probably not the best. The new API is far better, so I briefly introduce it and end this by showing how the new API uses clever bytecode modification to achieve the configurability and speediness it is cherished for.", "code": "YY7DGS", "state": "submitted", "created": "2024-12-02", "speaker_names": "Johannes Bechberger", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Processing Large Geospatial Datasets with Dask & Xarray", "abstract": "Geospatial datasets are growing in size, often exceeding 100TB and reaching into Petabyte scale. Let's explore how technologies to process these datasets are evolving to handle the ever growing amount of data and how Xarray and Dask are a great choice to process these datasets efficiently.", "full_description": "Geospatial datasets are growing in size, often exceeding 100TB and reaching into Petabyte scale. Many of these datasets are publicly available, providing a great resource for analysis, but working with them requires increasingly large computational resources and a diverse set of tools.\r\n\r\nWe will start by briefly introducing Dask and Xarray, which form the backbone of the geospatial stack in Python. Using the ERA5 dataset as a case study, we will demonstrate how Xarray can be used to explore large-scale climate data effectively from your local laptop.\r\n\r\nBuilding on this foundation, we will delve into recent advancements in Dask Array. Originally designed as a parallel NumPy API, Dask Array was used to handle much larger datasets over the last few years. We\u2019ll explore the latest developments in Dask and Xarray that continue to expand the scalability and capabilities of these tools to catch up with the scale requirements of modern datasets.\r\n\r\nThis discussion will highlight improvements in ease of use, scalability, and performance. Additionally, we\u2019ll present the first-ever set of geospatial benchmarks, collected earlier in 2024 from the community. These benchmarks provide a clear illustration of the scale at which Xarray and Dask are required to operate.\r\n\r\nFinally, we\u2019ll offer a peak behind the scenes of an ongoing project aimed at building the first ever query optimizer for large scale array computations.", "code": "FQCMBL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Patrick Hoefler", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Deploying AI Models on Edge Devices For Diagnostic Support in Healthcare.", "abstract": "The  World Health Organization reports that nearly 60% of low-income countries lack enough healthcare workers, leaving essential diagnostic needs unmet.\r\nThis simply implies that in areas with limited resources, poor connectivity and lack of infrastructure make traditional cloud-based AI solutions to healthcare impractical. This is where edge AI steps in, it offers a practical approach to bringing AI capabilities directly to edge devices ( smartphones, drones or even industrial robots ).\r\n\r\nIn this talk, I will educate people on how lightweight AI models can be deployed on edge devices to enable diagnostic support in  healthcare, and provide  help where it\u2019s needed.  Using Open Neural Network Exchange (ONNX), I will show how to optimize machine learning models for significant applications such as detecting abnormalities all without reliance on cloud dependent systems.", "full_description": "Edge AI refers to the practice of running artificial intelligence algorithms and models directly on edge devices, like smartphones, smart watches, IoT devices, cameras, or other embedded systems, instead of relying on centralized cloud servers.\r\nAsides from the fact that this reduces reliance on cloud servers which can be expensive, it also enables real time monitoring and improves accessibility to AI powered diagnostics.\r\n\r\nFor instance, Imagine wearing a smartwatch that monitors your heart rate, blood pressure and oxygen level and then processes this data locally to detect anomalies in your health.\r\nIn addition to being quick and portable, it solves the problem of unmet diagnostic needs.\r\n\r\nThis talk, focuses on how to deploy AI models on Edge devices to address diagnostics challenges and increase efficiency in healthcare settings.\r\nUsing Open Neural Network Exchange(ONNX), a framework valued because of its cross platform compatibility and ability to seamlessly convert  frameworks, I will demonstrate how to optimize models for tasks like detecting medical imaging abnormalities, monitoring vital signs, and identifying early warning signs of chronic conditions. Attendees will learn techniques to provide efficient AI solutions on devices like smartphones and IoT sensors, which will help to reduce the need for constant cloud connectivity and enhance  healthcare accessibility in remote ans underserved areas.", "code": "9QKJUJ", "state": "submitted", "created": "2024-12-31", "speaker_names": "Kelechi Chibundu", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond Prompts: Building Autonomous Agents with Generative AI and LLMs", "abstract": "Generative AI and Large Language Models have rapidly advanced beyond simple question-answering and text summarization tasks. The next frontier lies in harnessing these technologies to create autonomous agents\u2014systems capable of reasoning, planning, and executing complex sequences of actions with minimal human intervention. This talk explores the principles and challenges of designing such agents, focusing on how LLMs can function as core components for decision-making and communication.\r\nWe will explore strategies for integrating LLMs with external tools, APIs, and feedback loops, enabling adaptive behavior in dynamic environments. Real-world applications will be highlighted, from task automation and research assistance to creative problem-solving and collaborative workflows. Attendees will gain insights into the architectural frameworks, theoretical foundations and technical considerations necessary to deploy robust and responsible autonomous systems. Whether you're an AI enthusiast, developer, or researcher, this session will provide a roadmap for navigating this exciting and transformative domain.", "full_description": "Generative AI and Large Language Models have rapidly advanced beyond simple question-answering and text summarization tasks. The next frontier lies in harnessing these technologies to create autonomous agents\u2014systems capable of reasoning, planning, and executing complex sequences of actions with minimal human intervention. This talk explores the principles and challenges of designing such agents, focusing on how LLMs can function as core components for decision-making and communication.\r\nWe will explore strategies for integrating LLMs with external tools, APIs, and feedback loops, enabling adaptive behavior in dynamic environments. Real-world applications will be highlighted, from task automation and research assistance to creative problem-solving and collaborative workflows. Attendees will gain insights into the architectural frameworks, theoretical foundations and technical considerations necessary to deploy robust and responsible autonomous systems. Whether you're an AI enthusiast, developer, or researcher, this session will provide a roadmap for navigating this exciting and transformative domain.", "code": "PGUHFU", "state": "withdrawn", "created": "2024-12-21", "speaker_names": "Catalin Hanga", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Challenges and Lessons Learned While Building a Real-Time Lakehouse using Apache Iceberg and Kafka", "abstract": "How do you build a large-scale data lakehouse architecture that makes data available for business analytics in real time, while being more cost-effective, more flexible and faster than the previous proprietary solution? With Python, Kafka and Iceberg, of course!\r\n\r\nWe built a large-scale data lakehouse based on Apache Iceberg for the Schwarz Group, Europe's largest retailer. The system collects business data from thousands of stores, warehouses and offices across Europe.\r\n\r\nIn this talk, we will present our architecture, the challenges we faced, and how Apache Iceberg is shaping up to be the data lakehouse format of the future.", "full_description": "The Schwarz Group is present in thirty-two countries around the world with over ten thousand stores, hundreds of warehouses, an assortment of over two thousand different products and a single ERP system to manage them all.\r\n\r\nEvery country maintains its own databases for operational purposes but all the data is also gathered in one central analytics platform for all countries. Not only does this platform need to be stable and reliable, the data also needs to be made available for the consumers in near real-time \u2013within mere minutes. The existing analytics platform was based on proprietary solutions which were expensive and required niche knowledge, severely limiting the number of available developers.\r\n\r\nTherefore, we set out on a journey to completely redesign the analytics platform; a new solution, based as much as possible on Open Source technologies like Python, Kafka and Iceberg. Leveraging Python for its great ecosystem and ease of use. Kafka for fast and reliable message processing of over one thousand tables per country into one central hub. And Iceberg at the core, as our data lakehouse format for its fully transparent schema evolution and high performance through its rich metadata layer.\r\nThrough our presentation we will showcase the different challenges we faced during the desing of our new architecture, how our selected tech stack allowed us to tackle each of them and the lessons we learnt. We will focus on our challenges in four areas: scalability, performance, continuity of service and data quality.\r\n\r\nScalability: Ingesting changes on over one thousand tables coming from servers across thirty-two countries supporting operations of Europe\u2019s largest retailer is no easy task. Our architecture needs to support receiving tens of thousands of events per second. We will present how we set up Kafka to support our current load and potential for future growth, how we use Tabular\u2019s Iceberg sink connector to ingest all our tables, and how we leverage avro serialization and snappy compression of messages to reduce network traffic.\r\n\r\nPerformance: The large amounts of data we handle, paired with the influx of small files that can result from real-time data ingestion, made ensuring performance an extremely challenging aspect of our application. We will show how we designed our data lake house; using Icebergs hidden partitioning to ensure performance while remaining flexible to evolve them over time and how we designed and implemented an effective maintenance job to reduce small files in our iceberg tables.\r\n\r\nContinuity of Service: The existing analytics platform contains the core of the business data which is used for many analytics and forecasting use cases across the organization. One of main requirements was to ensure a smooth transition to the new architecture with as little downtime as possible. This meant facilitating the access to existing users by allowing them to retrieve the data in the same way they were doing it in the past, with minimal changes. We will show how our architecture ensures flexibility by allowing access to the data from many engines and show an example on how we integrated our architecture with Snowflake. \r\n\r\nData quality: We faced some challenges when it came to the consolidation of all the data we receive from all countries. For some tables, the schemas diverged across countries having different sets of columns, different data types being used and even different primary keys. We will talk about how we handled data quality issues coming from the operational databases by using iceberg\u2019s schema evolution capabilities, a schema registry and Kafka Connect single message transforms (SMTs).", "code": "JUAF3S", "state": "confirmed", "created": "2024-12-19", "speaker_names": "Jonas B\u00f6er, Elena Ouro Paz", "track": "PyData: Data Handling & Engineering"}, {"title": "Learning to Rank at Scale: idealo's Challenging Journey to Better Search Ranking", "abstract": "Search is one of the pillars of modern e-commerce platforms, directly influencing user experience and business outcomes. To boost search conversion rates at idealo, one of Europe's leading price comparison platforms, we built an in-house Learning to Rank system that leverages user behavior data to optimize search rankings.\r\n\r\nHandling millions of daily user searches across our German catalog of more than 500 million offers and 5 million products required a robust and scalable solution. This talk dives deep into the practical challenges of building and deploying a Learning to Rank system at scale.", "full_description": "In this talk, we'll take you behind the scenes of building our Learning to Rank pipeline. Attendees will learn how to:\r\n\r\n - Integrate Machine Learning Lifecycle and MLOps practices to ensure continuous improvement and scalability of our system with AWS Sagemaker, AWS Glue, Airflow, Github Actions and Grafana\r\n - Generate effective Learning to Rank training targets from implicit user feedback\r\n - Design robust evaluation metrics for online and offline evaluation of Learning to Rank systems, even in the absence of explicit user purchase signals\r\n - Identify and mitigate common biases in user behavior data\r\n - Optimize LightGBM models for low-latency inference, especially when dealing with high cardinality categorical features\r\n\r\nWe'll also share hard-earned lessons from our production deployment, including how daily retraining, combined with inherent position bias in user feedback, led our model down a path of reinforcing its own errors (and how we escaped this feedback loop!).", "code": "KNZ7PQ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Atakan Filg\u00f6z", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Mastering Demand Forecasting: Lessons from Europe's Largest Retailer", "abstract": "Ever craved your favorite dish, only to find its key ingredient missing from the store? You're not alone - stock outs can have significant consequences for businesses, resulting in frustrated customers and lost sales. On the other hand, overstocking can lead to wasted storage costs and potential write-offs. The replenishment system is responsible for striking the right balance between these opposing risks.\r\nThe key to successful replenishment is making accurate predictions about future demand. \r\n\r\nThis presentation takes a deep dive into the intricate world of demand forecasting, at Europe's largest retailer. We will demonstrate how enhancing simple machine learning methods with domain knowledge allows to generate hundreds of millions of high-quality forecasts every day.", "full_description": "This talk will provide an in-depth look at the forecasting engine, the heart of Lidl's replenishment system.\r\n\r\nEach day at Lidl, hundreds of millions of various products journey from suppliers to warehouses before reaching the shelves. Our so-called forecasting engine helps to automate the supply chain at every step along the way. \r\nEven with the vast amount of data at our disposal, the problem is still extraordinarily intricate. Each item, store or warehouse has unique demand patterns influenced heavily by a wide range of factors, such as holidays. While most of the effects are quantifiable, others remain unavailable and a certain degree of stochasticity is inherent to the process. The objective of our demand prediction may also vary based on their usages. Accuracy on the day level typically matters for short-term predictions, while it doesn't for long-term predictions. \r\n\r\nWe'll present our pragmatic modeling methodology on a simplified version of the problem at hand: The warehouse forecasting of single items. \r\n\r\nWe explain the rationale for training separate models for each item-warehouse combination and go into the reasons why we opted for using a LGBM model and why we believe it is best suited for our application. In addition to outlining our high-level modeling approach, we demonstrate how business and domain expertise are integrated into the modeling process through the use of sample and feature weighting and examine the impact of this integration on prediction quality. Following the base model, extensions are introduced that enable the incorporation of higher-level information at the finest level of granularity. This is achieved through decomposition and recomposition of the time-series at hand. In detail, we will present uplift decomposition for different use-cases, which include handling of promotions and holidays.\r\n\r\nTo conclude, we will give an overview of how all the presented methods synergize in delivering reliable forecasts for happy customers, so that you will hopefully never find yourself in front of an empty shelf!", "code": "NNGWGC", "state": "accepted", "created": "2024-12-21", "speaker_names": "Moreno Schlageter, Yovli Duvshani", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Crafting Great APIs for Libraries: Lessons from the Good, the Bad, and the Delightful", "abstract": "APIs serve as the critical interface between users and libraries, yet designing an effective API remains a nuanced and often underestimated challenge. In this talk, we\u2019ll explore what makes an API great by focusing on three core principles: simplicity, consistency, and flexibility.\r\n\r\nThrough a hands-on case study, we\u2019ll iteratively improve a real-world API, showcasing practical techniques for identifying and addressing pain points. Along the way, we\u2019ll discuss common mistakes that make APIs confusing or difficult to use and how to avoid them. By the end of the session, attendees will have a deeper understanding of the art of API design and the tools to create interfaces that developers love.\r\n\r\nWhether you\u2019re an experienced library developer or just starting your journey, this talk will equip you with the knowledge to build APIs that not only work but shine.", "full_description": "Designing an API is more than just defining methods and parameters; it\u2019s about creating a seamless connection between the user and the library. A well-crafted API can elevate the user experience, making a library a go-to choice for developers. But what separates a great API from a mediocre one? And what makes an API actively frustrating to use?\r\n\r\nThis talk explores the principles of effective API design\u2014simplicity, consistency, and flexibility\u2014and their real-world applications. Attendees will learn to identify common pitfalls in API design and how to avoid them. Through an iterative approach, we\u2019ll work on improving a real-world API, transforming it from cumbersome to delightful. This process will highlight the importance of understanding your users\u2019 needs and designing with empathy, ensuring that your API becomes a tool developers genuinely enjoy working with.\r\n\r\nWhether you\u2019re designing APIs for internal tools, open-source libraries, or enterprise systems, this session will provide actionable insights and practical techniques to help you craft APIs that stand out.", "code": "FDHAS7", "state": "submitted", "created": "2024-12-24", "speaker_names": "Syed Ansab Waqar Gillani, Syed Hassan Gilani", "track": "PyCon: Programming & Software Engineering"}, {"title": "\"FastHTML: Simple Starts, Unlimited Possibilities - Web Development the Python Way", "abstract": "Are you frustrated with complex JavaScript frameworks? Tired of hitting walls with inflexible Python web tools? Want to build web apps without leaving the Python ecosystem?\r\n\r\nFastHTML, created by Fast.ai and Answer.ai founder Jeremy Howard, is revolutionizing Python web development. Built on the same foundation as FastAPI (Starlette) and supercharged by HTMX, it brings modern web capabilities to Python developers.\r\n\r\nUnlike many popular higher-level python web development tools, FastHTML grows with your needs. No more rewriting applications when you outgrow your framework's limitations.\r\n\r\nIn this talk you'll discover:\r\n\r\n1. How FastHTML enables production-ready web apps in pure Python\r\n2. Why its architecture makes it both simple to start and infinitely extensible\r\n3. Real-world examples of FastHTML in action\r\n4. The philosophy behind \"hackable\" web development\r\n\r\nJoin us for 90 minutes of practical, hands-on learning that will transform how you think about web development with Python!", "full_description": "Are you frustrated with complex JavaScript frameworks? Tired of hitting walls with inflexible Python web tools? Want to build web apps without leaving the Python ecosystem?\r\n\r\nFastHTML, created by Fast.ai and Answer.ai founder Jeremy Howard, is revolutionizing Python web development. Built on the same foundation as FastAPI (Starlette) and supercharged by HTMX, it brings modern web capabilities to Python developers.\r\n\r\nUnlike many popular higher-level python web development tools, FastHTML grows with your needs. No more rewriting applications when you outgrow your framework's limitations.\r\n\r\nIn this talk you'll discover:\r\n\r\n1. How FastHTML enables production-ready web apps in pure Python\r\n2. Why its architecture makes it both simple to start and infinitely extensible\r\n3. Real-world examples of FastHTML in action\r\n4. The philosophy behind \"hackable\" web development\r\n\r\nJoin us for 90 minutes of practical, hands-on learning that will transform how you think about web development with Python!", "code": "PJPTFC", "state": "submitted", "created": "2025-01-05", "speaker_names": "Rens Dimmendaal", "track": "PyCon: Django & Web"}, {"title": "Design, Generate, Deploy:  Contract-First with FastAPI", "abstract": "This talk explores a contract-first approach to API development using the OpenAPI generator, a powerful tool for automating API generation from a standardized specification.  We will cover (1) what would you need to run to have a standard implementation of the FastAPI endpoints and data models; (2) how to customize the mustache templates that are used to generate the API stubs; (3) share some ideas how to customize the CLI and (4) how to maintain the contract and how to handle breaking changes to the contract. We will close the session with a discussion of the challenges of implementing the OpenAPI generator.", "full_description": "Let me share a story with you about two developers working at a Malt, Europe's leading freelance management system & marketplace. \r\n\r\nDev-1: Hi there! We have an issue on production. It seems that a request was sent where \u201ccompany id\u201d is not given. \r\nDev-2: Oops! But I thought we agreed on an anonymous mode?\r\nDev-1: That\u2019s actually a great idea. You mean that company id is not required? \r\nDev-2: Exactly!\r\nDev-1: Thanks! I will update the data model and push the changes! \r\n\r\nAs the conversation above suggests, sending data between two applications can easily fail if the requirements are not defined up front. Even for simple requests a lot of decisions have to be made: are the fields optional or mandatory? What about the returned payloads and their data types? Do we need default values? If we are not clear what we will expect (from the request) and what we will return (in the response), in the worst case, the request will fail and we spend time debugging, like above.  \r\n\r\nTo overcome this issue, we decided to move to a contract-first approach, where we define the exact request and response and generate the endpoints and data models from there using the OpenAPI generator. The OpenAPI generator is a powerful tool that allows you to automatically generate API client libraries, server stubs, documentation, and configuration from an OpenAPI specification, or a \u201ccontract\u201d between two applications. This contract forms the basis for generating the endpoint stubs for our python applications but also for the client models and code. Starting with the contract can significantly speed up the development process and improve the consistency of your API implementations.\r\n\r\nDuring this talk we will address the following topics: \r\n- The vanilla implementation that generates endpoints and data models: what would you need to run to have a first version of the FastAPI endpoints. If the setting allows for it, we would show a short demonstration. \r\n- How to use customisable templates: we customised the mustache templates that generated the endpoints and data models so we could generate our custom FastAPI app. Also we added examples to the generated data models as these were not available in the default implementation. \r\n- How to customise the CLI tool and ideas for setting up your CI pipeline: we will share some ideas how to customise the CLI and how we used it in our CI pipeline to prevent discrepancies between the contract and the generated stubs.\r\n- how to maintain the contract and how to handle breaking changes to the contract\r\nWe will close the session with a discussion of the challenges and benefits of implementing the OpenAPI Generator. While it offers standardisation and best practices, it can introduce additional complexity, especially with the tool still in beta. We'll share our experiences navigating this trade-off.", "code": "ZACM3E", "state": "submitted", "created": "2024-12-19", "speaker_names": "Dr. Evelyne Groen, Kateryna Budzyak", "track": "PyCon: MLOps & DevOps"}, {"title": "Decoding AI's evolution using Big Data: a methodological approach", "abstract": "This talk explores AI's impact on jobs by analyzing the tasks and online job postings. It reveals AI-driven shifts in job roles and skills identifying key skill clusters. The findings guide educators and policymakers in aligning workforce competencies with emerging trends.", "full_description": "This study investigates the transformative impact of Artificial Intelligence on occupations by analyzing the Atlas of Work dataset and online job postings. Through advanced data preparation and model selection, the research reveals how AI reshapes job roles, emphasizing dynamic changes in required skills. Focusing on sectors like ICT, telecommunications, and mechatronics, it identifies distinct skill clusters and highlights the convergence of digital, soft, and hard skills. The findings offer actionable insights for educators, policymakers, and industry leaders to adapt workforce development strategies in response to emerging AI-driven demands.", "code": "R9FX39", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Generative AI"}, {"title": "OpenAI o1/o3: the New Scaling Laws for LLMs that can reason", "abstract": "The renowned Chinchilla paper changed the way we train Large Language Models (LLMs). The authors - including the current Mistral CEO - derived the so-called **scaling laws** to maximise your model performance under a compute budget, balancing the number of parameters and data *during training*.\r\n\r\nHowever, with o1, OpenAI ushered a new era of LLMs: reasoning capabilities. This new breed of models broadened the concept of scaling laws, shifting focus from **train-time** to **test-time** (or inference-time) compute. How do these models work? What do we think their architectures look like, and what data do we use to train them? And finally - and perhaps more importantly: how expensive can they get, and what can we use them for?", "full_description": "OpenAI o1, or Qwen's QwQ, make us feel we are in an \"Alpha Zero moment\" for LLMs. While have plenty of GPT4-like capable models, this new breed of LLMs promises to achieve new levels of performance through a very simple principle: by spending more time to answer tougher questions.\r\n\r\nWe tried to achieve this effect in several ways, both at the prompt engineering and the neural network level. However, none has proven this effective thus far. The talk will cover the following points.\r\n\r\n**LLaMAs and Chinchillas** (6 min). A brief recap of the Chinchilla findings and how we departed ways from the old scaling laws with models like LLaMA.\r\n\r\n**From train-time to test-time** (6 min). What test-time compute is, when it is advantageous to scale test-time compute, and what are the use-cases for reasoning models.\r\n\r\n**How do models reason?** (8 min) What \"reasoning\" consists of, and how it's been currently implemented.\r\n\r\n**Benchmark review** (5 min) A couple of words to caution the audience against the benchmarks that have been published so far, especially when it comes to pricing.\r\n\r\n**Conclusion** (5 min). Wrap up and questions.", "code": "F77EP9", "state": "submitted", "created": "2024-12-22", "speaker_names": "Luca Baggi", "track": "PyData: Generative AI"}, {"title": "GraphQL: Does it replace SQL, REST or Something Else?", "abstract": "Join us for an in-depth exploration of GraphQL's impact on API development. We'll analyze its unique approach to data querying and manipulation, comparing it with traditional SQL and REST architectures. Discover the advantages and limitations of GraphQL, and explore its potential to reshape modern web development practices. Whether you're a seasoned developer or new to the field, this talk offers valuable insights into navigating the evolving landscape of API technologies. At the end you will get recommendations when to choose what.", "full_description": "Attendees will gain insights into GraphQL's unique approach to API development, understand its comparative advantages and limitations when contrasted with SQL and REST, and learn about its potential applications in modern web development. This session equips participants with knowledge to navigate the evolving API landscape effectively. The demo application showcases how to manage an airline database using both GraphQL and REST interfaces. The application uses Flask for the web server and Couchbase for the database. It provides CRUD operations (Create, Read, Update, Delete) for managing airlines.", "code": "VQLKZG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Gregor Bauer", "track": "PyCon: Programming & Software Engineering"}, {"title": "I want to deploy my Flask app on Kubernetes, what are my options?", "abstract": "After coding your amazing Flask app, now you have to deploy and operate it. Kubernetes is a great way to do this, but it isn't easy to get started with creating a container and knowing what your deployment options are. In this talk we look at a few options so that the next time you have to deploy your Flask application you know which option is best for you!", "full_description": "After creating a great web app using Python such as with flask, the next hurdle to production is how to make it available to users and operate it. And not just your app, but also ingress, the database, observability and the list goes on. We will go through your options for simplifying the operations of your web app using open source tooling. This will include using k8s directly with helm charts, PaaS using fly.io and new tooling developed by Canonical using juju. By the end of the talk you will have seen the benefits and drawbacks of each which will help you make an informed decision on which tool best suits your needs!", "code": "KHTQJQ", "state": "submitted", "created": "2024-12-02", "speaker_names": "David Andersson", "track": "PyCon: Django & Web"}, {"title": "Reproducible Dependency Management with pixi", "abstract": "Learn how to manage your conda- and pip-based Python dependencies with pixi.\r\nWith its declarative approach, pixi provides a reliable way for reproducible\r\nPython environments.\r\nIts design is based on experiences drawn from tools such as `pip`,\r\n`conda` and `mamba` as well as Rust's `cargo`.", "full_description": "Managing dependencies can be a complex task,\r\nespecially when it comes to multi-platform support and extensions written in\r\ncompiled languages, such as C/C++ or Rust, are involved.\r\nThere are many tools for Python that can help to take care of different aspects\r\nof this problem.\r\nThese tools are external Python packages that are not distributed with Python\r\nitself.\r\nOnly `pip` is installed by default.\r\nOther, more recent languages such as Rust, include the tooling for dependency\r\nmanagement as part of the core language.\r\nOn the other hand, the Python ecosystem for dependency management is very\r\nfragmented.\r\nIn general, there are two main approaches\r\n(a) based on pip packages and\r\n(b) based on conda packages.\r\nPixi offers an approach that can work with both package types.\r\nIts design is inspired by Rust's cargo.\r\nIn addition to Python packages, pixi can also manage software written in other\r\nlanguages.\r\nFor example, using conda-forge as the default conda repository,\r\npixi can install tools such as `gcc`, `pandoc`, or `git`.\r\nPixi can work tother with existing tools such as `setuptools` or `poetry`.\r\nIn fact, it uses `uv` internally to install pip packages.", "code": "NW3PP8", "state": "submitted", "created": "2024-12-22", "speaker_names": "Mike M\u00fcller", "track": "PyCon: Programming & Software Engineering"}, {"title": "Lessons learned in bringing a RAG chatbot with access to 50k+ diverse documents to production", "abstract": "Retrieval-Augmented Generation (RAG) chatbots are a key use case of GenAI in organizations, allowing users to conveniently access and query internal company data. A first RAG prototype can often be created in a matter of days. But why are the majority of prototypes still in the pilot stage? [\\[1\\]](https://www2.deloitte.com/content/dam/Deloitte/us/Documents/consulting/us-state-of-gen-ai-q3.pdf)\r\n\r\nIn this talk we share our insights from developing a production-grade chatbot at Merck. Our RAG chatbot for R&D experts accesses over 50,000 documents across numerous SharePoint sites and other sources. We identified three key success factors:\r\n1. Developing a robust data pipeline that syncs documents from source systems and that handles enterprise features such as replicating user permissions. \r\n2. Establishing a comprehensive evaluation framework with a clear optimization metric.\r\n3. Driving adoption through an onboarding training and ongoing user engagement, such as regular office hours.\r\n\r\nWe think that many of these lessons are broadly applicable to RAG chatbots, making this talk valuable for practitioners aiming to implement GenAI solutions in business contexts.", "full_description": "Building a prototype RAG chatbot with frameworks like LangChain can be straightforward. However, scaling it into a production-grade application introduces complex challenges. In this talk, we share our lessons learned from developing a RAG chatbot designed to assist research and development (R&D) experts.\r\n\r\nOur chatbot was developed to effectively handle and provide access to a large collection of unstructured knowledge, consisting of over 50,000 documents stored across more than 20 SharePoint sites and other sources. We faced significant hurdles in:\r\n- **Data Pipeline Engineering**: Crafting a modular and scalable pipeline capable of periodically syncing documents, handling dynamic user permissions, and efficiently processing large volumes of unstructured data.\r\n- **Evaluation Framework Development**: Implementing an effective testing strategy without the availability of static ground truth data. We employed automated testing with frameworks like pytest, utilized LLM-as-a-judge, and integrated tracing to iteratively refine our dataset and maintain high answer quality.\r\n- **RAG Design and Prompting Strategies**: Addressing challenges in document chunking, citation integration, reranking retrieved results, and applying permission and PII filters to ensure compliance and accuracy in responses.\r\n- **User Adoption**: Driving user adoption through onboarding training and ongoing engagement, such as regular office hours and feedback mechanisms.\r\n\r\nWe emphasize the importance of applying data science principles to GenAI projects:\r\n- **Start Simple and Iterate**: Begin with a basic implementation as a baseline and iteratively enhance functionality based on testing and user feedback.\r\n- **Test-Driven Development**: Identify key test scenarios early and use them to drive development, ensuring that improvements are measurable and aligned with growing user needs.\r\n- **Focus on Key Metrics**: Establish clear metrics to optimize against, aiding in making informed decisions throughout the development process.\r\n  \r\n**Main Takeaways for the Audience:**\r\n- Understand the critical role of robust, modular data pipelines in handling dynamic and unstructured data sources for LLM applications.\r\n- Learn strategies for developing effective evaluation frameworks in complex domains where traditional ground truth data may be lacking.\r\n- Gain insights into advanced RAG design techniques that enhance chatbot performance and reliability.\r\n- Recognize the substantial data engineering and software development efforts required to transition a prototype to a production-grade LLM solution.\r\n\r\nBy sharing our experiences, attendees will gain practical insights into deploying robust RAG chatbots, transforming a functional prototype into a reliable, scalable application that fulfills enterprise requirements.", "code": "XLZQFA", "state": "submitted", "created": "2024-12-20", "speaker_names": "Bernhard Sch\u00e4fer", "track": "PyData: Generative AI"}, {"title": "GreenPython: Measuring and Reducing Software's Carbon Footprint Through Python", "abstract": "Every line of code has an environmental cost. In this talk, attendees will understand how to measure and optimize your Python applications' energy consumption using tools you already know. We'll combine familiar libraries like pandas and scikit-learn with monitoring tools to create a complete picture of your code's carbon footprint \u2013 from database queries to ML model inference. Through practical examples, you'll learn how different implementation choices affect energy usage and how to make your code more environmentally friendly without sacrificing performance.", "full_description": "In today's digital age, our code's environmental impact often goes unnoticed. This talk sheds light on how Python applications affect energy consumption and what we can do about it. From AI training to web services, digital infrastructure accounts for a significant portion of global energy usage \u2013 but it doesn't have to be this way. Through practical demonstrations, attendees will explore how different Python implementations affect power consumption and learn to make environmentally conscious coding decisions.\r\n\r\nWe'll dive into how seemingly innocent code choices can significantly impact carbon emissions, from database queries to ML model training. You'll see real-time monitoring of environmental impact in action and discover predictive approaches to estimate carbon footprints of different architectural choices. By examining patterns in computational resource usage and their energy implications, we'll uncover simple yet effective strategies to make Python applications more sustainable.", "code": "PJVKCM", "state": "submitted", "created": "2024-12-22", "speaker_names": "Neeraj Pandey", "track": "General: Others"}, {"title": "How to solve a Python mystery", "abstract": "Has any of your Python applications become unresponsive? Has it deadlocked or is it busy doing something? How can you know?\r\nIn this talk, I will introduce useful Linux performance and observability tools that can be used to understand which files, connections, and OS system calls your application is performing and share real-world mysteries that it helped to solve.", "full_description": "Has any of your Python applications become unresponsive? Has it deadlocked or is it busy doing something? How can you know?\r\nWhen troubleshooting some Python applications you often don\u2019t have time to analyze the source code and to understand how exactly it is working. Instead, you have to act fast and treat the application as a black box. In this talk, I will introduce useful Linux performance and observability tools that can be used to understand which files, connections, and OS system calls your application is performing. I will also share real-world mysteries that this approach has helped to solve.", "code": "ZMX3JT", "state": "withdrawn", "created": "2024-11-27", "speaker_names": "Aivars Kalv\u0101ns", "track": "PyCon: Programming & Software Engineering"}, {"title": "Real-time visualization using dash and plotly", "abstract": "In this paper, a simple live dashboard will be developed using plotly and dash on a practical dataset. This will ease the presentation of data job by abstracting the technicalities and codes from the non-data persons.", "full_description": "Attendees will learn how to create responsive dashboards using Dash, with an emphasis on real-time data processing, integrating live data streams, and efficiently visualizing large datasets. Areas to be discussed are:\r\n- Introduction to Dash and Plotly\r\n- Building interactive Dash applications\r\n- Streaming and updating visualizations in real-time.\r\n- Best practices for optimizing performance and ensuring smooth user experiences.\r\n- Use cases\r\nThere is a possibility that after this session, the participants will have the skills to create real-time dashboards.", "code": "RDEG7P", "state": "rejected", "created": "2024-12-09", "speaker_names": "Hampo, JohnPaul A.C.", "track": "PyData: Visualisation & Jupyter"}, {"title": "Empowering Your ML Models: Building Robust and Accessible APIs for Seamless Integration.", "abstract": "Unlock the potential of your machine learning models by transforming them into accessible APIs. In this talk, we will delve into the practical aspects of building and deploying APIs using powerful tools like Flask and FastAPI. You'll learn the core principles of API design, along with best practices for handling requests, input validation, and security. We'll also explore real-world deployment strategies, including an engaging example of deploying a model on a Raspberry Pi Zero W. This talk aims to equip you with the skills needed to create APIs that make your ML models easily integrable with various applications, ensuring they deliver maximum value.", "full_description": "Elevate your machine learning projects by transforming your models into accessible APIs. This talk delves into the process of building APIs to expose ML models for integration with various applications. We'll cover the fundamentals of RESTful API design, and explore tools like Flask and FastAPI to create efficient and scalable APIs. Learn how to handle requests, manage input validation, and ensure security. \r\nAdditionally, we'll discuss deployment strategies to make your APIs production-ready, including a fun demonstration of deploying a model on a Raspberry Pi Zero W. By the end of this session, you'll have the knowledge and skills to create and deploy efficient APIs that enhance the usability and accessibility of your machine learning models.. \r\n\r\n**Outline** \r\n\r\n**1- Introduction to Model Deployment (5 minutes)**\r\n\r\nTopics:\r\n- Importance of deploying ML models.\r\n- Picking the right tool for your needs\r\n- Modular and bespoke pipelines\r\n- Open source tool support: TensorFlow, PyTorch, Jupyter Notebooks and scikit-learn\r\n- Data labelling\r\n- Challenges in transitioning from development to production.\r\n\r\nObjectives:\r\n- Set the context for the importance of deploying ML models.\r\n- Highlight common challenges faced during deployment.\r\n\r\n\r\n**2- Deployment Strategies (10 minutes)**\r\n\r\nTopics:\r\n- Overview of different deployment strategies (batch, real-time, A/B testing).\r\n- Pros and cons of each strategy.\r\n\r\nObjectives:\r\n- Provide a comparative understanding of deployment strategies.\r\n- Help attendees choose the right strategy for their use case.\r\n\r\n\r\n**3- Containerization with Docker (10 minutes)**\r\n\r\nTopics:\r\n- Introduction to Docker and containerization.\r\n- Creating Docker containers for ML models.\r\n\r\nObjectives:\r\n- Demonstrate how to use Docker to containerize ML models.\r\n- Ensure attendees understand the benefits of containerization.\r\n\r\n\r\n**4- Deploying on Cloud Platforms (10 minutes)**\r\n\r\nTopics:\r\n- Overview of popular cloud platforms (AWS, GCP, Azure).\r\n- Deploying models using cloud services.\r\n\r\nObjectives:\r\n- Show how to deploy ML models on different cloud platforms.\r\n- Provide practical examples and step-by-step guides.\r\n\r\n\r\n**5- Monitoring and Maintenance (5 minutes)**\r\n\r\nTopics:\r\n- Setting up monitoring and logging for deployed models.\r\n- Handling model updates and retraining.\r\n\r\nObjectives:\r\n- Teach attendees how to monitor model performance.\r\n- Discuss strategies for maintaining and updating models.\r\n\r\n\r\n**6- Q&A (5 minutes)**\r\nTo address attendee questions and clarify any doubts.", "code": "KRDFYU", "state": "submitted", "created": "2024-12-20", "speaker_names": "Ariane Djeupang", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Decoding Demand: The Power of Scalable Model Explainability", "abstract": "In the fast-paced world of demand prediction, understanding the 'why' behind predictions is as crucial as the predictions themselves. This presentation explores the power of explainability, enabling the standardization of insights across diverse models at scale. This approach ensures that everyone - from data enthusiasts to industry professionals - can grasp complex data with clarity and confidence.", "full_description": "Demand forecasting is a crucial element in the retail and manufacturing sectors, ensuring products are produced and delivered efficiently while minimizing waste. To enhance decision-making and refine model development workflows, it's vital to understand and explain model predictions by evaluating the influence of input data and identifying deviations from actual outcomes.\r\nShapley values provide a powerful framework for model explainability by offering additive marginal contributions of model features to predictions. Their nature, where the sum of all Shapley values equals the model prediction, makes them easily interpretable and widely adopted. However, calculating Shapley values can be computationally demanding, especially when scaling up to handle large datasets.\r\nIn our approach, we employ, alongside other models, generalized additive model (GAM)-inspired architectures with exponential link functions for demand predictions, which inherently provide multiplicative factor explanations. I will demonstrate how we efficiently compute Shapley values from these factor explanations, allowing us to generate them alongside predictions with minimal overhead, directly into a Snowflake database.\r\nThis talk will delve into the challenges and solutions for scaling model explanations, highlighting the fundamental differences between additive and multiplicative model explanations and their impact on interpretation. Additionally, I will discuss how Snowflake empowers us to aggregate Shapley values across extensive customer datasets, enabling the visualization of model explainability at various scales. This capability is crucial for providing actionable insights and ensuring that model transparency is maintained even as data volumes grow.", "code": "LDUS7U", "state": "submitted", "created": "2024-12-19", "speaker_names": "Tim Daniel Rose", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Securing RAG Pipelines with Fine Grained Authorization", "abstract": "Using LLMs and AI in your Enterprise? Make sure you build Fine Grained Authorization to ensure your LLMs access only the data they are authorized to. \r\n\r\nThis talk will show how you can build Relationship Based Access Control (ReBAC) for fine-grained authorization for your RAG pipelines. The talk also includes a demo using Pinecone, Langchain, OpenAI, and SpiceDB.", "full_description": "Building enterprise-ready AI requires ensuring users can only augment prompts with data they're authorized to access. Relationship-based access control (ReBAC) is particularly well-suited for fine-grained authorization in Retrieval-Augmented Generation (RAG) because it makes decisions based on relationships between objects, offering more precise control compared to traditional models like RBAC and ABAC.\r\n\r\nThis talk covers how ReBAC systems can safeguard sensitive data in RAG pipelines. We'll start with why Authorization is critical for RAG pipelines, and how Google Zanzibar achieves this with ReBAC. We'll then illustrate how pre-filtering vector database queries with a list of authorized object IDs can improve efficiency & security. \r\n\r\nThe talk will also include a demo implementing fine-grained authorization for RAG using Pinecone, Langchain, OpenAI, and SpiceDB.", "code": "GRWYQB", "state": "submitted", "created": "2024-12-20", "speaker_names": "Sohan Maheshwar", "track": "PyData: Generative AI"}, {"title": "Coaching people with AInxiety", "abstract": "Some people are a big fan of AI, they are seeing an avalanche of new and exciting opportunities. Others are afraid of AI and the impact on their jobs and their lives in general. Afraid of their 'imagination' AI will help governments, companies and surveillance to control their lives.\r\n\r\nOne of the biggest foundations of anxiety is the lack of understanding and knowledge about a specific topic or scenario. In this talk we are going to use the Iceberg Methodology to understand some of the most common fears and anxieties about AI. What's below the surface? What's the real fear? What's the real problem?\r\n\r\nHow do we use the Iceberg Methodology to coach people finding out what AI is all about? How do we help them to understand AI and its impact? Can we help them to see the opportunities next to the risks? And how do we use some of their ground fears to adjust the models and algorithms to make them feel more trustworthy for more people?\r\n\r\nThis talk is about mixing AI knowledge and coaching skills.", "full_description": "This talk is about how we handle people that don't trust AI for various reasons. one of the reasons can be a lack of knowkedge.\r\n\r\nHow do we make the basics of AI accessible and understandable for non-IT people? This is needed because AI is coming very close to everyone in this day and age.\r\n\r\nI will teach following topics:\r\n- The Coaching methodology\r\n- Non IT-minded people\r\n- Explaining IT basics to everyone\r\n- ...", "code": "TCZ8EV", "state": "submitted", "created": "2024-12-26", "speaker_names": "Dennie Declercq", "track": "General: Ethics & Privacy"}, {"title": "GluCINDa: Turning messy data from continuous monitoring systems into research data.", "abstract": "Continuous glucose monitoring (CGM) systems provide real-time measurements every 5-15 minutes via a small sensor placed under the skin. Interstitial glucose measured with CGM sensors is a reliable proxy for blood glucose, thus reducing the need for frequent fingerpricks. The rich datasets generated by CGM systems are invaluable for both clinical care and diabetes research. However, CGM exports \u2013 even though available in CSV or XLSX formats \u2013 are often inconsistent, non-machine-readable and lack metadata, making data preparation time-consuming and error-prone. \r\n\r\nTo address these challenges, we developed GluCINDa (Glucose: Coherent Import. Neat Data.), a Python-based tool, that automates CGM data processing by supporting CGM exports in various file and table formats and turning them into harmonized datasets with additional documentation making every step traceable. Moreover, GluCINDa has a user-friendly interface with simple and advanced modes to meet project-specific needs. GluCINDa, therefore, reduces time required for data preparation, prevents (human) errors and dataloss and, consequently, enhances overall scientific quality and rigor in diabetes research.", "full_description": "Continuous glucose monitoring (CGM) is a major player in diabetes care, providing real-time glucose measurements from the interstitial fluid via a small sensor placed under the skin. Measurements happen every 5 to 15 minutes and serve as a reliable proxy for blood glucose, thus reducing the need for frequent fingerprick tests. CGM systems generate rich datasets that are not only useful for individual disease management and clincial care, but invaluable for diabetes research. However, because CGM data are not originally intended for research purposes, exports from CGM devices and online portals often fail to meet scientific standards. Even though the exports are available in CSV or XLSX format, they are rarely machine-readable, have inconsistent structures and lack metadata or documentation. These challenges make importing, harmonizing and preparing data for research time-consuming, error-prone and vulnerable to data loss.\r\n\r\nTo address these challenges, we developed GluCINDa (Glucose: Coherent Import. Neat Data.), a Python-based tool designed to automate CGM data processing. GluCINDa supports various file and table formats, detects glucose measurements with associated date and time information, and produces harmonized datasets suitable for research. She also generates comprehensive documentation files, ensuring every parsed value is traceable, alongside summary reports that provide an overview of the extracted data. To make the software accessible to a wide range of users, GluCINDa features a graphical user interface (GUI) with a simple mode for basic tasks and an advanced mode offering customizable settings.\r\n\r\nGluCINDa significantly outperforms traditional data import methods, delivering faster processing while minimizing (human) error and preventing data loss from messy input files. By adhering to scientific standards of traceability, transparency, and rigorous documentation, GluCINDa enhances the quality and reliability of CGM data for diabetes research.", "code": "LAYJY8", "state": "submitted", "created": "2024-12-22", "speaker_names": "Petra M. Baumann", "track": "PyData: Research Software Engineering"}, {"title": "Transformers for Game Log Data", "abstract": "The Transformer architecture, originally designed for machine translation, has revolutionized deep learning with applications in natural language processing, computer vision, and time series forecasting. Recently, its capabilities have extended to sequence-to-sequence tasks involving log data, such as telemetric event data from computer games.\r\n\r\nThis talk demonstrates how to apply a Transformer-based model to game log data, showcasing its potential for sequence prediction and representation learning. Attendees will gain insights into implementing a simple Transformer in Python, optimizing it through hyperparameter tuning, architectural adjustments, and defining an appropriate vocabulary for game logs.\r\n\r\nReal-world applications, including clustering and user level predictions, will be explored using a dataset of over 175 million events from an MMORPG. The talk will conclude with a discussion of the model's performance, computational requirements, and future opportunities for this approach.", "full_description": "The paper[1] introducing the Transformer architecture has been cited almost 150k times. By now, this deep learning architecture has been used for a large number of use cases. Obviously, language generation and large language models are among the most prominent use cases. However, the architecture has also been successfully employed to solve problems in computer vision and to forecast time series data to name only a few other examples.\r\n\r\nAt its core, the Transformer architecture is a deep neural network designed for sequence-to-sequence prediction tasks. E.g., mapping a sequence of words in one language to a sequence of words in another language as it is done in machine translation tasks. This architecture has recently gained attention for another application well-suited to sequence-to-sequence mapping: the analysis of telemetric log data from games[2]. While log data from games is one specific area that has been explored lately, this approach generally works for log data in other domains arising from websites or mobile apps.\r\n\r\nIn this talk, I will walk the audience through a simple Transformer architecture in Python that can be used to train a model on game log data. I will discuss the challenges of constructing a vocabulary and tokenizer based on log data. Unlike language data, game logs often contain structured events with properties, making vocabulary design non-trivial. I will highlight design choices in the model construction to balance the predictive power of the model and computational efficiency. This includes hyper-parameter selection for the model (e.g., embedding size, number of layers, etc.) and the training procedure (e.g., batch size, learning rate, etc.). I will also explain how to adapt the Transformer architecture to handle long sequences of log data efficiently, including architectural changes to the basic network.\r\n\r\nI will demonstrate how representations derived from the model can be applied to various use cases, such as clustering and prediction tasks arising in game data science. Typical prediction tasks in game data science are survival time prediction for regression or purchase prediction for classification. Insights from clustering or player level predictions can help to improve retention or optimize monetization models. To evaluate the effectiveness of this approach, I trained multiple models on a publicly available 100GB game log dataset containing over 175 million events from NCSOFT\u2019s MMORPG Blade and Soul. In addition to presenting qualitative results, I will compare the computational resources and hardware requirements of this method to those of a simple baseline algorithm. \r\n\r\nBy the end of the talk, attendees will gain actionable insights into building and training Transformers for log data, equipping them to tackle similar challenges in their own domains.\r\n\r\nTentative agenda of the talk:\r\n5  - Intro\r\n5  - Review of the Transformer architecture and its usage in GPT\r\n10 - Adjusting the architecture to game log data\r\n10 - Training of different models\r\n10 - Obtaining player representations from the models for clustering and prediction tasks\r\n5  - Outlook & Conclusion\r\n\r\n[1] \u201cAttention is all you need\u201d, Vaswani et al., 2017\r\n[2] \u201cplayer2vec: A Language Modeling Approach to Understand Player Behavior in Games\u201d, Wang et al., 2024\r\n[3] \u201cGame Data Mining Competition on Churn Prediction and Survival Analysis using Commercial Game Log Data\u201d, Lee et al., 2018", "code": "9NFHAS", "state": "submitted", "created": "2025-01-01", "speaker_names": "Fabian Hadiji", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond Code: Fostering Diversity and Inclusion in Open Source", "abstract": "Open source thrives not just on code, but on the diverse perspectives and inclusive practices that shape our communities. In this dynamic talk, we explore the intersection of open source and diversity, shedding light on how to bridge existing gaps and build welcoming environments for underrepresented groups. Through real-world examples, practical strategies, and inspiring stories from existing researches made on African communities, we'll uncover the immense benefits of diversity and equip you with actionable steps to foster inclusivity. Join us to learn how you can help create a more vibrant and inclusive open-source community that goes beyond code.", "full_description": "Open source is more than just a collaborative coding effort -- it's about creating communities where everyone feels valued and included. This talk delves into the critical intersection of open source and diversity, examining the current landscape and identifying gaps that need to be addressed. We'll explore practical strategies to make open source communities more inclusive and welcoming to underrepresented groups. Attendees will gain insights into the benefits of a diverse community, understand the challenges faced by marginalized contributors, and learn actionable steps to foster inclusivity. By the end of this session, you'll be equipped with the knowledge and tools to advocate for and implement diversity initiatives within your open-source projects. \r\n\r\n**Outline**\r\n**1- Introduction to Diversity in Open Source (5 minutes)**\r\n\r\nTopics: Importance of diversity, current landscape and challenges.\r\nObjectives: Set the context for the importance of diversity in open-source communities, highlight existing gaps and challenges.\r\n\r\n\r\n**2- Benefits of a Diverse Community (5 minutes)**\r\n\r\nTopics: Innovation, community health, and project sustainability.\r\nObjectives: Illustrate the tangible benefits of diversity in open-source projects, inspire attendees to prioritize inclusivity.\r\n\r\n\r\n**3- Challenges Faced by Underrepresented Groups (5 minutes)**\r\n\r\nTopics: Barriers to entry, experiences of marginalized contributors.\r\nObjectives: Provide a deeper understanding of the obstacles faced by underrepresented groups, foster empathy and awareness.\r\n\r\n\r\n**4- Practical Strategies for Inclusion (5 minutes)**\r\n\r\nTopics: Mentorship programs, inclusive language, community-building activities.\r\nObjectives: Offer actionable steps for creating inclusive environments, provide practical examples and best practices.\r\n\r\n\r\n**5- Case Studies and Success Stories (5 minutes)**\r\n\r\nTopics: Real-world examples of successful diversity initiatives.\r\nObjectives: Share inspiring stories of diverse open-source communities, demonstrate the positive impact of inclusion efforts.\r\n\r\n**6- Q&A (5 minutes)**", "code": "LRL7MS", "state": "submitted", "created": "2024-12-21", "speaker_names": "Ariane Djeupang", "track": "General: Community & Diversity"}, {"title": "Who does Python trust, and why?", "abstract": "We all know how important it is to make sure that you see the \"secure\" padlock icon when paying for something online. It's simple, easy to understand, and automatic. \r\n\r\nIs your code the same? How would you start to make sure it is? Let\u2019s dive into those questions in this talk.", "full_description": "First off, the basics: security is based on trust, but trust must be verified. This is especially true in the design of 'zero-trust' networks. Knowing who your code trusts, and how you verify that trust, is therefore essential. \r\n\r\nThis talk will briefly go through some of the basics, such public/private key cryptography, DNS, and x509 certificate signing, before examining some of the wider implementation and regulation of PKI (Public Key Infrastructure) online. We'll look at how python interfaces with this, along with some history, and assess the future of important integrations like certifi and truststore, before touching on the important issues of DNS security, as well as supply chain attacks.\r\n\r\nHopefully, this talk will give you a headstart or a refresh on the knowledge required to have genuinely constructive paranoia when coding. \r\n\r\nFailing that, at the very least, I promise to make you the go-to expert on how your company's proxy works, and ensure you never again type \"pip config set global.verify_ssl false\".", "code": "UCSDWD", "state": "submitted", "created": "2024-12-21", "speaker_names": "Dom Weldon", "track": "PyCon: Security"}, {"title": "DIY as THE way towards understanding", "abstract": "I will try to convince you, that correct answer to the \"there's a library for this\" is \"so what?!\". Understanding through building is the best way to understand first principles. I will show this on my experience of writing my own GPT.", "full_description": "\"Build or buy\" is a question asked regularly in software engineering world. I will show you some examples, when you can live without libraries and tools. Just to build up to the main point, building something from the ground up is an excellent way how to learn - for example how to learn about Large Language Models. I will show you how I coded my way towards small gen AI language model. What it brings and why you should too.", "code": "7QRMML", "state": "submitted", "created": "2024-12-22", "speaker_names": "Martin M\u00fcnch", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Impostor syndrome in Tech", "abstract": "Do you ever get that awful feeling that you're not good enough to do your job? That you lack skills, intelligence and talent? That everyone around you knows what they're doing, and you just slip through by luck, chance, and appearing to be better than you really are? Do you live in fear that it's only a matter of time before someone discovers you're just a fraud? \r\n\r\nYou are not alone. \r\n\r\nAccording to research, Impostor Syndrome affects almost 90% of people working in IT. In this presentation, I will unravel this phenomenon, explore why it is so dominant in our industry, how one gets into a vicious circle and where it can lead to. I will discuss how the awareness of simple mechanisms will help you see your competencies and what tricks to use to defy the demons of insecurity. \r\n\r\n*May contain traces of rubber ducks.", "full_description": "It\u2019s a very important topic. A tremendous amount of people struggle with impostor syndrome, waste their potential and degrade their mental health.\r\nThis talk was presented at several conferences, both online and in person with up to 2k audience, there were always a lot of questions and positive feedback.\r\nThe material is useful even for people familiar with impostor syndrome, as I\u2019m presenting over 20 particular tips and associated psychological mechanisms. Familiarity with those might be an eye-opener as well.", "code": "JPDFA3", "state": "submitted", "created": "2024-12-16", "speaker_names": "Pawe\u0142 Zaj\u0105czkowski", "track": "General: Education, Career & Life"}, {"title": "Inclusive Data for 1.3 Billion: Designing Accessible Visualizations", "abstract": "According to the World Health Organization (WHO), an estimated 1.3 billion people (1 in 6 individuals) experience a disability, and nearly 2.2 billion people (1 in 5 individuals) have vision impairment. Improving the accessibility of visualizations will enable more people to participate in and engage with our data analyses.\r\n\r\nIn this talk, we\u2019ll discuss some principles and best practices for creating more accessible data visualizations. It will include tips for individuals who create visualizations, as well as guidelines for the developers of visualization software to help ensure your tools can help downstream designers and developers create more accessible visualizations.", "full_description": "Specifically, we will cover:\r\n\r\n- What makes data visualizations inaccessible? We will cover accessibility fundamentals like color contrast, alternative text descriptions, keyboard navigation support, screen reader compatibility, and more, with specific examples and demonstrations.\r\n- Are Python data visualization tools accessible? We will teach how to analyze the visualization landscape and discuss how tool developers can begin and prioritize improvements. \r\n- How accessible is my visualization? We will demonstrate how to conduct accessibility audits for data visualization tools by performing and documenting two accessibility evaluation tests live.\r\n\r\nThis talk will include specific examples from our ongoing work to improve the accessibility of Bokeh, a Python library for creating interactive data visualizations for web browsers. We hope this talk enables you to take the first few steps in making your next data visualization and your visualization tools, more accessible.", "code": "LNW3KE", "state": "accepted", "created": "2024-12-22", "speaker_names": "Dr. Tania Allard, Pavithra Eswaramoorthy", "track": "PyData: Visualisation & Jupyter"}, {"title": "Accuracy Is Not Enough: Building Trustworthy AI with Conformal Prediction", "abstract": "Building a good scoring model is just the beginning. In the age of critical AI applications, understanding and quantifying uncertainty is as crucial as achieving high accuracy. This talk highlights conformal prediction as the definitive approach to both uncertainty quantification and probability calibration, two extremely important topics in Deep Learning and Machine Learning. We\u2019ll explore its theoretical underpinnings, practical implementations using TorchCP, and transformative impact on safety-critical fields like healthcare, robotics, and NLP. Whether you're building predictive systems or deploying AI in high-stakes environments, this session will provide actionable insights to level up your modelling skills for robust decision-making.", "full_description": "When deploying machine learning models in the real world, especially in domains like healthcare, robotics, or natural language processing, the stakes are high. It\u2019s not enough to train a model, evaluate its accuracy, and call it a day. Questions of how confident the model is, how reliable its predictions are, and how to act on these predictions are critical yet often overlooked. This talk takes you beyond conventional metrics and into the world of uncertainty quantification and probability calibration, with conformal prediction as the definitive tool for both.\r\n\r\n\r\n\r\nWe\u2019ll start of the presentation by exploring the fundamental need for uncertainty in AI systems\u2014why it matters, how it\u2019s quantified, and how it can be used to make informed decisions. From there, we\u2019ll introduce conformal prediction, a mathematically rigorous yet practical framework that provides guarantees on prediction reliability while remaining model-agnostic. Core concepts such as probability calibration and uncertainty quantification will be highlighted as key parts in the modelling process, establishing their importance in the domain.\r\n\r\n\r\n\r\nThe session will also feature real-world examples and use cases such as:\r\n\r\n- Healthcare: Predict disease likelihood with quantifiable confidence, such as identifying risks in MRI scans.\r\n\r\n- Robotics: Navigate dynamic environments safely using calibrated vision-based models.\r\n\r\n- Natural Language Processing: Improve outputs of large language models with uncertainty-aware predictions.\r\n\r\n\r\n\r\nFinally, we\u2019ll showcase the TorchCP toolbox, a GPU-accelerated library for integrating conformal prediction into deep learning pipelines, an area of Data Science that has a lot of hype but often overlooks the importance of such tools. Through a live demonstration, you\u2019ll see how to implement these methods step-by-step, empowering you to build trustworthy AI systems that go beyond accuracy.\r\n\r\n\r\n\r\nAttendees will leave with:\r\n\r\n- A solid understanding of uncertainty quantification, probability calibration and their importance.\r\n\r\n- Practical knowledge of conformal prediction and how to implement it.\r\n\r\n- A new perspective on AI reliability and decision-making in critical domains.\r\n\r\n\r\n\r\nWhether you're an ML researcher, data scientist, or practitioner deploying AI models in critical environments, this session will equip you with the right tools and philosophy to create AI systems that are not only accurate but also reliable and robust.", "code": "UDDTBS", "state": "submitted", "created": "2025-01-04", "speaker_names": "Chris Aivazidis", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Write your own code coverage tool in Python", "abstract": "Coverage tools are essential for ensuring that your tests exercise significant parts of your Python application. But what exactly is code coverage, how do coverage tools work behind the scenes, and how can you build one yourself? This is what this talk is all about.", "full_description": "Coverage tools are great, but how do they work? Python 3.12 introduced a new low-impact monitoring API through PEP 669, enabling the development of significantly faster tools, including debuggers and coverage analyzers. This talk explores how to leverage the new API to implement efficient coverage tools, what insights they provide, and how they can enhance your development workflow, allowing you a peak behind the curtain.", "code": "PNRGT7", "state": "submitted", "created": "2024-12-02", "speaker_names": "Johannes Bechberger", "track": "PyCon: Python Language & Ecosystem"}, {"title": "From Tensors to Clouds \u2014 A Practical Guide to Zarr V3 and Zarr-Python 3", "abstract": "A key feature of the Python data ecosystem is the reliance on simple but efficient primitives that follow well-defined interfaces to make tools work seamlessly together (Cf. http://data-apis.org/). NumPy provides an in-memory representation for tensors. Dask provides parallelisation of tensor access. Xarray provides metadata linking tensor dimensions. **Zarr** provides a missing feature, namely the scalable, persistent storage for annotated hierarchies of tensors. Defined through a community process, the Zarr specification enables the storage of large out-of-memory datasets locally and in the cloud. Implementations exist in C++, C, Java, Javascript, Julia, and Python, enabling.\r\n\r\nThis talk presents a systematic approach to understanding and implementing the newer version of [Zarr-Python](https://github.com/zarr-developers/zarr-python), i.e. Zarr-Python 3 by explaining the new API, deprecations, new storage backend, improved codec pipeline, etc.\r\n\r\nI will also show the performance improvements in ZP-3 while creating, reading and writing async Zarr arrays across local and remote storage like AWS S3.", "full_description": "Zarr is a data format for storing chunked, compressed N-dimensional arrays and is sponsored by [NumFOCUS]((https://numfocus.org/project/zarr)) under their umbrella.\r\n\r\nIt is based on open-source technical specification and has implementations in several languages, with [Zarr-Python](https://github.com/zarr-developers/zarr-python) being the most used.\r\n\r\nAfter the successful adoption of Specification V3, our team has worked tirelessly over the last year to ensure the Python library's compliance with the latest spec.\r\n\r\n## Outline\r\n\r\nFirst, I\u2019d be talking about:\r\n\r\n### Understanding Zarr basics (5 mins.)\r\n\r\n- What is Zarr, and how it works?\r\n    - The inner workings of Zarr using illustrated graphics\r\n- What is the Zarr Specification?\r\n    - What's new in Zarr Spec V3?\r\n\r\nThen, I'll be talking about the new Zarr-Python 3 and its significant features:\r\n\r\n### What's new in Zarr-Python 3? (15 mins.)\r\n\r\n- Major design updates\r\n    - New storage backend\r\n    - Creating Zarr arrays and groups asynchronously\r\n    - New and improved codec pipeline\r\n    - Native GPU support for creating and writing arrays\r\n- Changes and deprecations\r\n    - Overview of the new API\r\n    - Optimising performance for large arrays\r\n    - Deprecation of several stores like LMDBStore, SQLStore, MongoDBStore, etc.\r\n- 3.0 Migration guide\r\n    - Steps to migrate from Zarr-Python 2 to Zarr-Python 3\r\n- Extensions\r\n    - How can Zarr-Python 3 be extended to add new custom data types, stores, chunking strategies, etc.?\r\n\r\nThen, I\u2019d be doing a hands-on session, which would cover the following:\r\n\r\n### Hands-on (5 mins.)\r\n\r\n- Creating Zarr arrays and groups using Zarr-Python 3\r\n    - Plus walkthrough of the new features (mentioned above)\r\n- Writing and reading from Cloud object storage\r\n    - Using S3/GCS/Azure to create Zarr arrays and write data to it\r\n- Looking under the hood\r\n    - Use store and info functions to explain how your Zarr data is stored and display important information\r\n\r\n### Conclusion (5 mins.)\r\n \r\n- Key takeaways\r\n- How can you get involved?\r\n- QnA\r\n\r\nThis talk aims to address an audience that works with large amounts of data and is looking for a transparent, open-source, reliable, cloud-optimised, and environmentally friendly format.\r\n\r\nThe tone of the talk is set to be informative, story-telling and fun.\r\n\r\nIntermediate knowledge of Python and NumPy arrays is required for the attendees to attend this talk.\r\n\r\n### After this talk, you\u2019d:\r\n\r\n- understand the basics of Zarr and what's new in V3,\r\n- leverage the new functionalities of Zarr-Python 3 with improved performance,\r\n- make an informed decision on what data format to use for your data", "code": "ABWHSD", "state": "submitted", "created": "2024-12-22", "speaker_names": "Sanket Verma", "track": "PyData: Data Handling & Engineering"}, {"title": "How Narwhals is silently bringing pandas, Polars, DuckDB, PyArrow, and more together", "abstract": "If you were writing a data science tool in 2015, you'd have ensured it supported pandas and then called it a day.\r\n\r\nBut it's not 2015 anymore, we've fast-forwarded to 2025. If you write a tool which only supports pandas, users will demand support for Polars, PyArrow, DuckDB, and so many other libraries that you'll feel like giving up.\r\n\r\nLearn about how Narwhals allows you to write dataframe-agnostic tools which can support all of the above, with zero dependencies, low overhead, static typing, and strong backwards-compatibility promises!", "full_description": "Suppose you want to write a data science tool to do feature engineering. Your experience may go like this:\r\n- Expectation: you can focus on state-of-the art techniques for feature engineering.\r\n- Reality: you keep having to make you codebase more complex because a new dataframe library has come out and users are demanding support for it.\r\n\r\nOr rather, it might have gone like that in the pre-Narwhals era. Because now, you can focus on solving the problems which your tool set out to do, and let Narwhals handle the subtle differences between different kinds of dataframe inputs!\r\n\r\nNarwhals is a lightweight and extensible compatibility layer between dataframe libraries. It is already used by several open source libraries including Altair, Marimo, Plotly, Scikit-lego, Vegafusion, and more. You will learn how to use Narwhals to build dataframe-agnostic tools.\r\n\r\nThis is a technical talk aimed at tool-builders. You'll be expected to be familiar with Python and dataframes. We will cover:\r\n- 2-3 minutes: motivation. Why are there so many dataframe libraries?\r\n- 2-3: minutes: life before vs after Narwhals - real-world examples of how the data landscape is changing\r\n- 7-8 minutes: basics of Narwhals, wrapping native objects, expressions vs Series, lazy vs eager\r\n- 7-8 minutes: advanced Narwhals concepts: row order, non-elementary group-by aggregations, multi-indices, null values, backwards-compatibility promises\r\n- 2-3 minutes: what comes next?\r\n- 5 minutes: engaging Q&A / awkward silence\r\n\r\nTool builders will benefit from the talk by learning how to build tools for modern dataframe libraries without sacrificing support for foundational classic libraries such as pandas.", "code": "CPCNRZ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Marco Gorelli", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "What's expression expansion in Polars?", "abstract": "[Polars](https://github.com/pola-rs/polars) uses [expressions](https://docs.pola.rs/user-guide/concepts/expressions-and-contexts/#expressions) to provide the user with a flexible and modular API to create queries.\r\n\r\nPolars expressions are lazy representations of data manipulations and it's only within a Polars context that these expressions materialise into concrete results.\r\nIn turn, [expression expansion](https://docs.pola.rs/user-guide/concepts/expressions-and-contexts/#expression-expansion) is an underused Polars feature that lets you write a single expression and apply it to multiple columns, making it a very convenient feature that is loved by all Polars users.", "full_description": "This talk will start by explaining what it really means for expressions to be lazy representations of data manipulations and within that context we give a motivating example that explains why we need expression expansion.\r\n\r\nThen, we explore all of the ways in which expression expansion can be used, namely in column selection and within structs, and we explore the different patterns and types of selections that can be used within expression expansion.\r\n\r\nFinally, we conclude with some miscellaneous tips on how to make the best use of expression expansion and show a real world example of a data query that was greatly simplified and sped up by the introduction of expression expansion.", "code": "LNGW88", "state": "submitted", "created": "2024-12-24", "speaker_names": "Rodrigo Gir\u00e3o Serr\u00e3o", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Job-Me - An AI System for Analyzing Resumes", "abstract": "Job-Me leverages cutting-edge AI to revolutionize the hiring process, significantly reducing time and costs for firms. By combining advanced text mining and generative AI techniques, the system parses resumes, extracts and standardizes skills, and evaluates their relevance to job profiles. Leveraging large language models (LLMs) and the innovative spike model, Job-Me enhances the precision of matching job seekers with employers, ensuring an efficient and seamless recruitment experience.", "full_description": "In this presentation, I will introduce an innovative data pipeline built with PySpark and powered by large language models (LLMs) to transform the way skills are classified from resumes. By combining advanced NLP and machine learning techniques, the system tackles a multi-class, multi-topic classification challenge, aligning extracted skills with a standardized classification framework. Additionally, the AI-driven algorithm evaluates the significance of each skill for specific profiles, enhancing precision and relevance. A working prototype is already accessible at www.job-me.it, showcasing the potential of this cutting-edge approach.", "code": "DCJYZW", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Optimising your Database for Analytics", "abstract": "Data analytics isn\u2019t always done in a dedicated analytics database. The business wants to glean insights and value from the data that\u2019s generated over time by its OLTP applications, and the simplest way to do that is often just to run analytics queries directly on the application database.\r\n\r\nThe database and code are probably optimised for performance of the day-to-day application activity. Someone may have spent many hours, weeks or months getting things to that state. But now, you\u2019re being asked to run complex analytics queries against this carefully tuned database!\r\n\r\nFortunately, there are some tools and techniques that you can use to optimise your database for an analytics workload without compromising the performance of your existing application.", "full_description": "Data analytics isn\u2019t always done in a dedicated analytics database. The business wants to glean insights and value from the data that\u2019s generated over time by its OLTP applications, and the simplest way to do that is often just to run analytics queries directly on the application database.  But the database and code are probably optimised for performance of the day-to-day application activity, and performance is likely to take a hit if you suddenly add complex analytics queries to the mix.\r\n\r\nIn this talk, we'll discuss the challenges associated with running data analytics on an existing OLTP database. We'll look at some of the impacts this type of workload could have on the application, and why it could cause the analytics queries themselves to perform poorly.\r\n\r\nWe'll then look at some of the strategies, tools and techniques that you can use to optimise your database for an analytics workload without compromising the performance of your application. This will include architecture choices, configuration parameters, materialized views and more.", "code": "ZA3DLL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Karen Jex", "track": "PyData: Data Handling & Engineering"}, {"title": "Accelerating privacy-enhancing data processing", "abstract": "Our mission is simple but profound: to improve and extend lives by learning from the experience of every person with cancer. Achieving this requires seamless feedback loops between scientists, engineers, and clinicians working with sensitive data across heterogeneous environments.\r\n\r\nYou might expect a story about how we tried and failed. And yes, we\u2019ll share some failures and surprises along the way. But this is, at its core, a success story - because it works.\r\n\r\nIn this talk, we\u2019ll dive into the data architecture and core technologies that enable us to learn from every patient\u2019s journey with cancer. We\u2019ll reveal how our cross-functional teams - scientists, engineers, and clinicians - collaborate to transform raw data into research-grade datasets. Along the way, we\u2019ll share the challenges we faced, the lessons we learned, and the tools we developed to ensure the high data quality required for groundbreaking research.\r\n\r\nWe\u2019ll also show how we leverage domain knowledge, data science, and established technologies to create tools that maintain this quality and accelerate feedback loops. By shifting insights generation left, we empower teams to iterate faster and drive more impactful outcomes.\r\n\r\nThis talk is ideal for anyone with data warehouse or lakehouse experience curious about harnessing the Pythonic data stack to iteratively generate insights from sensitive data in highly regulated environments.", "full_description": "In this talk, we\u2019ll begin by introducing the concept of real-world evidence datasets and their transformative impact on cancer research. We\u2019ll explore the significant challenges of building high-quality real-world evidence datasets, including the fragmented healthcare data landscape, the complexity of source data, and stringent regulatory constraints.\r\n\r\nTo address these challenges, we\u2019ll introduce our privacy-enhancing data architecture and foundational technology stack, which includes AWS, Snowflake, Python, DLT, DBT, Pandas, and DuckDB. We\u2019ll explain how we leverage these tools to establish critical feedback loops and accelerate actionable insights. Key topics include:\r\n\r\n- Integrating notebooks with production pipelines to bridge the gap between clinical domain expertise and production workflows.\r\n- Employing a write-audit-publish pattern to ensure continuous data quality through unit tests, data tests, regression tests, and cohort-spanning statistical tests.\r\n- Using DLT to move normalization steps closer to the data source, unlocking iterative data modeling and enabling more agile development cycles.\r\n\r\nOne of our most impactful innovations was shifting data investigations to the left. Previously, raw data inspections were delayed until pre-processed data was available in our data warehouse, causing prolonged feedback loops and inefficiencies. Disorganized file formats often required data scientists to manually inspect data, limiting their ability to draw meaningful, cohort-wide conclusions.\r\n\r\nTo overcome these challenges, we established local databases directly on compute instances where raw data is stored, leveraging the flexibility and transparency of this approach. Transformations developed within this setup can seamlessly transfer to the cloud data warehouse while remaining portable and adaptable to various environments. This enabled our data scientists to explore and understand the data earlier in the process, eliminating bottlenecks. With immediate access to organized datasets, our team could quickly assess incoming data, provide actionable feedback, and begin building analytical models.\r\n\r\nThis proactive, iterative approach not only streamlined workflows but also fostered deeper collaboration between data engineers and data scientists, ultimately accelerating the generation of high-quality insights for cancer research.", "code": "LDNYWY", "state": "submitted", "created": "2024-12-20", "speaker_names": "Florian Stefan, Anna Schwarz", "track": "PyData: Data Handling & Engineering"}, {"title": "Bayesian Neural Networks for Beginners", "abstract": "Bayesian Neural Networks (BNNs) enhance traditional neural networks by providing uncertainty estimates in predictions, making them ideal for applications where confidence quantification is crucial. This talk offers a conceptual overview of BNNs, with minimal mathematical detail, and concludes with a practical demo using a simple dataset. This talk is beginner friendly.", "full_description": "- Why Bayesian Neural Networks?\r\nBayesian Neural Networks (BNNs) has a unique advantage over traditional neural networks: they provide uncertainty estimates in their predictions. While other techniques can approximate uncertainty, the Bayesian framework naturally incorporates it, providing confidence interval in model outputs. This is important for applications where understanding model uncertainty is just as important as making accurate predictions.\r\n\r\n- Conceptual Overview\r\nIn the Bayesian paradigm, the neural network's parameters (weights and biases) are treated as random variables, rather than fixed values. To generate predictive distributions, BNNs assume these parameters follow some probability distributions. Instead of sampling from the posterior distribution (which is computationally expensive), variational inference is used for scalability. Here, an approximate distribution (usually simpler) is optimized by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximation. At inference time, the BNN behaves as an ensemble of models, with weights and biases sampled from the learned distributions. This enables the generation of multiple outputs for the same input, from which confidence intervals can be computed.\r\n\r\n- How It Works - An Example\r\nWe will walk through a short demo where a BNN is applied to a dataset.", "code": "JYQANY", "state": "submitted", "created": "2024-12-18", "speaker_names": "Wenting Jiang", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Microtonal music theory with xenharmlib", "abstract": "The talk gives a quick introduction to xenharmlib, a GPL-licenced generalized music theory library that supports non-Western harmony traditions and novel microtonal tunings. The library is meant as a tool belt for programmer-composers to find harmonizations, chord progressions, improvisation scales and more in unfamiliar terrain. As an illustration the talk will feature strange and other-wordly sounds.", "full_description": "The Western music tradition is ubiquitous. Nearly all musical production of the last and current century is build on the assumption that melodies and chords are chosen from a selection of 12 notes per octave. This is reflected in music theory libraries (like music21 and mingus), most of which have built this assumption deep into their design. However the Western system is only one among many: Modern Arabic Music selects from a set of 24 notes, Turkish music even from 53. Other experimental systems have been proposed and aroused recent interest from composers of various genres.\r\n\r\nTo address this problem, xenharmlib was created, a python library for computational music theory that allows the programmer-composer to freely choose the system of tonality underlying chords, scales and melodies. The talk will make you familiar with the basic concepts of alternative tonality and how to model useful structures with xenharmlib. Examples of experimental tonality will be played accompanying the code snippets.", "code": "P89QHD", "state": "submitted", "created": "2024-12-19", "speaker_names": "Fabian Vallon", "track": "General: Others"}, {"title": "Predicting skill shortages using online job ads for European employer insights.", "abstract": "A journey of predicting skills shortages with real-time data. The study integrates online job ads with traditional datasets, offering a nuanced understanding of skill demands. We will present the results of the model tuning and applications, and the results on Italian and UK data.", "full_description": "Explore the predictive power of online job ads in anticipating skill shortages, a critical aspect for effective workforce planning. Leveraging machine learning and multi-dimensional datasets, this study combines national statistical sources with job postings data to enhance the accuracy and granularity of predictions. Delve into the methodology, from quantifying skill importance to forecasting shortages and discover actionable insights to guide policies and strategies for a more aligned and efficient labor market.", "code": "YDFV9W", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "The aesthetics of AI: from cyberpunk to fascism", "abstract": "Let\u2019s explore the visual grammars, references and cultural norms at play in the field of AI; from Kismet to Spot\u00ae, from Clippy to Claude. As a sector we can be hyper-focused on technical process and function, to the extent that it blinkers our understanding of the cultural and political impacts of our work. Aesthetics infuse every aspect of technology. Aesthetic interpretations are manifold and mutable, constructed in-congress with the observer and not fully defined by the original designer. AI technologies add additional layers of subtext: character, consciousness, agency, intent.\r\n\r\nDespite this murkiness, or perhaps because of it, this talk makes an passionate argument for engaging with historical aesthetic movements, for building our shared professional knowledge of fads and fashions\u23afnot just from the past 40 years of internet culture\u23afbut also the past 140 years of ideology, technology, and thought.", "full_description": "**Talk Outline** \r\n- Define aesthetics\r\n- Differentiate aesthetics from visual design\r\n- Aesthetics of contemporary AI\r\n- History of aesthetics in technology\r\n- Link current technologies to historical aesthetic movements\r\n\r\n**Detailed description**\r\nThe field of artificial intelligence has long been dominated by discussions of technical capabilities, algorithmic improvements, and functional benchmarks. Beneath this technical layer lies a rich but often unexamined tapestry of visual and cultural decisions that profoundly shape how we perceive, interact with, and ultimately integrate AI systems into our society. \r\n\r\nThis talk moves beyond simple visual design to explore how aesthetics \u2013 study of the principles of beauty and artistic taste \u2013 shapes both the creation and interpretation of AI technologies. The aesthetic choices woven into today\u2019s AI interfaces, both for end-users and industry practitioners, reveal our deep-seated assumptions about the world. \r\n\r\nPhilosophers of art ask us to introspect: What is goodness? What is beauty? Which endeavours are most worthy of our attention? We can use these same questions to explore the ideas framing AI.\r\n\r\n\u201cAll watched over by machines of loving grace\u201d, a poem by Richard Brautigan, imagines a utopian future where the natural and technological worlds achieve balance and harmony, and where humans are free to pursue creative, embodied pursuits, freed of menial labour. Is this the utopia imagined by OpenAI or DeepMind when they describe the imminent arrival of AGI? Does the world as described by the big brands of AI actually align with our own imaginings of progress, of utopia? \r\n\r\nA brief historical overview will trace how technological aesthetics have evolved, examining how different eras have visualized and presented technological innovations. This context sets the stage for drawing direct connections between current AI aesthetics and historical movements \u2013 revealing how contemporary design choices often unconsciously echo past ideological and artistic approaches.\r\n\r\nThrough these connections, I\u2019ll demonstrate why developing a broader aesthetic literacy is crucial for AI practitioners. Understanding these historical and cultural reference points can lead to more thoughtful and effective uses of AI. As our field continues to shape the future of human-machine interaction, this aesthetic awareness becomes not just an academic exercise, but a practical necessity: providing both the groundwork for nuanced critique, and the capacity to clearly define how we expect technologies to fit into and improve our lives.", "code": "933YXH", "state": "submitted", "created": "2024-12-22", "speaker_names": "Laura Summers", "track": "General: Others"}, {"title": "Everything, all the time, right now! An introduction to stream processing", "abstract": "Processing large volumes of data at low latency is key to enabling real-time and predictive products, but with millions of data points and complex processing requirements, traditional batched data pipelines quickly develop into hour-long dredges and when results are needed in a timely fashion using more resources and running these pipelines at a higher frequency is a pathway to exploding cost and diminishing returns.\r\nWhere traditional batch processing approaches just don't cut it anymore, stream processing is the answer. In this talk you'll get an overview of what stream processing is, when to use it, important techniques and frameworks, Python code examples, as well as some tips, caveats and best practices gathered from hard-earned real world experience.\r\nBy the end you'll be equipped with all the knowledge you need to start writing streaming pipelines of your own, enabling you to deliver data and insights at sub-second latency.", "full_description": "The goal of this talk is giving you all the expertise you need to start writing streaming data pipelines and programs, both in terms of concepts and practical coding knowledge.\r\n\r\nWe will start with a quick introduction on what stream processing is, when it is useful, when it is not, and how it compares against traditional databases and batch processing paradigms.\r\n\r\nUsing code examples we will learn how to write some basic and then more complex streaming pipelines, introducing the most important stream processing techniques and concepts such as \"stateful processing\", \"event time\" and \"out-of-orderness\".\r\n\r\nNext we will explore the limitations and complications of streaming pipelines when compared to more traditional processing models and how we can work around them, like dealing with incomplete data and non-replayable datasources\r\n\r\nWe will do a brief excursion into the niche (but immensely useful) topic of _distributed_ stream processing before getting to the hard-learned experience from several years of developing and operating business critical streaming pipelines at the worlds largest aviation company. This covers topics like debugging pipelines, making them reliable and how to reduce maintenance effort with fancy greek-letter architectures which make you sound smart at tech conferences.\r\n\r\nFinally you will get an ecosystem overview with the most important stream processing frameworks (Python and others) as well as related technologies so that by the end of the talk you are equipped to architect and develop streaming pipelines, a surprisingly uncommon yet incredibly useful skill.", "code": "KJKFNB", "state": "submitted", "created": "2024-12-22", "speaker_names": "Damion Werner", "track": "PyData: Data Handling & Engineering"}, {"title": "Towards Intelligent Monitoring: Detecting Degraded Flame Torch Nozzles", "abstract": "Flame cutting is a method where metals are efficiently cut using precise control of the oxygen jet and consistent mixing of fuel gas. The condition of the nozzle is changing over time: deposits formed during the cutting process can degrade the flame quality, reducing the precision of the cut. Traditionally, nozzles suspected of wear are sent back for manual inspection, where experts evaluated the flame visually and audibly to determine whether repair or replacement is needed. This project leverages machine learning to optimize this process by analyzing acoustic emission data.", "full_description": "Flame cutting is a method where metals are efficiently cut using precise control of the oxygen jet and consistent mixing of fuel gas. The condition of the nozzle is changing over time: deposits formed during the cutting process degrade the flame quality, reducing the precision of the cut. Currently, nozzle testing relies on manual inspections, where experts visually and audibly evaluate the flame. This method is risky, as worn nozzles may remain in operation, posing safety hazards through ejection of high-temperature material. Additionally, the process is costly, especially when industrial equipment is damaged.\r\n\r\nLaboratory Evaluation: This section outlines the preliminary experiments aimed at assessing whether this sensor is suitable for distinguishing different machine states. The experiments focus on identifying the optimal sensor placement and analyzing how various machine states impact sensor readings. The design process for the laboratory experiments and the subsequent systematic data collection is shown. The results suggest that while detecting every machine state may not be feasible, the sensor shows promise in identifying degraded nozzles.\r\n\r\nData Preprocessing & Annotation: For a proof of concept, the raw acoustic emission data required manual labeling, as prior assessments depended on expert evaluations. Here, we utilized Label Studio, an annotation tool that streamlines the labeling process. \r\nModelling  & Feature Engineering: We extract features using statistical methods and transform the acoustic emission signals into the frequency domain through scipy, focusing on features in  the frequency domain.\r\n\r\nEvaluation: We discuss the approach for splitting the data, considering that multiple observations from the same nozzle are present. In a computational study, we evaluate the feature sets developed in the previous step using two different classification models: Support Vector Classifier and Multilayer Perceptron. This section explains how the experiments are computed and parallelized including the time required for execution.\r\n\r\nLastly, we discuss the dataset's limitations and the challenges faced during development. We also highlight steps taken to improve generalization and provide an outlook on future objectives, mostly aimed at a broader applicability of the models.", "code": "89BX8V", "state": "submitted", "created": "2024-12-21", "speaker_names": "Dominik Falkner", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "FastAPI Meets Langchain: Crafting a RAG Solution", "abstract": "RAG (Retrieval-Augmented Generation) is an emerging paradigm in AI that combines the strengths of retrieval systems and generative models to deliver context-aware, highly relevant responses. \r\nBy leveraging FastAPI\u2019s robust and Langchain\u2019s modular tools for interacting with language models and knowledge sources, you'll learn how to create an AI assistant application for a shop.\r\n\r\nThe tutorial covers key concepts, including integrating a vector database for document retrieval, configuring FastAPI endpoints for seamless interaction, and using Langchain to enhance the capabilities of large language models with retrieved context. Whether you're a developer aiming to expand your AI toolkit or a data enthusiast exploring RAG, this step-by-step guide will provide you with the knowledge and skills to design and deploy intelligent, context-driven applications.", "full_description": "In this tutorial, you will learn how to build a *Retrieval-Augmented Generation* (RAG) application using *FastAPI* and *Langchain*. \r\nRAG combines two powerful components of modern AI systems: retrieval mechanisms to fetch relevant information and generative models to produce contextually accurate outputs. This hybrid approach enables applications to provide precise, knowledge-driven responses by grounding generative AI in reliable data sources.\r\n\r\nKey concepts covered include:\r\n\r\n- Data Storage and Retrieval: Learn how to ingest data into a vector database.\r\n- Langchain Pipelines: Use *Langchain* to link your language model to external knowledge bases, enabling it to retrieve context and generate grounded responses. \r\n- FastAPI Integration: Build RESTful endpoints to facilitate interaction between the client and the backend\r\n\r\nBy the end of this tutorial, you\u2019ll have a fully functional RAG application that can serve as a foundation for solving real-world challenges. Whether you're building an AI-powered assistant, improving customer support systems, or enhancing research tools, this guide provides practical, hands-on knowledge for turning ideas into reality with FastAPI and Langchain.", "code": "PQ3M3C", "state": "submitted", "created": "2025-01-03", "speaker_names": "Giunio", "track": "PyCon: Programming & Software Engineering"}, {"title": "Why Don\u2019t Customers Want My Free Goods? \u2013 Why Forecasting Models Don\u2019t Answer 'What If' Questions", "abstract": "Forecasting and causal inference are distinct but fundamental tasks in data science. While forecasting predicts future outcomes based on history, causal inference explores the \"why\" behind those outcomes and helps simulate \"what if\" scenarios. Confusing the two can lead to misleading results.\r\n\r\nAt Blue Yonder, we encountered a case where a customer's forecasting model predicted demand accurately based on price. However, when they used the model for simulations to explore \"what if\" scenarios, the results were counterintuitive: lower prices led to lower demand. I will share how we resolved this issue and emphasize the importance of incorporating causal thinking when addressing questions like, \"Why did this happen?\" or \"What if I do X?\"\r\n\r\nIn this talk, I\u2019ll show how to identify common pitfalls, like confounders, when integrating causal inference into forecasting workflows. We\u2019ll also explore Bayesian models, powered by Markov Chain Monte Carlo (MCMC) methods, to bridge the gap between forecasting and causality using the PyMC library on practical examples.\r\n\r\nBy the end of this talk, you\u2019ll learn how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses with Directed Acyclic Graphs (DAGs) to understand relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.", "full_description": "Forecasting and causal inference are fundamental yet distinct tasks in data science. While forecasting predicts future outcomes based on historical patterns, causal inference seeks to understand the \"why\" behind these outcomes and explore \"what if\" scenarios to simulate the impact of interventions. These tasks often overlap but have different requirements\u2014and confusing them can lead to unexpected results.\r\n\r\nAt Blue Yonder, we observed an intriguing case where a customer's forecasting model used price information to predict demand accurately. However, when they used the model for causal simulations\u2014adjusting prices to explore \"what if\" scenarios\u2014the predictions behaved counterintuitively: lower prices led to lower predicted demand! I would like to share the details of how we helped the customer resolve this issue in this real-world example and highlight the importance of taking a causal perspective when tackling questions like, \"Why did this happen?\", \"What if I do X?\", or \"What if I had done Y instead?\"\r\n\r\nIn this talk, I will demonstrate how to identify and avoid common pitfalls, such as confounders, when incorporating causal inference into traditional forecasting workflows. I\u2019ll show how we can use Bayesian models powered by Markov Chain Monte Carlo (MCMC) methods to bridge the gap between forecasting and causality. Together, we\u2019ll walk through practical examples using the PyMC library to illustrate these concepts.\r\n\r\nBy the end of this talk, you\u2019ll gain actionable insights on how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses using Directed Acyclic Graphs (DAGs) to understand causal relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.\r\n\r\nWhether you\u2019re interested in forecasting, causal inference, Bayesian modeling, or all of them, this talk will equip you with practical tools to approach each with clarity and confidence\u2014grounded in a real-world example from the Blue Yonder experience.", "code": "LRRVGG", "state": "accepted", "created": "2025-01-05", "speaker_names": "Matthias Binder", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Scaling Python: An End-to-End ML Pipeline for ISS Anomaly Detection with Kubeflow", "abstract": "Building and deploying scalable, reproducible machine learning pipelines can be challenging, especially when working with orchestration tools like Slurm or Kubernetes. In this talk, we demonstrate how to create an end-to-end ML pipeline for anomaly detection in International Space Station (ISS) telemetry data using only Python code.\r\n\r\nWe show how Kubeflow Pipelines, MLFlow, and other open-source tools enable the seamless orchestration of critical steps: distributed preprocessing with Dask, hyperparameter optimization with Katib, distributed training with PyTorch Operator, experiment tracking and monitoring with MLFlow, and scalable model serving with KServe. All these steps are integrated into a holistic Kubeflow pipeline.\r\n\r\nBy leveraging Kubeflow's Python SDK, we simplify the complexities of Kubernetes configurations while achieving scalable, maintainable, and reproducible pipelines. This session provides practical insights, real-world challenges, and best practices, demonstrating how Python-first workflows empower data scientists to focus on machine learning development rather than infrastructure.", "full_description": "Among popular open-source MLOps tools, **Kubeflow** stands out as a Kubernetes-native platform designed to support the entire ML lifecycle, from data preprocessing to model training, deployment, and retraining. Its modular structure enables the integration of a wide range of tools, making it a highly versatile framework for building scalable and reproducible ML workflows. Despite this, most existing resources focus on individual components rather than demonstrating how these can be orchestrated into a seamless, end-to-end pipeline.\r\n\r\nIn this talk, we present a practical case study that highlights the potential of Kubeflow in a real-world application. Specifically, we showcase how an automated ML pipeline for anomaly detection in International Space Station (ISS) telemetry data can be built and deployed using Kubeflow and other open-source MLOps tools. The dataset, originating from the Columbus module of the ISS, introduces unique challenges due to its complexity and high-dimensional nature, providing an excellent testbed for MLOps workflows.\r\n\r\n### **What makes this approach unique?**\r\n\r\nOur workflow is built entirely in Python, leveraging Kubeflow\u2019s Python SDK to orchestrate every stage of the pipeline. This eliminates the need for manual interaction with Kubernetes or container configurations, making the process accessible to ML engineers and data scientists without extensive DevOps expertise.\r\n\r\n### **Key takeaways for attendees:**\r\n\r\n*   **Tool integration:** Learn how to combine Dask for distributed preprocessing, Katib for hyperparameter optimization, PyTorch Operator for distributed training, MLFlow for experiment tracking and monitoring, and KServe for scalable model serving. These tools are orchestrated into a unified pipeline using Kubeflow Pipelines.\r\n*   **Overcoming challenges:** Gain insights into the technical hurdles faced during the implementation of this pipeline and discover the strategies and best practices that made it possible.\r\n*   **Real-world impact:** Understand how to apply MLOps principles to complex, real-world datasets and how these principles translate into scalable, maintainable, and reproducible workflows.\r\n\r\nTo ensure reproducibility and accessibility, the entire pipeline, including configurations and code, is publicly available in our GitHub repository [here](https://github.com/hsteude/code-ml4cps-paper). Attendees will be able to replicate the workflow, adapt it to their own use cases, or extend it with additional features.\r\n\r\n### **Who should attend?**\r\n\r\nThis session is designed for data scientists, ML engineers, and Python enthusiasts who want to simplify the development of scalable ML pipelines. Whether you're new to Kubernetes or looking to streamline your MLOps workflows, this talk will provide actionable insights and tools to help you succeed.", "code": "TRUUVL", "state": "submitted", "created": "2024-12-20", "speaker_names": "Christian Geier, Henrik Sebastian Steude", "track": "PyCon: MLOps & DevOps"}, {"title": "Improving and Monitoring a RAG System with Open Source Components", "abstract": "The city of Munich offers extensive information about municipal services on its website, but findability is often a problem. This talk presents a solution: an AI-based search with Retrieval Augmented Generation (RAG) that can process questions in natural language.\r\n\r\nThe topic is relevant as many stakeholders face similar challenges. The presentation explains the implementation using open source components, the step-by-step improvement of the search function and the monitoring during operation. Participants will learn how to build, optimise and monitor an AI-based search.", "full_description": "The city of Munich offers comprehensive information on municipal services on its website. However, despite the wide range of information available, there is often a problem that citizens have difficulty finding the desired information. To address this widespread issue, an AI-based search was developed that can process natural language questions, thus improving the accessibility of information.\r\n\r\nThis presentation addresses the relevant problem of information retrieval, which many cities and institutions face, and shows how Munich has solved it using Retrieval Augmented Generation (RAG). RAG is a technique that enhances the capabilities of language models (LLMs) by integrating information from external data sources to generate more precise and relevant answers.\r\n\r\nThe implementation of the AI-based search was carried out using various open-source components:\r\n- Qdrant: A vector database that holds the content as vectors.\r\n- Langchain: An abstraction layer for LLMs.\r\n- Langfuse: Used for monitoring and evaluating LLM based applications.\r\n- Litellm: A proxy layer for LLMs and embedding models, which simplifies access to and integration of various models.\r\n\r\nSpecial emphasis is placed on the continuous improvement of the search function through the use of test datasets. These data sets are used to evaluate the performance of the search function and to make targeted improvements.  Additionally, monitoring is supplemented by user feedback to ensure that the search function meets user requirements.\r\n\r\nThe main points of the presentation include:\r\n- The detailed structure and individual components of the RAG system.\r\n- The steps for continuous improvement and evaluation of the search function.\r\n- Monitoring methods during live operation, including user feedback.\r\n- The integration and use of the open-source components Langchain, Langfuse, and Litellm.\r\n\r\nBy the end of the presentation, attendees will have understood how to implement, continuously improve and monitor an AI-based search in live operation. They will also have learned the basics of RAG and the benefits of the open source tools used.", "code": "DFZMLG", "state": "submitted", "created": "2025-01-03", "speaker_names": "Michael Jaumann", "track": "PyData: Generative AI"}, {"title": "Responsible AI with fmeval - an open source library to evaluate LLMs", "abstract": "The term \"Responsible AI\" has seen a threefold increase in search interest compared to 2020 across the globe. As developers, the questions like \"How can we build large language model-enabled applications that are responsible and accountable to its users?\" encountered in the conversation more often than before. And the discussion is further compounded by concerns surrounding uncertainty, bias, explainability, and other ethical considerations.\r\n\r\nIn this session, the speaker will guide you through fmeval, an open-source library designed to evaluate Large Language Models (LLMs) across a range of tasks. The library provides notebooks that you can integrate into your daily development process, enabling you to identify, measure, and mitigate potential responsible AI issues throughout your system development lifecycle.", "full_description": "The term \"Responsible AI\" has seen a threefold increase in search interest compared to 2020 across the globe. As developers, the questions like \"How can we build large language model-enabled applications that are responsible and accountable to its users?\" encountered in the conversation more often than before. And the discussion is further compounded by concerns surrounding uncertainty, bias, explainability, and other ethical considerations.\r\n\r\nIn this session, the speaker will guide you through fmeval, an open-source library designed to evaluate Large Language Models (LLMs) across a range of tasks. The library provides notebooks that you can integrate into your daily development process, enabling you to identify, measure, and mitigate potential responsible AI issues throughout your system development lifecycle.\r\n\r\nTarget Audience: Machine Learning Engineers/Data Scientists, AI/ML Researchers, Software Developers, AI/ML Project Managers, Solutions Architectures.", "code": "KTJY9V", "state": "submitted", "created": "2025-01-05", "speaker_names": "Mia Chang", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Atomic Habits Bot: Build a Habit in 66 Days", "abstract": "### Abstract  \r\n\r\n**Atomic Habits Bot: Build a Habit in 66 Days**  \r\n\r\nForming new habits can be challenging, but technology can simplify the process. In this talk, you will learn how to build an **Atomic Habits Bot** using **Python**, **Langchain**, **LLMs**, and the **Telegram API** to send personalized notifications via Telegram. Inspired by James Clear's *Atomic Habits*, this bot will guide users through the principles of habit formation, helping them develop a new habit in 66 days.  \r\n\r\nBy leveraging **LLMs** and vectorization, the bot provides tailored interactions based on user progress. Additionally, **Langchain** is used to connect language models with external platforms, creating a seamless user experience. This session will combine psychology, coding, and AI to demonstrate how technology can help individuals stay consistent in their habit-building journey.", "full_description": "### Description  \r\n\r\nBuilding habits is essential for personal growth, but sticking to them can often feel overwhelming. This session introduces the **Atomic Habits Bot**, a Python-powered chatbot designed to help users form habits effectively by integrating James Clear's *Atomic Habits* principles.  \r\n\r\nThe bot utilizes advanced AI tools, including **LLMs** (such as GPT-3 or GPT-4) for personalized interactions and **Langchain** to create dynamic integrations with external platforms. Through the **Telegram API**, the bot sends automated reminders and motivational messages, adapting to the user\u2019s progress over the 66-day habit formation period.  \r\n\r\nThis talk covers:  \r\n1. **Psychology of Habits**: An overview of *Atomic Habits* principles and their real-world applications.  \r\n2. **Role of LLMs**: How large language models generate natural, personalized conversations.  \r\n3. **Bot Construction**: Step-by-step guide to building the bot using Python and Langchain.  \r\n4. **Telegram Integration**: Connecting the bot to Telegram for real-time notifications.  \r\n5. **Live Demonstration**: A live showcase of the bot\u2019s functionality and features.  \r\n\r\nBy the end of the session, attendees will be inspired and equipped to combine Python and AI tools to develop their own habit-building applications.", "code": "FKG9WU", "state": "submitted", "created": "2025-01-02", "speaker_names": "Carla Marcela Florida Rom\u00e1n", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "20 Years of Python for Finance", "abstract": "In December 2024, The Python Quants celebrated its 20th company anniversary. This talk reflects on two decades of leveraging Python in the financial industry, highlighting the evolution of the ecosystem, key milestones, and the role education has played in shaping this journey. Drawing from my experiences as the author of seven books on Python for Finance and as an educator and consultant, I will share insights into past developments and future trends in this transformative domain.", "full_description": "Python has become the programming language of choice in the financial industry, revolutionizing everything from data analysis to algorithmic trading. In December 2024, The Python Quants marked its 20th anniversary, underscoring two decades of innovation and progress in this exciting field. During this 30-minute talk, I will delve into the fascinating journey of Python for Finance\u2014how it started, how it has matured, and where it is heading.\r\n\r\nKey highlights include:\r\n\r\n* The pivotal role of education in spreading Python adoption within the financial sector, supported by my work as the author of seven influential books on the topic and director of the Certificate in Python for Finance (CPF) Program.\r\n* A retrospective on the major shifts in the Python ecosystem, from libraries like NumPy and pandas to the emergence of AI and machine learning frameworks.\r\n* Real-world applications that have reshaped quantitative finance, asset management, and algorithmic trading.\r\n* A forward-looking perspective on the challenges and opportunities that lie ahead, including advancements in open-source technologies and the growing importance of AI and Machine Learning in Finance.", "code": "SUAFJX", "state": "submitted", "created": "2025-01-04", "speaker_names": "Dr. Yves J. Hilpisch", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Running every street in Paris with Python and PostGIS", "abstract": "In this talk, we'll explore how to extract street networks from OpenStreetMap, process GPS tracking data from running activities, and build a system to track progress toward covering every street in a city. We'll dive into challenges like handling GPS inaccuracies, matching runs to streets, and maintaining a database of covered streets.\r\n\r\nThis talk is aimed at Python developers interested in working with geospatial data using Python libraries like osmnx, shapely, geopandas, and storing it for efficient querying in Postgres and PostGIS.", "full_description": "In 2006, Tom Murphy started a project of running every street in Pittsburgh (over 1,500 miles in total). He finished the project in 2022, covering 3661 miles in 269 runs. In this talk, we'll look at how we can do the same in our cities and track our progress, with Paris as an example.\r\n\r\nWe'll explore how to extract street networks from OpenStreetMap, process GPS tracking data from running activities, and build a system to track progress toward covering every street in a city. We'll dive into challenges like handling GPS inaccuracies, matching runs to streets, and maintaining a database of covered streets.\r\n\r\nThis talk is aimed at Python developers interested in working with geospatial data using Python libraries like osmnx, shapely, geopandas, and storing it for efficient querying in Postgres and PostGIS.", "code": "RNFBT3", "state": "submitted", "created": "2024-12-20", "speaker_names": "Vinayak Mehta", "track": "PyCon: Programming & Software Engineering"}, {"title": "Understanding API Dispatching in NetworkX", "abstract": "Hi! Have you ever wished your pure Python libraries were faster? Or wanted to fundamentally improve a Python library by rewriting everything in a faster language like C or Rust? Well, wish no more... NetworkX's backend dispatching mechanism redirects your plain old NetworkX function calls to a FASTER implementation present in a separate backend package by leveraging the Python's [`entry_point`](https://packaging.python.org/en/latest/specifications/entry-points) specification!\r\n\r\n[NetworkX](https://github.com/networkx/networkx) is a popular, pure Python library used for graph(aka network) analysis. But as the graph size increases (like a network of everyone in the world), the NetworkX algorithms could take days to solve a simple graph analysis problem. So, to address these performance issues, a backend dispatching mechanism was recently developed. In this talk, we will unveil this dispatching mechanism and its implementation details, and how we can use it just by specifying a `backend` kwarg like this:\r\n\r\n    >>> nx.betweenness_centrality(G, backend=\u201cparallel\u201d)\r\n\r\nor by passing the backend graph object(type-based dispatching):\r\n\r\n    >>> H = nxp.ParallelGraph(G)\r\n    >>> nx.betweenness_centrality(H)\r\n\r\nWe'll also go over the limitations of this dispatch mechanism. Ending with a brief overview of how this API dispatch mechanism could be integrated in other scientific Python libraries, along with the potential challenges that may arise with them.", "full_description": "In the first few minutes, we will familiarize ourselves with the NetworkX library and graph analysis in general. Moving on to a quick demo of the performance limitations of existing NetworkX algorithms such as `betweenness_centrality` and `square_clustering` when applied to a larger [SNAP graph dataset](https://snap.stanford.edu/data/ca-HepTh.html), underscoring the critical necessity for more optimised implementations. Balancing this need for more efficient and faster algorithms with the core values of NetworkX-- to remain free from any external dependencies and to uphold its pure Python nature, led to the development of a new backend dispatching system in NetworkX.\r\n\r\nNext, we will understand what API dispatching is in a broader sense, what are `entry_points`, how NetworkX utilises them to discover the backend packages and then redirect the NetworkX function calls to the faster backend implementations. We will then delve into some of the details of the features a backend developer can utilise, like the usage of `NETWORKX_TEST_BACKEND` environment variable, the second `entry_point` for backend meta-data, the `backend_priority`, `can_run`, `should_run` etc. And a summary of other NetworkX backends and some future to-dos for NetworkX\u2019s dispatching.\r\n\r\nAfter that, we will understand how and when this API dispatching mechanism could be adopted by a scientific Python library, and what are some of the challenges that they might face during its integration. We will also go over some of the recent developments in integrating API dispatching in the scikit-image project and the challenges of integrating API dispatching in an array-based library. Finally, concluding with an interactive Q&A.\r\n\r\nIntended audience:\r\n- Contributors of Python libraries who are interested in offering their users faster algorithms without changing their codebase and the user-API much\r\n- People who work with large graph datasets\r\n- anyone interested in API dispatching, graphs or any of the above stuff :)\r\n\r\nSome basic knowledge of Python is expected; should know what objects and classes are.\r\n\r\n\r\nThank you :)", "code": "TLBU3Z", "state": "submitted", "created": "2024-12-22", "speaker_names": "Aditi Juneja", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Conquering PDFs: document understanding beyond plain text", "abstract": "NLP and data science could be so easy if all of our data came as clean and plain text. But in practice, a lot of it is hidden away in PDFs, Word documents, scans and other formats that have been a nightmare to work with. In this talk, I'll present a new and modular approach for building robust document understanding systems, using state-of-the-art models and the awesome Python ecosystem. I'll show you how you can go from PDFs to structured data and even build fully custom information extraction pipelines for your specific use case.", "full_description": "For the practical examples, I'll be using spaCy, and the new Docling library and layout analysis models. I'll also cover Optical Character Recognition (OCR) for image-based text, how to convert tabular data to pandas DataFrames, and strategies for creating training and evaluation data for information extraction tasks like text classification and entity recognition using PDFs and other documents as inputs.", "code": "FUX3FR", "state": "confirmed", "created": "2024-11-28", "speaker_names": "Ines Montani", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Stock Market Forecasting with Conformal Prediction", "abstract": "Dive into the future of financial analysis and the power of uncertainty quantification using conformal prediction techniques, applied through the robust capabilities of Neural Prophet, Nixtla, and MAPIE libraries. Gain insights into how these tools provide reliable predictive intervals, enhancing decision-making in volatile markets.", "full_description": "-Advantages and Fundamentals concepts of Conformal Prediction.\r\nExplore the impact of conformal prediction on financial time series forecasting. This talk will introduce the fundamentals of conformal prediction, highlighting its ability to provide reliable uncertainty quantification by generating predictive intervals that encompass real market behaviours. In this way conformal prediction allows investors to make informed decisions despite inherent market volatility.\r\n\r\n-MAPIE, Nixtla, and Neural Prophet Python libraries.\r\nI'll present Neural Prophet, Nixtla, and MAPIE libraries that simplify the implementation of Conformal Prediction.\r\n\r\n-Practical example of a financial time series.\r\nTo bring theory into practice, I'll walk through a use case using Yahoo! Finance's API with a comparison of several statistical and machine learning models.", "code": "F39V8J", "state": "submitted", "created": "2024-12-22", "speaker_names": "Claudio G. Giancaterino", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Django's Dilemma: Balancing Simplicity with Scalability", "abstract": "Django's model-view-serializer approach works great for small apps, but as projects grow, you might face challenges like scattered database operations and APIs that are too closely linked to your database models. These issues can make unit testing and scaling harder. I'll share real-world examples of these problems and show how to refactor an app using ideas from Domain-Driven Design (DDD) and hexagonal architecture. We'll look at a before-and-after example to see how these changes can make your app easier to use and debug. Plus, we'll discuss when Django's simplicity is enough and when it's worth adopting a more structured approach. You'll leave with practical tips for transforming your Django projects into systems that can handle increased complexity.", "full_description": "Django's model-view-serializer framework is a powerful tool for building small to medium-sized applications quickly and efficiently. Its straightforward approach allows developers to create CRUD operations with minimal effort, making it an excellent choice for projects with clear and simple requirements. However, as applications grow in complexity and scale, developers often encounter several challenges that can hinder the application's maintainability and scalability.\r\nOne common issue is the scattering of database operations across the codebase. This pattern often leads to tightly coupled APIs and database models, making it difficult to isolate logic for individual testing and increasing the fragility of the application as it scales. As database logic becomes intertwined with business logic, maintaining and expanding the application becomes increasingly challenging.\r\nIn this talk, we will explore these common pitfalls through real-world examples and demonstrate how to refactor a Django application for improved scalability and maintainability. Using a webhook server app as a case study, we'll discuss how client-driven response customization can complicate the traditional model-view-serializer approach. We'll explore the limitations of handling complex serialization and response formatting within the view layer, which often results in bloated views that are hard to test and scale.\r\nTo address these challenges, we'll introduce concepts from Domain-Driven Design (DDD) and hexagonal architecture. By applying these principles, we can decouple the application's components, making it easier to manage and extend. We'll refactor the application using a builder pattern for payload formatting, ensuring that response structures are flexible yet maintainable. Additionally, we'll introduce an injected service layer and a repository layer, which separate concerns and allow for more targeted unit testing. This architecture not only improves testability but also enhances the application's ability to adapt to changing requirements.\r\nWe'll provide a clear before-and-after comparison, highlighting the benefits of this refactored approach. Attendees will learn how these changes can lead to cleaner, more scalable code that is easier to debug and maintain. We'll also discuss the balance between Django's simplicity and the need for more sophisticated architecture, helping developers decide when it's appropriate to adopt these patterns.\r\nBy the end of this session, attendees will gain practical insights into transforming their Django projects into robust systems capable of handling increased complexity. Whether you're a developer working on scaling an existing application or someone interested in learning more about advanced architectural patterns, this talk will provide valuable takeaways and actionable strategies for improving your Django applications.", "code": "YCAUMC", "state": "submitted", "created": "2024-11-28", "speaker_names": "Anette Haferkorn", "track": "PyCon: Django & Web"}, {"title": "Beginner's Guide to building Autonomous AI Agents with Eliza Framework", "abstract": "AI agents are becoming fundamental building blocks of modern applications, thanks to their capability of understanding context with the help of the LLMs, but with added capability to interact with systems, files. AI agents that can meaningfully interact across platforms while maintaining context aren't just for large tech companies anymore as we have a number of open source AI Agent frameworks now which developers can leverage. This beginner-friendly, hands-on tutorial shows the process of creating autonomous AI agents using the Eliza framework which is an open source Python based AI Agent framework. From learning the core concepts of Agents, to understanding how to create, deploy, and manage AI agents that can handle real-world tasks.", "full_description": "We'll break down the complexities of AI agent development into understandable components. Starting with basic concepts and progressing to functional implementations, participants will learn how to create, deploy, and manage AI agents that can handle real-world tasks with the help of the Eliza AI Agent framework. Finally we will have a hands on demo \r\n\r\n\r\nKey Learning Outcomes:\r\n- Understand the fundamental architecture of AI agents\r\n- Build a basic AI agent using the Eliza framework\r\n- Design agent-to-agent communication patterns\r\n- Deploy agents that can interact across platforms\r\n\r\nTutorial Outline (90 minutes):\r\n\r\nPart 1: Understanding AI Agents and Setup (25 minutes)\r\n\r\n- Introduction to AI Agents (15 minutes)\r\nWhat are AI agents and why they matter\r\nUnderstanding agent components:\r\n- Memory and context management\r\n- Decision making capabilities\r\n- System interactions\r\n \tIntroduction to Eliza framework architecture\r\n\r\n- Environment Setup (10 minutes)\r\n\r\nPart 2: Building Your First Agent (40 minutes)\r\n\r\n- Creating a Basic Agent (20 minutes)\r\nBasic agent structure walkthrough\r\nSetting up agent configuration\r\nImplementing first interactions\r\nSetting up agent personality and behavior\r\nTesting basic responses\r\n\r\n- Adding Memory and System Interaction (20 minutes)\r\nWhy do agents need persistent storage?\r\nSetting up Couchbase for agent memory\r\n\r\n Managing agent state\r\nHandling context in conversations\r\n\r\nPart 3: Advanced Features and Deployment (25 minutes)\r\n- Extending Agent Capabilities (15 minutes)\r\nAdding custom skills\r\nImplementing system commands\r\nBasic error handling\r\nMonitoring agent behavior\r\n\r\n- Production Considerations (10 minutes)\r\nDeployment best practices\r\nCommon pitfalls and solutions\r\nScaling considerations\r\nNext steps in development", "code": "LLVQNE", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Suvrakamal Das, Rudraksh Karpe", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Harnessing LLLMs for Clinical Entity Extraction and Medical Coding in German Clinical Text", "abstract": "The healthcare sector generates vast amounts of unstructured text, presenting challenges in extracting valuable medical information and assigning standardized medical codes. My work investigates the application of Large Language Models (LLMs) and fine-tuned BERT models for Clinical Named Entity Recognition (NER) and Medical Coding in German clinical texts.\r\n\r\nBy comparing generative LLMs (like Llama-3.1-70B) with domain-specific BERT models (e.g., medBERT.de), I propose a hybrid approach that leverages synthetic data generated by LLMs to enhance the performance of BERT models. Using the Berlin-T\u00fcbingen-Oncology Corpus (BRONCO), my study demonstrates that LLMs offer rapid solutions with minimal data requirements, while fine-tuned BERT models achieve higher recall for complex clinical tasks.\r\n\r\nAdditionally, I present innovations in structured JSON output and error handling to optimize LLM applications in medical extraction and medical coding tasks, achieving notable precision gains. This work underscores the transformative potential of combining generative and discriminative NLP models for improving clinical text processing in non-English domains, paving the way for efficient, scalable, and domain-specific AI solutions in healthcare.", "full_description": "Introduction\r\n------------\r\n\r\nThe healthcare industry faces a critical challenge: converting the vast quantities of unstructured clinical text generated daily into actionable insights. German clinical documents, in particular, demand specialized Natural Language Processing (NLP) solutions due to linguistic and regulatory complexities. My worl explores the intersection of generative Large Language Models (LLMs) and fine-tuned BERT models to address two core tasks: Clinical Named Entity Recognition (NER) and Medical Coding. These tasks are pivotal for enhancing patient care, streamlining medical billing, and advancing clinical research.\r\n\r\nThis research introduces innovative methodologies and evaluates their effectiveness in processing German clinical texts, focusing on three core elements:\r\n\r\n1.  Leveraging generative LLMs for rapid entity extraction with minimal training data.\r\n\r\n2.  Fine-tuning BERT-based models on domain-specific datasets for improved recall.\r\n\r\n3.  Combining synthetic data generation with model fine-tuning to enhance performance.\r\n\r\nThrough a detailed evaluation using the Berlin-T\u00fcbingen-Oncology Corpus (BRONCO), this work establishes benchmarks for Clinical NLP tasks in German and sets the stage for scalable and efficient applications in healthcare.\r\n\r\n* * * * *\r\n\r\nChallenges in Clinical NLP for German Texts\r\n-------------------------------------------\r\n\r\nClinical texts present unique challenges, especially in the German language:\r\n\r\n1.  **Heterogeneous Data Sources**: Documents vary significantly in structure, style, and content, making uniform processing difficult.\r\n\r\n2.  **Domain-Specific Terminology**: Medical texts often use highly specialized terms, abbreviations, and shorthand, complicating entity recognition and standardization.\r\n\r\n3.  **Data Scarcity and Privacy**: German clinical datasets are limited due to strict data protection regulations, increasing the reliance on synthetic data and domain-specific models.\r\n\r\n4.  **Complex Entity Linking**: Mapping clinical entities to standardized medical codes, such as ICD, OPS, and ATC, requires precise contextual understanding.\r\n\r\n* * * * *\r\n\r\nMethodologies Explored\r\n----------------------\r\n\r\n### 1\\. Generative LLMs for Clinical NER\r\n\r\nLLMs, such as Llama-3.1-70B, OpenBioLLM, and BioMistral, have demonstrated potential for zero-shot and few-shot learning, making them suitable for tasks with limited labeled data. In this work:\r\n\r\n-   Prompt engineering is employed to guide LLMs in identifying entities like diagnoses, medications, and treatments.\r\n\r\n-   Structured output generation in JSON format is implemented to simplify downstream processing.\r\n\r\n-   Error-handling mechanisms are developed to improve consistency, including replaying invalid responses and flagging ambiguous entities.\r\n\r\nThe results show that LLMs provide rapid and flexible solutions, but their performance varies by entity type. They excel in identifying medications and treatments but struggle with diagnosis-related terms, highlighting the need for fine-tuned models.\r\n\r\n### 2\\. Fine-Tuned BERT Models\r\n\r\nBuilding on previous research with medBERT.de, this study fine-tunes BERT-based models on the BRONCO dataset:\r\n\r\n-   The BERT models are trained to recognize entities in German clinical text, achieving superior recall for complex tasks.\r\n\r\n-   Fine-tuning parameters, such as learning rate and batch size, are optimized for clinical NLP tasks.\r\n\r\n-   The models' ability to process large text chunks in parallel enhances their efficiency compared to sequential LLM inference.\r\n\r\n### 3\\. Hybrid Approach: Synthetic Data for Model Augmentation\r\n\r\nA novel hybrid approach combines the strengths of LLMs and BERT models:\r\n\r\n-   Synthetic data is generated using LLMs with carefully designed prompts in the BIO tagging format.\r\n\r\n-   This synthetic data augments the BRONCO training set, improving BERT's performance by up to 4% in precision for diagnosis-related entities.\r\n\r\n-   The hybrid approach bridges the gap between generative and discriminative modeling, offering a robust solution for resource-constrained settings.\r\n\r\n* * * * *\r\n\r\nKey Innovations\r\n---------------\r\n\r\n### Structured Output Generation\r\n\r\nTo ensure reliable and interpretable outputs from LLMs, structured JSON generation is enforced. This includes:\r\n\r\n-   Defining strict schemas for entity recognition.\r\n\r\n-   Utilizing Python's Pydantic library for schema validation and error handling.\r\n\r\n-   Implementing fallback strategies for invalid or incomplete responses.\r\n\r\n### Synthetic Data Generation\r\n\r\nSynthetic data addresses the scarcity of annotated clinical datasets. By:\r\n\r\n-   Generating diverse and high-quality examples tailored to the target domain.\r\n\r\n-   Enhancing model generalization and robustness.\r\n\r\n### Entity Linking with Cross-Encoders\r\n\r\nA critical aspect of this research is Clinical Entity Linking, mapping extracted entities to medical codes (ICD, OPS, ATC). The xMEN framework is fine-tuned with domain-specific encoders, achieving high precision and recall in medical coding tasks.\r\n\r\n* * * * *\r\n\r\nEvaluation and Results\r\n----------------------\r\n\r\n### Performance Metrics\r\n\r\nThe models are evaluated on precision, recall, and F1-score across three entity types:\r\n\r\n1.  **Diagnoses**\r\n\r\n2.  **Medications**\r\n\r\n3.  **Treatments**\r\n\r\nKey findings include:\r\n\r\n-   Generative LLMs achieve precision rates of up to 88.4% for medications but underperform in diagnosis recognition.\r\n\r\n-   Fine-tuned BERT models excel in recall, particularly for complex entities.\r\n\r\n-   The hybrid approach improves overall F1-scores, demonstrating the synergy of combining LLMs with fine-tuned models.\r\n\r\n### Computational Efficiency\r\n\r\nInference speed and resource utilization are also analyzed:\r\n\r\n-   LLMs offer flexibility but require significant computational resources.\r\n\r\n-   BERT models, once fine-tuned, provide faster and more resource-efficient solutions for large-scale deployments.\r\n\r\n* * * * *\r\n\r\nPractical Implications and Future Directions\r\n--------------------------------------------\r\n\r\n### Enhancing Healthcare Delivery\r\n\r\nBy improving the accuracy and efficiency of Clinical NLP tasks, this research has direct applications in:\r\n\r\n-   Automated medical coding for billing and insurance.\r\n\r\n-   Clinical decision support systems.\r\n\r\n-   Patient safety and adverse event monitoring.\r\n\r\n### Advancing AI in Healthcare\r\n\r\nThe findings highlight the need for:\r\n\r\n-   Developing specialized LLMs for medical applications.\r\n\r\n-   Expanding synthetic data generation techniques to other low-resource domains.\r\n\r\n-   Improving entity linking frameworks for multilingual healthcare data.\r\n\r\n### Open Challenges\r\n\r\nWhile promising, the research identifies several areas for improvement:\r\n\r\n-   Balancing precision and recall across diverse entity types.\r\n\r\n-   Reducing computational overhead for real-time applications.\r\n\r\n-   Addressing ethical considerations in using synthetic data and AI-generated insights.\r\n\r\n* * * * *\r\n\r\nConclusion\r\n----------\r\n\r\nThis work underscores the transformative potential of integrating LLMs and BERT models for Clinical NLP in German healthcare. By addressing challenges in data scarcity, domain adaptation, and computational efficiency, it sets a foundation for scalable and impactful AI solutions in the medical domain. Through innovative methodologies and rigorous evaluation, this research contributes to advancing healthcare delivery and NLP research, fostering a future where AI seamlessly integrates into clinical practice.", "code": "NWK7BW", "state": "submitted", "created": "2024-12-17", "speaker_names": "Kunal Runwal", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Detecting AI-Generated Text", "abstract": "The rapid advancement of large language models (LLMs) has enabled the generation of highly coherent text, which is nearly indistinguishable from human-written content. While useful in many applications, LLMs can also be used for harmful purposes such as spreading disinformation, automating spam, and fostering plagiarism. This can negatively impact the reputation of companies and undermine the trust of users. To mitigate these risks, various detection methods have been proposed to determine whether a text is created by a machine or a human. \r\n\r\nIn this talk, we will discuss differences between AI-generated and human-written texts. Next, we will overview state-of-the-art AI text detectors and discuss their differences, limitations, and empirical performance. We will analyze how detection performance changes depending on the text domain, information available to the detector, prompt complexity, and other factors. Finally, we will also touch on approaches to evade detectors, such as paraphrasing and spoofing.", "full_description": "Recent advancements in natural language processing have led to the development of large language\r\nmodels (LLMs) capable of generating highly coherent and contextually appropriate text. The rapid growth in the volume and quality of AI-generated content emphasizes the need for robust detection systems to prevent misuse of these technologies in dangerous applications such as misinformation propagation, phishing, and academic dishonesty. Accurately identifying AI-generated text is equally crucial in various business applications, including moderating online content, assessing the news trustworthiness and classifying product reviews.\r\n\r\nThe ability to accurately detect AI-generated text is especially critical for businesses that have a variety of customer-facing products where users can misuse LLMs. For instance, users of e-commerce businesses may suffer from product listings containing generic texts generated by a language model, e.g., \u00abI cannot fulfill that request, it goes against our current policy\u00bb. This makes listings look unprofessional and leads to poor user experience. Likewise, online product reviews have a high likelihood of being generated by an LLM, which undermines the trust for the platform and can be seen as irrelevant by customers. While some organizations can proactively ask users to disclose whether the AI-generated text was used in their content submission, many business use cases still lack trustworthiness and require additional moderation systems that could reliably identify such texts on a massive scale.\r\n\r\nIn this talk, we will focus on the topic of AI text detectors and discuss possible approaches and their performance in various settings. Our goal is to provide insights into the strengths and weaknesses of existing detectors and propose potential future directions to mitigate the misuse of LLMs in business applications. We will also talk about approaches to evade such detectors, such as paraphrasing and spoofing.", "code": "79YG9T", "state": "submitted", "created": "2025-01-05", "speaker_names": "Nikita Kozodoi", "track": "PyData: Generative AI"}, {"title": "Bridging Causal thinking and Media Mix Modeling", "abstract": "In today's data-driven landscape, understanding causal relationships is essential for effective marketing strategies. This talk will explore the link between Bayesian causal thinking and media mix modeling, utilizing Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP).\r\n\r\nWe will examine how DAGs represent causal assumptions, how SCMs define relationships in media mix models, and how to implement these models within a Bayesian framework. By using media mix models as causal inference tools, we can estimate counterfactuals and causal effects, offering insights into the effectiveness of media investments.", "full_description": "In the era of data-driven decision-making, understanding causal relationships is crucial for effective marketing strategies. This talk delves into the underexplored connection between Bayesian causal thinking and media mix modeling, linking Directed Acyclic Graphs (DAGs), Structural Causal Models (SCMs), and the Data Generation Process (DGP). By navigating through these key concepts, we will demonstrate how we can build models that not only predict outcomes but also represent causal mechanisms within the marketing ecosystem.\r\n\r\nStarting from foundational principles, we will explore how DAGs serve as a formal language for encoding causal assumptions, how Structural Causal Modeling define relationships in media mix models, and how we implement those in the Bayesian framework through the famous DGP. We will further illustrate how media mix models can be employed as causal inference tools to estimate counterfactuals and causal effects, providing actionable insights into the effectiveness of media investments.\r\n\r\nFinally, we\u2019ll show how Bayesian inference enables us to update these causal beliefs in light of data. This synthesis of causal reasoning and probabilistic modeling is not only theoretically rich but practically powerful\u2014offering a robust framework for constructing media mix models that more accurately reflect the complexities of real-world marketing dynamics.\r\n\r\nAttendees will leave with an understanding of how to apply Bayesian causal discovery (guided by an example in an IPython notebook) to develop causally valid models that can be applied to real-world marketing data. They will learn how to use Media Mix Models as causal inference tools to estimate counterfactual scenarios and causal effects, unlocking deeper insights into the effectiveness of media investments. This presentation aims to reveal a new pathway for marketers, data scientists, and researchers to harness the potential of these powerful methodologies together, empowering them to drive more informed, causally grounded decisions.", "code": "MNTFRG", "state": "submitted", "created": "2024-11-28", "speaker_names": "Carlos Trujillo", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Intuitive A/B Test Evaluations for Coders", "abstract": "A/B testing is a critical tool for making data-driven decisions, yet its statistical underpinnings\u2014p-values, confidence intervals, and hypothesis testing\u2014are often challenging for those without a background in statistics. Coders frequently encounter these concepts but lack a straightforward way to compute and interpret them using their existing skill set.\r\nThis talk presents a practical approach to A/B test evaluations tailored for coders. By utilizing Python\u2019s random number generator and basic loops, it introduces bootstrapping as an accessible method for calculating p-values and confidence intervals directly from data. The goal is to simplify statistical concepts and provide coders with an intuitive understanding of how to evaluate test results without relying on complex formulas or statistical jargon.", "full_description": "Making A/B Test Evaluations Intuitive for Coders: A Python-Based Approach\r\n\r\nA/B testing is an essential method for data-driven decision-making, but interpreting the results can be daunting. Complex jargon around p-values and confidence intervals often creates barriers to understanding. This talk simplifies A/B testing by introducing a practical, Python-powered approach using bootstrapping\u2014a flexible and accessible method that aligns with how software engineers think and works without requiring statistical knowledge.\r\n\r\nSession Highlights:\r\n\r\n1. Statistical Significance and Hypothesis Testing:\r\n    * Why is statistical testing crucial for A/B tests? Simple comparisons overlook randomness.\r\n    * Using Python, we\u2019ll demonstrate how to simulate \"what-if\" scenarios by shuffling and resampling data, allowing participants to compute p-values and understand the likelihood of observed differences occurring by chance.\r\n2. Confidence Intervals with Bootstrapping:\r\n    * Confidence intervals clarify the range of plausible outcomes.\r\n    * We\u2019ll explore how to resample experiment data repeatedly to estimate variability and construct intuitive confidence intervals\u2014all using basic tools like random number generators and loops, without requiring advanced math.\r\n    * \r\nKey Takeaways:\r\n* Hands-on skills to compute p-values and confidence intervals using basic programming concepts.\r\n* Clear, step-by-step demonstrations of shuffling, resampling, and generating statistical insights.\r\n* Practical knowledge to move beyond black-box libraries and understand the \"why\" and \"how\" behind A/B test evaluations.\r\n\r\nBy the end of the session, attendees will be equipped to demystify A/B testing with a coder-friendly workflow, empowering them to make confident, data-driven decisions in their projects.\r\n\r\nTalk Outline:\r\n\r\n1. Setting the Stage (5 minutes)\r\n    * What is A/B testing?\r\n    * Why isn't it enough to just compare numbers? Why do we need statistics to interpret results?\r\n2. Statistical Significance and P-Values (5 minutes)\r\n    * Statistical tests (t-test, z-test, binomial test) are frequently used, but what is the intuition behind them?\r\n    * Introducing the basic idea of bootstrapping.\r\n3. Bootstrapping Explained (8 minutes)\r\n    * Step-by-step illustration of the bootstrapping approach.\r\n    * What is a p-value? An intuitive description using resampling.\r\n4. Confidence Intervals Explained (7 minutes)\r\n    * Importance of confidence intervals and how they help interpret results.\r\n    * Intuitive computation of confidence intervals using bootstrapping.\r\n    * Impact of sample size on confidence intervals and certainty.\r\n5. Why These Statistics Matter (5 minutes)\r\n    * Discussion on the practical necessity of statistical techniques.\r\n    * How these methods ensure data-driven decision-making in A/B testing.", "code": "RQ8JBM", "state": "submitted", "created": "2024-12-21", "speaker_names": "Thomas Mayer", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Build & Deploy Apps like a (pro) Data Scientist using Streamlit", "abstract": "Do you ever find it complicated to learn the complexities of a traditional web framework to push your data science work online? Worry no more! Streamlit might help speed things up as it is designed for the required purpose - creating beautiful data-related web apps that can be deployed in minutes. \r\n\r\nIn the hands-on tutorial, we\u2019ll go through various features of Streamlit and build a small lyric fetcher app based on the available dataset (which I have curated and will be sharing the Github link before the talk) of around 24K Billboard top-100 songs. \r\n\r\nBasic understanding of HTML, Python, and libraries such as Numpy, Matplotlib, and Pandas should be good. Ensure you have the said libraries (and wordcloud python library), an editor (Sublime or VSCode), and Streamlit installed in your system.", "full_description": "0:01-0:05 minutes: In the first section, I will discuss with you the basics of Streamlit and some examples of applications made through it. I will also show you the expected final version of what we\u2019ll create during the tutorial. \r\n\r\n00:05 - 0:15 minutes: In the second section, we will ensure that all libraries are set up correctly in your system, and we can run a small \u201cHello World\u201d code on the local server. \r\n\r\n00:15 - 0:55 minutes: In the third section, we will build our application step-by-step by creating a layout and adding the required elements. These elements would include two drop-down buttons for selecting the song & artist for which we want lyrics, A lyric showcasing column, and a word cloud visualization of the respective lyrics.\r\n\r\nIn the last 5 minutes, I will show you how we can deploy the app online using Heroku and Streamlit, which you can further attempt after the talk on your own.", "code": "QVMLSJ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Siddharth Gupta", "track": "PyData: Visualisation & Jupyter"}, {"title": "Distributed file-systems made easy with Python's fsspec", "abstract": "The cloud native revolution has impacted all aspects of engineering, and data engineering is not exempt. One of the ongoing challenges in the data engineering world remains the local and distributed cloud native storage. In this talk we\u2019ll explore working with distributed file systems in Python, through an intro to fsspec: a popular python library that is well-positioned to address the growing challenge of interacting with storage systems of different kinds in a consistent way.\r\n\r\nIn this talk we\u2019ll show hands-on examples of working with fsspec with some of the most popular data tools in the Python community: Pandas, Tensorflow and PyArrow. We\u2019ll demonstrate a real world implementation of fsspec and how it provides easy extensibility through open source tooling.\r\n\r\nYou\u2019ll come away from this session with a better understanding for how to implement and extend fsspec to work with different cloud native storage systems.", "full_description": "### **1. Setting the Stage: Local vs. Distributed Storage (5 minutes)**  \r\n- **What\u2019s the Big Deal with Storage?**  \r\n  - First, let\u2019s talk about the shift from local storage (where we keep files on our own machines) to cloud-native storage (where data is spread across servers in the cloud).  \r\n  - This shift is awsome but comes with new challenges: distributed systems can be tricky to work with, especially when you need to access them in a consistant way.  \r\n\r\n### **2. Enter fsspec: A Game Changer for File Systems (10 minutes)**  \r\n- **What is fsspec?**  \r\n  - fsspec is a Python library that makes working with any kind of file system\u2014whether it's local, in the cloud, or on a distributed system\u2014much easier.  \r\n  - It does this by giving us a unified way to interact with storage, no matter where the files actaully live.  \r\n\r\n- **Why is fsspec Awesome?**  \r\n  - It simplifies file operations (like opening and reading files) across different storage systems, saving us time and mental enery.  \r\n  - Plus, it\u2019s open-source, which means you can extend it and make it work for your own unique storage setup.  \r\n\r\n### **3. fsspec in Action: How It Works with Popular Python Tools (15 minutes)**  \r\n\r\n#### **A. Using fsspec with Pandas**  \r\n- **Pandas & fsspec:**  \r\n  - If you work with Pandas, you\u2019re probably familiar with loading and saving data. fsspec helps make this process smoother by letting you pull data from cloud storage (like AWS S3) with no fuss.  \r\n  - We\u2019ll see how this works in practise, making it easy to work with large datasets in the cloud. \r\n\r\n#### **B. Using fsspec with TensorFlow**  \r\n- **TensorFlow & fsspec:**  \r\n  - If you\u2019re building machine learning models, TensorFlow needs to access training data and models, sometimes stored in the cloud.  \r\n  - With fsspec, TensorFlow can seamlessly interact with cloud storage, making your ML pipelines more streamlined and less frustraiting.  \r\n\r\n#### **C. Using fsspec with PyArrow**  \r\n- **PyArrow & fsspec:**  \r\n  - PyArrow is great for high-performance data processing. When working with big data files like Parquet, fsspec makes it easy to load and save them from cloud storage without missing a beat.  \r\n\r\n### **4. Extending fsspec: Building Your Own Solutions (5 minutes)**  \r\n- **What if I Need Something Custom?**  \r\n  - Sometimes, you need to work with storage systems that aren\u2019t \u201cout of the box.\u201d The cool part about fsspec is that it\u2019s highly extensible.  \r\n  - I\u2019ll walk through how you can easily extend fsspec to work with your own custom storage systems, using a real-world example of how we did this.  \r\n\r\n### **5. Wrap-Up & Key Takeaways (5 minutes)**  \r\n- **The Big Picture:**  \r\n  - fsspec is a simple yet powerful tool for making cloud-native storage work seamlessly with Python data tools like Pandas, TensorFlow, and PyArrow.  \r\n  - It\u2019s the tool you didn\u2019t know you needed to simplify your cloud storage tasks.  \r\n\r\n- **Final Thought:**  \r\n  - With fsspec, working with distributed storage doesn\u2019t have to be hard. It makes everything feel like you\u2019re working with local files, even when they\u2019re scattered across the cloud.  \r\n\r\n### **6. Q&A Session (5 minutes)**", "code": "DEHZHK", "state": "submitted", "created": "2024-12-17", "speaker_names": "Einat Orr", "track": "PyData: Data Handling & Engineering"}, {"title": "Take the Polar Plunge: A Fearless Introduction to Apache Iceberg\u2122", "abstract": "In this beginner-friendly session, we'll introduce Apache Iceberg\u2122 and core features, like schema evolution, hidden partitioning, and ACID-compliant transactions. Learn how Iceberg solves key data lake challenges and integrates with open-source tools. A live demo will show how to apply these capabilities to your data needs.", "full_description": "New to Apache Iceberg\u2122 or just starting to test the waters? Time to take the plunge! In this beginner-friendly session, we\u2019ll introduce you to the core features of this high-performance table format and what it has to offer your data team as you build out your modern data lakehouse.\r\n\r\nWe\u2019ll start by exploring where Iceberg fits within the context of data warehouses, data lakes, and other data storage formats. From there, you\u2019ll be introduced to some of Iceberg\u2019s key features, like schema evolution, hidden partitioning, and ACID-compliant transactions. You\u2019ll see how these capabilities help to solve real-world challenges in scalability, performance, and data consistency. We won\u2019t stop there, though! Next, we\u2019ll go even deeper beneath the surface and explore the open-source tools, query engines, and catalogs that make up the Iceberg ecosystem. Through an end-to-end demo, you\u2019ll see Iceberg in action and learn how you can leverage its features to streamline your data management and drive better insights.\r\n\r\nBy the end of the session, you\u2019ll have the foundation you need to get started applying Iceberg to solve your own data challenges. So come along, and we\u2019ll take the polar plunge together!", "code": "7BXGA7", "state": "submitted", "created": "2024-12-10", "speaker_names": "Danica Fine", "track": "PyData: Data Handling & Engineering"}, {"title": "Climbing the DOM - Introducing textual, the lean application framework", "abstract": "Text-based User Interfaces (TUIs) have experienced a renaissance, offering lightweight,\r\nhighly customizable interfaces for developers and end-users alike.\r\n`Textual`, an open source framework to build cross-platform TUI applications, combines the simplicity of terminal-based applications with the sophistication of graphical interface design.", "full_description": "`Textual` is an open source framework to build cross-platform TUI applications with a simple python API, which is developed by `Textualize`, also known for `rich`.\r\n\r\nOver the last years the development of `textual` made a lot of progress and finally in Dezember 2024  v1.0.0 was released.\r\n\r\n`Textual`'s API combines modern Python with the best of developments from the web world, for a lean app development experience. De-coupled components and an advanced testing framework ensure you can maintain your app for the long-term.\r\n\r\nIn this talk we will dive into `textual`'s features, develop a small `textual` application together and see what other developers already have build.", "code": "HRRLCM", "state": "submitted", "created": "2025-01-05", "speaker_names": "Lars Grams", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Inference with embeddings and convex optimization", "abstract": "Since the \u2018LLM Big Bang\u2019 a couple of years ago, there has been a renewed interest in word embeddings, especially (though not exclusively) in retrieval-augmented generation tasks. Yet, most applications still rely on simple, pairwise embedding comparisons. In this talk, I will introduce a straightforward convex optimization approach that leverages the \u201cmeaning compositionality\u201d property of embeddings to perform explainable inferences over sets of embeddings. This method goes beyond one-to-one similarity checks to determine, for example, whether one embedding is implied by a collection of others. Such inference capabilities can enhance re-ranking processes by removing redundancy, and they can also improve content-based recommendation systems. On a practical level, this technique is readily implementable using tools like the CVXPY library, which typically solves these problems with sub-second latency, even when dealing with hundreds of embeddings.\r\nIn sum, convex-optimization can provide NLP practitioners with an additional lightweight method to achieve explainable inferences in production systems.", "full_description": "Using a furniture e-commerce site as a motivating example, this talk will illustrate why content-based recommendations sometimes require going beyond pairwise feature comparisons. I will demonstrate how mapping these flexible general feature comparisons to a convex optimization problem offers a powerful solution\u2014one that can be readily implemented in Python using the well-known CVXPY library.\r\n\r\nTo help attendees fully grasp this approach, I will introduce the basics of embeddings and explain how meaning compositionality in embedding spaces allows for arithmetic reasoning. I will also provide a concise overview of convex optimization and how it can enhance the afore-mentioned reasoning.\r\n\r\nAdditionally, I will introduce a second use case of the same convex problem: finding non-redundant decompositions in retrieval-augmented generation (RAG) pipelines. By applying this method, we can ensure more diverse sets of retrieved information.\r\n\r\nBy the end of the talk, attendees will have a foundational understanding of how convex optimization can enhance NLP pipelines.", "code": "PLYEUA", "state": "submitted", "created": "2024-12-22", "speaker_names": "Ivan Herreros", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "AWS S3 security best practices for your Python applications", "abstract": "Misconfigured S3 buckets cause countless massive data leaks. You sure don't want to be responsible for the next one. In this talk, you'll learn how to apply security best practices to your S3 bucket for your Python applications using Terraform.", "full_description": "## Introduction - 5min\r\n\r\nWe'll discuss why security is essential. I'll present some examples of data leaks caused by misconfigured S3 buckets.\r\n\r\n\r\n## S3 buckets - usage - 5min\r\nWe'll discuss the most typical S3 bucket usages - storing images, data lake, hosting websites, ...\r\n\r\n\r\n## S3 buckets - security - 15min\r\nWe'll look at which things we need to enable/disable (public access, MFA deletion, ...). I'll show how to do that with Terraform. We'll also look into leveraging Terraform modules to ensure that all our S3 buckets use secure defaults.\r\n\r\n## Q & A - 5min\r\nQuestions from the audience.", "code": "KBXTNS", "state": "submitted", "created": "2024-12-20", "speaker_names": "Jan Giacomelli", "track": "PyCon: Security"}, {"title": "Introduction to Creating Unit Tests for PySpark Applications Using unittest and pytest Libraries", "abstract": "Software testing, and in particular, unit testing, is a crucial step in modern Data Engineering. Pytest and unittest are great tools for developing unit tests for PySpark applications. In this talk, I will provide code examples using both libraries. Also, I discuss the advantages and disadvantages that each of them brings.", "full_description": "1. Introduction\r\nIn this session, I will talk about how we can create unit tests for PySpark applications using Python unittest and pytest libraries. Before jumping into the topic, it makes sense to talk briefly about what testing is in Software Development, what types of tests exist, what unit testing is and why it is even needed?\r\n\r\n2. What is Testing in Software Development?\r\nTesting in software development refers to the process of evaluating and verifying that a software program or application works as intended. It\u2019s a crucial step in the software development life cycle, ensuring that the software meets specified requirements and is free of defects or bugs that could adversely impact its performance, reliability or security.\r\n\r\nIn this session, I won\u2019t go into details of the benefits of testing and Test Driven Development (TDD). But as a data engineer/data scientist/ML engineer (i.e., data person) you should care about the quality of the software you are building. However, I will still briefly talk about the importance of unit testing below.\r\n\r\n2.1 What are the Test Types?\r\nThere are numerous types of tests in software development, each designed for a specific purpose and stage in the development process. The most commonly implemented test types are:\r\n\r\nUnit Testing: These are the smallest tests which validate that an individual component (e.g., function) of the software works as intended. That\u2019s why it is expected to run the unit tests in the Continuous Integration (CI) pipeline such as after each commit or when a PR is opened depending on your DevOps workflow.\r\n\r\nIntegration Testing: These tests check if multiple components work well together. The number of integration tests is less than unit tests but higher than E2E tests. They also take longer to execute than unit tests but take less time than E2E tests. Integration tests can also be included in the CI pipeline (e.g., when PR opened into develop and/or master). Since the decision of when to run which test and which branch strategy to use are DevOps-related topics, I won\u2019t dive into this topic for now.\r\nEnd-to-End Testing: Checks the application flow from beginning to end making sure that everything works well. This is very costly and most of the time done manually (especially at the beginning of the development) in the data engineering world. It is also possible to automate E2E tests but the execution time will still be much longer than other types of tests. Be aware that, in the Web and Mobile Development contexts E2E testing is different where the goal is to simulate real user actions using various tools.\r\n\r\n2.2 Understanding Unit Testing and Its Importance\r\nUnit testing is one of the foundational aspects of software development. At its core, a unit test is a piece of code written to test a specific function or module in isolation from other parts of the application. The primary goal is to validate each unit (e.g., a function).\r\n\r\n2.3 What are the Purposes of the unittest and pytest Libraries?\r\n\r\nunittest is a built-in Python testing library that follows the xUnit style and provides a test framework along with test discovery.\r\npytest is a popular third-party testing library for Python that offers a flexible and concise way to write tests.\r\n\r\nDemo time.\r\n\r\n\r\n2.4 How Can I Execute These Tests?\r\nDemo time.\r\n\r\n2.5 Summary and Comparison\r\nCompare the advantages and disadvantages of pytest and unittest libraries\r\n\r\n3. Conclusion\r\nThis was an introduction to testing in software development and in particular unit testing for PySpark applications. Of course, there are many more details to be covered such as mocking, pytest plugins, pytest conftest.py which is used for sharing fixtures across multiple files etc\u2026Please let me know if you are interested in more advanced topics and I will try to cover these.\r\n\r\nTo conclude, both libraries are great for developing unit tests for PySpark applications and in general for Python applications. You should choose depending on your needs and familiarity in both tools. If you are just starting with unit testing in Python, I would personally recommend to start with the unittest library first, especially if you already have experience with JUnit. Then move to pytest once you feel more comfortable.", "code": "VUUCMK", "state": "submitted", "created": "2024-12-12", "speaker_names": "Mete Can Akar", "track": "PyData: Data Handling & Engineering"}, {"title": "pytest - simple, rapid and fun testing with Python", "abstract": "The pytest tool offers a rapid and simple way to write tests for your Python code. This training gives an introduction with exercises to some distinguishing features, such as its assertions, marks and fixtures.\r\n\r\nDespite its simplicity, pytest is incredibly flexible and configurable. We'll look at various configuration options as well as the plugin ecosystem around pytest.", "full_description": "- (20 minutes) **pytest feature walkthrough:**\r\n    * Automatic test discovery\r\n    * Assertions without boilerplate via the assert statement\r\n    * Configuration and commandline options\r\n    * Marking and skipping tests\r\n    * Data-driven tests via parametrization\r\n    * Exercises\r\n\r\n- (60 minutes) **pytest fixture mechanism:**\r\n    * Setup and teardown via dependency injection\r\n    * Declaring and using function/module/session scoped fixtures\r\n    * Using fixtures from fixture functions\r\n    * Parametrizing fixtures\r\n    * Looking at useful built-in fixtures (managing temporary files, patching, output capturing)\r\n    * Exercises\r\n\r\n- (10 minutes) **Where to go next:**\r\n    * Useful CLI arguments to deal with failing tests\r\n    * Overview of the plugin ecosystem around pytest", "code": "PDBAXQ", "state": "submitted", "created": "2024-12-01", "speaker_names": "Florian Bruhin", "track": "PyCon: Testing"}, {"title": "Demo-Proof: Building AI Demos that Survive Reality", "abstract": "Demos have a talent for failing at the worst possible moments. Drawing from real-world experience building demos for thousands of developers, I'll show you how to transform fragile AI pipelines into robust teaching tools that just work. Leave with practical patterns and Python tricks that turn demo disasters into learning delights.", "full_description": "Building reliable AI demos is an art that balances wow-factor with actual usefulness. We'll explore battle-tested patterns for graceful degradation, resource management, and interactive learning - all packaged into demos that won't break when you need them most. Through live examples using Gradio and HuggingFace, learn to build demos that both teach and impress, without the \"it worked on my machine\" syndrome.", "code": "USBMXJ", "state": "submitted", "created": "2024-12-03", "speaker_names": "Hila Weisman Zohar", "track": "PyData: Generative AI"}, {"title": "Peering into the Black Box: Developing an NLP Analytics System to Assess Your Engine's Weaknesses", "abstract": "As NLP models become increasingly prevalent in modern applications, the industry faces a critical challenge: how can we effectively evaluate the limitations of black-box NLP systems? This session introduces a model-agnostic evaluation paradigm that automatically generates test datasets enriched  with labeled linguistic challenges for each example. These labels enable precise analysis of failure cases, uncovering critical flaws and actionable insights. We will also present the development of a performance dashboard that visualizes engine capabilities, fostering better collaboration between data science and product teams. Designed for data scientists and product managers, this session offers practical tools to enhance NLP system performance and reliability.", "full_description": "With NLP engines becoming integral to modern applications, the lack of standardized tools to assess their robustness poses significant risks to product reliability and user experience. This session introduces a new approach to evaluating NLP engines that addresses this gap.\r\nBy leveraging the capabilities of large language models (LLMs), we developed an automated method to generate comprehensive test datasets. Each dataset example includes labels pinpointing specific linguistic challenges. Using this Attribute-Aware dataset to evaluate an NLP engine, provides a clearer identification of it\u2019s strengths and weaknesses. This approach uncovered several critical issues in our own parser, providing deep insights into areas requiring improvement.\r\nBeyond dataset generation, we implemented a dashboard that visually maps the engine's performance across various linguistic dimensions. This dashboard streamlines the development process and fosters collaboration between data scientists and product teams, ensuring the NLP engine aligns with real-world product requirements.\r\nOur methodology is model-agnostic, making it applicable across diverse NLP systems. It is particularly valuable for black-box models such as LLMs and generative AI systems, where understanding internal limitations is often challenging. This session is designed to be accessible to a broad audience, including data scientists and product managers, offering practical tools and insights for building more robust NLP applications.\r\nAttendees will leave with an understanding of how to implement these techniques, improve collaboration within their teams, and ensure their NLP engines deliver consistent and reliable results.", "code": "ELNFTD", "state": "submitted", "created": "2024-12-03", "speaker_names": "Oren Matar", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "NeuroAI: Brains, Biology and Cognition", "abstract": "NeuroAI is an emerging area of research into the fundamental principles of cognition and intelligence shared by living organisms and AI.\r\n\r\nThere is still a large conceptual gap between brains and machine learning models. The brain forms abstractions and learns continuously within a single life-time, while machine learning has focused on large data-sets and static function approximation. How can this gap be bridged?\r\n\r\nLearn about the key topics, challenges and intriguing work in the emerging area of NeuroAI (and pickup some ideas about the future of AI along the way!)", "full_description": "NeuroAI is an emerging area of research into the fundamental principles of cognition and intelligence shared by living organisms and AI. Most significant developments in the history of AI have been inspired by biology; there are still many systems and properties of the brain that are yet to be replicated in AI algorithms. There continues to be a large conceptual gap between brains and machine learning models. The brain forms abstractions and learns continuously within a single life-time, while machine learning has focused on large data-sets and static function approximation. How can this gap be bridged?\r\n\r\nDespite the recent commercial success of large language models; these models are still brittle and resource hungry. While creating something with the capabilities of an LLM is impressive, reaching this point required training on dumps of the entire internet and whole libraries of books; even then progress has slowed as the \"fossil fuel\" of text written by humans is exhausted.\r\n\r\nThe bridge between AI and neuroscience is once again a promising path to walk. Understanding and then replicating the brain's ability to learn rapidly, flexibly and to create abstractions is as exciting and important as ever. This talk introduces the field of NeuroAI, discusses the upcoming challenges in the field, explores the intriguing and exciting successes and reflects on how this approach will shape the future of AI.", "code": "VGGYEQ", "state": "submitted", "created": "2024-12-22", "speaker_names": "Andy Kitchen", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Going Global: Taking code from research to operational open ecosystem for AI weather forecasting", "abstract": "When I was hired as a Scientist for Machine Learning, experts said ML would never work in weather forecasting. Nowadays, I get to contribute to Anemoi, a full-featured ML weather forecasting framework used by international weather agencies to research, build, and scale AI weather forecasting models. \r\n\r\nThe project started out as a curiosity by my colleagues and soon scaled as a result of its initial success. As machine learning stories go, this is a story of change, adaptation and making things work. \r\n\r\nIn this talk, I'll share some practical lessons: how we evolved from a mono-package with four people working on it to multiple open-source packages with 40+ internal and external collaborators. Specifically, how we managed the explosion of over 300 config options without losing all of our sanity, building a separation of packages that works for both researchers and operations teams, as well as CI/CD and testing that constrains how many bugs we can introduce in a given day. You'll learn concrete patterns for growing Python packaging for ML systems, and balancing research flexibility with production stability. As a bonus, I'll sprinkle in anecdotes where LLMs like chatGPT and Copilot massively failed at facilitating this evolution.\r\n\r\nJoin me for a deep dive into the real challenges of scaling ML systems - where the weather may be hard to predict, but our code doesn't have to be.", "full_description": "What does it take to go from \"ML will never work in weather forecasting\" to running AI models in production at weather agencies? This talk chronicles the journey of Anemoi, a framework that evolved from research code to an operational ML weather forecasting system - and the technical challenges we faced along the way.\r\n\r\nStarting as experimental code and notebooks by a small team of four, Anemoi grew into a robust ecosystem supporting 40+ developers across multiple international weather agencies. I'll share our experience of scaling both the team and codebase, including the interesting challenge of conducting weekly code tours for new team members while maintaining development velocity.\r\n\r\nThe technical evolution of Anemoi mirrors many challenges in scaling ML systems. We'll explore how the codebase transformed from research artifacts and notebooks into a structured mono-package with proper separation of concerns. Then, how we split this into an ecosystem of specialized packages - only to later realize that some components were too tightly coupled and needed reunification. This journey offers valuable lessons about when to split packages and when to maintain unified codebases. \r\n\r\nConfiguration management evolved alongside our architecture. I'll demonstrate how we leveraged Hydra to tame over 300 configuration options into a hierarchical system that enables component composition without sacrificing usability. This system now powers everything from dataset creation to model inference, with full traceability of configurations and artifacts throughout the ML lifecycle.\r\n\r\nA unique aspect of developing ML systems at ECMWF is integrating with decades of expertise in weather forecast validation. We'll look at how we connected modern ML tooling like MLFlow with traditional meteorological evaluation systems, creating a bridge between ML innovation and established meteorological practices.\r\n\r\nThe talk will cover practical challenges that every growing ML system faces:\r\n\r\n- Making model components truly configurable and replaceable\r\n- Implementing model sharding for global weather predictions\r\n- Supporting flexible grids for regional weather services\r\n- Managing CI/CD across multiple packages\r\n- Streamlining release processes with modern tools\r\n- The eternal struggle with changelog management\r\n\r\nThroughout the presentation, I'll share real examples of what worked, what didn't, and why - including our experiments with AI coding assistants and where they fell short. You'll walk away with concrete patterns for scaling Python ML systems, strategies for managing growing complexity, and insights into balancing research flexibility with production requirements.\r\n\r\nWhether you're scaling an ML system, managing a growing Python codebase, or interested in how weather forecasting is being transformed by AI, this talk offers practical lessons from the frontier of operational ML systems.", "code": "WMBDJ8", "state": "submitted", "created": "2024-12-23", "speaker_names": "Jesper Dramsch", "track": "PyCon: MLOps & DevOps"}, {"title": "A path to AGI: Autonomous, collaborative multi-agents", "abstract": "What if machines could teach one another? \ud83e\udde0\ud83e\udd16 Do you think teamwork is only for humans? \ud83e\udd1d Learn how collaborative multi-agent systems are paving the way for Artificial General Intelligence (AGI) with Arma\u011fan Amcalar. Don't miss this mind-bending journey into the (very near) future of AI! \ud83d\ude80\ud83d\udca1", "full_description": "In this talk, Arma\u011fan Amcalar addresses the transformative journey toward Artificial General Intelligence (AGI) through autonomous, collaborative multi-agent systems. Drawing from his extensive experience in developing and applying these systems, Arma\u011fan will explore how independent agents can work together, learn from each other, and adapt to complex environments to achieve common goals. Participants will gain insights into the latest developments, challenges faced, and future directions in this dynamic field, understanding how collaborative multi-agent systems pave the way for the next generation of intelligent machines. Join us to explore the potential and complexities of these systems in the pursuit of AGI.", "code": "AACFNT", "state": "submitted", "created": "2024-12-20", "speaker_names": "Armagan Amcalar", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Leveling Up Your LinkedIn With NLP", "abstract": "Are you looking to take your LinkedIn engagement to the next level? This research project explores actionable strategies to boost LinkedIn performance, using Python\u2019s Natural Language Toolkit (NLTK) for text analysis and R\u2019s ggplot2 for data visualization. By diving deep into the most frequent words in LinkedIn posts, the analysis uncovers key themes and topics that capture audience attention.  \r\n\r\nA highlight of this study is its comparison of engagement rates for video vs. photo posts, offering insights into which content format resonates more effectively with audiences. Beyond format, the research examines the relationship between impressions (views) and engagement metrics such as likes, comments, and shares. The findings reveal a positive correlation between impressions and engagement, emphasizing the importance of reach in driving interaction.  \r\n\r\nAdditionally, the study identifies weak engagement rates for posts shared in forums, suggesting that tailored, platform-specific content strategies are essential. On the other hand, posts tied to holidays and observances consistently show the highest engagement rates, providing a powerful strategy for individuals and organizations aiming to amplify their visibility.  \r\n\r\nThis session will leave attendees with actionable insights, from understanding text-based trends to using data-driven strategies for content optimization. Learn how to transform your LinkedIn presence with smart, engaging content that connects with your audience.", "full_description": "What Problem Am I Addressing?\r\n\r\nHave you ever wondered why some LinkedIn posts get tons of engagement while others barely receive a like? I wanted to crack the code to discover which kinds of posts perform well on my LinkedIn and how I could increase engagement. While many professionals rely on guesswork to optimize their social media presence, I took a data-driven approach using Python to analyze post performance and uncover actionable insights. Specifically, I examined how engagement rates differ between video posts and photo posts, uncovering trends that can be applied to improve content strategies.  \r\n\r\nWhy Is This Problem Relevant to the Audience?\r\nFor professionals early in their careers or building their personal brand, LinkedIn is a powerful tool for visibility and networking. However, maximizing its potential can be challenging without understanding what drives engagement. This talk will not only help you analyze and decipher trends in your social media performance but also provide you with a replicable framework to improve engagement for your own LinkedIn page. By analyzing content formats like videos vs. photos alongside text-based trends, you\u2019ll learn to make data-backed decisions to elevate your presence.  \r\n\r\nWhat Are Your Solutions to the Problem?\r\n\r\nThrough my analysis, I discovered actionable insights about my LinkedIn content:  \r\n1. Holiday-Centric Posts Perform Well:\r\nPosts tied to holidays see higher engagement due to their timely relevance and universal appeal.  \r\n\r\n2. Video vs. Photo Engagement: Video posts generally drive higher engagement rates compared to photo posts, but there are key nuances to consider in content framing.  \r\n\r\n3. Strategic Categorization of Content: Shifting focus to underutilized but promising content categories can drive better performance.  \r\n\r\n4. Data-Informed Adjustments: Analyzing patterns helps refine how posts are framed and timed to resonate with your audience.  \r\n\r\n---\r\n\r\nWhat Are the Main Takeaways from Your Talk?\r\n\r\nBy the end of this session, attendees will have learned:  \r\n1. How to Define and Analyze Engagement: Understand metrics like impressions and interactions, and how to interpret them in the context of LinkedIn posts.  \r\n\r\n2. How to Use the NLTK Package: Learn to leverage Python\u2019s Natural Language Toolkit (NLTK) for text analysis, including sentiment and content categorization. \r\n \r\n3. Video vs. Photo Content Insights: Analyze and compare engagement rates for videos vs. photo posts to understand which format works best for different audiences.  \r\n\r\n4. Visualizing Success: Explore how to visualize the differences between high-performing and low-performing posts for clearer insights.  \r\n\r\n5. Actionable Insights from Text Analysis: Discover how to frame your content better based on data-driven trends.  \r\n\r\n6. End-to-End Project Setup: Learn how to structure a project using Python, from defining the problem and analyzing data to creating actionable solutions that have real-life impact.  \r\n\r\n---\r\n\r\nThis talk is perfect for those who want to enhance their LinkedIn strategy, explore the intersection of data science and personal branding, or learn how to apply NLP to solve real-world problems. By the end, you\u2019ll walk away equipped to make smarter, data-backed decisions to grow your LinkedIn presence and engagement.", "code": "ACRBBU", "state": "submitted", "created": "2024-12-18", "speaker_names": "Anjola Adeyemo", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Efficient Range Joins in Pandas", "abstract": "Ever wanted to compute a range join in Pandas but find the current implementation memory expensive and generally inefficient? Well, look no more. This talk will show you how to compute range joins in a performant manner in Pandas.", "full_description": "Imagine a manufacturer wishing to minimise the cost of storage while maximising profits (increasing the inventory of the more profitable product, while decreasing the storage for the less profitable product), or a tax audit to find out which employers earn more, but pay less tax. It could be as simple as efficiently finding the range of dates that dates from another dataframe fit into. These are problems that can be solved by range joins. At the moment, the way to solve this in Pandas is with a cartesian join, which can be expensive memory wise, and generally is inefficient. This talk aims to show a better, more efficient way of solving range joins with Pandas.\r\n\r\nThe talk will contain a description of the algorithms implemented, as well as some speed tests with regards to performance.", "code": "BJYCCT", "state": "submitted", "created": "2024-12-22", "speaker_names": "Samuel Oranyeli", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Scaling AI Agents Intelligence with Database Adapters", "abstract": "AI agents, powered by LLMs, are transforming modern applications by not only understanding context but also interacting with systems and files seamlessly. Open-source AI agent frameworks like Ai16z are democratizing access to these capabilities, enabling developers to build intelligent systems that maintain context and interact meaningfully across platforms.\r\n\r\nThis talk is about the pivotal role of vector databases in building scalable, robust memory systems for AI agents. We'll explore how distributed architectures, real-time search, and vector similarity matching empower agents to efficiently store, retrieve, and leverage contextual information across massive datasets with millisecond-level performance., We'll examine how database adapters provide a vital persistence layer for AI agents, enabling them to store relationships, manage memories, and handle complex interactions across multiple systems.", "full_description": "AI agents are becoming fundamental building blocks of modern applications, thanks to their capability of understanding context with the help of the LLMs, but with added capability to interact with systems, files. AI agents that can meaningfully interact across platforms while maintaining context aren't just for large tech companies anymore as we have a number of open source AI Agent frameworks now which developers can leverage. \r\n\r\nBuilding intelligent AI agents requires robust memory systems that can scale. This talk explores how vector databases provide the critical infrastructure for agent memory, enabling efficient storage and retrieval of contextual information. We'll examine how distributed architectures, real-time search capabilities, and vector similarity matching allow agents to maintain context, learn from interactions, and make informed decisions across massive datasets - all while maintaining millisecond-level performance. Moreover Database Adapters provide the persistence layer for AI Agents, enabling storage and retrieval of memories, relationships which are critical components for AI Agents when interacting with multiple systems and for more complex use-cases", "code": "VEELYF", "state": "submitted", "created": "2024-12-22", "speaker_names": "Shivay Lamba, Suvrakamal Das", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building pure Django REST API", "abstract": "The new version of Django has several important features that allow us to avoid installing additional modules. Libraries like DRF, yasg, spectacular have always been recommended for REST API development. Now the rules have changed.", "full_description": "In this workshop: \r\n1. Creating the modern art of project development in Django. (\u00b5-Django style). \r\n2. Adding Built-in serializer tools.\r\n3. JsonResponse, an underrated class in Django for API. \r\n4. Async ORM and Async Class-based-views . The best parts of modern Django.\r\n5. Adding OpenApi Documentation for our views.\r\n\r\nAfter this workshop, all participants got information about the new features of Django.", "code": "FRYF8H", "state": "submitted", "created": "2025-01-05", "speaker_names": "Maxim Danilov", "track": "PyCon: Django & Web"}, {"title": "Cutting the price of Scraping Cloud Costs", "abstract": "A case study of rewriting a simple data pipeline involving Python, a pinch of Go, Git workflows, Airflow, Postgres and Cloud. Investigating some common assumptions and principles of designing data pipelines.\r\nThe benefits and issues with the tools and how these may be handled.\r\nGet answers to the following options and more, as to what is the cheapest and most maintainable solution for this case\u2026\r\n\r\n1. Do the data scraping ourselves OR use an established 3rd party aggregated data source\r\n2. Use temporary embedded DBs OR a cloud master DB server \r\n3. Use a standard ELT pipeline pattern OR do ETL within each pipeline step\r\n4. Implement purely via the pipeline workflow / DAGs and SQL OR create a separate Python package\r\n\r\nI hope this case study of a pipeline rewrite will give you insights that are applicable to Python use for your own data pipelines, and into cloud pricing.", "full_description": "# Cutting the price of  Scraping Cloud Costs\r\n\r\nTalk outline, header lines relate to separate slides.\r\n\r\n### How does cloud pricing work?\r\n\r\nIt is complicated! \u2026 and the price lists are not small.\r\n\r\nThe full price lists for the 3 main CSPs, AWS, Google and Azure together constitute about 5 million prices.\r\n\r\nFor example, hard disk storage cost should be simple?  \r\n\r\nCalculate based on hardware type, size, throughput, iops, different price depending on zone (us-east2 vs ap-south1 etc.), charges for traffic between regions vs within regions Then there are the backup costs based on its retention size/time, schedule etc.\r\n\r\n### The tools and data sources for cloud pricing\r\n\r\nKubecost (now IBM) has open source tools for analyzing real time k8s suppliers costs.\r\n \r\nInfracost.io - builds on kubecost and also adds its own tools to this.\r\n\r\nAll the cloud providers have bulk and query APIs for getting their current pricing.\r\n\r\n### What pricing data does our case study need?\r\n\r\n### The original pipeline\r\n\r\n### Examine the assumptions behind the old pipeline\u2019s architecture\r\n\r\n### Plus/minus of pipeline frameworks -  \r\n\r\n1. Using generic CI/CD pipelines - Github workflows (Codefresh, Jenkins/Artifactory etc)\r\n2. Airflow - the old standard, but what issues does it have?\r\n3. New generation, Dagster or Prefect\r\n\r\n### Create separate click based pkgs for fast dev and test\r\n\r\n### Demo\r\n\r\nA quick run through of the new pipeline for context \r\n\r\n### Performance testing and simplicity of the solution?\r\n\r\nResearch and trial - timings plus costings\r\n\r\n### Pipeline development, deployment and maintenance issues\r\n\r\n### Pricing strategy (or lack of it)\r\n\r\nThe Rule of 40 is a financial metric used to assess the performance of a Software-as-a-Service (SaaS) company - one way to keep above the 40% profit margin is by keeping tighter control of costs.\r\n\r\nSadly a common approach is that engineers / data scientists put together whatever they think is technically the correct solution, without thinking about cost.\r\n\r\nA few months later, the bill starts coming in and management tell everyone to turn off non-prod pipelines, delete dev environments \u2026 stop wasting money. The bill goes down a bit and the process repeats again. \r\n\r\nThere can be more formal processes put in place for cost reduction and control (details of those.)\r\n\r\n\r\n### What is the cost of the different pipeline architectures\r\n\r\nCharges for cloud SaaS are more concealed than raw Cloud PaaS. But it must be assessed too, and they have some interesting pricing. For example if your data pipeline is open source and public, Github workflows are free! If it is private they charge twice as much for compute as using direct Azure. \r\nSimilarly cloud pipeline providers are big in this space, but how does Astronomy SaaS pricing stack up vs. running yourself on cloud PaaS\r\n\r\n### What pipeline design principles can we derive from this case?\r\n\r\n### Conclusion and summing up", "code": "NWPKPL", "state": "submitted", "created": "2024-12-14", "speaker_names": "Ed Crewe", "track": "PyData: Data Handling & Engineering"}, {"title": "No more robots, no more glowing brains. It's time for Better Images of AI.", "abstract": "Do you work in AI? Are you tired of people using images of the terminator, robots, or glowing brains when they use your work? Do they make you uncomfortable but are you having trouble articulating why?\r\n\r\nI have been there.\r\n\r\nIn this talk I will introduce you to the *\"Better Images of AI\"** project. It consists of an image bank of better images, and a guide that explains why those tropes are unhelpful and what you should look for in alternative images.\r\n\r\nAs a bonus, I will also share how I created the images I have contributed to the image bank, using not Stable Diffusion, but good ol' Scikit-Learn!\r\n\r\nAt the end of this talk you will be able to discuss with your peers what's wrong with popular images of AI , where to find better images of AI, and even have some inspiration on how to create your own better images of AI.", "full_description": "\u2022 Spark interest with clich\u00e9 images of AI \r\n\u2022 Introduce the Better Images of AI project \r\n\u2022 Explain the problem\r\n  o Why do we need better images of AI? \r\n  o Who needs better images of AI? \r\n  o What Tropes To Avoid\r\n  o Why are unhelpful images of AI used so much?\r\n\u2022 Provide guidance \r\n  o Guidance for choosing and creating better images of AI \r\n  o Examples of better images of AI\r\n\u2022 Call to Action\r\n  o Stop using bad images of AI and start using better images of AI \r\n  o Contribute to the library, anyone can do it, I did it and so can you", "code": "SJTUQY", "state": "submitted", "created": "2025-01-05", "speaker_names": "Rens Dimmendaal", "track": "General: Ethics & Privacy"}, {"title": "Autonomous Browsing using Large Action Models", "abstract": "The browser serves as our gateway to the internet\u2014the largest repository of knowledge in human history. Proficiency in its use is a core skill across nearly all professions and is becoming increasingly important for Artificial Intelligence. But can Large Action Models (LAMs) autonomously operate a browser? What exactly are LAMs that promise to translate human intentions into actions? We report on a project that fully automates the job application process using AI: from navigating unfamiliar website structures and filling out forms to handling document uploads and cookie banners.", "full_description": "Large Action Models (LAMs) were first introduced by Rabbit with the launch of their R1 device, aiming to create end-to-end trained models that automatically translate human instructions into actions. Since then, the definition of LAMs has evolved to encompass Large Language Models (LLMs) utilized in multi-agent settings. Notable examples include Anthropic's \"Computer Use\" feature in their Claude model and Google's Project Mariner. These projects allow LLMs to operate a web browser or computer in a human-like manner by viewing the screen, moving the cursor, clicking buttons, and typing text, thereby fulfilling the original promise of LAMs by effectively translating human instructions into automated actions.\r\n\r\nWe present an innovative application of LAMs that automates the job application process using AI. Our system autonomously navigates unfamiliar website structures, fills out forms, handles document uploads, and manages cookie banners without human intervention. This level of automation streamlines the application process for job seekers while ensuring accurate and timely submissions.\r\n\r\nTo achieve this, we leveraged the LaVague framework, which employs a modular, agent-based approach:\r\n\r\n1.\t**Coordinator Agent:** A central agent powered by a multimodal model coordinates the entire process. It has access to website visuals, user data (e.g., personal details, CV information), previous instructions, and the overall objective. Based on this information, it delegates tasks to specialized agents.\r\n2.\t**Navigation Control Agent:** For simple website navigation, this agent utilizes a browser driver such as Selenium to directly interact (e.g., scroll) with the webpage.\r\n3.\t**Knowledge Agent:** When additional information is required, this agent performs knowledge-intensive tasks using an LLM. Examples include researching specific details or restructuring CV data.\r\n4.\t**Navigation Engine Agent:** For complex website interactions like inputting values or uploading files, this agent generates custom code for the browser driver. Using an LLM with access to the HTML code, it creates the necessary commands.\r\n\r\nThese agents work iteratively, performing tasks step by step until either the objective is achieved, or a maximum number of steps is reached.\r\n\r\nBy building a custom solution around the LaVague framework tailored specifically for the job application process, we successfully automated the entire workflow. In our presentation, we discuss our overall architecture, the challenges encountered during development and share valuable lessons learned for practical adoption.\r\n\r\nLarge Action Models like these highlight the transformative potential of AI in automating intricate tasks, bridging the gap between understanding human intentions and executing them in dynamic, real-world scenarios.", "code": "HYE8EX", "state": "submitted", "created": "2024-12-20", "speaker_names": "Nico Kreiling, Arne Grobr\u00fcgge", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "PyData: Generative AI - Bridging Data Science and Creative Machine Learning", "abstract": "Python has transcended being just a programming language\u2014it's a vibrant ecosystem that continuously reshapes technological innovation. This talk explores Python's journey, its current landscape, and the powerful interconnections that make it more than just a tool, but a collaborative platform driving global technological advancement.", "full_description": "Dive into the vibrant world of Python's ecosystem\u2014where code meets innovation! Explore how Python has transformed from a simple scripting language to a powerful technological platform driving global change. Uncover the intricate connections between community, technology, and creativity that make Python more than just a programming language. Discover emerging trends, collaborative models, and the future of technological development through the lens of Python's dynamic ecosystem.", "code": "GWASHJ", "state": "submitted", "created": "2024-12-08", "speaker_names": "ABHISHEK SAINI", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Unforgettable, that's what you are: Evaluating Machine Unlearning and Forgetting", "abstract": "Can deep learning/AI models forget? In this talk, you'll explore the realm of machine unlearning, where researchers and practitioners aim to remove memorized examples from machine learning models. This is relevant for training increasingly overparameterized models and growing GDPR/Privacy concerns with large scale model development and use.", "full_description": "Deep learning memorization is a known phenomena, where deep learning / AI models memorize parts of their training dataset. This happens often for repeated examples, novel examples and occurs more often in overparameterized models.\r\n\r\nThis presents problems for guiding machine learning behavior, requiring much effort in guardrails and output monitoring, as well as questioning whether the models can be GDPR-compliant (i.e. the right to be forgotten).\r\n\r\nA growing area of research on machine unlearning or machine forgetting has emerged to investigate ways a model might unlearn or forget particular memorized examples. In this talk, you'll learn about the field of machine unlearning and related topics like data anonymization to evaluate exactly what's truly unforgettable. Jokes aside: you'll have some practical take-aways to apply to your work in data and machine learning development.", "code": "SZFRRA", "state": "confirmed", "created": "2025-01-05", "speaker_names": "Katharine Jarmul", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "From stockouts to happy customers: Proven solutions for time series forecasting in retail", "abstract": "Time series forecasting in the retail industry is uniquely challenging: Datasets often include stockouts that censor actual demand, promotional events cause irregular demand spikes, new product launches face cold-start issues, and diverse demand patterns within an imbalanced product portfolio create modeling challenges.\r\nIn this talk, we\u2019ll explore proven, real-world strategies and examples to address these problems. Learn how to successfully handle censored demand caused by stockouts, effectively incorporate promotional effects, and tackle the variability of diverse products using clustering and ensembling strategies. Whether you\u2019re a seasoned data scientist or a Python developer exploring forecasting, the goal of this session is to introduce you to the key challenges in retail forecasting and equip you with actionable insights to successfully overcome them in real-life scenarios.", "full_description": "Retail time series forecasting is uniquely challenging: stockouts censor true demand, promotions cause irregular demand spikes, cold-start products lack historical data, and diverse product portfolios introduce modeling complexities. These challenges can lead to inefficiencies such as over- or understocking in the warehouses and therefore also to dissatisfied customers. This talk explores proven strategies to tackle these issues and deliver actionable insights.\r\n\r\nLearn how to handle constrained demand caused by stockouts both with adequate imputation as well as machine learning strategies, incorporate promotional effects with suitable feature engineering techniques that also help in cases of incomplete promotional data, predict demand for new products using transfer learning and also discover how ensembling strategies and clustering can simplify forecasting for diverse, imbalanced datasets.\r\n\r\nWe\u2019ll also highlight tools like statsforecast, neuralforecast, scikit-learn and our AutoML framework with a strong stacking ensembling mechanism in it's core. Whether you\u2019re a seasoned data scientist or a Python developer exploring forecasting, the goal of this session is to introduce you to the key challenges in retail forecasting and equip you with actionable insights to successfully overcome them in real-life scenarios.", "code": "QN3BTA", "state": "submitted", "created": "2025-01-05", "speaker_names": "Robert Haase", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "sieves: A Toolkit for Prototyping NLP Tasks With Structured Generation", "abstract": "While LLMs steal all the thunder these days, classic NLP tasks - classification, NER, etc. - remain ubiquitous. Whether or not generative AI is involved, decomposing NLP applications into a series of modular, pipelined tasks is a widely used and effective software engineering pattern.\r\n\r\nGenerative and predictive zero-/few-shot (ZFS) models complement this approach by enabling rapid prototyping of NLP tasks with little to no data. However, they come with their own challenges:\r\n\r\n- **Generative models** CAN struggle to produce outputs in the required format, leading to the need for *structured generation*\u2014a set of methods designed to enforce or fine-tune generative models to precisely match the desired output structure.\r\n- **Predictive ZFS models**, while task-specific, typically lack generalization across different tasks.\r\n\r\nWhat does that mean for your friendly neighbourhood ML engineer, looking for the best way to quickly prototype a NLP pipeline? Structured generation remains an active area of research, and existing tools have varying strengths and limitations. Switching between them is not always straightforward, making the choice potentially a sticky one. Ideally, engineers need a flexible solution that supports rapid experimentation with minimal setup and sensible defaults to get started quickly.\r\n\r\nCue `sieves`: a toolkit for rapid prototyping of NLP tasks with ZFS models. It provides a unified interface for a variety of models, tasks, and methods for structured output generati", "full_description": "In NLP, implementing pipelines of established tasks is a common practice. Prototypes that require little to no data are often very valuable during early system development, as fully supervised models typically demand extensive annotated datasets. Generative and predictive zero-/few-shot learning models address this limitation by eliminating the immediate need for full annotation, enabling a \u201cwarm start\u201d over a \u201ccold start\u201d with fully supervised models. This approach accelerates development and enables faster feedback loops.\r\n\r\nDespite the richness of the open NLP ecosystem, there currently is no unified toolkit supporting rapid task prototyping without tight coupling to a specific (1) model, (2) structured generation library (e.g. https://github.com/stanfordnlp/dspy, https://github.com/dottxt-ai/outlines, https://github.com/1rgs/jsonformer, https://github.com/BoundaryML/baml) or (3) task. All three components should be as interchangeable as possible to improve on the developer experience and efficacy. `sieves` aims to fill this gap. \r\n\r\n### Agenda\r\n\r\nThis talk will cover the following topics:\r\n\r\n1. **The Value of Pipelines and Task Isolation**: Exploring their usefulness as abstractions, even when using generative models.\r\n2. **Warm Starts With Prototyping**: How to iterate effectively without relying on a training set.\r\n3. **Overview of Structured Generation Tools**: A summary of the current landscape in structured generation.\r\n4. **Introducing `sieves`**: Bringing it all together with pipelines, warm starts/prototyping, and structured generation.\r\n\r\nThe largest part of the presentation will focus on `sieves`, its core concepts, and its benefits:\r\n\r\n- **Loose coupling**: Decoupling models, tasks, and structured generation backends for flexibility.\r\n- **Unified format**: Standardized input/output formats across models, tasks, and structured generation backends.\r\n- **Generative and predictive ZFS support**: Compatibility with both approaches for versatility.\r\n- **Pipeline architecture**: Designed for maintainability and easy debugging.\r\n- **Comprehensive task support**: Prebuilt tasks for ZFS regimes, enabling single-line execution for tasks such as:\r\n    - Classification, NER, entity linking, REL, translation, etc.\r\n- **Auxiliary component integration**: Support for essential tools often needed in practical NLP workflows.\r\n- **Customizability**: Lightweight encapsulation of tasks, allowing users to easily customize or add new tasks.\r\n- **Supervised training support**: Enables compilation and export of training, validation, and test datasets in popular formats like Hugging Face\u2019s `datasets`.\r\n\r\n### Takeaways\r\n\r\nBy the end of this talk, attendees will understand:\r\n\r\n1. **The Benefits of Pipelines and Prototyping For Warm Starts**: How pipelines and warm starts - leveraging prototypes with ZFS techniques - can speed up the development of NLP applications, also when using generative models.\r\n2. **The Role of Structured Generation**: Insights into the current state of structured generation for text models and its impact on task implementation.\r\n3. **Contributions of `sieves`**: The place of `sieve` in the NLP ecosystem and how it contributes to a smoother and more productive developer experience in building out NLP applications by simplifying prototyping and accelerating development.", "code": "YTBGHG", "state": "submitted", "created": "2024-12-22", "speaker_names": "Raphael Mitsch", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Disability Inclusion is Us: Building Inclusive Open Source Communities Through Intentional Action", "abstract": "In today's world, where disability affects a significant percentage of the population, it is crucial for open-source communities to address the challenges faced by persons with disabilities (PWDs) and work towards their inclusion. This talk will delve into practical measures such as referral programs, internal disability disclosures, and integrating disability into existing agendas rather than treating it as a separate issue. We will dive into disability mainstreaming with a focus on its role in promoting universal design and inclusivity. Attendees will gain insights into establishing disability mainstreaming committees, formulating action plans, implementing best practices, and monitoring and evaluating progress.", "full_description": "Persons with Disabilities face challenges that often go unnoticed in our Open Source Communities.This talk explores the challenges faced by persons with disabilities (PWDs) and presents effective strategies to foster disability inclusion within open-source communities. Globally, disability affects a significant portion of the population, resulting in high levels of unemployment and low community participation of PWDs. PWDs often find themselves limited to low-paying entry-level jobs, perpetuating inequality and poverty. Women with disabilities and individuals with intellectual disabilities, or multiple disabilities face even greater employment disparities. By giving them a chance as Open Source Communities we will be opening them up to several opportunities.\r\n\r\nThe talk will present a range of effective strategies, initiatives, and action plans to foster disability inclusion within open-source communities. It will discuss the importance of transformational leadership that questions existing systems, paradigms, and assumptions to drive change. Additionally, creating a compelling vision for social change will be explored, emphasizing the need to address social inequalities in Open Source Communities.\r\n\r\nWe will delve into practical measures and solutions to this, listed below:\r\n- How to set-up disability inclusion committees in our open source communities, who should be a member and how to sustain them\r\n- Coming up with internal referral programs and a streamlined disability disclosure system within the community.\r\n- Drafting a Disability Mainstreaming Action plan, its contents and how to customize it to suit your community\r\n- Effectively using quarterly reports to show progress on specified Disability Mainstreaming Indicators.\r\n\r\nBy the end of the talk, attendees will leave with actionable strategies to foster disability inclusion within their open-source communities. The aim is to create an environment that is more inclusive, welcoming, and empowering for persons with disabilities, ultimately enriching the open-source movement as a whole.", "code": "BPRSJL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Brayan Kai Mwanyumba", "track": "General: Community & Diversity"}, {"title": "Building Interactive Data Applications with Taipy: From Prototype to Production", "abstract": "Are you tired of using static notebooks for your data analysis? This practical guide demonstrates how to turn Python data operations into well-designed web apps that people want to use. We'll use Taipy to bridge the gap between web development and data research while maintaining the familiar Python ecosystem. Attendees will discover how to create apps that are ready for production, manage real-time data updates, and create responsive user interfaces.", "full_description": "Transform your data workflows into interactive web applications using Taipy. Learn how to build, customize, and deploy data-driven applications that bridge the gap between data science and web development, all without leaving the Python ecosystem.\r\n\r\nIn this hands-on tutorial, attendees will learn how to transform data analysis workflows into production-ready web applications using Taipy. Moving beyond static notebooks and basic dashboards, we'll explore how to build interactive, data-driven applications that can handle real-world scenarios.\r\n\r\nStarting with the basics, we'll progressively build towards complex applications, covering everything from data pipelines to interactive visualizations. Attendees will learn how to create responsive user interfaces, handle real-time data updates, and implement best practices for production deployment.", "code": "JN9SEV", "state": "submitted", "created": "2024-12-22", "speaker_names": "Neeraj Pandey", "track": "PyCon: Django & Web"}, {"title": "How my ADHD made me a better engineering manager.", "abstract": "How on earth can you manage the lives of other people if you struggle managing your own? With this logic I convinced myself for so much of my career that management wasn\u2019t for me. Here\u2019s why I was wrong, and why tech needs more neurodivergent people leading teams.", "full_description": "How on earth can you manage the lives of other people if you struggle managing your own? With that logic, I ruled myself out of management for years. Then I took a leap and became an engineering manager. And you know what? I realised I was good at it. And whilst my ADHD will always ensure I play life on hard-mode (and management is no different), I was surprised at the ways my past experiences, my quirks, my traumas and all the eccentricities bundled into my messy brain were in fact the reasons I succeeded as a manager. \r\n\r\nIn this whirlwind tour of my three years so far as an EM, I\u2019ll knit together a series of lessons I learnt (sometimes very difficult ones), I\u2019ll sprinkle a dusting of practical advice you won\u2019t find in neurotypical management books, and I\u2019ll cover a few of the areas and weaknesses in which I still struggle and how I\u2019m working on them.\r\n\r\nDon\u2019t worry, this won\u2019t be one of those \u201cADHD can be your superpower\u201d talks. But I hope my experiences as a neurodivergent Engineering Manager during one of the most turbulent periods in tech in recent history will at least banish the thoughts in your head that you could never do it. And that your own experiences, far from hindering you, will in fact likely give you the empathy and understanding you need to lead a fulfilled and effective team.", "code": "KVQCHR", "state": "submitted", "created": "2025-01-05", "speaker_names": "Chris Ward", "track": "General: Community & Diversity"}, {"title": "The Art of API Versioning: Ensuring Evolution Without Chaos", "abstract": "As APIs evolve to meet the needs of growing user bases, managing versioning effectively becomes a crucial challenge. Poor versioning strategies, such as duplicating entire applications for every iteration, can lead to significant maintenance burdens and scalability issues. This talk explores the art of API versioning, highlighting why it matters and how to balance flexibility and stability.", "full_description": "API versioning is at the heart of maintaining compatibility while enabling evolution, but it\u2019s often a source of frustration for developers and users alike. This session dives into the importance of API versioning, the challenges it presents, and best practices for addressing those challenges. Attendees will explore the drawbacks of \u201cversioning by suffering,\u201d where entire applications or endpoints are duplicated for each version, and discover scalable alternatives like schema-only migrations. \r\n\r\nThe talk also examines the principles of Conway\u2019s Law, showing how organizational structure influences API design and how reversing the process can lead to cleaner, more coherent systems. Through real-world examples and actionable insights, this talk provides the tools necessary to design APIs that grow with confidence and clarity, ensuring long-term stability and user satisfaction.", "code": "JG7H8C", "state": "submitted", "created": "2024-12-24", "speaker_names": "Syed Ansab Waqar Gillani, Syed Hassan Gilani", "track": "PyCon: Programming & Software Engineering"}, {"title": "Building a Self-Hosted MLOps Platform with Kubernetes", "abstract": "Many managed MLOps platforms, while convenient, often fall short in providing flexibility, requiring complex integrations, and causing vendor lock-in. In this talk, we\u2019ll share our experience transitioning from managed MLOps tools to a self-hosted solution built on Kubernetes. We\u2019ll focus on how we leveraged open-source tools like Feast, MLflow, and Ray to build a more flexible, scalable, and customizable platform that is now in use at Rewe Digital. By migrating to this self-hosted architecture, we gained greater control over our ML pipelines, reduced our dependency on third-party services, and created a more adaptable infrastructure for our ML workloads.", "full_description": "Many managed MLOps platforms, while convenient, often fall short in providing flexibility, requiring complex integrations, and causing vendor lock-in. In this talk, we\u2019ll share our experience transitioning from managed MLOps tools to a self-hosted solution built on Kubernetes. We\u2019ll focus on how we leveraged open-source tools like Feast, MLflow, and Ray to build a more flexible, scalable, and customizable platform that is now in use at Rewe Digital. By migrating to this self-hosted architecture, we gained greater control over our ML pipelines, reduced our dependency on third-party services, and created a more adaptable infrastructure for our ML workloads.\r\n\r\nTalk Outline: \r\n\r\n1. Introduction (5 minutes):\r\n- The challenges of using managed MLOps platforms: vendor lock-in, integration complexity, and lack of flexibility.\r\n- Why transitioning to a self-hosted solution on Kubernetes can be beneficial.\r\n\r\n2. Proposed Solution (10 minutes):\r\n- Why Kubernetes for MLOps?\r\n- How open-source tools like Feast, MLflow, and Ray come together to form the core of a robust self-hosted MLOps stack.\r\n- Benefits of building a flexible, scalable platform that fits your needs.\r\n\r\n3. Building the Platform (10 minutes):\r\n- Practical steps for setting up and configuring Feast, MLflow, and Ray on Kubernetes.\r\n- Integration strategies and how to manage pipelines, model tracking, and feature storage.\r\n\r\n4. Lessons Learned and Q&A (5 minutes):\r\n- Challenges and takeaways during the migration process\r\n- Q&A", "code": "3CYZUH", "state": "submitted", "created": "2024-12-22", "speaker_names": "Josef Nagelschmidt", "track": "PyCon: MLOps & DevOps"}, {"title": "Are LLMs the answer to all our problems?", "abstract": "Generative AI models have shaken up the German market. Since the release of ChatGPT, AI is available and usable for everyone. The number of ChatGPT-based agents is growing rapidly, but concerns about privacy, copyright and ethics remain. Regulation and ethical AI go hand in hand, but are often seen as barriers. The presentation will cover the different aspects of ethics and how they are addressed by regulation. It will give an overview of how to use large language models in a safe and practical way. This won't only address the various ethical issues, but also convince your next customer to invest in your AI-based product.", "full_description": "The talk will delve into the complexities of large language models (LLMs), exploring their capabilities and challenges. We'll look at bias in face recognition and word2vec, highlighting cases such as the COMPAS system and Amazon's recruitment tool, which have raised concerns about fairness and accuracy. The intersection of LLM and copyright will also be discussed, including the use of copyrighted material in training data and potential infringement issues.  When talking about data, regulations such as the EU AI Act, the CLOUD Act and data privacy will be examined, raising important questions about data sovereignty and cross-border data transfers. \r\n\r\nThe environmental impact of LLMs will be addressed, focusing on their significant carbon footprint and the need for sustainable solutions. An overview of the LLM landscape will be provided, including English models and European alternatives. By exploring these topics, participants will gain a deeper understanding of the opportunities and challenges presented by LLMs, as well as the regulatory frameworks and best practices that can help mitigate their risks.", "code": "EN3QPQ", "state": "submitted", "created": "2024-12-17", "speaker_names": "Dr. Maria B\u00f6rner", "track": "General: Ethics & Privacy"}, {"title": "Turning the web into page(s)", "abstract": "Did you know that HTML and CSS are very useful tools for generating PDF documents? \ud83e\udd2f\r\n\r\nWhen we talk about document layout, such as invoices, letters, reports, and the like, we often think of Word, LibreOffice, LaTeX and so on.\r\n\r\nWe often think of all the little problems of adjusting that address that's longer than usual, that logo that should be repeated on every page, that table that should be cut here but not there\u2026\r\n\r\nIt would be so handy to finally have a solid template and be able to automate all this instead of wasting time dealing with these problems.\r\n\r\nAre you tired of laying out your documents by hand? Managing invoices, letters, reports\u2026 with that \ud83e\udd2c Word/LibreOffice template you'll still have to tweak? Maybe you've tried LaTeX but it's still complex and inflexible? Come and discover how web technologies can simplify your life and be used to generate PDF documents!", "full_description": "During this tutorial, we'll present how web technologies (HTML/CSS) are adapted to automatic document generation.\r\n\r\nThen we'll show examples of documents we've done with HTML and CSS, along with CSS tips.\r\n\r\nFinally, we'll guide participants through the layout of an HTML document to generate a PDF version.\r\n\r\nFor this tutorial, we'll be using the WeasyPrint Python library, which transforms HTML/CSS documents into PDFs. (Note: we\u2019re the maintainers of this library)\r\n\r\nPrerequisite:\r\n* WeasyPrint https://weasyprint.org/ installed (we can help you during the workshop)\r\n* Basic knowledge of HTML and CSS", "code": "K7F7WV", "state": "submitted", "created": "2024-12-17", "speaker_names": "Lucie Anglade, Guillaume Ayoub", "track": "General: Others"}, {"title": "How to use Data Science Superpowers in real life, a Bayesian perspective", "abstract": "In the data science field, we use all these powerful methods to solve important problems. Most of the time, we do this very well because our data science and machine-learning toolbox fits the problems we tackle quite precisely. Yet, what about our everyday choices or even our most important life decisions? Can we use for our private lives what we advocate for in our jobs or are these choices inherently different?\r\nMany of this real life decisions are a little different than textbook machine-learning problems. There is often less or hard-to-come-by data and the decisions are infrequent, but sometimes very consequential. This talk will dive into what makes everyday decisions difficult to handle with our data science toolbox. It will show how Bayesian thinking can help to reason in such cases, especially when there is not a lot of data to rely on.", "full_description": "In this talk, I want to have a look on decision making from a slightly different angle. In a world that produces an ever growing amount of data in every domain, data scientists can shine with their tools to make data-driven decisions. Often there is even too much data and the most tedious part of the work is to remove the noise from the signal with clever feature engineering. Though the world gets covered more and more by big data, this development is not distributed evenly.\r\n\r\nLots of decisions we need to make in real life do not follow this pattern. In fact, there are often surprisingly few data points that help us here. Yet, are there fundamental differences between everyday decisions and the type of decisions we automate so well with machine learning in our jobs? In this part of the talk, I will attempt a characterisation of both types of decisions. We will have a closer look at what implicit assumptions we make to use our machine learning toolbox. After this we might get a first explanation why these tools might be unsuited to answer questions like \u2019how longe should I study for an exam\u2019 or \u201a\u2019should I  accept this new job or not\u2019.\r\n\r\nEnter Bayesian statistics: This part of the talk will introduce Bayesian statistics for beginners using simple examples and images. It will highlight the benefits of the method when we are short of data but have some additional experience not encoded in the data. I will show how in these circumstances prior distributions come in really handy.\r\n\r\nAfter laying the groundwork on Bayesian methods we will circle back to the everyday decisions and see how well both things fit together. On a higher level, this will show what makes problems in decision making a great fit for Bayesian methods. I will introduce this using a practical example. The example will deal with the decision how long one should study for a test or exam. Taking a step-by-step approach, we explore how this decision can be informed with just a few data points. Set aside finding the key to successful exam preparation, the example is also helpful to see some of the basics for working with the pymc library.\r\n\r\nThe talk will end with some more general thoughts. This will answer where to go from here and for which decisions a thorough investigation like the presented one is worthwhile.,Yet, once one is familiar with the basics of Bayesian thinking, there might be shortcuts. I will show that we can use the principles as a great tool to improve discussions about important decisions on a broader scale.", "code": "RLTZTC", "state": "submitted", "created": "2024-12-17", "speaker_names": "Tim Lenzen", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Teamwork makes the dream work", "abstract": "Ask any successful software developer - it takes a lot more than technical skills to win at this game. Software development is a team sport.\r\n\r\nWhat does good teamwork mean to you? Is it about empathy? Making sure voices are heard? Clear communication? Mutual respect? Well...yes.\r\n\r\nBut there is more to it than that.  A lot more.\r\n\r\nWhen it comes to software development, a lot of these fluffy definitions of good teamwork can be implemented in very concrete ways. \r\n\r\nThis talk will cover a lot of the \"soft\" skills that come into play in software development, as well as the harder skills that come into play (eg testing and proper use of Git). These are a few of the skills that will make people want to work with you again and again.  \r\n\r\nPrepare yourself for ball-sport analogies.", "full_description": "We'll be talking about teamwork. First of all,  I'll talk a bit about why teamwork is the norm and why it is worth paying attention to very closely in software development.\r\n\r\nI'll then talk about some forms of waste that can come into play, with reference to principles borrowed from value stream mapping and agile.  \r\n\r\nAll the while I'll be talking about specific behaviours and how they are helpful.  We'll touch on things like:\r\n\r\n- clear communication and asking for help\r\n- why testing is good teamwork \r\n- fast feedback cycles\r\n- prioritisation heuristics \r\n- ball-sport analogies o.O", "code": "FFKT9X", "state": "submitted", "created": "2024-12-13", "speaker_names": "Sheena", "track": "General: Education, Career & Life"}, {"title": "From Documents To Data: Automating Document Digitalization with AI and Python", "abstract": "Digitalizing paper documents to structured data is a common challenge in not only data analysis but also business activities, especially when the volume of documents is large or when one encounters documents with diverse formats. In this proposal, we will share insights from our use case of menu digitalization, where we leverage Google DocumentAI to extract key information such as names, prices, and descriptions from menus. Following this, we enhance the digitalized content, including grouping extracted items, using Gemini. This approach streamlines the menu digitalization process, enabling automation that delivers efficient and accurate outputs for end users. \r\n\r\nWe will explore the technical implementation of this workflow, the challenges we faced, and how we addressed them using Python-based tools and frameworks. We hope to provide audiences practical knowledge about applying similar techniques to their own document-processing needs, building up end-to-end solutions. This talk aims to provide valuable insights into integrating AI-powered tools to solve real-world business challenges effectively.", "full_description": "Digitalizing paper documents is a common challenge in both data analysis and business operations. In this session, we will dive deep into a real-world use case of menu digitalization, where we encounter files with large volumes and diverse formats. These features lead to significantly increased complexity, especially when we aim at returning structured data, if we use traditional text extraction methods. We present an end-to-end solution that is not only suitable for batch process, but also for individual usage from end users. \r\n\r\nIn our solution, we leverage Google DocumentAI to extract critical information from menus, including menu item names, prices, volumes, descriptions and allergy info. The response from DocumentAI can be easily transferred into relational data. Beyond extraction, we employ Gemini to enhance the digitalized content by grouping the extracted data. This enables us to gain valuable info beyond menu itself for seamless usability. \r\n\r\nThe session will provide a step-by-step walkthrough of the entire workflow, highlighting how Python-based tools and frameworks power the implementation. We will discuss: \r\n1. How to set up and use Google DocumentAI for document data extraction. \r\n2. Techniques to clean, structure, and validate the extracted data for accuracy and usability. \r\n3. How Gemini contributes to grouping and enhancing the digitalized content, making it ready for applications such as POS (Point of Sale) onboarding. \r\n\r\nWe will also share the challenges faced during the process, such as handling inconsistent document formats and ensuring reliable outputs, along with the strategies we used to overcome them. The talk will include practical demonstrations of key Python libraries and tools used to build this end-to-end solution, ensuring attendees gain actionable knowledge to tackle their own document-processing challenges. \r\nBy the end of this session, we hope that attendees will: \r\n1. Understand how to integrate AI-powered document processing tools into their workflows. \r\n2. Gain insights into building scalable, automated pipelines for efficient and accurate data extraction and enhancement.", "code": "7V8PMR", "state": "submitted", "created": "2024-12-20", "speaker_names": "Vu Tran, Yiqiu Chen", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Data Warehouses Meet Data Lakes", "abstract": "Many organizations have migrated their data warehouses to datalake solutions in recent years.\r\nWith the convergence of the data warehouse and the data lake, a new data management paradigm has emerged that combines the best of 2 approaches: the botton-up of big data and the top-down of a classic data warehouse.", "full_description": "In this talk, I will explain the current challenges of a datalake and how we can approach a \r\nmoderm data architecture with the help of pyspark, hudi, delta.io or iceberg.\r\nWe will see how organize data in a data lake to support real-time processing of applications \r\nand analyzes across all varieties of data sets, structured and unstructured, how provides \r\nthe scale needed to support enterprise-wide digital transformation and creates one unique source of data \r\nfor multiple audiences.", "code": "XX9WK9", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Data Handling & Engineering"}, {"title": "Introduction to Machine Learning with Python", "abstract": "Machine learning can seem daunting to beginners, but Python makes it accessible to everyone. This talk introduces attendees to the core concepts of machine learning, the Python libraries commonly used, and a hands-on walkthrough of building a simple ML model. By the end of this session, attendees will feel confident taking their first steps into the exciting world of machine learning.", "full_description": "Many beginners find it challenging to understand the fundamentals of machine learning or how to get started with Python.\r\nPython\u2019s ML ecosystem is vast and beginner-friendly, making it ideal for those new to the field.\r\nThis session will provide:\r\nA gentle introduction to ML concepts like supervised learning, unsupervised learning, and model evaluation.\r\nDemonstrations using popular Python libraries such as scikit-learn and pandas.\r\nKey Takeaways:\r\nUnderstanding basic ML workflows.\r\nLearning how to preprocess data and train a simple model.\r\nAccessing resources for further exploration.", "code": "XGNQX7", "state": "rejected", "created": "2024-12-22", "speaker_names": "Olaleye Aanuoluwapo Kayode", "track": "General: Education, Career & Life"}, {"title": "Building Serverless Python AI skills as WASM components", "abstract": "Frameworks like `llama-stack` and `langchain` allow for quick prototyping of generative AI applications. However, companies often struggle to deploy these applications into production quickly. This talk explores the design of a Python SDK that enables the development of AI skills in Python and their compilation into WebAssembly (WASM) components, targeting a specific host runtime that offers interfaces for interacting with LLMs and associated tooling.", "full_description": "Why do companies struggle so hard to get their AI skills into production quickly?\r\n\r\nThis talk is about building an SDK that enables the development of production-ready AI skills in Python that can be run as serverless functions within a WASM runtime and interact with LLMs via a WIT (WASM Interface Type) world. On a less technical note, we will explore the design of an SDK that offers a streamlined development experience for AI skills.\r\n\r\nWe will explore the implications for topics such as testability, traceability, and the evaluation of AI logic. How can software engineering best practices, such as separation of concerns and modularity, be applied to the design of AI applications?\r\n\r\nFred Brooks' excellent essay, _No Silver Bullet,_ distinguishes between accidental complexity and essential complexity. This talk will explore how an SDK for AI skills can reduce accidental complexity during development and deployment, providing developers with a focused environment for innovating prompts and retrieval strategies.\r\n\r\nHow can a WIT that supports running AI applications be designed, and how can bindings to such a WIT world be generated and consumed in a Python module? We will examine abstractions that allow local testing and debugging without the compilation step by encapsulating the WIT host interface behind a `Protocol`.\r\n\r\nThe talk also covers benefits of running AI skills as WASM components: When compiling a Python module to a component, the Python interpreter is part of the compiled component. Although this results in longer start-up times compared to components written in compiled languages like Rust, it provides a key advantage: The interpreter can securely execute Python code generated by an LLM within a highly restricted environment, ensuring no network or file system access.\r\n\r\nKey takeaways include developing a foundational understanding of WASM and WIT, and how they can interface with Python. You will gain insights into the challenges of deploying AI skills into production and discover how testing, tracing, and evaluation can simplify this process.", "code": "X9UA7Q", "state": "submitted", "created": "2024-12-21", "speaker_names": "Moritz Althaus", "track": "PyData: Generative AI"}, {"title": "How to interact with PostgreSQL data using natural language", "abstract": "Data-NL is a data-driven project designed to interact with data available in a PostgreSQL database, using natural language. Utilizing FastAPI for the backend, it integrates with Llama Index SQLDatabase to interact with the PostgreSQL database and integrates with OpenAI's API to generate insights and responses based on user prompts. The project is containerized using Docker, ensuring easy deployment and scalability. \r\n\r\nWith a focus on providing valuable insights, Data-NL aims to support research and decision-making processes across multiple fields.", "full_description": "Data-NL is an innovative project designed to interact with data using natural language across various domains. Below is a detailed outline:\r\n\r\n* Introduction\r\n\r\n    * Objectives and goals\r\n\r\n        * Natural Language\r\n        * Data Analysis\r\n        * User Prompt Responses\r\n    \r\n* Architecture\r\n\r\n    * Backend: FastAPI \r\n    * Database: PostgreSQL\r\n    * Integration\r\n\r\n        * Llama Index\r\n        * OpenAI API\r\n    \r\n    * Containerization: Docker\r\n\r\n        * Scalability\r\n        * Ease of Deployment\r\n\r\n* Use Cases\r\n\r\n    * Research Support\r\n    * Decision-Making Processes\r\n    * Cross-Domain Applications\r\n\r\n* Future Enhancements\r\n\r\n    * Additional Data Sources\r\n    * Enhanced Analytics\r\n    * Expanded Use Cases", "code": "UK9ERD", "state": "submitted", "created": "2024-12-19", "speaker_names": "Jo\u00e3o Gomes", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Topological data analysis: How to quantify \"holes\" in your data and why?", "abstract": "Do you need to compare sets of points in a plane? Identify a potential cyclic event in high-dimensional time series data? Find the second or the third highest peak of a noisily sampled function? Topological data analysis (TDA) is not a universal hammer, but it might just be the 16 mm wrench for your 16 mm hex head bolt. There is no shortage of Python libraries implementing TDA methods for various settings, but navigating the options can be challanging without prior familiarity with the topic. In my talk I will demonstrate the utility of the tool with several simple examples, list various libraries used by the TDA community, and dive a bit deeper into the methods to explain what the libraries implement and how to interpret and work with the outputs.", "full_description": "For specific tasks, topological data analysis can be a more rigid, straightforward and interpretable alternative to complicated machine learning pipelines. However, it is not so widely known and can be intimidating to get into when starting from zero. The goal of this talk is to introduce persistent homology, the main tool of topological data analysis, show concrete examples of how to apply it using available Python libraries, and reveal more details about what is going on \"under the hood\", which is important to correctly utilize the methods. I will start with several examples showcasing the possible uses of persistent homology and how to establish an analysis pipeline in Python. Then I will describe more about different variants within such a pipeline, like a choice of a filtered complex or vectorization, and their advantages and disadvantages.", "code": "HQWAYP", "state": "submitted", "created": "2024-12-22", "speaker_names": "Ondrej Draganov", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "What we talk about when we talk about AI skills.", "abstract": "Defining what constitutes AI skills has always been ambiguous. As AI adoption accelerates across industries and the European AI Act mandates companies to ensure AI literacy among their staff, organizations face growing even more challenges in defining and developing AI competencies. In this talk, we'll present a comprehensive framework developed by the appliedAI Institute's experts that categorizes AI skills across technical, regulatory, strategic, and innovation domains. We'll also share initial data on current AI skills levels and upskilling needs and provide practical strategies for organizations to assess, develop, and acquire the AI capabilities required for their specific needs.", "full_description": "What it means to \"work in/with AI\" and the corresponding roles, tasks, and required skills have been ambiguous since the emergence of AI professionals in industry. And while the demand for AI-skilled professionals continues to grow, both organizations and individuals seeking to work in the AI field often struggle with two challenges: 1) clearly defining the competencies and responsibilities that positions and projects require, and 2) identifying appropriate upskilling opportunities to match these needs.\r\nThe urgency to upskill professionals in AI topics has not only become more nuanced since the emergence of generative AI but is also growing rapidly. This trend is further amplified by the upcoming European AI Act, which will soon require companies to \"ensure, to their best extent, a sufficient level of AI literacy among their staff.\" This regulation has created an urgent need to define and understand what constitutes AI literacy and AI skills in practical terms.\r\n\r\nTo help organizations and professionals navigate this landscape, we have developed a comprehensive framework categorizing AI skills into distinct domains spanning technical competencies, regulatory knowledge, AI strategy, and ecosystem understanding. Our framework, developed by the multidisciplinary team of AI experts in the appliedAI Institute for Europe, provides a structured approach to defining skill requirements, guiding career development, identifying training gaps, and helping educational providers align their offerings with market demands.\r\n\r\nIn this presentation, we will introduce our framework and share initial data reflecting the current state of AI skills levels and upskilling needs across a sample of companies. We will also discuss practical strategies for implementing this AI Skills framework within organizations, enabling them to better assess, develop, and acquire the AI capabilities they need to fulfill their specific needs.", "code": "98FQDY", "state": "submitted", "created": "2025-01-05", "speaker_names": "Paula Gonzalez Avalos", "track": "General: Education, Career & Life"}, {"title": "Unlocking the Predictive Power of Relational Data with Automated Feature Engineering", "abstract": "Relational data can be a goldmine for classical Machine Learning applications \u2014 yet extracting useful features from multiple tables, time windows, and primary-foreign key relationships is notoriously difficult. In this code tutorial, we\u2019ll use the H&M Fashion dataset to demonstrate how\u00a0getML\u00a0FastProp automates feature engineering for both classification (churn prediction) and regression (sales prediction) with minimal manual effort, outperforming both\u00a0Relational Deep Learning\u00a0and a skilled\u00a0human data scientist according to the RelBench leaderboard.\r\n\r\nThis code tutorial is perfect for data scientists looking to leverage their relational and time-series data data effectively for any kind of predictive analytics applications.", "full_description": "This tutorial tackles a common pain point in data science \u2013 extracting useful features from relational data spread across multiple interconnected tables. Manually crafting these features is often tedious, error-prone, and heavily reliant on domain expertise.\r\n\r\nWhy is this important? Relational data powers industries from e-commerce and healthcare to finance. Yet, building predictive models on such datasets often involves laborious feature engineering. getML FastProp \u2013 the fastest open-source algorithm for automated feature engineering \u2013 streamlines this process, helping data scientists move faster and build better models.\r\n\r\nIn this hands-on tutorial, we\u2019ll work through two tasks from Stanford\u2019s Relational Learning Benchmark (RelBench) using the H&M Fashion dataset: 1) Predict customer churn with a classification model, 2) Forecast item sales using regression model.\r\n\r\nWe\u2019ll walk through the code and concepts needed to solve these tasks with getML FastProp, achieving state-of-the-art performance and outperforming both Relational Deep Learning models and an experienced human data scientist.\r\n\r\nBy the end of this tutorial, you'll learn how to:\r\n- Understand relational learning \u2013 Grasp the core challenges and concepts of working with multi-table datasets.\r\n- Reproduce results \u2013 Run the provided notebooks and code to reproduce the results at your own pace.\r\n- Automate feature engineering \u2013 Use getML\u2019s FastProp to extract features directly from relational data.\r\n- Build and optimize getML pipelines \u2013 Develop pipelines for both classification and regression tasks.\r\n- Integrate into MLOps workflows \u2013 Leverage getML alongside LightGBM and Optuna.\r\n\r\nThis tutorial provides a practical, reproducible framework for working with relational and time-series data, applicable across industries and domains.", "code": "C3RVM3", "state": "submitted", "created": "2025-01-05", "speaker_names": "Alexander Uhlig", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Navigating the Security Maze: An Interactive Adventure", "abstract": "How to integrate security into a software development project? Without jeopardizing timeline or budget?  You decide! \r\nThis interactive session covers crucial decisions for software security, and the audience decides how the story ends...", "full_description": "Although DevSecOps has been a trend topic for years, it is still far from being a solved problem.\r\n\r\nThis interactive session brings the challenges of security in the development process to life: Participants are confronted with several scenarios from everyday project work and their decisions help shape the further course of the presentation. They have to reconcile security requirements with budget, development speed and user-friendliness and bring the project safely from the idea to live operation.\r\n\r\nThe session covers the entire development process, but each run is different as the audience decides the course of the story via online-voting: How to proceed with the development project and think about security at the same time?", "code": "3WLDMQ", "state": "submitted", "created": "2024-12-20", "speaker_names": "Clemens H\u00fcbner", "track": "PyCon: Security"}, {"title": "Agentic AI: Build a Multi-Agent Application with CrewAI", "abstract": "This hands-on tutorial will dive into the fundamentals of building multi-agent systems using the CrewAI Python library. Starting from the basics, we\u2019ll cover key concepts, explore advanced features, and guide you step-by-step through building a complete application from scratch. We\u2019ll discuss implementing guardrails, securing interactions, and preventing query injection vulnerabilities along the way.", "full_description": "### **Short Abstract**  \r\n**Agentic AI: Build a Multi-Agent Application with CrewAI**  \r\nIn this hands-on tutorial, we\u2019ll dive into the fundamentals of building multi-agent systems using the CrewAI Python library. Starting from the basics, we\u2019ll cover key concepts, explore advanced features, and guide you step-by-step through building a complete application from scratch. Along the way, we\u2019ll discuss implementing guardrails, securing interactions, and preventing query injection vulnerabilities.\r\n\r\n---\r\n\r\n### **Detailed Description**  \r\nThis tutorial introduces **Agentic AI**\u2014a design approach where multiple agents collaborate to solve complex tasks efficiently. Using the **CrewAI Python library**, we\u2019ll start with the fundamentals and progressively move towards advanced concepts, focusing on practical implementation.\r\n\r\n#### **What We\u2019ll Cover:**  \r\n1. **Understanding Agentic AI:** Core principles and why multi-agent systems are valuable.  \r\n2. **Getting Started with CrewAI:** Setting up the library and creating simple agents.  \r\n3. **Advanced Agent Interactions:** Defining workflows, collaboration patterns, and communication protocols.  \r\n4. **Building from Scratch:** Step-by-step guide to developing a complete multi-agent application.  \r\n5. **Implementing Guardrails:** Techniques to ensure agents operate within defined constraints.  \r\n6. **Preventing Query Injection:** Strategies for securing agent queries against malicious inputs.  \r\n\r\n#### **Why Attend?**  \r\nBy the end of this session, you\u2019ll have hands-on experience building an agent-based application, understand how to implement security measures, and be equipped with best practices for maintaining control over agent behavior. Whether you're new to agentic systems or looking to refine your skills, this tutorial will provide both the theory and the practical insights needed to start building with CrewAI.  \r\n\r\n**Prerequisites:** Familiarity with Python and basic AI concepts will help you get the most out of this session.", "code": "SVLRGG", "state": "submitted", "created": "2024-12-23", "speaker_names": "Alessandro Romano", "track": "PyData: Generative AI"}, {"title": "The Pok\u00e9mon trainer's guide to Polars", "abstract": "[Polars](https://github.com/pola-rs/polars) is a fast, easy to use, and open source, dataframe library. It's clean API and its contexts and expressions allow writing readable but performant data wrangling code.\r\n\r\nAimed at both data newbies and experts from other libraries, in this hands-on tutorial we'll use data from the Pok\u00e9mon franchise to explore the [Polars API](https://docs.pola.rs/api/python/stable/reference/index.html) and the ideas that set it apart from other dataframe libraries.", "full_description": "This hands-on workshop will focus on teaching the fundamental concepts that differentiate Polars from the other dataframe libraries in use.\r\n\r\nYou will learn about expressions and contexts, and how they provide the modular building blocks that make the Polars API so intuitive. You will also learn about expression expansion, an often-overlooked feature that makes it much more convenient to write flexible expressions.\r\n\r\nThe lazy API is another key factor for the high performance we get from Polars and, in this tutorial, you will learn how to work with it.\r\nThis lets the Polars query engine optimise the data queries you write so that you can focus on expressing what you need to do to your data while Polars focuses on how to do it efficiently.", "code": "M8E8Q8", "state": "submitted", "created": "2024-12-24", "speaker_names": "Rodrigo Gir\u00e3o Serr\u00e3o", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "The State of Production Machine Learning in 2025", "abstract": "In this talk Alejandro dives into the state of production machine learning in 2024, and covers the concepts that make production machine learning so challenging, as well as some of the recommended tools available to tackle these challenges.", "full_description": "As the number of production machine learning use-cases increase, we find ourselves facing new and bigger challenges where more is at stake. Because of this, it's critical to identify the key areas to focus our efforts, so we can ensure we're able to transition from machine learning models to reliable production machine learning systems that are robsut and scalable. In this talk Alejandro dives into the state of production machine learning in 2024, and covers the concepts that make production machine learning so challenging, as well as some of the recommended tools available to tackle these challenges.\r\n\r\nWe will be covering a deep dive of the production ML tooling ecosystem and dive into best practices that have been abstracted from production use-cases of machine learning operations at scale, as well as how to leverage tools to that will allow us to deploy, explain, secure, monitor and scale production machine learning systems.", "code": "QHSEVJ", "state": "rejected", "created": "2024-12-23", "speaker_names": "Alejandro Saucedo", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Human-Centered Design for Retrieval Augmented Generation: A Path to Trust and Usability", "abstract": "There is no single RAG system that will suddenly solve all your problems. Although there are a lot of promises around integrating a chat window in your application and suddenly being blessed with happy users, this is unfortunately not the case. It is crucial to understand what the user needs to make the most of these new technologies. In our talk we will explore different aspects centered around the User Interface of a RAG system that will help you to build more effective machine learning systems. Concretely we will explore the following topics: \r\n\r\n1. Balancing Human and Machine Intelligence in RAG Systems: Provide effective but safe workflows through balancing user control and automation.\r\n2. Building Trust Through Transparency and Explainability: Increase safety and build user trust through traceable decisions.\r\n3. Designing Intuitive and Usable Interfaces for Diverse Users: Reducing cognitive load and adapting to user needs.\r\n4. Feedback and Iteration Loops for Continuous Improvement: Refine your system over time and improve alignment with user expectations.\r\n\r\nFor each of those we will show you typical pitfalls to avoid and patterns to follow for leveling up your RAG systems and improving the experience for your users. Additionally, we will also provide you with concrete suggestions for implementation, using open-source tooling.", "full_description": "Human-Centered Design for Retrieval Augmented Generation: A Path to Trust and Usability \r\n\r\nIntroduction\r\n\r\nCurrently, many companies are trying to leverage the power of LLMs to speed up a variety of tasks and to enable their employees to gain insights from data like never before. However, a lot of times the value actually provided stays well below the initial expectations. When looking into use cases where this was the case, we found the following: \r\n\r\n1. A lot of times, people tried generic solutions like Microsoft Copilot which simply lack the customizability needed to seamlessly support the workflow. \r\n2. Other times, people aim at building a custom solution but neglect the importance of providing a suitable user interface for leveraging the new technology. \r\n\r\nBoth of those cases can be caused by a variety of different factors. One could for example be overestimating the abilities of LLMs and thus neglecting the importance of providing transparency in the decision-making process. Another one could also be simply not having the abilities for implementing a user interface themselves together with the lack of tooling that focuses on the user interface aspect. \r\n\r\nThis is why we embrace the importance of Human-Centered Design for LLM powered systems, especially RAG systems. Concretely we have identified the following areas that we focus on when designing RAG systems that provide value to their users and are safe to use: \r\n\r\n1. Balancing Human and Machine Intelligence in RAG Systems: Provide effective but safe workflows through balancing user control and automation \r\n2. Building Trust Through Transparency and Explainability: Increase safety and build user trust through traceable decisions. \r\n3. Designing Intuitive and Usable Interfaces for Diverse Users: Reducing cognitive load and adapting to user needs. \r\n4. Feedback and Iteration Loops for Continuous Improvement: Refine your system over time and improve alignment with user expectations.\r\n\r\nIn the following sections we will thus outline those general themes, together with common pitfalls and patterns to follow. We also will provide guidance on how to leverage open-source tooling for concrete implementation of these principles. \r\n\r\n\r\nBalancing Human and Machine Intelligence in RAG Systems \r\n\r\nBeing able to find the right balance of automation and user control is critical for designing an effective RAG system. Simply providing a chat window where the user can interactively ask any question, he or she wants will often not be efficiently supporting the task at hand through lack of guidance and automation. On the other hand, simply automating the whole process will often also not work as the model will make mistakes which depending on the use case can be costly or even unacceptable. Thus, you should care about thinking deeply about what the LLM can provide you and what the user must bring to the table. Below you find our lists of pitfalls and patterns to follow to avoid this trouble: \r\n\r\n1. Deeply think about your users' use case and workflow! Also, iterate frequently, it is impossible to get is perfect the first time. \r\n2. Think about humans in the loop approaches early on. A lot of times, people are too focused on making the model a tiny bit more accurate although the much more effective solution is changing the way the model is used. \r\n3. Make the transitions between things the LLM does and things the human does as easy and transparent as possible. E.g., going out of an app and opening a file in a PDF viewer to verify some information is usually not the way to go. Also, the user should know what the LLM just did and be able to take over seamlessly. \r\n4. Provide sensible defaults for workflows and let experienced users override them if necessary. A lot of times you will need to cater to both. Trying to fit everything in one interface will make your app unusable and weaken the guidance your app can provide to a user. Thus, find sensible defaults than can be adapted by experts without distracting the ones that don\u2019t need the flexibility. \r\n\r\n\r\nBuilding Trust Through Transparency and Explainability \r\n\r\nLike also in other machine learning applications, a lot of the success of a machine learning system depends on being safe to use and building user trust. This is especially true for LLM-based systems as the fear and real risk of hallucinations is ubiquitous. Imagine having to make a decision in a business environment and simply trusting the aggregated answer of the LLM. This will not work, even in domains that are not especially high stakes. As a result, we have defined the following principles: \r\n\r\n1. Make it as easy as possible for the user to follow the decision-making process of the LLM. Even simple mechanisms like making the sources for an answer easily accessible can help already. \r\n2. Think about additional mechanisms that can further reduce cognitive load when interpreting a result, e.g., further narrow down the information used for generating an answer based on explainable AI techniques. \r\n3. Do not overwhelm the user with technical details in the explanations. Stay as close to the representations of information the user is used to and do not clutter the UI with additional elements. \r\n4. Provide granularity in explanations, meaning, first provide high-level summaries and let the user switch to detailed information on demand. \r\n\r\n\r\nDesigning Intuitive and Usable Interfaces for Diverse Users \r\n\r\nBesides providing transparency about what the model does there is also a component about simply making the application easy and fun to use. Users probably won\u2019t feel comfortable using an app where they feel they must do a lot of unnecessary actions.  One large part of this is really knowing the user, another part is having the technical ability to implement this using e.g. a front-end framework like React. In general, we propose to adhere to the following principles: \r\n\r\n1. Focus on tailoring your app to the user. Even tiny differences in the layout of the app or the way an action is dispatched can make a huge difference, especially in highly repetitive tasks. \r\n2. When creating a POC, be aware of that. If you can, incorporate thinking about suitable patterns for user interaction as soon as possible. If you cannot do that in a first iteration, at least explicitly outline how the app could be developed in a more usable direction. \r\n3. Use diverse methods for collecting feedback, such as having weekly meetings with real users, shadowing the users when doing their task using the new app, and many more. \r\n\r\n\r\nFeedback and Iteration Loops for Continuous Improvement \r\n\r\nAs mentioned in the previous section, feedback is key for developing your system in a direction where the users will be effective with it and like using it. This has many layers and can affect adapting the underlying machine learning model itself but also making changes to the user interface. A lot of times however, we see that this feedback loop is neglected, especially in POCs, claiming that dealing with this later will be fine. The problem with this is that often those POCs then fail to prove value and do not move on to be a production ready ML system. So, it is important to think about how you can try incorporating Feedback into your ML project. We on our side usually try to follow these principles: \r\n\r\n1. Gather feedback continuously to improve your system. This should be one of your main sources for prioritizing tasks. Otherwise, your development will be undirected, no matter if it is about UI or model improvements. \r\n2. Make giving feedback as easy as possible and be transparent about how you use the feedback. This will incentivize users to give you feedback in the first place. The best is to make them immediately feel the changes they can make with this. \r\n3. If possible, try to avoid clutter in the User Interface as much as possible and use non-explicit feedback signals as much as possible, e.g., if a user adapts sources manually after a query was answered then your system might have gotten the wrong sources. \r\n\r\n\r\nConclusion \r\n\r\nBeing human-centered when designing RAG systems is important. As described previously, although it seems like the natural language interface by itself makes the LLM technology really accessible to the user this is not necessarily the case. It is still important to actively push towards finding the right interfaces and workflows to be effective in implementing a solution for a specific problem. Cross-cutting principles like frequent iteration, or balancing automation and user control, as well as specific mechanisms like explainability techniques can thus help tremendously with developing a solution that really is helping people to fulfilling their tasks. We hope with this presentation we can raise awareness, as well as provide hands-on guidance on what to avoid and what to embrace when building your next RAG-based machine learning system.", "code": "AMRYQL", "state": "submitted", "created": "2024-12-22", "speaker_names": "Daniel Klitzke, Stefan Suwelack", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Vector Databases as an Instrument for Big Data Analysis", "abstract": "Vector databases are widely recognized as tools for similarity search, for example, in trendy (Agentic) Retrieval Augmented Generation. Yet, few pay attention to vector databases' feature - the ability to determine DISsimilarity. With similarity and dissimilarity, vector databases can be used to analyze billions of data points to detect anomalies and outliers, classify unstructured data, perform variety sampling and discovery search, and, in general, discover hidden patterns in data. This talk will demonstrate using vector databases as a big data analysis tool.", "full_description": "Vector databases store vector representations (embeddings) of diverse data modalities (text, images, audio, etc). These embeddings capture objects' semantic information\u2014regardless of data modality or, for example, language. That's why vector databases are widely used for semantic similarity search. Lately, this capability has made them a popular choice as a backbone of (Agentic) Retrieval Augmented Generation (RAG) systems, where they serve as knowledge bases.\r\nYet, vector databases can do far more than just similarity-based lookups. Their often overlooked strength is their ability to measure dissimilarity.\r\nOne can transform vector databases into large-scale data analysis tools that handle billions of data points by using similarity and dissimilarity.  \r\nThis tool can perform anomaly and outlier detection, variety sampling, recommendations, discovery search, etc.\r\nThis talk will discuss best practices for using vector databases as big data analysis engines/tools. You'll come away with a new perspective on how to use similarity and dissimilarity to solve real-world problems on a massive scale.", "code": "YWCEMN", "state": "submitted", "created": "2024-12-22", "speaker_names": "Evgeniya", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Refactoring and Maintaing Software : Building code you won't hate tomorrow", "abstract": "Writing code from scratch, especially with recent progress in AI, has never been easier, however maintaining and fixing the code as your codebase grows has been and still remains one of the biggest challenges. \r\n\r\nI'll go over practical advice on programming something that will make your team's and your life easier in years to come instead of be the constant source of frustrations.", "full_description": "While has made it easy to generate code that \"works\",  the long-term challenges of maintaining, updating, and bugfixing codebases remain as critical as ever.\r\n\r\nThis talk will cover the following:\r\n- Introduction (3 minutes)\r\n- Common Maintainability Pitfalls (5 minutes)\r\n- Practical Solutions (8 minutes)\r\n- Team Practices (6 minutes)\r\n- Actionable Takeaways (3 minutes)\r\n- Q&A Session (5 minutes)\r\n\r\n This talk combines real-world examples with battle-tested principles to help you build software that grows gracefully with your project's needs. We will also go over how you can use and more importantly how NOT to use LLMs to help you code.", "code": "9M39FV", "state": "submitted", "created": "2024-12-22", "speaker_names": "Bojan Miletic", "track": "PyCon: Programming & Software Engineering"}, {"title": "Empowering Ethiopian Youth with Python: Bridging the Gap to the Future Tech World", "abstract": "In today's world, knowing how to code is like knowing how to read and write. It opens doors to amazing opportunities! Python is a fantastic language to learn first. It's easy to understand, fun to use, and you can build all sorts of cool things with it.\r\n\r\nImagine teaching Python to high school students in Ethiopia. It's a chance to give them a head start in the exciting world of technology. We want to make sure everyone has a chance to succeed in tech, and Python is a great way to do that.\r\n\r\nPython is like a friendly guide. It helps you learn to think like a programmer without being too complicated. You can use it for websites, analyzing data, and so much more!\r\n\r\nEthiopia has a lot of talented young people, and we want to help them reach their full potential. But sometimes it's hard to get access to good technology and education. Teaching Python in schools can change that. It gives students valuable skills and shows them that a career in tech is possible.\r\n\r\nOur plan involves creating a special learning program, training teachers, and making sure students have the resources they need. We also want to work with the community to make sure everyone feels welcome in the world of Python.\r\n\r\nThe goal is to make the Python community more diverse and give Ethiopian youth amazing career opportunities. By inspiring a love for technology early on, we can create a brighter future for Ethiopia and the world of tech.", "full_description": "This proposal emphasizes the transformative potential of introducing Ethiopian high school students to programming through Python. Python's simplicity, readability, and versatility make it an ideal language for beginners. By teaching Python, we can empower Ethiopian youth, fostering a new generation of tech enthusiasts and ensuring greater diversity and inclusion within the global tech community. This initiative aims to provide essential skills, inspire careers in technology, and bridge the educational gap in Ethiopia.", "code": "HMMYWV", "state": "submitted", "created": "2024-12-22", "speaker_names": "Bitseat Aragaw", "track": "General: Community & Diversity"}, {"title": "What you'll never learn on an IT course", "abstract": "I'm, as an award-winning engineering mentor and i work with many mentees from different countries, different genders and different cultural backgrounds. They are learning different programming languages and solving different problems.... but they all ask the same question that their studies or courses haven't answered.\r\n\r\nWhy we were not learned how to deal with this \u201cfear against complexity\u201d. And how to stop being scared.", "full_description": "In my talk, I offer some suggestions on how you can deal with this \u201cfear of complexity\u201d. Although my advices are not a silver bullet, but they can really help:\r\nI will cover several topics in my talk:\r\n1. How to stay motivated: little obvious tricks.\r\n2. The challenge of making the right choice. It doesn't matter what technology in what field you choose - it's the feeling of being right that matters.\r\n3. Supporting sources. Something that can increase motivation and get sources of strength to keep going.", "code": "R9NTMA", "state": "submitted", "created": "2025-01-05", "speaker_names": "Maxim Danilov", "track": "General: Community & Diversity"}, {"title": "Building and Deploying Autonomous AI Agents on Social Media Platforms Using an Agentic Framework", "abstract": "Integrating AI agents into ever-changing digital communities poses significant challenges. Developers must contend with high-volume, high-velocity streams of user-generated content while ensuring that these agents remain autonomous, adaptable, and contextually aware across varied linguistic and cultural landscapes. Achieving consistently intelligent behavior involves dynamically interpreting shifting social patterns, adjusting to new trends, and refining responses without the need for continuous human intervention.\r\n\r\nA dynamic orchestration environment for autonomous AI agents offers a forward-thinking solution. This environment not only simplifies the process of real-time data ingestion and interpretation but also supports the seamless evolution of agent capabilities. Its inherently flexible architecture enables agents to adapt on-the-fly, maintaining coherent identities, knowledge bases, and conversational personalities, even as social discourse rapidly changes.\r\n\r\nIn this talk, we will explore how such an orchestration environment addresses the intricate demands of real-time social engagement. Attendees will gain insights into techniques for handling fast-moving data streams, strategies for agent memory management, and methods for integrating multiple AI models to enhance overall functionality. We will also discuss the modular extensions that allow developers to tailor agent behavior, ensuring a wide array of potential applications and continuous innovation.", "full_description": "The integration of AI agents on social media platforms for real-time interaction faces significant hurdles, including the complexity of managing real-time data streams, ensuring agent autonomy, and maintaining consistent performance across various languages and contexts. Developers often struggle with creating agents that can react intelligently to the dynamic environment of social media without the need for constant human oversight.\r\n\r\nThe ai16z's Eliza framework offers a solution by providing a robust, open-source platform specifically designed for building and deploying autonomous AI agents that can operate with real-time data. Eliza simplifies the process by offering tools for consistent agent behavior, real-time data processing, decentralized inference and scalability.\r\n\r\nIn this talk, we will explore how the framework handles the intricacies of real-time data, allowing agents to engage in meaningful, context-aware conversations autonomously. Attendees will learn about the architecture that supports this, including data management, agent memory systems, and the integration of multiple AI models for enhanced functionality.\r\n\r\nThis talk covers\r\n- The challenges of integrating AI agents with real-time social media data streams.\r\n-  How Eliza framework leverages real-time data to enhance agent autonomy and interaction on social media platforms.\r\n- Techniques for managing agent memory to ensure consistent personalities and knowledge bases.", "code": "HYXHA7", "state": "submitted", "created": "2024-12-22", "speaker_names": "Suvrakamal Das, Rudraksh Karpe", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Inside the Mind of an LLM", "abstract": "Recent advances in Large Language Model (LLM) **explainability** have yielded intriguing results. \r\n\r\nThis talk will explore the most recent breakthroughs: how we discovered Llamas think in English and altered Claude\u2019s belief system, inducing it to give the highest importance to the Golden Gate Bridge. \r\n\r\nFinally, we'll examine the implications of these findings for AI privacy and security.", "full_description": "In 2022 and 2023, explaining the inner workings of large language models (LLMs) seemed like a daunting task. However, recent studies in 2024 have led to significant breakthroughs in this area. This talk will explore three of the most important discoveries in LLM explainability.\r\n\r\n**1. Llamas \u201cThink\u201d in English [1]**\r\n\r\nResearchers from EPFL have revealed that models from the Llama 2 family of LLMs use English as their internal representation, regardless of the input or output language. This behaviour may account for certain biases in the models' style when used with non-English languages.\r\n\r\n**2. Monosemantic Features in Claude 3 and GPT-4 [2]**\r\n\r\nResearchers from Anthropic, later followed by OpenAI, have succeeded in collapsing the internal representation of Claude 3 Sonnet and GPT-4 into monosemantic features. This discovery enables a deeper understanding of which areas of the model are associated with specific topics and allows for the adjustment of the relative importance of these topics. This technique holds promise for aligning LLMs with ethical values.\r\n\r\n**3. LLMs Memorize Unusual Data [3]**\r\n\r\nRecent research has shown why LLMs tend to memorize specific samples that are outliers compared to the normal data distribution. Unique strings, such as names and personal information, are particularly prone to memorization and can be reproduced by the models, especially when exploring similarly unusual data spaces. This finding explains instances where GPT-4 have shared personal information when prompted to repeat the same word indefinitely.\r\n\r\nFinally, we will discuss the implications of these findings on privacy and security in the context of LLMs.\r\n\r\n**Time Split**\r\n\r\n**Talk Structure**\r\n\r\n- **0-5 mins:** Introduction - Why LLMs are challenging to explain\r\n- **5-10 mins:** Llamas \u201cthink\u201d in English\r\n- **10-20 mins:** Monosemantic features in Claude 3 and GPT-4\r\n- **20-25 mins:** LLMs Memorize Unusual Data\r\n\r\n**Resources**\r\n\r\n1. Wendler, Chris, et al. \"Do llamas work in English? on the latent language of multilingual transformers.\" arXiv preprint arXiv:2402.10588 (2024).\r\n2. Templeton, Adly. Scaling monosemanticity: Extracting interpretable features from Claude 3 sonnet. Anthropic, 2024.\r\n3. Nasr, Milad, et al. \u201cExtracting Training Data from ChatGPT\u201d, arXiv preprint arXiv:2311.17035 (2023)", "code": "DM3DDY", "state": "submitted", "created": "2024-12-22", "speaker_names": "Emanuele Fabbiani", "track": "PyData: Generative AI"}, {"title": "Duplicate Code Dilemma: Unlocking Automation with Open Source!", "abstract": "\"Don't Repeat Yourself\" \u2013 a phrase that we have all heard many times. In this talk, we will have an overview how to deal with code duplication and how open-source template libraries such as Copier and Cookiecutter can assist us in managing similarly structured repositories. Furthermore, we will explore how code updates can be automated with the help of the open-source library Renovate Bot. By the end of this session, you will gain insights into these solutions while also questioning whether they truly eliminate repetition or merely contribute to another cycle of automation.", "full_description": "\u201cDon\u2019t Repeat Yourself\u201d (DRY) is one of the first principles that every programmer encounters in the early stages of their coding journey. Some of us even had to learn it the hard way. We promised ourselves to avoid repetitive code to never again deal with the extensive refactoring required for every small change.\r\n\r\nThis simple principle has found a fundamental place in every programmer's heart. It may also be the reason why, from time to time, every programmer doubts their code and begins to refactor it in the early stages of coding.\r\n\r\nThis talk provides an overview of different solutions for preventing code repetition. We will start with the most common solutions, such as using functions and classes, and then explore more intermediate approaches for managing similarly structured repositories with the help of open-source template libraries such as Copier and Cookiecutter. Finally, we will address a more complex problem and examine how to automate updates using open-source tools like Renovate Bot.\r\n\r\nAs a takeaway, participants will gain insights into various solutions and a glimpse into the usability of each open-source library. Participants are also encouraged to reconsider the entire process: Are these solutions truly preventing repetitive code, or are we merely caught in an endless cycle of automation?", "code": "KZKT9W", "state": "submitted", "created": "2024-12-16", "speaker_names": "Raana Saheb-Nassagh", "track": "PyCon: Programming & Software Engineering"}, {"title": "NLP for the Preservation of the Indigenous Quechua Language in Bolivia", "abstract": "### Abstract  \r\n\r\nQuechua, spoken by over 2.8 million people in Bolivia, is one of the country's most vital indigenous languages, yet it faces significant challenges due to modernization and globalization. This presentation introduces *Quechua AI Preservation*, a project that combines cutting-edge Natural Language Processing (NLP) techniques, embeddings, and generative AI to revitalize and preserve Quechua.  \r\n\r\nBy leveraging embeddings for semantic understanding and generative AI to create educational and cultural resources, this initiative aims to address the scarcity of digital linguistic tools and promote the language\u2019s integration into the digital world. Join us as we explore how innovation and cultural heritage can converge to ensure Quechua\u2019s survival and growth for future generations.", "full_description": "### Description  \r\n\r\nQuechua is more than a language; it is a gateway to the rich cultural identity of Bolivia, spoken by over 2.8 million people. However, it is increasingly at risk due to declining intergenerational transmission and the effects of globalization. This talk delves into the intersection of technology and cultural preservation, showcasing how cutting-edge NLP techniques can help address these challenges through an innovative project called *Quechua AI Preservation*.  \r\n\r\nThe project leverages advanced embeddings and generative AI models to create robust tools for the revitalization and preservation of Quechua. By focusing on building accessible digital resources, such as text corpora, translation systems, and conversational agents, *Quechua AI Preservation* aims to bring the language into the digital sphere.  \r\n\r\nWe will discuss the current state of Quechua linguistic resources, the technical challenges of working with low-resource languages, and how embeddings can provide semantic understanding even with limited data. Additionally, we will explore how generative AI can assist in creating educational materials, cultural content, and digital tools that empower Quechua-speaking communities.  \r\n\r\nThis session is designed to inspire the Python and NLP community to actively participate in *Quechua AI Preservation*. By integrating state-of-the-art technologies, we can ensure that Quechua not only survives but thrives in the digital age, paving the way for its continued transmission to future generations.", "code": "DA7D3G", "state": "submitted", "created": "2025-01-02", "speaker_names": "Carla Marcela Florida Rom\u00e1n", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Advanced Polars: Lazy Queries and Streaming Mode", "abstract": "Do you find yourself struggling with Pandas' limitations when handling **massive datasets** or **real-time data streams**?\r\n\r\nDiscover **Polars**, the lightning-fast DataFrame library built in Rust. This talk presents two advanced features of the next-generation dataframe library: **lazy queries** and **streaming mode**.\r\n\r\nLazy evaluation in Polars allows you to build complex data pipelines without the performance bottlenecks of eager execution. By deferring computation, Polars optimises your queries using techniques like **predicate and projection pushdown**, reducing unnecessary computations and memory overhead. This leads to significant performance improvements, particularly with datasets larger than your system\u2019s physical memory.\r\n\r\nPolars' **`LazyFrame`s** form the foundation of the library\u2019s streaming mode, enabling **efficient streaming pipelines**, real-time transformations, and seamless integration with various data sinks.\r\n\r\nThis session will explore use cases and technical implementations of both lazy queries and streaming mode. We\u2019ll also include **live-coding demonstrations** to introduce the tool, showcase best practices, and highlight common pitfalls.\r\n\r\nAttendees will walk away with practical knowledge of lazy queries and streaming mode, ready to apply these tools in their daily work as data engineers or data scientists.", "full_description": "Do you find yourself struggling with Pandas' limitations when handling **massive datasets** or **real-time data streams**?\r\n\r\nDiscover **Polars**, the lightning-fast DataFrame library built in Rust. This talk presents two advanced features of this next-generation dataframe library: **lazy queries** and **streaming mode**.\r\n\r\nLazy evaluation in Polars allows you to build complex data pipelines without the performance bottlenecks of eager execution. By deferring computation, Polars optimises your queries using techniques like **predicate and projection pushdown**, reducing unnecessary computations and memory overhead. This leads to significant performance improvements, particularly with datasets larger than your system\u2019s physical memory. For instance, if you need to filter a large CSV file based on certain criteria, Polars can push down the filter operation to the scan level, reading only the necessary rows from the file. This can drastically reduce the amount of data loaded into memory and speed up query execution.\r\n\r\nPolars' **`LazyFrame`s** form the foundation of the library\u2019s streaming mode, enabling **efficient streaming pipelines**, real-time transformations, and seamless integration with various data sinks. This means you can process data in chunks as it arrives, without having to load the entire dataset into memory **2**. Imagine building a real-time analytics dashboard that processes data from a Kafka stream, applying aggregations and transformations on the fly, and updating the dashboard with the latest insights. Polars' streaming mode makes this possible with minimal effort.\r\n\r\nThis session will explore use cases and technical implementations of both lazy queries and streaming mode. We\u2019ll also include **live-coding demonstrations** to introduce the tool, showcase best practices, and highlight common pitfalls.\r\n\r\nAttendees will walk away with practical knowledge of lazy queries and streaming mode, ready to apply these tools in their daily work as data engineers or data scientists.\r\n\r\n**Talk Outline**\r\n1. Introduction and motivation for lazy queries and streaming mode (5 mins)\r\n2. Lazy queries: discussion, implementation and live coding (10 mins)\r\n3. Streaming mode: implementation, main settings, most common issues, and live coding (10 mins)\r\n4. Q&A (5 mins)", "code": "EFSDWY", "state": "submitted", "created": "2024-12-22", "speaker_names": "Emanuele Fabbiani", "track": "PyData: Data Handling & Engineering"}, {"title": "Revolutionizing Customer Experience: A Comprehensive Contact Center Solution with AI-Powered Insight", "abstract": "In today's digital world, exceptional customer experiences are vital for business success. Our platform leverages AI and ML to transform contact centers, combining NLP, machine learning algorithms, and multi-agent architecture for unparalleled customer service. Key features include Customer360, an analytics module providing a 360-degree view of customer behavior, and SmartGuide, a real-time guidance system for agents. These tools offer real-time insights into customer preferences and sentiment, enabling data-driven decisions, improved efficiency, and enhanced customer satisfaction. Our scalable, flexible solution integrates seamlessly into existing operations, driving business growth and customer loyalty.", "full_description": "In today's digitally-driven world, delivering exceptional customer experiences is crucial for business success. The contact center industry is no exception, with an increasing demand for personalized and efficient service. In this talk, we want to introduce a cutting-edge platform that leverages Artificial Intelligence (AI) and Machine Learning (ML) to transform the way contact centers operate.\r\n\r\nOur platform combines natural language processing (NLP), machine learning algorithms, and multi-agent architecture to deliver unparalleled customer experience. At its core, our solution is designed to provide real-time insights into customer behavior, preferences, and sentiment. This enables contact center managers to identify areas for improvement, optimize operations, and make data-driven decisions that drive business growth.\r\n\r\nOne of the key features of our platform is Customer360, a cutting-edge analytics module that provides a 360-degree view of customer behavior across all touchpoints, including phone calls, voicemails and chat sessions. By analyzing this vast dataset, Customer360 delivers rich insights into customer preferences, sentiment, and pain points, enabling contact center managers to pinpoint areas for improvement and optimize their strategies. This information is then used to identify trends, patterns, and areas where customers are struggling with our products or services, allowing us to develop targeted solutions that meet their evolving needs. In our use case, Customer360 revealed common pain points in our customer support process, which we addressed by integrating a voice bot that provides proactive assistance on these topics. As a result, our agents are now able to devote more time to addressing complex issues and providing personalized support, leading to increased customer satisfaction and loyalty.\r\n\r\nIn addition to Customer360, our platform also features SmartGuide, a real-time guidance system that provides employees with relevant information and expertise at any given time. SmartGuide is designed to reduce response times, improve overall efficiency, and enhance customer satisfaction. By providing agents with access to accurate and up-to-date information, SmartGuide helps ensure that customers receive the best possible experience every time. In it's core, SmartGuide is an agent, who is able to understand the conversation between agent and customer in real time, searches for relevance information in the background and shows these information on the agent's screen.\r\n\r\nThe benefits of our platform are far-reaching, extending beyond just improved customer experience to increased efficiency, reduced manual labor, and enhanced decision-making. By leveraging AI and ML, we are able to provide a more personalized and efficient service that meets the evolving needs of customers. Our solution is designed to be scalable, flexible, and adaptable, making it easy to integrate into existing contact center operations.\r\n\r\nIn conclusion, our platform represents a significant shift in the way contact centers operate. By leveraging AI and ML, we are able to provide real-time insights into customer behavior, preferences, and sentiment. With features like Customer360 and SmartGuide, we offer a comprehensive solution that can help drive business growth and improve overall customer satisfaction.", "code": "A3VMXL", "state": "submitted", "created": "2024-12-20", "speaker_names": "Vu Tran", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Power up your Polars code with Polars extention", "abstract": "While Polars is written in Rust and has the advantages of speed and multi-threaded functionalities., everything will slow down if a Python function needs to be applied to the DataFrame. To avoid that, a Polar extension can be used to solve the problem. In this workshop, we will look at how to do it.", "full_description": "We love Polars because it is written in Rust so we can use Rust's security and speed. However, it is not the most efficient if we still have to call in a Python function to perform specific aggregation. In this workshop, we will use the Polars plugin. You will be writing simple functions in Rust, and then you will use it together with Polars in your Python data pipeline.\r\n\r\n## Target Audience\r\n\r\nEngineers and data scientists who use Polars and are confident to write a bit of Rust code. We expect you to have knowledge of Python and Polars and have a bit of Rust experience (or be able to pick it up relatively quickly). Not all concepts in Rust will be explained but we will link to material where you can find explanations.\r\n\r\n## Goal\r\n\r\nTo empower Polars users who want to do more and do better with Polars. For folks who don't mind learning a new programming language, it is also a good opportunity to learn and practice writing in Rust.\r\n\r\n## Outline\r\n\r\n- Introduction (15 mins): \r\n    1. What is Polars plugin \r\n    2. How does it work (using Maturin to develop packages)\r\n    3. How to use it with Polars (exercises)\r\n- Simple numerical functions (35 mins): \r\n    1. Creating numerical functions with 1 input (exercise)\r\n    2. Creating numerical functions with multiple inputs in the same row (exercise)\r\n    3. Creating numerical functions that support multiple types (exercise)\r\n- Advance usage with Polars plugin (40 mins):\r\n    1. Creating functions with multiple inputs across different rows (exercise)\r\n    2. Functions with user-set parameters (exercise)\r\n    3. Working with strings and lists (exercise)", "code": "CP3TKB", "state": "submitted", "created": "2024-12-17", "speaker_names": "Cheuk Ting Ho", "track": "PyData: Data Handling & Engineering"}, {"title": "Triton Inference Server: The Magic Behind Efficient AI Model Serving", "abstract": "Picture this: A world where your machine learning models don't just sit pretty in Jupyter notebooks but actually come to life, performing like elite athletes, scaling effortlessly across complex infrastructure.\r\n\r\nIn the world of machine learning, deployment is where potential meets performance. In our journey with NVIDIA's Triton Inference Server, we've transformed complex ML infrastructure into a streamlined, powerful deployment ecosystem.\r\n\r\nI\u2019ll share tips and tricks to maximize Triton\u2019s features like a pro, helping you get the most out of your resources while delivering fast, reliable inference. In this session we\u2019ll explore some of the key highlights and Triton's game-changing features with real-world Impact:\r\n\r\n- Seamless multi-framework model serving (TensorFlow, PyTorch, ONNX), simplifying complex inference workflows\r\n- Automatic scaling and zero-downtime model updates, handling high throughput with minimal latency\r\n- Advanced performance monitoring and optimization, providing deep visibility into model performance\r\n\r\nBut that\u2019s not all. Triton\u2019s support for Python backends opened up another dimension of possibilities, enabling us to integrate preprocessing and custom logic directly into the inference pipeline. This flexibility allowed us to eliminate bottlenecks traditionally found in data preparation workflows, bringing inference closer to real-time performance.", "full_description": "Deployment is the critical moment where machine learning\u2019s potential meets its true performance, and NVIDIA\u2019s Triton Inference Server has been a game-changer in transforming complex ML infrastructures into streamlined, powerful ecosystems.\r\n\r\nIn this presentation, I\u2019ll walk you through the challenges of deploying machine learning models in production. From managing multiple frameworks like TensorFlow, PyTorch, and ONNX to meeting the demands of high throughput and low latency, these obstacles often hinder even the most innovative projects. This is where Triton steps in, offering a robust solution that simplifies workflows while enhancing scalability and performance.\r\n\r\nWe\u2019ll explore some of Triton\u2019s standout features and their real-world impact. With its ability to serve models from multiple frameworks seamlessly, Triton eliminates the need for extensive engineering to unify different workflows. Paired with automatic scaling and zero-downtime model updates, your infrastructure can handle high demand without interruptions, while advanced monitoring and optimization tools provide deep insights into model performance, allowing you to fine-tune for maximum efficiency.\r\n\r\nTriton\u2019s Python backend opens up another dimension of possibilities. By integrating preprocessing and custom logic directly into the inference pipeline, Triton removes traditional bottlenecks and brings inference closer to real-time performance. I\u2019ll share how this flexibility enabled us to streamline data preparation workflows, saving time and resources while delivering faster results.\r\n\r\nThroughout the session, I\u2019ll also provide practical tips and tricks for getting the most out of Triton. From optimizing resource utilization and configuring model ensembles to leveraging advanced features for specific deployment scenarios, these insights will help you deploy like a pro.\r\n\r\nFinally, I\u2019ll reflect on lessons learned during this journey - from overcoming challenges to achieving breakthroughs in production-ready ML. My goal is to leave you inspired and equipped to harness Triton\u2019s potential, transforming your models into high-performing assets ready to excel in real-world applications.", "code": "BUTDWC", "state": "submitted", "created": "2024-12-17", "speaker_names": "Salvatore Visaggi", "track": "PyCon: MLOps & DevOps"}, {"title": "Modern NLP for Proactive Harmful Content Moderation", "abstract": "Despite an array of regulations implemented by governments and social media platforms worldwide (i.e. famous DSA), the problem of digital abusive speech persists. At the same time, rapid advances in NLP and large language models (LLMs) are opening up new possibilities\u2014and responsibilities\u2014for using this technology to make a positive social impact. Can LLMs streamline content moderation efforts? Are they effective at spotting and countering hate speech, and can they help produce more proactive solutions like text detoxification and counter-speech generation?\r\n\r\nIn this talk, we will dive into the cutting-edge research and best practices of automatic textual content moderation today. From clarifying core definitions to detailing actionable methods for leveraging multilingual NLP models, we will provide a practical roadmap for researchers, developers, and policymakers aiming to tackle the challenges of harmful online content. Join us to discover how modern NLP can foster safer, more inclusive digital communities.", "full_description": "The rise of large language models (LLMs) has revolutionized natural language processing (NLP), creating opportunities to address complex societal challenges, including the pervasive issue of harmful online content. Despite global regulations and platform-specific policies, abusive speech and toxic content continue to plague digital spaces, highlighting the need for smarter, scalable, and multilingual solutions.\r\n\r\nThis talk explores how modern NLP technologies can play a transformative role in content moderation, moving beyond traditional detection methods to proactive measures that promote healthier online interactions. We will cover key topics, including:\r\n\r\n* Understanding the Landscape: Definitions and nuances of harmful content categories, including hate speech, misinformation, and harassment. We will bring practices not only from CS field, but from communication with social scientists and NGOs.\r\n* Hate Speech Detection: Can LLMs detect hate speech? How the models can be adapted to new languages?\r\n* Text Detoxification: Diving into nuances of toxicity of 9 languages (from our recent shared task) and sharing best practice on LLMs prompting for texts detoxification.\r\n* Counter-Speech Generation: Our recent research results on how make LLMs generate not a very general \"Please, it is not ok to talk like this report\" but indeed address the targeted group.\r\n* Ethical Considerations: Who, in the end, responsible for the content moderation? How the community can help to bring best practices? How the measure the \"effectiveness\" of LLMs for content moderation?", "code": "F9EFXA", "state": "submitted", "created": "2024-12-20", "speaker_names": "Daryna Dementieva", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "How I built OrchestrIA - a new agent orchestrator", "abstract": "Want to know how to create agents and tools in few steps? This is the talk for you!\r\nAt the end you\u2019ll know how to quickly create new tools ad hoc for your use case and how to use them in your agent with ease.", "full_description": "Having worked in the AI world for a while in different projects I've seen lots of different libraries and frameworks to create agents. All of them try to simplify creation and use of agents and their tools, with different level of success.\r\n\r\nThough most of them left me unsatisfied because they don't simplify enough. I think we can do better, that's why I decided to build OrchestrIA.\r\n\r\nThe OrchestrIA goal is to simplify the creation process of both the agents and their tools using existing projects within the Python ecosystem.\r\n\r\nIn this talk I'll show you how I created the project from scratch and how easy it is to use and create your own agents alongside their tools, in just few steps.", "code": "DNK38Q", "state": "submitted", "created": "2024-12-19", "speaker_names": "Silvano Cerza", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "From Rules to Reality: Python's Role in Shaping Roundnet", "abstract": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community. However, like many emerging sports, it faces challenges in balancing competition, optimizing rules, and increasing accessibility for both players and spectators. This is where Python and data analysis come into play.\r\n\r\nIn this talk, I'll share insights from my role as Data Lead on the International Roundnet rule committee, where we use Python-powered data analysis to make informed decisions about the future of the sport. We'll explore how analyzing gameplay patterns and testing rule changes with simulation can lead to fairer, more exciting games and attract a broader audience.", "full_description": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community.  But what's truly unique about Roundnet is the opportunity it offers: as a new and emerging sport, we have the rare chance to shape its global rule changes entirely through data analysis. This is a groundbreaking approach \u2013 a first for any sport in the modern era.\r\n\r\nIn this talk, I\u2019ll share insights from my role as Data Lead on the International Roundnet Rule Committee and take you through how we are leveraging Python and data analysis to guide these changes. Over the past year, we\u2019ve collected rule proposals and have set up a series of experiments designed to test their effects. Using Python and statistical modeling, we\u2019re planning to select key tournaments worldwide this year to observe how rule adjustments impact gameplay data.\r\n\r\nOur ultimate goal is to discover if specific combinations of rule changes can make Roundnet fairer, more exciting, and accessible for players and spectators alike. This journey is an exploration of how data-driven decision-making can transform a sport from the ground up, using real-world insights and experimentation.\r\n\r\nThis talk will take you on the journey of how we set up our testing framework, what tools we\u2019re using, and how we\u2019ve employed Python-powered analysis to bring empirical evidence into the decision-making process. It will equip you with a new perspective on data's role in shaping real-world change \u2013 especially in grassroots movements, community building, and sports.", "code": "ZT3MGL", "state": "accepted", "created": "2024-12-17", "speaker_names": "Larissa Haas", "track": "PyData: Data Handling & Engineering"}, {"title": "From Queries to Confidence: Ensuring SQL Reliability with Python", "abstract": "SQL remains a foundational component of data-driven applications, but ensuring the accuracy and reliability of SQL logic is often challenging. SQL testing can be cumbersome, time-consuming, and error-prone. However, these challenges can be addressed by leveraging the simplicity of Python's testing framework such as pytest, enabling clean, robust, and automated SQL testing.", "full_description": "SQL is an essential part of data-driven applications, powering everything from simple queries to complex data transformations. However, ensuring the accuracy and reliability of SQL code is often challenging, particularly when dealing with intricate logic or large-scale datasets. Also, deploying changes in SQL code to production is another complex task, as it requires careful validation to avoid breaking the query logic.\r\n\r\nFortunately, integrating Python\u2019s testing framework such as pytest into SQL workflows provides a streamlined solution for these challenges. Such approach enables creating clean, efficient, and automated testing processes for SQL code and database logic. Therefore, we can validate query results, enforce schema consistency, and simulate complex data scenarios, all while reducing manual effort and improving test coverage.\r\n\r\nThis talk will address:\r\n- configuring lightweight database fixtures\r\n- verifying SQL query result and testing scripts seamlessly\r\n- data mocking\r\n- schema validation\r\n- testing non-deterministic queries\r\n- handling large datasets\r\n\r\nAttendees will gain insights into improving SQL code quality, identifying issues early in the development process, and ensuring the reliability of data-driven products. This presentation is particularly beneficial for Data Scientists, Engineers, and Analysts seeking to enhance the efficiency and precision of their testing practices.", "code": "FSK3PE", "state": "submitted", "created": "2024-12-20", "speaker_names": "Anna Varzina", "track": "PyCon: Testing"}, {"title": "GPS doesn't work! Can a model alert us before this happens?", "abstract": "Have you ever happened to use GPS and realised that it is not working properly? The Sun and a Space Weather effect called Travelling Ionospheric Disturbances (TIDs) could be responsible. We will present an explainable TIDs forecasting model, based on CatBoost and using several physical drivers to make forecasts.", "full_description": "Travelling Ionospheric Disturbances (TIDs) are wave-like fluctuations in plasma density that ripple through the ionosphere, excited mainly by acoustic and gravity waves. TIDs contribute significantly to the degradation of Global Navigation Satellite Systems (GNSS) and radio communications at mid-latitudes, so it is crucial to reliably detect and potentially forecast their occurrence.\r\n\r\nSince there are no empirical models to forecast TIDs, in the context of the Travelling Ionospheric Disturbances Forecasting System (T-FORS) project, funded by the European Union, Istituto Nazionale di Geofisica e Vulcanologia (INGV) developed an explainable machine learning model, which is based on gradient boosting over an ensemble of decision trees (CatBoost) and aims at forecasting the occurrence of TIDs over the European sector within 3 hours.\r\n\r\nThe SHapley Additive exPlanation (SHAP) framework is introduced to test features influence on the model output from both global and local (event-level) aspects, so as to improve interpretability and explainability, which are very desirable features in potentially high-risk settings such as GNSS and HF communications.\r\n\r\nOur research shows that the proposed algorithm, in its current development stage, achieves a reasonable level of performance \u2013 given the imbalanced nature of the problem and the still incomplete understanding of the physical phenomenon.", "code": "8NSN9Z", "state": "submitted", "created": "2024-12-13", "speaker_names": "Vincenzo Ventriglia", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Reinforcement Learning for Finance", "abstract": "Reinforcement Learning and related algorithms, such as Deep Q-Learning (DQL), have led to major breakthroughs in different fields. DQL, for example, is at the core of the AIs developed by DeepMind that achieved superhuman levels in such complex games as Chess, Shogi, and Go (\"AlphaGo\", \"AlphaZero\"). Reinforcement Learning can also be beneficially applied to typical problems in finance, such as algorithmic trading, dynamic hedging of options, or dynamic asset allocation. The workshop addresses the problem of limited data availability in finance and solutions to it, such as synthetic data generation through GANs. It also shows how to apply the DQL algorithm to typical financial problems. The workshop is based on my new O'Reilly book \"Reinforcement Learning for Finance -- A Python-based Introduction\".", "full_description": "Reinforcement Learning and related algorithms, such as Deep Q-Learning (DQL), have led to major breakthroughs in different fields. DQL, for example, is at the core of the AIs developed by DeepMind that achieved superhuman levels in such complex games as Chess, Shogi, and Go (\"AlphaGo\", \"AlphaZero\"). Reinforcement Learning can also be beneficially applied to typical problems in finance, such as algorithmic trading, dynamic hedging of options, or dynamic asset allocation. The workshop addresses the problem of limited data availability in finance and solutions to it, such as synthetic data generation through GANs. It also shows how to apply the DQL algorithm to typical financial problems.\r\n\r\nThe workshop covers the following topics:\r\n\r\n* Learning through interaction\r\n* Deep Q-Learning applied to Finance\r\n* Synthetic Data Generation\r\n* Dynamic Asset Allocation with DQL\r\n\r\nThe workshop is based on my new O'Reilly book \"Reinforcement Learning for Finance -- A Python-based Introduction\".", "code": "VBW3EK", "state": "submitted", "created": "2025-01-04", "speaker_names": "Dr. Yves J. Hilpisch", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Strategies to Solve LLM Hallucinations", "abstract": "Hallucinations have been a big pain in products that use LLM. Users expect high accuracy in answers and rely increasingly on LLM or AI assistants. Hallucinations would result from destroying the users' trust in real-life consequences. Solving it has been a high-priority task for AI researchers.", "full_description": "In this talk, the speaker will explain, in technical terms, what causes LLM to hallucinations. From there, the speaker will introduce a few strategies to minimize hallucination and show examples of designs and use cases.\r\n\r\n## Topics covered\r\n\r\n- what causes AI hallucinations\r\n- how to avoid hallucinations by working inside the model\r\n- how to avoid hallucinations by working outside of the model\r\n- how to avoid hallucinations as a user\r\n\r\n## Goal\r\n\r\nTo educate the audience about LLM hallucination and to explore methods to minimize it.\r\n\r\n## Target Audiences\r\n\r\nEngineers who integrate LLM in their products.", "code": "BUHT3N", "state": "submitted", "created": "2024-12-17", "speaker_names": "Cheuk Ting Ho", "track": "PyData: Generative AI"}, {"title": "Getting started with Healthcare AI Agents Python", "abstract": "Using AI agents to tackle challenges in clinical trials, adverse event reporting, and patient care management. AI agents have the potential to transform healthcare by automating tasks, improving decision-making, and enhancing patient outcomes.", "full_description": "The healthcare sector is uniquely positioned to benefit from AI agents due to its complex workflows, massive data requirements, and the need for timely, accurate decision-making. Here\u2019s how to get started:\r\n\r\nUnderstand the Role of AI Agents in Healthcare AI agents in healthcare can:\r\nAssist in diagnostics by analyzing patient data, medical images, and lab results. Enable clinical decision support through predictive analytics and evidence-based recommendations. Enhance patient engagement with virtual assistants for scheduling, symptom checks, or post-care follow-ups. Optimize administrative workflows like billing, coding, and claims management.\r\n\r\nIdentify a Problem to Solve Start by identifying a specific pain point. For example:\r\nReducing manual effort in medical data entry. Accelerating the diagnosis process with AI-powered imaging tools. Personalizing patient treatment plans using predictive modeling.\r\n\r\nLearn the Basics of AI for Healthcare Machine Learning (ML): Predict patient outcomes or recommend treatments based on historical data. Natural Language Processing (NLP): Extract insights from unstructured medical records or facilitate patient-agent conversations. Reinforcement Learning (RL): Optimize treatment pathways or resource allocation in hospitals. Familiarize yourself with healthcare-specific data challenges, such as privacy regulations (e.g., HIPAA, GDPR) and handling noisy, unstructured data.\r\n\r\nExplore AI Agent Frameworks and Tools LangChain: Ideal for building conversational agents that assist with patient inquiries or doctor consultations. Hugging Face Transformers: Use pre-trained models for medical text analysis. Healthcare-Specific Platforms: Look into tools like Google Health AI or IBM Watson Health for specialized healthcare applications.\r\n\r\nBuild Your First AI Agent Define the Scope: Start small with a single task, such as appointment scheduling or symptom checking. Prepare Data: Use publicly available datasets like MIMIC-III for training models, ensuring compliance with ethical standards. Choose Models: Pre-trained models or fine-tuned versions (e.g., BioBERT for medical NLP). Integrate with Systems: Connect the agent to existing EMRs or hospital systems via APIs.\r\n\r\nAddress Challenges in Healthcare AI Data Privacy and Security: Ensure compliance with regulations like HIPAA. Bias in Models: Validate models against diverse datasets to prevent biased outcomes. Interoperability: Ensure agents can communicate with multiple healthcare systems seamlessly.", "code": "9CDDSS", "state": "submitted", "created": "2025-01-03", "speaker_names": "shahid qadri", "track": "PyCon: Python Language & Ecosystem"}, {"title": "PyData Stack: Building and deploying pure Python, open source data platforms", "abstract": "Modern open source Python data packages offer the opportunity to build and deploy pure Python, production-ready data platforms. Engineers can and do play a big role in helping companies become data-driven by centralising this data, cleaning and modelling it and presenting back to the business. Now more than ever it allows engineers and companies of any size the ability to build data products and insights for relatively low cost. In this talk we\u2019ll walk through the key components of this stack, tooling options available and demo a deployable containerised Python data stack.", "full_description": "Modern data platforms can be built and deployed using completely open source, Python packages. In this talk, I\u2019ll cover what constitutes a modern data stack and what open source Python packages can be used to build a stack suitable for the needs of most developers and companies. Rather than a one size fits-all approach, I\u2019ll initially demonstrate the rich ecosystem of technologies available and the pros and cons of the technology choices.\r\n\r\nTo be concrete, we will demo an instance of this type of self-contained, deployable platform that is composed of specific technology choices for the key components: data pipelines, transformation engine, data warehouse, presentation layer and orchestration. This implementation will use Docker, Python and yes, even some SQL. \r\n\r\nStructure\r\n1. What is a data platform? [5 mins]\r\n    1. Why are they useful\r\n    2. When do companies need them? \r\n2. What are the key components [5 mins]\r\n    1. Data pipelines\r\n    2. Transformation engine\r\n    3. Data store\r\n    4. Presentation layer\r\n    5. Orchestration\r\n3. Python packages available for this? [10 mins]\r\n    1. Data pipelines: dlt/PyAirbyte\r\n    2. Transformation engine: dbt or sqlmesh\r\n    3. Data store: s3/blob store/data warehouse/duckdb\r\n    4. Presentation layer: Streamlit/Superset\r\n    5. Orchestration: Prefect/Dagster\r\n4. The platform [10 mins]\r\n    1. Design\r\n    2. Technology choices\r\n    3. Joining it all together\r\n5. Deployment [10 mins]\r\n    1. IAC via Pulumi-Python\r\n\r\nOutcomes\r\n\r\nThe aim of this talk  is to equip attendees with an understanding of the availalbe technology choices and  the knowledge to build their own data platforms. This would specifically be useful for attendees who may be software or backend engineers who may also be called upon to own the data stack to support business and analyst use cases. It may also help engineers who may be looking to re-platform legacy, expensive data platforms to a more modern data stack. \r\nFor research and personal projects, spinning up a modern platform could be useful for compute heavy analytics that have outgrown local development.", "code": "PRRPQ3", "state": "submitted", "created": "2025-01-03", "speaker_names": "Eric Thanenthiran", "track": "PyData: Data Handling & Engineering"}, {"title": "Securing digital resources via the implementation of blockchain using python and flask", "abstract": "Information integrity and security is a crucial aspect of our daily lives, hence, the concept of blockchain technology. Blockchain technology can be leveraged to build secure web apps that the developer of such apps cannot cheat the users of the app. Hence, having secured digital assets for both parties.", "full_description": "I will demonstrate the concept of Blockchain and cryptography with a practical example on how it can be used to secure any digital information such as a loan credit application or a sales application. In the past, I have successfully demonstrated security information integrity in a loan business and also in drug counterfeiting. There will be a live demonstration of the web app built with python flask framework.", "code": "B7Z7KD", "state": "rejected", "created": "2024-12-20", "speaker_names": "Hampo, JohnPaul A.C.", "track": "PyCon: Security"}, {"title": "Nowcasting financial crisis with deep learning techniques", "abstract": "This tutorial shows how apply classical models and new time series models (based on LLMs) to \r\nnowcasting stock markets crisis events.\r\nSpecifically, we'll how the transmission mechanisms across stock markets can \r\nbe used to train machine learning models to predict crisis events. The tutorial'll show \r\nthe entire pipeline: from the preparation of the dataset,\r\nhow balance observations and how measure our performances.", "full_description": "In this tutorial, I'll show how train machine learning models to predict a financial crisis event in the next 20 days.\r\nSpecifically, we'll speak about:\r\n- the preparation of the dataset\r\n- the selection of the variables with RandonForest  and Boruta method\r\n- the training of different models  (Classification Trees, Random Forests and Extreme Gradient Boosting)\r\n- the prediction / the measure of our performances.", "code": "7ALSWE", "state": "rejected", "created": "2024-12-30", "speaker_names": "Mauro Pelucchi", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Mastering Excel Automation with OpenPYXL", "abstract": "Tired about complex VBA macros and functions? Join the Excel revolution as we see how OpenPYXL Python library changes how we work with spreadsheets. We'll discover how to handle data with a touch of Python magic. By the end, you\u2019ll be an Excel expert, easily automating tasks and managing complex data with a few clicks. Get ready to improve your data skills!", "full_description": "Tired about complex VBA macros and functions? Join the Excel revolution as we see how OpenPYXL Python library changes how we work with spreadsheets. We'll discover how to handle data with a touch of Python magic. By the end, you\u2019ll be an Excel expert, easily automating tasks and managing complex data with a few clicks. Get ready to improve your data skills!\r\n\r\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "code": "TE3VDJ", "state": "rejected", "created": "2024-12-31", "speaker_names": "Jordi Bosch", "track": "PyCon: Programming & Software Engineering"}, {"title": "pytest tips and tricks for a better testsuite", "abstract": "pytest lets you write simple tests fast - but also scales to very complex scenarios: Beyond the basics of no-boilerplate test functions, this training will show various intermediate/advanced features, as well as gems and tricks.\r\n\r\nTo attend this training, you should already be familiar with the pytest basics (e.g. writing test functions, parametrize, or what a fixture is) and want to learn how to take the next step to improve your test suites.\r\n\r\nIf you're already familiar with things like fixture caching scopes, autouse, or using the built-in `tmp_path`/`monkeypatch`/... fixtures: There will probably be some slides about concepts you already know, but there are also various little hidden tricks and gems I'll be showing.", "full_description": "We'll cover things like:\r\n\r\n- Recommended pytest settings for more strictness\r\n- What's xfail and why is it useful?\r\n- How to mark an entire test file or single parameters\r\n- Ways to deal with parametrize IDs and syntax\r\n- Useful built-in pytest fixtures\r\n- Caching for fixtures\r\n- Using fixtures implicitly\r\n- Advanced fixture and parametrization topics\r\n- How to customize fixtures behavior based on markers or custom CLI arguments\r\n- Basics of patching, mocking, and alternatives\r\n- Short intro to property-based testing with Hypothesis\r\n- Various useful plugins, and a short teaser about how to write your own", "code": "SSHXXH", "state": "submitted", "created": "2024-12-01", "speaker_names": "Florian Bruhin", "track": "PyCon: Testing"}, {"title": "supplyseer: Computational Supply Chain with Python", "abstract": "This talk introduces supplyseer, an open-source Python library that brings advanced analytics to Supply Chain and Logistics. By combining time series embedding techniques, stochastic process modeling, and geopolitical risk analysis, supplyseer helps organizations make data-driven decisions in an increasingly complex global supply chain landscape. The library implements novel approaches like Takens embedding for demand forecasting, Hawkes processes for modeling supply chain events, and Bayesian methods for inventory optimization. Through practical examples and real-world use cases, we'll explore how these mathematical concepts translate into actionable insights for supply chain practitioners.", "full_description": "Supplyseer bridges the gap between theoretical supply chain analytics and practical implementation by providing a pythonic interface to advanced mathematical concepts. This talk will walk through the library's core components and demonstrate how they solve real-world supply chain challenges.\r\n\r\nOutline:\r\n\r\n1. Introduction to Modern Supply Chain Analytics\r\n- The need for sophisticated analytics in today's complex supply chains\r\n- Why traditional methods fall short\r\n- The role of probabilistic modeling and topological analysis\r\n\r\n2. Core Mathematical Foundations\r\n- Time series embedding techniques using Takens' theorem\r\n- Stochastic process modeling for demand forecasting\r\n- Bayesian approaches to Economic Order Quantity (EOQ)\r\n- Point process modeling with Hawkes processes\r\n- Network analysis for supply chain risk assessment\r\n\r\n3. Library Architecture and Design Philosophy\r\n- Object-oriented design for supply chain analytics\r\n- Integration of multiple analytical approaches\r\n- Extensible architecture for custom analytics\r\n- Performance considerations and optimizations\r\n\r\n4. Key Features Deep Dive\r\na) Demand Forecasting Module\r\n   - Stochastic demand process simulation\r\n   - Time-delay embedding for pattern recognition\r\n   - Mixture density networks for uncertainty quantification\r\n\r\nb) Risk Analysis Tools\r\n   - Geopolitical risk assessment\r\n   - Supply chain network visualization\r\n   - Real-time monitoring and alerting\r\n   - Trade restriction impact analysis\r\n\r\nc) Inventory Optimization\r\n   - Bayesian EOQ implementation\r\n   - Multi-echelon inventory optimization\r\n   - Stockout probability calculation\r\n   - Vector field analysis for inventory dynamics\r\n\r\n5. Practical Applications\r\n- Route optimization with geopolitical risk consideration\r\n- You and your suppliers play cooperative games: game-theoretic Supply Chain\r\n- Supply Chain Digital Twins\r\n- Real-time risk monitoring and mitigation\r\n\r\n6. Integration with Data Science Ecosystem\r\n- Compatibility with pandas and polars\r\n- Integration with scikit-learn pipeline\r\n- Visualization with matplotlib and seaborn\r\n- Performance optimization with numpy\r\n\r\n7. Future Directions\r\n- Planned features and enhancements\r\n- Community contribution opportunities\r\n- Integration with other supply chain tools\r\n- Research directions in supply chain analytics\r\n\r\n8. Interactive Demonstrations\r\n- Live coding examples\r\n- Real-world data analysis\r\n- Visualization of supply chain dynamics\r\n- Risk assessment workflows\r\n\r\nThe talk will include code examples and practical demonstrations, showing how to:\r\n- Implement stochastic demand forecasting\r\n- Analyze supply chain risks using network analysis\r\n- Optimize inventory levels using Bayesian methods\r\n- Visualize supply chain dynamics using vector fields\r\n- Monitor and assess geopolitical risks\r\n\r\nTarget Audience:\r\nThis talk is aimed at data scientists, supply chain analysts, and Python developers interested in applying advanced analytics to supply chain problems. Attendees should have intermediate Python knowledge and basic familiarity with data science libraries like pandas and numpy.\r\n\r\nPrerequisites:\r\n- Python programming experience\r\n- Basic understanding of supply chain concepts\r\n- Familiarity with pandas and numpy\r\n- Basic knowledge of probability and statistics\r\n\r\nTakeaways:\r\nAttendees will learn:\r\n- How to implement advanced supply chain analytics in Python\r\n- Practical applications of mathematical concepts in supply chain\r\n- Best practices for supply chain data analysis\r\n- Techniques for visualizing and monitoring supply chain dynamics\r\n- Methods for quantifying and managing supply chain risks\r\n\r\nAll code examples and demonstrations will be available in a GitHub repository, allowing attendees to experiment with the concepts presented and apply them to their own supply chain challenges.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b", "code": "N9CAUM", "state": "submitted", "created": "2024-12-19", "speaker_names": "rostami.jako@gmail.com", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Why E.ON Loves Python", "abstract": "Join me as I share my 20-year journey with Python and its pivotal role at E.ON. Discover how we transitioned fully to Python, streamlined our development framework, and embraced MLOps principles. Learn about some of our AI projects, including image analysis and real-time inference, and our steps towards open-sourcing code to foster innovation in the energy sector. Explore why Python is our go-to language for data science and collaboration.", "full_description": "In this talk, I will share my journey with Python, spanning over 20 years, and how it has become an integral part of our work at E.ON. My experience with open source began over 30 years ago during my research as a Theoretical Particle Physicist, where sharing insights and code was a daily practice. Transitioning to a software developer role at a start-up, I initially used Perl for various tasks but soon realized the challenges of code readability and collaboration. Python, with its enforced indentation and readability, quickly became my language of choice.\r\n\r\nAt E.ON, Python is our go-to language for Data Science tasks. In our team we recently migrated another programming language codebase to Python to streamline our development framework and attract top talent. Python's straightforward modularization into packages and modules simplifies maintenance and lineage, especially in cloud-based pipelines, and helps prevent vendor lock-in. The robust toolchain for code quality checks, testing, and building packages makes Python a no-brainer for development and supports our MLOps principles.\r\n\r\nI will discuss how Python facilitates collaboration globally at E.ON and share examples of our MLOps principles in action. Highlights include image analysis projects like object detection with batch inferencing and instance segmentation with real-time inference endpoints. Additionally, I will detail E.ON's steps towards open-sourcing some of our codebases, enabling other energy companies to build on our projects.\r\n\r\nJoin me to explore why Python is not just a tool but a catalyst for innovation and collaboration at E.ON.", "code": "JM3G8S", "state": "submitted", "created": "2024-12-13", "speaker_names": "Christer Friberg", "track": "PyCon: MLOps & DevOps"}, {"title": "RAG: Tricks from the trenches", "abstract": "RAG. You\u2019ve heard about it. Your entire family has heard about it. However, most LLM based projects never make it to production. In this talk, I\u2019ll teach you some tricks on how to make a successful RAG app - so you don\u2019t have to make the same mistakes.", "full_description": "This talk is about deconstructing the myths and misconceptions around a good RAG workflow. I will give the attendees practical advice on how to make their retrieval workflow better. It will be filled with practical advice that attendees can go out an apply directly.\r\n\r\nSpoiler? It\u2019s all about good Python.\r\n\r\nRough Outline: \r\n    Intro Slide \r\n    RAG - what is it? \r\n    The Langchain 4 liner and what\u2019s wrong with it \r\n    It\u2019s all about the data and validation \r\n    Leverage Pydantic validation and structured data \r\n    Evaluate very early \r\n    Representative vs. Keyword based \r\n    Summarization tricks \r\n    Pulling everything together\r\n    Logging everything \r\n    Final thoughts and QA\r\n\r\nNote: The structure might change slightly", "code": "NAU9S8", "state": "submitted", "created": "2024-12-20", "speaker_names": "Duarte O.Carmo", "track": "PyCon: MLOps & DevOps"}, {"title": "\ud83e\udd80 R\u00fcstzeit: Asynchronous Concurrency in Python & Rust", "abstract": "Many Python developers are enhancing their Rust knowledge and want to take the next step in translating their understanding of advanced concepts like asynchronous programming. \r\n\r\nIn this talk, I'll help you take that step by juxtaposing Python's asyncio with Rust's async ecosystems, tokio and async-std. Through real-world examples and insights from conversations with graingert, co-author of Python's Anyio, we'll explore how each language approaches asynchronous execution, highlighting similarities and differences in syntax, performance, and ecosystem support. \r\n\r\nThis talk aims to persuade you that by leveraging Rust's powerful type system and compiler guarantees, we can build fast, reliable async code that's less prone to race conditions and concurrency bugs. Whether you're a Pythonista venturing into Rust or a Rustacean curious about Python's concurrency model, this session will provide practical insights to help you navigate async programming across both languages.\r\n\r\nWelcome to R\u00fcstzeit: Prepare to navigate async programming across both ecosystems.", "full_description": "Talk Timings (30 minutes):\r\n\r\nIntroduction and Hybrid Programming in Python and Rust [5 mins]\r\nAsynchronous Programming in Python [5 mins]\r\nAsynchronous Programming in Rust [5 mins]\r\nPerformance Comparison: Python vs. Rust [1 min]\r\nLeveraging Rust's Type System and Compiler Guarantees [5 mins]\r\nCase Study: \"A Million Large Language Monkeys at a Million Typewriters\" \u2013 Building Scalable Microservices with Tokio [7 mins]\r\n(Optional) Tom's Library: AnyIO and Unified Async in Python [3 mins]\r\nConclusion and Takeaways [3 mins]\r\n\r\n---\r\n\r\nMany Python developers are enhancing their Rust knowledge and want to take the next step in translating their understanding of advanced concepts like asynchronous programming. \r\n\r\nIn this talk, I'll help you take that step by juxtaposing Python's asyncio with Rust's async ecosystems, tokio and async-std. Through real-world examples and insights from conversations with graingert, co-author of Python's Anyio, we'll explore how each language approaches asynchronous execution, highlighting similarities and differences in syntax, performance, and ecosystem support. \r\n\r\nThis talk aims to persuade you that by leveraging Rust's powerful type system and compiler guarantees, we can build fast, reliable async code that's less prone to race conditions and concurrency bugs. Whether you're a Pythonista venturing into Rust or a Rustacean curious about Python's concurrency model, this session will provide practical insights to help you navigate async programming across both languages.\r\n\r\nWelcome to R\u00fcstzeit: It's time to prepare for async programming in Python and Rust.\r\n\r\nFurther Resources:\r\n\r\nhttps://rust-lang.github.io/async-book/\r\nhttps://anyio.readthedocs.io/en/stable/\r\nhttps://github.com/graingert", "code": "FGFFEE", "state": "submitted", "created": "2025-01-05", "speaker_names": "Jamie Coombes", "track": "General: Rust"}, {"title": "Mastering Git & GitHub for Data Science and Machine Learning: From Version Control to Collaboration.", "abstract": "Discover how to harness Git and GitHub to boost your data science and machine learning projects. This talk will explore the full spectrum of version control essentials and some collaboration tools specifically for data scientists and ML engineers. Starting with foundational Git commands and progressing to advanced features, you'll learn to efficiently manage your code, track changes, and work seamlessly with others. We'll also introduce you to GitHub Codespaces, Dev Containers, and Jupyter Notebooks in VS Code, providing robust environments for development and experimentation. Whether you're flying solo or collaborating with a team, this talk will empower you with the skills to enhance productivity and maintain high code quality. \r\nExpect live demonstrations of typical Git commands and GitHub workflows, alongside discussions on best practices for repository management, security enhancement, handling large files, and automating processes with GitHub Actions. We'll also share tips to avoid common pitfalls and troubleshoot issues effectively.", "full_description": "Unlock the power of Git and GitHub to streamline your data science and machine learning workflows.\r\nIn this talk, we\u2019ll start with learning essential version control practices and collaborative techniques tailored for data scientists and ML engineers. We'll cover everything from basic commands to advanced features like how to track changes, manage experiments, and collaborate effectively on your projects. We\u2019ll also dip into GitHub Codespaces, Dev Containers and Jupyter Notebooks in VS Code, development and experimentation environments essential for novice to advanced Data Scientists. Whether you're working on solo projects or in a team, this talk will equip you with the tools and strategies needed to elevate your productivity and maintain code quality. \r\n\r\n**Outline**\r\n\r\n**1- Introduction to Version Control (4 minutes)**\r\nTopics:\r\n-Importance of version control in data science and ML projects.\r\n- Brief overview of Git and GitHub.\r\n\r\nObjectives:  \r\n- Explain why version control is essential for collaborative projects.\r\n- Set the stage for the detailed exploration of Git and GitHub.\r\n\r\n\r\n**2- Getting Started with Git (5 minutes)**\r\nTopics:\r\n- Installing Git.\r\n- Basic Git commands (init, clone, add, commit).\r\n- Creating and managing repositories.\r\n\r\nObjectives:\r\n- Provide a hands-on introduction to using Git.\r\n- Ensure attendees can initialize and manage their own repositories.\r\n\r\n\r\n**3- Collaboration with GitHub and Codespaces (6 minutes)**\r\nTopics:\r\n- Setting up a GitHub account.\r\n- Pushing local repositories to GitHub.\r\n- Using GitHub features (Issues, Pull Requests).\r\n- Open existing or creating new projects with Codespaces\r\n- Opening it Codespaces with VS Code, PyCharm or other IDE Dev Container\r\n\r\nObjectives:\r\n- Demonstrate how to use GitHub for collaboration with Codespaces\r\n- Show practical examples of GitHub features that facilitate teamwork with Codespaces and Dev Containers\r\n\r\n\r\n**4- Branching, Merging and Rulesets (6 minutes)**\r\nTopics:\r\n- Creating and switching branches. \r\n- Managing accesses to your branches with rulesets \r\n- Merging branches and resolving conflicts.\r\n\r\nObjectives:\r\n- Teach attendees how to manage different project branches.\r\n- Equip them with skills to handle and resolve merge conflicts.\r\n\r\n\r\n**5- Best Practices and Tips (4 minutes)**\r\nTopics:\r\n- Best practices for Git commit messages.\r\n- Managing large files in Git.\r\n- Using GitHub Actions for CI/CD.\r\n\r\nObjectives:\r\n- Share practical tips and best practices.\r\n- Highlight common pitfalls and how to avoid them.\r\n\r\n\r\n**6- Q&A (5 minutes)**\r\nTo address attendee questions and clarify any doubts.", "code": "NHXSWS", "state": "submitted", "created": "2024-12-20", "speaker_names": "Ariane Djeupang", "track": "PyCon: MLOps & DevOps"}, {"title": "How I tried to explore Italian government financials with agents and Python", "abstract": "I wanted to start a political party to change Italian politics, but first I needed to understand where our taxes money goes. So I built an agent to crack open government spending data and figure it out!", "full_description": "In this talk I'll tell you the story on how I used Python and AI agents to analyse italian government financials.\r\n\r\nI live in Italy, and I'm not a fan of the current government in lots of ways. So I wanted to make a change somehow, and after lots of thinking I thought I should found a party. A political party.\r\n\r\nI started brainstorming ideas, reforms to help those in need, create positive changes for the people. Though at a certain point I faced the hard truth. I don't know much about government finance, I can't make plans without knowing how money is spent.\r\n\r\nThat's when I started toying with the idea of using AI to try and make sense of it all.", "code": "FZNSR8", "state": "submitted", "created": "2024-12-19", "speaker_names": "Silvano Cerza", "track": "PyData: Generative AI"}, {"title": "Safeguard your precious API endpoints built on FastAPI using OAuth 2.0", "abstract": "Is implementing authorization on your API endpoints an afterthought? Who should have access to your API endpoints? Is it secure? This talk covers using OAuth 2.0 to secure API endpoints built on FastAPI following industry-recognized best practices. Come on a journey with me from taking your API endpoints to being functional AND secure. When you follow secure identity standards, you\u2019ll be equipped with a deeper understanding of the critical need for authorization.", "full_description": "Audience Level: Beginners, Pythonistas who build on FastAPI who are not necessarily security experts but still need to deploy secure APIs.\r\n\r\nHistory of OAuth 2.0? (3 mins)\r\n- Background/history on OAuth \r\n- Why do we need OAuth 2.0?\r\n\r\nAuthorization Challenge (2 mins)\r\n- Why implement secure authorization now rather than later?\r\n- Data sensitivity\r\n\r\nOAuth 2.0 Overview (3 mins)\r\n- Core concepts\r\n- Key features: What are JWTs?\r\n- Benefits of using OAuth 2.0\r\n\r\nTechnical Implementation (4 mins)\r\n- Components of OAuth 2.0 \r\n- Different types of authorization flows and use cases\r\n- API setup on FastAPI\r\n\r\nDemo with FastAPI (12 mins) \r\n- Create an endpoint in FastAPI framework and secure it with OAuth 2.0\r\n- What are the different identity providers that can provide authorization?\r\n- Troubleshooting common issues \r\n\r\nBest Practices (4 mins)\r\n- Industry-standard protocol\r\n- Token-based security \r\n- Should you build your authorization server?\r\n\r\nNext Steps (2 mins)\r\n- Ability to integrate/provide SSO with various IdPs\r\n- Share resources to learn more including blogs, GitHub repo, etc.\r\n- Got questions? Connect with me!", "code": "K9ACTV", "state": "submitted", "created": "2024-12-22", "speaker_names": "Semona Igama", "track": "PyCon: Security"}, {"title": "From Desktop to Browser: Crafting Browser-Native Game Engine with Webassembly", "abstract": "Have you ever attempted to run a Python game in a browser only to have it crawl? We'll take care of that. This session demonstrates how to use Pygbag and WebAssembly to transform your Pygame creations into lightning-fast web games. No more having to choose between accessibility and performance; we'll see in real time how to create games that function flawlessly on all platforms, including desktop and web browsers. Gain useful skills to make cross-platform games that are enjoyable to play, whether you're developing a straightforward platform or a simulation with a lot of physics.", "full_description": "Have you ever wondered how to turn your Python games into lightning-fast web experiences without compromising speed? In this session, overcome the browser barrier and learn how to create powerful game engines that smoothly connect desktop and web platforms using Pygame, pygbag, and WebAssembly.\r\n\r\nStarting with familiar Pygame foundations, discover how pygbag transforms Python games into browser-ready experiences. Learn techniques for managing state, particle effects, and physics computations with near-native performance. Through live demonstrations, explore practical strategies for optimizing rendering and maintaining smooth gameplay across different environments.\r\n\r\nBy the end of the talk, learn how to create WebAssembly-powered games that function flawlessly on any platform without sacrificing user experience, and understand the architectural patterns that enable high-performance browser-based game engines.", "code": "Q8V9MP", "state": "submitted", "created": "2024-12-22", "speaker_names": "Neeraj Pandey", "track": "PyCon: Django & Web"}, {"title": "Detecting Complex Concepts in Unstructured Text Data", "abstract": "How do we detect complex ideas like attitudes toward science or politics in large amounts of text? Traditional methods struggle with abstract concepts, but domain-aware contextual representations make this possible. This innovative approach uncovers hidden patterns in how ideas are framed, whether in speeches, articles, or social media. By analyzing text using an advanced LLM-driven, domain-informed approach, we can reveal new insights into topics that are hard to measure. It\u2019s a powerful tool for unlocking meaning in unstructured data.", "full_description": "How do we measure abstract concepts, like attitudes toward politics or science, from vast amounts of unstructured text? This talk introduces domain-aware contextual representations, a novel method that leverages advanced natural language processing to detect and analyze complex ideas embedded in speech corpora. Unlike traditional approaches, which often rely on predefined measures, this technique adapts to the specific context of the data, making it possible to extract meaningful insights from political speeches, social media, or other forms of discourse.\r\n\r\nUsing domain-aware contextual representations, we can uncover subtle patterns in how political leaders talk about science, even when the concept itself is implicit or abstract. By incorporating domain-specific knowledge into the analysis, the method generates richer, more accurate representations of complex attitudes, such as how science is framed in terms of its role in society, its values, or its institutional importance.\r\n\r\nThis approach leverages open LLMs, like Mistral 7B and Llama 3, along with powerful Python libraries like SentenceTransformers, to build contextual embeddings that capture both the general linguistic context and the specific nuances of discourse of interest: science in this case. I will demonstrate how this method can be applied to a large dataset of political speeches, showing how different political regimes and leaders use science rhetoric in diverse ways. Early results highlight striking differences: autocratic leaders often focus more on the normative value of science, while democratic leaders emphasize its institutional role. The method also reveals how science discourse peaks in specific domains, such as healthcare and technology, during times of crisis.\r\n\r\nThis talk will showcase how domain-aware contextual representations, powered by open-source tools, open new possibilities for detecting and analyzing abstract concepts in text data, offering exciting opportunities for both researchers and practitioners working with large-scale text analysis in Python.", "code": "TXGAYW", "state": "submitted", "created": "2024-12-22", "speaker_names": "Sukayna Younger-Khan", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Code & Community: The Synergy of Community Building and Task Automation", "abstract": "The Python community is built on a culture of support, inclusion, and collaboration. Sustaining this welcoming environment requires intentional community-building efforts, which often involve repetitive or time-consuming tasks. These tasks, however, can be automated without compromising their value\u2014freeing up time for meaningful human engagement.\r\n\r\nThis talk showcases my project aimed at supporting underrepresented groups in tech, specifically through building Python communities on Mastodon and Bluesky. A key part of this initiative is the \"Awesome PyLadies\" repository, a curated collection of PyLadies blogs and YouTube channels that celebrates their work. To enhance visibility, I created a PyLadies bot for social media. This bot automates regular posts and reposts tagged content, significantly extending their reach and fostering an engaged community.\r\n\r\nIn this session, I\u2019ll cover:\r\n- The role of automation in community building\r\n- The technical architecture behind the bot\r\n- A hands-on demo on integrating Google\u2019s Gemini into community tools\r\n- Upcoming features and opportunities for collaboration\r\n\r\nBy combining Python, automation, and modern AI capabilities, we can create thriving, inclusive communities that scale impact while staying true to the human-centered ethos of open source.", "full_description": "My planned outline for the talk is as follows:\r\n\r\n- **Introduction**: A brief overview of the project and its goals, focusing on community building and inclusivity within the Python ecosystem (3 minutes)\r\n- **The Importance of Visibility**: Explain the background of the project and why visibility is important (3 minutes)\r\n- **Bot Architecture and Setup**: A technical walkthrough of the bot, its architecture, and how it operates to extend the reach of community content on platforms like Mastodon or Bluesky (5 minutes)\r\n- **Hands-On Demo: Task Automation with Google\u2019s Gemini and GitHub Actions**: A step-by-step guide to integrating Google\u2019s Gemini and GitHub Actions for creating low-barrier, automated workflows tailored for community-building tasks (12 minutes)\r\n- **Looking Ahead**: Provide a forward-looking perspective (upcoming features of the project and future developments) (2 minutes)\r\n- **Q&A and Buffer** (5 minutes)\r\n\r\nI hope that the talk will inspire more Pythonistas to automate their tasks, and also more PyLadies to share material publicly and make the public perception of experts in the field more diverse.", "code": "PLMJZ8", "state": "submitted", "created": "2024-12-17", "speaker_names": "Cosima Meyer", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Bag-of-Tricks for Improving RAG Chatbots", "abstract": "Retrieval Augmented Generation (RAG) is one of the most popular applications of generative AI in the industry. In this talk, we will provide practical insights on how to build better conversational assistants that leverage RAG to tap into your company\u2019s data and provide accurate answers on user questions. \r\n\r\nWe will discuss important techniques to improve the quality of RAG chatbots that we learned from the practical experience of building generative AI solutions and scientific literature. The talk will cover tips for improving retrievers, including features like hybrid search, nested chunking, reranking, and others. We will also discuss improvements to the answer generation, including prompt engineering and caching.", "full_description": "Retrieval Augmented Generation (RAG) is one of the most popular applications of generative AI in the industry. Despite the novelty of RAG, many theoretical and empirical studies have been published recently, discussing the performance and implementation of RAG chatbots in different domains.\r\n\r\nThis talk will provide practical insights on how to build better conversational assistants that leverage RAG to tap into your company\u2019s data and provide accurate answers on user questions. We will combine insights from the scientific literature and our practical experience from building chatbots for different customers to focus on the most impactful RAG techniques that work in practice.\r\n\r\nThe talk will cover tips for improving RAG retrievers, including features like hybrid search, nested chunking, reranking, adding meta-data to chunks, and others. We will also discuss improvements to the answer generation, including prompt engineering, caching, and more. Finally, we will also touch on how to reliably evaluate RAG chatbots.", "code": "VXGCQ9", "state": "submitted", "created": "2025-01-05", "speaker_names": "Nikita Kozodoi", "track": "PyData: Generative AI"}, {"title": "From Algorithm to Action: Building a DIY Distributed Trading Platform with Open Source", "abstract": "In this talk, we'll explore how you can implement your own distributed system for algorithmic trading leveraging the power of open source without being dependent on trading bot providers.\r\n\r\nWe will discuss different challenges occurring in HFT inter alia processing massive amounts of data with low latency and reliable risk control and how to solve them. Furthermore we will touch on the topic of regulatory requirements in trading.\r\n\r\nThese challenges will be addressed through a distributed system implemented in Python, utilizing Kafka for real-time data streaming, PostgreSQL for persistent storage and DuckDB for high-performance analysis. We will examine approaches to decouple the components to re-use and scale them across different markets.\r\n\r\nCryptocurrency markets are used as a proving ground for the PoC due to easy availability for everyone.", "full_description": "## Who is this talk for\r\n\r\nThis talk is ideal for all software engineers interested in financial technology, quantitative developers looking to understand modern trading infrastructure, and technical architects exploring distributed systems in high-stakes environments.  \r\nThis talk will NOT discuss specific trading strategies or give any financial advice.\r\n\r\n## Outline\r\n\r\n* Motivation  \r\n* Fundamental trading concepts and market mechanics  \r\n* Market data ingestion and processing  \r\n* Order management and execution  \r\n* Implementation of trading strategies  \r\n* Post-trade analysis  \r\n* Outlook\r\n\r\n## Motivation\r\n\r\nThe landscape of financial trading has undergone a dramatic transformation over the past decades. What was once the exclusive domain of institutional players on physical trading floors has evolved into a digitized, accessible marketplace where individual traders can participate from anywhere in the world. The emergence of commission-free trading apps and cryptocurrency exchanges has brought market participation to millions of new retail traders.  \r\nThis enables everyone to participate with their own trading system in global markets.\r\n\r\nIn this talk, we'll explore how you can implement your own distributed system for exchange trading leveraging the power of open source without being dependent on trading bot providers. While we won't be able to cover every aspect in depth, we'll address the most essential elements.\r\n\r\nCryptocurrency markets are used as a proving ground for the PoC due to easy availability for everyone.\r\n\r\n## Fundamental Trading Concepts and Market Mechanics\r\n\r\nWe'll begin by exploring essential trading concepts:\r\n\r\n* Order book dynamics  \r\n* Orders, Trades and Positions  \r\n* Different types of orders and their implications for system implementation  \r\n* Regulatory requirements  \r\n* Performance of strategies\r\n\r\nThese lead to different considerations in system design and architecture:\r\n\r\n* De-coupling of exchange interfaces and trading strategies to use same strategy for different markets by using adapter pattern  \r\n* Horizontal scaling to handle data load  \r\n* Need of low latency components and their communication to properly react to market  \r\n* Need of streaming data for real-time risk management  \r\n* Need of persistent storage for regulatory data and post-trading-analysis  \r\n* Need of order action recording and post-trading analysis for performance evaluation\r\n\r\n## Market Data Ingestion and Processing\r\n\r\nThe foundation of any trading system is its ability to efficiently process market data. This includes a Python component responsible for real-time normalization and standardization of multi-venue data:\r\n\r\n* Efficient market data representation and storage structures  \r\n* Techniques for handling high-throughput data without compromising latency  \r\n* Market data recording for post-trading analysis using Kafka\r\n\r\n## Order Management and Execution\r\n\r\nCritical components for managing the trading lifecycle. This includes a Python component responsible for normalization and standardization of multi-venue order interfaces:\r\n\r\n* Order action handling (placing orders, modifying orders) and keeping track of orders  \r\n* Global real-time position tracking and risk calculation using Kafka  \r\n* State recovery and system restart procedures  \r\n* Audit trail implementation and transaction logging using Postgres\r\n\r\n## Implementation of Trading Strategies\r\n\r\nWe'll explore the practical aspects of implementing trading strategies in Python using the previously discussed system components:\r\n\r\n* Usage of provided market data  \r\n* Placing orders and keeping track of positions  \r\n* Fast communication with market data and order components using gRPC  \r\n* Recording of strategy internals for post-trade analysis\r\n\r\n## Post-trade Analysis and Performance Monitoring\r\n\r\nWe will take a closer look to evaluate trading strategies performance with:\r\n\r\n* High-performance post-trade analytics using DuckDB  \r\n* Live performance monitoring using Kafka\r\n\r\n## Outlook\r\n\r\nAt the end we will have a brief outlook what other challenges might occur e.g.:\r\n\r\n* Other market types (Finance/Equity/ETF and Energy)  \r\n* Latency considerations  \r\n* Taxes", "code": "BR3D83", "state": "submitted", "created": "2024-12-14", "speaker_names": "Eugen Geist", "track": "PyCon: Programming & Software Engineering"}, {"title": "Intro to using LLMs with Python", "abstract": "With the advent of AI, LLMs have become more powerful and there are many Python libraries that help you to leverage these LLMs in your application. But because of the novelty of these libraries and the speed with which things are constantly changing in this domain, it\u2019s hard to get hold of things. In this talk we will discuss how to structure your Python application to use LLMs and configure them to use your proprietary data to answer questions, we will go through some common libraries like LlamaIndex and Langchain that help you to use LLMs efficiently and we will discuss some common problems that such applications can face.", "full_description": "In this talk, we will discuss primarily these topics which revolve around using LLMs in a Python project - \r\n1. Different LLMs and their current state\r\n2. LlamaIndex and Langchain Python libraries\r\n3. Using LlamaIndex for indexing your data\r\n4. Langchain to connect all the AI agents\r\n5. Integrating with a vector storage\r\n6. Avoiding common pitfalls\r\n\r\nThe technical requirement of this talk is basic understanding of Python", "code": "N9NVEN", "state": "submitted", "created": "2024-12-30", "speaker_names": "Prabhu Pant", "track": "PyCon: Programming & Software Engineering"}, {"title": "Serverless Orchestration: Discovering What's Out There", "abstract": "Orchestration is a typical challenge in the data engineering world. Scheduling your data transformation jobs via CRON-jobs is cumbersome and error-prone. Furthermore, with an increasing number of jobs to manage it gets in-oversee able. Tools like Apache Airflow, Dagster, Luigi, and Prefect are known for addressing these challenges but often require additional resources or investment. With the advent of serverless orchestration tools, many of these disadvantages are mitigated, offering a more streamlined and cost-effective solution.\r\n\r\nThis session provides a comprehensive overview of combining serverless architecture with orchestration. We will start by defining the core concepts of orchestration and serverless technologies and discuss the benefits of integrating them. The talk will then analyze solutions available in the cloud vendor space. Attendees will leave with a well-rounded understanding of the tools and strategies available in serverless orchestration.", "full_description": "Orchestration is a typical challenge in the data engineering world. Scheduling your data transformation jobs via CRON-jobs is cumbersome and error-prone. Furthermore, with an increasing number of jobs to manage it gets in-oversee able. Tools like Apache Airflow, Dagster, Luigi, and Prefect are known for addressing these challenges but often require additional resources or investment. With the advent of serverless orchestration tools, many of these disadvantages are mitigated, offering a more streamlined and cost-effective solution.\r\nBeyond data engineering, serverless orchestration holds substantial potential for classical software engineering, especially as organizations explore serverless approaches for optimizing efficiency and reducing overhead.\r\n\r\nIn this talk you will explore:\r\n* Basic Introduction to Serverless Orchestration: \r\n      - What is orchestration about?\r\n      - What is serverless about?\r\n      - Why combining the two of them?\r\n* Offerings from Major Cloud Vendors:\r\n      - Analyzing solutions from leading cloud providers in the realm of serverless orchestration\r\n* Patterns and Solutions for Serverless Orchestration in Software Engineering:\r\n      - Exploring how serverless orchestration can be applied within classical software engineering contexts\r\n\r\nParticipants will leave this session equipped with a comprehensive understanding of the serverless orchestration landscape and its applications across different engineering disciplines.", "code": "AGY8CT", "state": "submitted", "created": "2024-12-06", "speaker_names": "Tim Bossenmaier", "track": "PyCon: Programming & Software Engineering"}, {"title": "Graph Machine Learning in all its glory!", "abstract": "Our world is complex: one approach to understanding and learning more about relationships within the data is to represent it as a network or a graph - with entities as nodes and relations between them as edges. Network applications are abundant - Facebook, knowledge databases like Wikipedia, traffic routes, molecular pathways, and the fun starts when we start thinking of the physical world as graphs. Where there\u2019s data, there\u2019s uncertainty and the need to predict the future to make it a *little less* uncertain, which is why we need machine learning.\r\n\r\n**This talk will be about modeling one such network using a Graph Neural Network (GNN) and predicting on it using Deep Graph Library, an efficient and scalable open source framework to train and serve GNNs. While GNNs can be used for any ML task, we\u2019ll focus on building a recommendation model.**\r\n\r\nBut the transition from traditional data structures to graph-based models is not straightforward. Unlike tabular data, graphs require adapting ML principles to accommodate their topological and relational complexities. \r\n\r\n**This talk will help participants gain practical insights into modeling their data as graphs and leveraging GNNs to build a recommendation system - from scratch. We\u2019ll go over each ML model building step and discuss how to adapt our learnings from tabular data to graph data.**", "full_description": "This talk is to get the audience excited about graph ML and hopefully inspire them to think of their data as a potential graph which can open doors for this new paradigm of ML. Graphs are complex, but then again so is most of the data now.\r\n\r\nThe talk will be supported by code snippets and will be using an open source recommendation dataset. \r\n\r\n### Outline: \r\n\r\n**1. Intro to Graph Machine Learning** (6 minutes)\r\n- Brief overview of traditional data vs. graph-based data and graph components\r\n- Networks in real-life and relevance\r\n- ML on graphs: recent advancements\r\n\r\n**2. Graph Neural Networks (GNNs) and DGL** (6 minutes)\r\n- What are GNNs and how do they work? \r\n- Overview of the Deep Graph Library (DGL) \r\n- Architecture for building recommendation models using GNNs\r\n- GNN Model Architecture\r\n\r\n**3. Training a GNN model in DGL** (15 minutes)\r\n\r\nThis section is meant to address the following key questions on training and serving models using DGL:\r\n- How do we transform tabular data into meaningful nodes and edges? \r\n- How do we prepare data for graph modeling?\r\n- Can we split train / test data randomly as is convention? \r\n- How do we represent categorical/numerical features? \r\n- How do we train and make predictions on this model? \r\n- (optional): How do we decide on a model architecture?\r\n- How do we evaluate this model?\r\n- How do we re-train this model? \r\n- What are the data / scalability limitations when training GNNs?\r\n\r\nWe will also discuss certain common pitfalls and how to avoid them. \r\n\r\n**4. Q&A and Closing Remarks** (3 minutes)\r\n\r\n\r\nAfter the talk, the audience should have a good understanding about:\r\n- How to structure their data as graph components and prepare it for modeling\r\n- What graph machine learning is all about, what a GNN is and how do we train and serve GNNs in DGL?\r\n- End-to-end ML tasks on graph data: best practices", "code": "CHAEXA", "state": "accepted", "created": "2024-12-19", "speaker_names": "Shreya Khurana", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Hey, what\u2019s your MTTR?", "abstract": "Even with hundreds of tools available today, monitoring or restoring a service is often a painful experience. Logs are always either too little or too many, services are not instrumented correctly, and often teams are chasing yet another tool instead of thinking about what they actually need. Feels familiar? Then you\u2019re invited.", "full_description": "Writing software and having good test coverage is not enough to ensure the reliability of your services. Once you hit production and get real traffic, magic things usually happen!\r\n\r\nYou find bugs, services go down, in the worst scenarios you get notified by your users, and if that\u2019s not enough you might spend hours debugging or trying to replicate an issue.\r\n\r\nMTTR is one of the DORA metrics, the time it takes to restore a service. In other words, how long does it take to discover that one of your services is either down or not working like expected.\r\n\r\nDuring this talk I\u2019ll talk about MTTR and I\u2019ll share real world experiences, on how to bring your MTTR back to something that doesn\u2019t frustrate you or your team, how to go from zero to hero and bring your services into a state where deploying to prod doesn\u2019t feel scary anymore.", "code": "FBRM8W", "state": "submitted", "created": "2024-12-22", "speaker_names": "Christian Barra", "track": "PyCon: MLOps & DevOps"}, {"title": "From Trees to Transformers: GetYourGuide\u2019s Journey Towards Deep Learning for Ranking", "abstract": "GetYourGuide, a global marketplace for travel experiences, reached diminishing returns with its XGBoost-based ranking system. We switched to a Deep Learning pipeline in just nine months, maintaining high throughput and low latency. We iterated on over 50 offline models and conducted more than 10 live A/B tests, ultimately deploying a PyTorch transformer that yielded significant gains. In this talk, we will share our phased approach\u2014from a simple baseline to a high-impact launch\u2014and discuss the key operational and modeling challenges we faced. Learn how to transition from tree-based methods to neural networks and unlock new possibilities for real-time ranking.", "full_description": "GetYourGuide is a global online marketplace that helps travelers discover and book the best experiences. One of our core challenges is ensuring users always see the most relevant activities first\u2014a task historically powered by an XGBoost-based ranking system. However, as we continued refining our tree-based models, returns on incremental improvements began to plateau. To spark our next step change in performance, we decided to adopt Deep Learning.\r\n\r\nIn this talk, we will share how, in just nine months, we migrated our ranking pipeline to a Deep Learning architecture while maintaining tight latency and high-throughput requirements. We will walk through our phased approach, starting with a minimal viable model to confirm our production setup and gradually increasing its complexity. Along the way, we tested over 50 iterations offline and ran more than 10 live A/B tests to validate the impact on our customers. Ultimately, we rolled out a PyTorch transformer-based model with significant business impact. We will also discuss the main challenges we faced on the operational and modeling sides, how we overcame them, and the lessons we learned.\r\n\r\nYou will leave with practical strategies for transitioning from traditional tree-based models to neural networks in production. Join us to learn how to advance your machine-learning capabilities and unlock new dimensions of relevance and personalization for real-time ranking.", "code": "83QH37", "state": "accepted", "created": "2024-12-21", "speaker_names": "Theodore Meynard, Mihail Douhaniaris", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "From Notebook to Production: Deploying Machine Learning Models with Confidence.", "abstract": "Transitioning machine learning models from development to production can often feel daunting. This session aims to demystify the deployment process, offering a comprehensive guide to moving your models into production environments securely and efficiently. We\u2019ll delve into various deployment strategies, including containers, serverless architectures, and dedicated ML platforms. Attendees will learn best practices for monitoring, scaling, and maintaining models post-deployment to ensure they deliver consistent real-world value. With actionable insights and practical tools, you'll leave this talk equipped to confidently deploy your machine learning models and make the leap from development to production seamless.", "full_description": "Transitioning machine learning models from development to production can be challenging. This talk demystifies the deployment process, offering a comprehensive guide to getting your models into production environments securely and efficiently. We'll explore various deployment strategies, including containers, serverless architectures, and dedicated ML platforms. Learn about best practices for monitoring, scaling, and maintaining your models post-deployment. By the end of this session, you'll be equipped with actionable insights and tools to deploy your models confidently and ensure they deliver real-world value.\r\n\r\n**Outline**\r\n\r\n**1- Introduction to Model Deployment (5 minutes)**\r\nTopics:\r\n- Importance of deploying ML models.\r\n- Picking the right tool for your needs\r\nModular and bespoke pipelines\r\n- Open source tool support: TensorFlow, PyTorch, Jupyter Notebooks and scikit-learn\r\n- Data labelling\r\n- Challenges in transitioning from development to production.\r\n\r\nObjectives:\r\n- Set the context for the importance of deploying ML models.\r\n- Highlight common challenges faced during deployment.\r\n\r\n**2- Deployment Strategies (10 minutes)**\r\n\r\nTopics: \r\n- Overview of different deployment strategies (batch, real-time, A/B testing), \r\n- Pros and cons of each strategy.\r\n\r\nObjectives:\r\n- Provide a comparative understanding of deployment strategies.\r\n- Help attendees choose the right strategy for their use case.\r\n\r\n**3- Containerization with Docker (10 minutes)**\r\nTopics:\r\n- Introduction to Docker and containerization.\r\n- Creating Docker containers for ML models.\r\n\r\nObjectives:\r\n- Demonstrate how to use Docker to containerize ML models.\r\n- Ensure attendees understand the benefits of containerization.\r\n\r\n**4- Deploying on Cloud Platforms (10 minutes)**\r\nTopics:\r\n- Overview of popular cloud platforms (AWS, GCP, Azure).\r\n- Deploying models using cloud services.\r\n\r\nObjectives:\r\n- Show how to deploy ML models on different cloud platforms.\r\n- Provide practical examples and step-by-step guides.\r\n\r\n**5- Monitoring and Maintenance (5 minutes)**\r\nTopics:\r\n- Setting up monitoring and logging for deployed models.\r\n- Handling model updates and retraining.\r\n\r\nObjectives:\r\n- Teach attendees how to monitor model performance.\r\n- Discuss strategies for maintaining and updating models.\r\n\r\n**6- Q&A (5 minutes)**\r\nTo address attendee questions and clarify any doubts.", "code": "TQY8WA", "state": "submitted", "created": "2024-12-20", "speaker_names": "Ariane Djeupang", "track": "PyCon: MLOps & DevOps"}, {"title": "Beyond Alembic and Django Migrations", "abstract": "ORMs like Django and SQLAlchemy have become indispensable in Python development, simplifying the interaction between applications and databases. Yet, their built-in schema migration tools often fall short in projects that require advanced database features or robust CI/CD integration.\r\n\r\nIn this talk, we\u2019ll explore how you can go beyond the limitations of your ORM\u2019s migration tool. Using Atlas\u2014a language-agnostic schema management tool\u2014as a case study, we\u2019ll demonstrate how Python developers can automate migration planning, leverage advanced database features, and seamlessly integrate database changes into modern CI/CD pipelines.", "full_description": "Talk Structure: \"Beyond Your ORM's Migration Tool\"\r\n\r\n1. Introduction \u2013 Why ORMs Build Migration Tools\r\n   - ORMs like SQLAlchemy and Django ORM simplify database interactions and include migration tools (e.g., Alembic, Django Migrations) for schema changes.\r\n   - These tools are robust for ORM-defined schemas but lack advanced features and native CI/CD integrations.\r\n\r\n2. Where Built-in Tools Fall Short\r\n   - ORM migration tools focus on basic schema changes but don\u2019t support advanced database objects like triggers, materialized views, or stored procedures.\r\n   - Lack native integration with modern CI/CD tools, leaving teams to implement custom, often suboptimal solutions.\r\n\r\n3. Presenting Atlas \u2013 Bridging the Gap\r\n   - Atlas complements ORM tools by reading their schemas (e.g., Django models, SQLAlchemy models) and enabling advanced extensions.\r\n   - Key features:\r\n     - Support for triggers, materialized views, and other advanced objects.\r\n     - Native CI/CD integration for automating and validating schema changes.\r\n\r\n4. How Atlas Integrates with ORMs\r\n   - Atlas reads ORM-defined schemas and enhances them with advanced features.\r\n   - Combines ORM workflows with Atlas\u2019s robust schema management capabilities, enabling automation and database-specific optimizations.\r\n\r\n5. Demo \u2013 Atlas in Action\r\n   - Example: A Django project adds a materialized view and a trigger using Atlas.\r\n   - Steps:\r\n     - Use Atlas to read the ORM schema and extend it with advanced features.\r\n     - Automate migration validation and deployment through CI/CD pipelines.\r\n   - Outcome: Simplified and automated schema management with modern tooling.\r\n\r\n6. Conclusion and Q&A\r\n   - Key Takeaways:\r\n     - ORM migration tools like Alembic and Django Migrations are great for standard use cases but fall short for advanced workflows and CI/CD integration.\r\n     - Atlas bridges this gap, enabling automation and advanced database features.\r\n   - Call to Action: Try Atlas to enhance schema workflows.\r\n   - Q&A: Open floor for questions.", "code": "KCV9RS", "state": "submitted", "created": "2024-12-26", "speaker_names": "Rotem Tamir", "track": "PyCon: Django & Web"}, {"title": "Mastering PDF Form-Filling with PyPDF", "abstract": "Tired about PDF paperwork? Join the PDF revolution as we see how PyPDF Python library changes how we work with PDF forms completion. We'll cover how to form-fill PDFs with a touch of Python magic. By the end, you\u2019ll be a PDF expert, eliminating manual data entry. Get ready to improve your data skills!", "full_description": "Tired about PDF paperwork? Join the PDF revolution as we see how PyPDF Python library changes how we work with PDF forms completion. We'll cover how to form-fill PDFs with a touch of Python magic. By the end, you\u2019ll be a PDF expert, eliminating manual data entry. Get ready to improve your data skills!\r\n\r\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "code": "LVTCS3", "state": "rejected", "created": "2024-12-31", "speaker_names": "Jordi Bosch", "track": "PyCon: Python Language & Ecosystem"}, {"title": "What\u2019s in a Picture? Creating a Simple Image Classifier with Python", "abstract": "If you've ever wondered how your phone recognizes your face when you unlock it or how your photos app categorizes different objects, this talk will answer your questions. We\u2019ll walk through the process of creating a simple image classifier using Python, breaking down the steps so you can understand and build your own. By the end, you\u2019ll have a clear picture of how it all works\u2014and the tools to try it yourself!", "full_description": "Have you ever wondered how your phone recognises faces or how apps can tell the difference between a cat and a dog? Behind the scenes, it\u2019s all image classification! In this talk, you\u2019ll learn how to build your own image classifier using Python\u2014even if you\u2019re completely new to machine learning.\r\n\r\nWe\u2019ll break down the process into fun, easy-to-follow steps. You\u2019ll discover how to:\r\n1) Prepare and Preprocess Images: Learn how to feed images into your classifier.\r\n2) Use Python Libraries: Explore tools like scikit-learn, TensorFlow, or Keras to train your model.\r\n3) Train a Simple Model: Create a basic classifier that can recognize different types of images.\r\n4) Test and Improve Your Classifier: See how well your model performs and tweak it to get better results.\r\n\r\nGet ready to see what\u2019s _really_ in a picture\u2014and teach your Python code to see it too!", "code": "3MPM7Z", "state": "submitted", "created": "2024-12-22", "speaker_names": "Ilerioluwakiiye Abolade, Toyibat Adele", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Programming not prompting LLMs with DSPy and friends: Beyond Manual Tuning", "abstract": "LLMs promise efficiency but often get bogged down by manual tuning for optimal performance. DSPy changes the game. Traditional LLM deployment is a high-effort, error-prone process, demanding extensive prompt engineering and fine-tuning across multiple steps. \r\n\r\nLet's demystify DSPy's role in enhancing LLM efficiency, offering practical, transformative insights. \r\n\r\nWe'll also explore alternatives and advice strategies when to use what.", "full_description": "LLMs promise efficiency but often get bogged down by manual prompt tuning for optimal performance. DSPy changes the game. Traditional LLM deployment is a high-effort, error-prone process, demanding extensive prompt engineering and fine-tuning across multiple steps. Imagine deploying LLMs where manual optimizations are replaced by DSPy's automated, efficient prompt and weight optimization, streamlining processes and slashing costs. We'll cover how DSPy revolutionizes LLM deployment, its direct impact on operational efficiency, cost-effectiveness, and model reliability through practical applications. \r\nLast but not least we'll have a look at this emerging field and compare DSPY to its alternatives.", "code": "7QDEGX", "state": "submitted", "created": "2024-12-31", "speaker_names": "Jeronim Morina", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Probably Fun: Board Games to teach Data Science", "abstract": "In this tutorial, you will speed-date with board and card games that can be used to teach Data Science. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\n\r\nAs a result, you will experience multiple ideas that you can use to make complex ideas more understandable and enjoyable. We would like to demonstrate how gamification can not only used to produce short puzzles and quizzes, but also as a tool to reason complex problem-solving strategies.\r\n\r\nWe will bring a set of carefully selected games that have been proven effective in teaching statistics, programming, machine learning and other Data Science skills. We also believe that it is probably fun to participate in this tutorial.", "full_description": "Games encourage people to put their brains to work in a focused, constructive and peaceful way. This makes games a fantastic tool in the classroom. Many board games contain sophisticated algorithms and statistical models right under the surface. Therefore, Data Science education can be boosted by playing carefully selected games.\r\n\r\nWe have applied popular board and card games such as Memory, Wizard, Machi Koro, Pandemic and Sky Team (the 2024 Game of the Year in Germany) to teach Data Science concepts in our courses. Learners would first play a game, discuss the mechanisms and only after that get exposed to the theory. Finally, they would move to practical applications using computers.\r\n\r\nThis game-driven approach provides learners with an intrinsic motivation to solve a real practical problem (succeeding at the game).\r\nAnalyzing a game makes it easier to grasp the core mechanism or algorithmic model and ask qualified questions about the details later.\r\nIt also makes sure learners will want to come back for the next class. We have documented practical lessons and made them available under a CC license on https://www.academis.eu/probably_fun/ .\r\n\r\nIn this tutorial, you will speed-date with several short games that can be used to teach Data Science concepts and skills. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\nThis way, you will experience multiple ideas you can use to make complex methods and ideas more accessible. Also, the tutorial is probably fun to participate in.\r\n\r\nThe tutorial will be executed according to the following pseudocode (or lesson plan):\r\n\r\n1. The presenters give a short introduction on why games matter (5 min)\r\n2. The presenters group participants into teams of up to 6 people.\r\n3. Each team is assigned to a game table with a game and a cheat sheet with instructions. The presenters facilitate with understanding rules and to remove other obstacles.\r\n4. The teams play the game for up to 15 minutes.\r\n5. The teams discuss 1-3 prepared reflection questions to make the transfer from the game to the data science concepts.\r\n6. Each team moves to the next table.\r\n7. Repeat for 3-4 rounds.\r\n8. Everybody gets together for a joint Q & A\r\n9. A QR-Code links to material with games that help learning Data Science and lesson plans", "code": "WELCVS", "state": "confirmed", "created": "2024-12-22", "speaker_names": "Paula Gonzalez Avalos, Dr. Kristian Rother", "track": "General: Education, Career & Life"}, {"title": "From SHAP to EBM: Explain your Gradient Boosting Models in Python", "abstract": "Imagine you\u2019ve just developed a **credit rating model** for a bank. The backtesting metrics look fantastic, and you get the green light to go live. But then, the first loan applications start rolling in, and suddenly your model\u2019s decisions are under scrutiny. \r\n\r\nWhy was this application rejected? Why was that one approved? You **must** have answers\u2014and fast.\r\n\r\nThis session has you covered.\r\n\r\nWe\u2019ll dive into SHAP (SHapley Additive exPlanations) and EBM (Explainable Boosting Machine), two widely used methods for interpreting tree-based ensemble models like XGBoost. You\u2019ll learn about their theory, strengths, and limitations, as well as see them in action with Python examples using the `shap` and `interpret-ml` libraries.\r\n\r\nFrom understanding feature contributions to hands-on coding, this talk will equip you with practical tools to make complex models transparent, understandable, and ready for critical applications.", "full_description": "**XGBoost** is considered a state-of-the-art model for regression, classification, and learning-to-rank problems on tabular data. Unfortunately, tree-based ensemble models are notoriously **difficult to explain**, limiting their application in critical fields. Techniques like SHapley Additive exPlanations (SHAP) and Explainable Boosting Machine (EBM) have become common methods for assessing how much each feature contributes to the model prediction.\r\n\r\nThis talk will introduce SHAP and EBM, **explaining the theory** behind their mechanisms in an accessible way and **discussing the pros and cons** of both techniques. We will also comment on Python snippets where SHAP and EBM are used to explain a gradient boosting model.\r\n\r\nAttendees will walk away with an understanding of how SHAP and EBM work, the limitations and merits of both techniques, and a tutorial on how to use these methods in Python, courtesy of the [shap](https://shap.readthedocs.io/en/latest/) and [interpret-ml](https://interpret.ml/docs/ebm.html) packages.\r\n\r\nTalk outline:\r\n\r\n- A brief reminder about gradient boosting and XGBoost (5 mins)\r\n- The challenge of explainability (5 mins)\r\n- EBM: theory and applications (5 mins)\r\n- SHAP: theory and applications (5 mins)\r\n- Case study with live coding (10 mins)", "code": "MBLX3W", "state": "withdrawn", "created": "2024-12-02", "speaker_names": "Emanuele Fabbiani", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond Single Models: The Secret Sauce of Predictive Success", "abstract": "In this talk, we will explore the powerful world of ensemble learning techniques in machine learning, a method that improves predictive performance by combining multiple models. We will cover the foundations of ensemble learning, beginning with an understanding of weak and strong learners and their roles in model accuracy.\r\n\r\nWe will delve into three key ensemble techniques:\r\n\r\nBagging: A technique where multiple base models are trained independently and in parallel, helping to reduce variance and increase stability in predictions.\r\nBoosting: A sequential technique that builds a strong model by combining multiple weak models, focusing on reducing bias and improving prediction accuracy.\r\nStacking: A method that combines multiple models to build a generalized and robust final model, leveraging the strengths of diverse learners.\r\nThroughout the session, we will break down the mechanics of each technique, explore their strengths and weaknesses, and demonstrate how they can be applied to real-world problems to elevate predictive performance.", "full_description": "In this talk, we\u2019ll delve into the fascinating world of ensemble learning techniques in Machine learning \u2014a powerful paradigm that combines the strengths of multiple models to enhance overall predictive performance. Here\u2019s what you can expect: 1. Foundations: Understanding Weak and Strong Learners 2. Bagging: An ensemble technique that has multiple base models trained independently and in parallel. 3. Boosting: A technique that sequentially builds a strong model by combining multiple weak models. 4. Stacking: Model fusion of multiple models, building a more generalized yet robust model.", "code": "MMWXLU", "state": "rejected", "created": "2024-11-29", "speaker_names": "Yashasvi Misra", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond Big Data: Redefining AI with Small Data", "abstract": "The deep learning revolution has largely been driven by the availability of big data, enabling significant advancements in data-rich industries like finance and e-commerce. But what about fields where data is scarce, inaccessible, or hard to collect?\r\nIn my recent work on crack detection in reinforced concrete structures, I realized how much we rely on data-hungry AI models and why we need systems designed to excel with smaller, more specialized datasets. This realization led me to start exploring the concept of 'small data' not as a limitation, but as an opportunity to rethink how we approach AI development.\r\nThis talk will explore how methodologies like self-supervised learning, meta-learning, and advanced data augmentation are enabling intelligent systems to thrive with minimal data. Attendees will gain practical insights into adapting AI to data-scarce domains and leave with a clearer vision of how small data is shaping the future of intelligent, adaptable systems.", "full_description": "Big data dominated AI\u2019s rise, but the future belongs to small data. Many of the world\u2019s most exciting opportunities exist in environments where large-scale datasets are unavailable or impractical. From rare disease diagnostics to natural disaster prediction and cultural heritage preservation, these fields require innovative AI systems designed to thrive with smaller, high-quality datasets.\r\nThis talk will introduce 'small data' as a transformative paradigm for building intelligent, adaptable systems.\r\n\r\nWe\u2019ll explore:\r\n1. The Paradigm Shift to Small Data: How the concept of small data redefines AI by prioritizing efficiency, adaptability, and inclusivity.\r\n2. Innovative Techniques: Explore how self-supervised learning, meta-learning, Bayesian neural networks, and advanced data augmentation can be applied effectively in small data environments.\r\n3. Critical Evaluation: Analyze the strengths, limitations, and practical applications of these techniques in healthcare, environmental monitoring, and cultural heritage.\r\n4. Real-World Applications: Case studies showcasing the role of small data in these domains.\r\nActionable Frameworks: Explore structured decision-making frameworks to identify and implement the most effective small data methodologies for various domains.\r\n\r\nWhat You\u2019ll Gain\r\n1. A deep understanding of how small data is enabling smarter, more adaptable AI systems across healthcare, environmental monitoring, and cultural preservation.\r\n2. Practical strategies to design AI solutions that excel with minimal data.\r\n3. A forward-looking perspective on small data\u2019s role in shaping AI\u2019s future, driving innovation, and enabling global inclusivity.", "code": "A8JTRZ", "state": "submitted", "created": "2025-01-05", "speaker_names": "Zaynab Awofeso", "track": "General: Community & Diversity"}, {"title": "Modern Data Architectures need Software Engineering", "abstract": "This talk takes a look into modern data architectures for analytics, emphasizing software engineering influences, integration with transactional systems, and architectural scenarios. It gives insights in what companies actually build as working data architectures for them.", "full_description": "This talk explores the rapidly evolving landscape of modern data architectures that build and used for analytics purposes. They continually adapt practices and tools from software engineering. \r\n\r\nWe will explore the growing influence of software engineering within the data ecosystem. Speaking of software engineering: the integration of analytics systems with the transactional/operative systems will also be taken into account. Typical scenarios and contextual factors of different architectural approaches we will also be discussed. \r\n\r\nWe take a look into recent developments such as Data Mesh and Data Lakehouse, the ELT pattern, Cloud DWHs and theirlike. \r\n\r\nAdditionally, I provide insights into the practical implementation of these architectures using tools and technologies commonly found in the \u201cModern Data Stack.\u201d\r\n\r\nWhat will you learn:\r\n\r\n- Data Architecture and Data Engineering is heavily entangled with Software Engineering\r\n- The methods and processes used are far away from the old UI tools for ETL but very close to existing engineering best practices\r\n- Providing and using data for analytics purpose is by now a topic for every developer team. It's worth to understand the key ideas behind data architectures. You will learn them here. This includes concepts like Data Mesh or Lake but also concrete tooling like dbt.", "code": "XSHDEN", "state": "submitted", "created": "2024-12-20", "speaker_names": "Matthias Niehoff", "track": "PyData: Data Handling & Engineering"}, {"title": "The limits of AI", "abstract": "Despite all the hype around AI, we are still in the early days of finding out the value that generative AI unlocks together with its limits. And while there have been many conversations about the opportunities AI creates, this talk is about its limitations as dictated from the scaling laws and its ability to represent the world.", "full_description": "On the one hand AI is supposed to replace knowledge workers and on the other the most popular use of ChatGPT is from students using it to draft essays and complete home assignments. There is no doubt that there are tremendous opportunities to be unlocked even if AI stops improving moving on but let\u2019s talk about the limitations of current technology as well. This can help us inform what we can build now and also which developments help move us forward.\r\n\r\nOne obvious place to start this discussion is the scaling law that underpins current AI together with a study on how much data is available and how the trend to use lower and lower precision affects our models. A natural next step is to explore the literature around how good of a world representation AI has which will lead us into a discussion of applications that are possible, others that require certain care and lastly those that are out of reach at the moment.\r\n\r\nIn this talk you will\r\n\r\n- get a better understanding and feel of the scaling law that underpins AI\r\n- learn about the world model AI creates and its limits\r\n- see opportunities and limitations of AI informed from that understanding", "code": "SPKYUD", "state": "submitted", "created": "2024-12-20", "speaker_names": "Nick Sorros", "track": "PyData: Generative AI"}, {"title": "Beyond Fine-Tuning: A Strategic Roadmap for Adapting Large Language Models to Real-World Business Ne", "abstract": "Adapting Large Language Models (LLMs) to specific business needs can dramatically reduce compliance risks, optimise operations and improve customer satisfaction. This session provides a step-by-step roadmap, starting with Supervised Fine-Tuning (SFT) and progressing to more advanced, domain-driven approaches such as Retrieval-Augmented Adaptation for Fine-Tuning (RAFT). We also explore adaptation techniques, including reinforcement learning (e.g., DPO), that integrate feedback from both human experts and AI systems to ensure that results reflect organisational goals and user preferences.\r\n\r\nWhat sets this talk apart is its focus on combining methodologies - layering RAFT on top of SFT, then adding DPO - to solve challenges in high-stakes areas such as financial compliance. We will show how real and synthetic data can be blended to reduce annotation costs and speed adaptation without sacrificing accuracy. By illustrating these techniques in concrete, compliance-based scenarios, attendees will see exactly how to balance model performance, resource constraints and domain complexity.\r\n\r\nUltimately, attendees will leave with a clear framework for selecting the right fitting strategy: from basic fine-tuning for moderate use cases to more sophisticated solutions for specialised domains. Attendees will also learn how to evaluate the trade-offs between cost, data availability, and model accuracy to maximise the business impact of AI in their own environments.", "full_description": "Adapting Large Language Models (LLMs) to specific business needs can dramatically reduce compliance risks, increase operational efficiency, and improve customer satisfaction. This session provides a clear, practical roadmap for achieving these benefits, starting with Supervised Fine-Tuning (SFT) and progressing to advanced, domain-focused adaptations such as Retrieval-Augmented Adaptation for Fine-Tuning (RAFT). We also explore alignment strategies, including reinforcement learning techniques (e.g., RLHF, DPO) that incorporate both human and AI feedback to ensure that model outputs are aligned with organisational goals and user preferences.\r\n\r\nA distinctive feature of this talk is its emphasis on combining multiple methods, such as layering RAFT on top of SFT, and then adding RLHF/DPO to address the unique needs of complex domains. We will highlight real-world financial regulatory compliance scenarios to illustrate how these techniques can dramatically improve response accuracy while preserving privacy and reducing annotation overhead. By interweaving real and synthetic data, we demonstrate how practitioners can accelerate model customisation and reduce costs, even under severe data or budget constraints.\r\n\r\nIn addition to detailing the \"how\" of each method, we offer insights into why these adjustments matter for business impact, including guidelines for selecting methods based on domain complexity, data availability, and cost considerations. Attendees will leave with a concrete blueprint for using advanced LLM adaptations in their own projects, fully equipped to navigate the trade-offs between model performance, resource allocation, and regulatory compliance.\r\n\r\nOutline:\r\n\r\n1. Introduction: Why Adapting LLMs Matters\r\n1.1 Very often, adaptation is the only way an LLM can learn the complex and unique requirements of a specific business domain.\r\n1.2 Illustrate how domain specialization can reduce risk, drive innovation, and improve decision-making.\r\n2. Foundations: Supervised Fine-Tuning & Beyond\r\n2.1 Basic concept and process.\r\n2.2 Common tools and frameworks.\r\n2.3 Strengths and Limitations\r\n2.3.1 Strengths: Simple and effective for moderate domain adaptation.\r\n2.3.2 Limitations: Data scarcity, potential overfitting to small domains, limited customization.\r\n2.4 Advanced Adaptations: RAFT and Beyond\r\n2.4.1 How RAFT leverages domain-specific external knowledge for more precise responses.\r\n2.4.2 Advantages over plain SFT in complex or information-rich domains.\r\n3. Using Synthetic Data to Address Scarcity\r\n3.1 Generating Synthetic Data: Methods, best practices, and common pitfalls (e.g., preserving domain realism, avoiding bias).\r\n3.2 Ensuring Data Quality and Addressing Bias: Techniques to evaluate and mitigate potential errors in synthetic datasets.\r\n4. Generating synthetic data and applying SFT and RAFT to financial regulatory compliance use case\r\n5. Alignment Strategies: Adapting the Model to Specific Business Needs\r\n5.1 RLHF\r\n5.2 DPO vs other RLHF-like methods\r\n5.3 Beyond DPO\r\n5.4 Balancing human feedback loops with AI-based evaluation.\r\n6. Applying alignment strategies to the financial regulatory compliance use case\r\n7. Best Practices\r\n7.1 Decision framework based on domain complexity (e.g., highly specialized vs. broad tasks), data availability, and budget constraints.\r\n7.2Matching SFT, RAFT, and/or alignment techniques to specific project goals.\r\n8. Common Pitfalls\r\n8.1 Over-reliance on synthetic data.\r\n8.2 Underestimating alignment challenges.", "code": "UZD9LE", "state": "submitted", "created": "2025-01-04", "speaker_names": "Michael Shtelma", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Get Your Lambda On: Smyth & Lynara's Guide to Lazy Dev Life", "abstract": "Learn how to optimize Python ASGI apps in AWS Lambda with Lynara! This talk explores Lambda's cost-effectiveness, concurrency limits, and cold starts. Discover Lynara's seamless integration with FastAPI, Django, plus strategies for local testing with Smyth to streamline serverless development.", "full_description": "This presentation explores the challenges of using AWS Lambda for high-traffic Python ASGI applications, including concurrency limits, scalability costs, and transitioning to EC2. We introduce Lynara, a tool that transforms Lambda events into ASGI requests, enabling seamless deployment of frameworks like FastAPI, Starlette, and Django, and simplifying migration to EC2.\r\n\r\nAdditionally, we showcase Smyth, a local development tool that simulates production environments, enhancing accuracy in testing. Attendees will gain insights into benchmarking framework performance, and improving developer workflows. Learn how Lynara and Smyth address AWS Lambda's limitations to streamline serverless application development.", "code": "SPKTR9", "state": "submitted", "created": "2025-01-05", "speaker_names": "Damian Wysocki", "track": "PyCon: Programming & Software Engineering"}, {"title": "NLP for Emergency Response and Resilience Activities", "abstract": "Natural Language Processing (NLP) offers transformative potential for enhancing program monitoring, evaluation, accountability, and learning (MEAL) in humanitarian contexts. In this tutorial presentation, we focus on emergency response and resilience projects. By analyzing vast amounts of textual and spoken data, NLP enables real-time insights into the effectiveness of program interventions, stakeholders' feedback, and evolving ground realities. For monitoring, NLP can extract actionable information from output reports, social media, and community feedback, ensuring programs remain contextualized to the program participant's needs. In evaluation, sentiment analysis and topic modeling identify patterns. Within the multi-language orientation of program participants, machine translation enables multilingual analysis in diverse humanitarian settings. NLP-powered tools streamline accountability by categorizing complaints to enable timely responses and foster trust. Furthermore, NLP drives organizational learning by synthesizing lessons from previous, and current projects and offering evidence-based recommendations for future programming. Despite its potential, challenges such as data quality, contextual relevance, and ethical considerations in data collection and use must be addressed. By leveraging NLP, emergency responses and resilience activities can enhance decision-making, ensure more adaptive programming, and ultimately improve outcomes for vulnerable populations.", "full_description": "Abstract\r\nNatural Language Processing (NLP) offers transformative potential for enhancing program monitoring, evaluation, accountability, and learning (MEAL) in humanitarian contexts. In this tutorial presentation, we focus on emergency response and resilience projects. By analyzing vast amounts of textual and spoken data, NLP enables real-time insights into the effectiveness of program interventions, stakeholders' feedback, and evolving ground realities. For monitoring, NLP can extract actionable information from output reports, social media, and community feedback, ensuring programs remain contextualized to the program participant's needs. In evaluation, sentiment analysis and topic modeling identify patterns. Within the multi-language orientation of program participants, machine translation enables multilingual analysis in diverse humanitarian settings. NLP-powered tools streamline accountability by categorizing complaints to enable timely responses and foster trust. Furthermore, NLP drives organizational learning by synthesizing lessons from previous, and current projects and offering evidence-based recommendations for future programming. Despite its potential, challenges such as data quality, contextual relevance, and ethical considerations in data collection and use must be addressed. By leveraging NLP, emergency responses and resilience activities can enhance decision-making, ensure more adaptive programming, and ultimately improve outcomes for vulnerable populations. \r\n\r\nDescription\r\nNatural Language Processing (NLP) offers transformative potential for enhancing program monitoring, evaluation, accountability, and learning (MEAL) in humanitarian contexts. By leveraging its ability to process and analyze large volumes of textual and spoken data, NLP can significantly improve decision-making and shock responsiveness, fostering program participant resilience and program activity adaptation.\r\n\r\n1.\tMonitoring\r\nNLP tools can process real-time feedback from various sources such as field reports, annual result reports, call logs, social media posts, and program participant surveys. For instance, text classification algorithms can identify recurring themes or emerging risks, such as supply shortages or security concerns. Named Entity Recognition (NER) can track locations, events, or organizations mentioned in reports, providing actionable insights into program performance.\r\n2.\tEvaluation\r\nNLP can analyze qualitative data, such as interviews, focus group transcripts, and open-ended survey responses. Sentiment analysis reveals trends in program participant satisfaction, while topic modeling identifies underlying themes or challenges. Multilingual machine translation tools allow organizations to evaluate programs across regions with diverse languages, ensuring inclusivity in feedback analysis.\r\n\r\n3.\tAccountability\r\nNLP-powered chatbots and automated systems can process complaints and feedback from affected populations. Categorization algorithms can prioritize urgent concerns, such as protection issues or overlooked program participant needs, for faster response. Sentiment analysis helps assess public trust and satisfaction, while text summarization ensures accountability reports are concise yet complete.\r\n4.\tLearning\r\nBy synthesizing information from past, and ongoing project activities, NLP can extract lessons learned and key success factors. Tools like document clustering and summarization can identify best practices across programs and sectors. Additionally, NLP can facilitate the creation of knowledge repositories, making insights easily accessible to staff and program stakeholders at various level for future planning.\r\n\r\nDespite its transformative potential, the application of NLP in humanitarian settings comes with challenges. These include ensuring data quality, addressing bias in algorithms, and adhering to ethical principles in data collection, especially in vulnerable communities. However, with considerate implementation, NLP can help humanitarian organizations make data-driven decisions, adapt to dynamic environments, and ultimately improve outcomes for crisis-affected populations.", "code": "GNYMEC", "state": "submitted", "created": "2025-01-03", "speaker_names": "BETHELHEM TEKA TELILA", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "A tour of the 9 namespaces of Polars expressions", "abstract": "[Polars](https://github.com/pola-rs/polars) expressions are at the core of why the Polars API is so flexible and intuitive, but there are hundreds of expressions available.\r\n\r\nThis talk will give an overview of the 9 namespaces of Polars expressions, namely `arr`, `bin`, `cat`, `dt`, `list`, `meta`, `name`, `str`, and `struct`.\r\n\r\nBy showing simple examples of when these namespaces come into play, the audience will become better acquainted with the methods that Polars expressions provide and thus will be better equipped to write efficient data queries with Polars.", "full_description": "This talk will introduce briefly the 9 namespaces that Polars expressions have with one or two motivating examples of when the namespace would be needed.\r\n\r\nThe examples will be simple but representative of what the namespace(s) are used for:\r\n- `arr` and `list` \u2013 namespaces that hold all of the functionality to work with the Array and List data types which are commonly mixed together, so we will see when to use one or the other;\r\n- `bin` \u2013 namespace that provides functionality to work with binary data;\r\n- `cat` \u2013 namespace dedicated to the categorical data type;\r\n- `dt` \u2013 large namespace that is useful when working with temporal data, time series, and more;\r\n- `meta` \u2013 a namespace that provides tools to manipulate expressions at a meta level;\r\n- `name` \u2013 a small but powerful namespace that is essential when working with expression expansion;\r\n- `str` \u2013 this namespace provides functionality to work efficiently with strings, which is typically the bane of dataframe libraries; and\r\n- `struct` \u2013 an entire namespace dedicated to the data type Struct.", "code": "J8NYMD", "state": "accepted", "created": "2024-12-24", "speaker_names": "Rodrigo Gir\u00e3o Serr\u00e3o", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Python on the Pitch: How Nationalelf will win World Cup 2026", "abstract": "We will dive into the fascinating world of football analytics, showcasing how to collect and process match data (e.g., Hudl Statsbomb, Sportmonks, and Understat), including player tracking, event logs, and tactical formations. Attendees will walk away with practical knowledge and Jupyter Notebooks, demonstrating Python's power in decoding modern football strategies.", "full_description": "In this talk, we will explore how Python can be leveraged to analyze and visualize football data for the [Germany national football team](https://en.wikipedia.org/wiki/Germany_national_football_team), managed by [Julian Nagelsmann](https://en.wikipedia.org/wiki/Julian_Nagelsmann), on their journey to winning the 2026 FIFA World Cup. We will dive into the fascinating world of **football analytics**, showcasing how to collect and process match data (e.g., Hudl Statsbomb, Sportmonks, and Understat), including player tracking, event logs, and tactical formations. We'll discover match data to demonstrate how the team's performance reflects Nagelsmann's tactical principles, such as [gegenpressing](https://en.wikipedia.org/wiki/Gegenpressing), offensive play, and compactness. Join us to unlock the power of Python in football analytics!", "code": "LZAVJT", "state": "submitted", "created": "2024-12-22", "speaker_names": "Ruslan Korniichuk", "track": "PyData: Visualisation & Jupyter"}, {"title": "Citation is Collaboration: Software Recognition in Research and Industry", "abstract": "The development of open source software is increasingly recognized as a critical contribution across many disciplines, yet the mechanisms for credit and citation vary significantly. This talk uses astronomy as a case study to explore shared challenges in attributing software contributions across research and industry. It will review the evolution of journal recommendations and policies over the past decade, alongside emerging publishing practices offering insights into their impact on the recognition of software contributions. An analysis of citation patterns for widely used libraries (numpy, scipy, astropy) highlights trends over time and their dependence on publication venues and policies. The talk will conclude with strategies for both developers and users for improving the recognition of software, fostering collaboration and sustainability in software ecosystems. All data and analysis code will be made available in a public repository, supporting transparency and further study.", "full_description": "In many fields, including research and industry, software is essential for driving innovation and scientific discovery, yet mechanisms for crediting software developers remain inconsistent and underdeveloped. This lack of recognition, particularly for open-source contributions, can discourage participation in software development and limit career opportunities for developers. Astronomy, as a computationally intensive discipline with a rich history of open-source software contributions, offers a valuable case study to examine these challenges. Over the past decade, changes in journal policies and emerging publishing practices have sought to address the issue, but their impact on credit attribution remains unclear.\r\n\r\nThis talk addresses the issue of software credit by analyzing publication and citation practices in astronomy. It evaluates how existing policies acknowledge software contributions and examines variations across journals and over time. Drawing on bibliometric data from the past decade, the analysis focuses on citation patterns for commonly used libraries, trends in citation rates, and the influence of journal policies. The study includes both foundational libraries, such as NumPy, and astronomy-specific libraries, such as Astropy. Based on these findings, the talk will offer recommendations to enhance the attribution of software contributions.\r\n\r\nThe issue of recognizing research software is not unique to astronomy or research. Participants from industry, other computationally driven fields, open-source communities, and publishing will find the insights applicable to their own disciplines. Understanding how software is cited and credited is critical for shaping more equitable recognition systems, which in turn support sustainable software development and community growth. The audience will leave with a clear understanding of how astronomy\u2019s experience can inform broader efforts to address similar challenges in their respective fields.\r\n\r\n The data and code for the analysis will be shared with participants giving participants access to a reproducible framework for analyzing software citation practices in other disciplines or software ecosystems.", "code": "VJR39N", "state": "submitted", "created": "2024-12-22", "speaker_names": "Ivelina Momcheva", "track": "General: Others"}, {"title": "Is coding assistant as good as we thought in coding?", "abstract": "Nowadays coding assistants are everywhere, many IDEs are offering them as plugins, and are becoming more and more powerful. But it prompts us questions, is coding assistant as good as we want it to be? What can and can\u2019t these AI agents do? Will AI take my job?", "full_description": "In this talk, the speaker will explain the current state of AI coding assistants, what is in the market, and what they promise. The speaker will also, with some real experience from developers who have used coding assistants, explore the potential and limitations of the assistants. From there, we will also look into the future, predicting the landscape of the software engineering industry and as a developer how we can take advantage of the coding assistants instead of getting our jobs taken by them.\r\n\r\n## Goal\r\n\r\nTo explain, in an as objective way as possible, the effect of AI coding assistants in a developer's career and to be proactive in preparing what's to come.\r\n\r\n## Target Audience\r\n\r\nEveryone who codes for a living or anyone who is enthusiastic about coding. The speaker expects all levels of familiarity with AI coding assistants.", "code": "EHZFLJ", "state": "submitted", "created": "2024-12-17", "speaker_names": "Cheuk Ting Ho", "track": "General: Education, Career & Life"}, {"title": "Building and distributing your own conda packages for fun and profit", "abstract": "Two major pain points in the Python ecosystem are the reproducibility and portability of your code. In this talk, we\u2019ll bundle code in a conda package and distribute it in an unconventional way. Instead of relying on public hosters, we\u2019ll use self-hosted infrastructure to create our own package index.", "full_description": "The conda ecosystem is a great way to distribute packages since it uses a language-agnostic approach to bundle binaries and code into a package. Unlike PyPi, this lets you install non-python dependencies and developer tools.\r\n\r\nModern tools such as [`rattler-build`](https://rattler.build) and [`pixi`](https://pixi.sh) provide a seamless experience during project setup and package building.\r\n\r\nWe will show you a low-effort way to self-host your own package indices using S3 as a storage backend. This is especially versatile since we can get S3-compatible blob storage anywhere \u2014 even in your own living room!\r\n\r\n### Agenda\r\n\r\n1. **Package your project as a conda package**\r\n    1. Writing a conda recipe\r\n    2. Using `rattler-build` to build the package\r\n2. **The conda package artifact**\r\n    1. A look inside the package\r\n    2. Using your package locally\r\n3. **Building your own package index using S3**\r\n    1. hosted providers vs. self-hosting\r\n    2. Setting up your own index\r\n    3. Using your index with pixi", "code": "NMXSJG", "state": "submitted", "created": "2024-12-20", "speaker_names": "Pavel Zwerschke, Moritz Wilksch", "track": "PyCon: MLOps & DevOps"}, {"title": "Bike Map: Computer Vision for Trip Routing", "abstract": "This talk introduces a computer vision-powered system for optimizing bike routes in urban areas like Berlin. By leveraging advanced models such as Mask2Former for semantic segmentation and DINOv2 for feature extraction, the system detects and categorizes bike lanes by safety and identifies cobblestone streets. It integrates these insights into a routing algorithm that prioritizes safe, smooth, and comfortable paths for cyclists. Learn how AI transforms urban cycling by combining safety-focused analysis with practical route planning tools.", "full_description": "Urban cycling safety and comfort are critical for encouraging sustainable transportation, yet existing routing tools often overlook these factors. This talk delves into the development of a computer vision-driven system to address this gap. Using street view imagery, the system detects and classifies bike lanes by safety, identifies cobblestone streets, and integrates these insights into a routing algorithm to create the safest and most comfortable bike paths. Attendees will explore the pipeline in detail, from using advanced models like Mask2Former for semantic segmentation and DINOv2 for feature extraction, to clustering techniques for safety assessment and binary classification for road surface detection. This session is ideal for AI enthusiasts, urban planners, and cycling advocates seeking to understand how AI can enhance urban mobility.", "code": "T3QJHM", "state": "submitted", "created": "2024-11-27", "speaker_names": "Antonio Rueda-Toicen", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "The Foundation Model Revolution for Tabular Data", "abstract": "What if we could make the same revolutionary leap for tables that ChatGPT made for text? While foundation models have transformed how we work with text and images, tabular / structured data (spreadsheets and databases) - the backbone of economic and scientific analysis - has been left behind. TabPFN changes this. It's a foundation model that achieves in 2.8 seconds what traditional methods need 4 hours of hyperparameter tuning for - while delivering better results. On datasets up to 10,000 samples, it outperforms every existing Python library, from XGBoost to CatBoost to Autogluon.\r\n\r\nBeyond raw performance, TabPFN brings foundation model capabilities to tables: native handling of messy data without preprocessing, built-in uncertainty estimation, synthetic data generation, and transfer learning - all in a few lines of Python code. Whether you're building risk models, accelerating scientific research, or optimizing business decisions, TabPFN represents the next major transformation in how we analyze data. Join us to explore and learn how to leverage these new capabilities in your work.", "full_description": "TabPFN shows how foundation model concepts can advance tabular data analysis in Python. Born as research published at ICLR 2023, it found strong community adoption with 1,200+ GitHub stars and 100,000+ downloads. Our upcoming January 2025 release introduces major improvements in speed, scale and capabilities that we're excited to preview at PyCon.\r\n\r\n**Detailed Outline:**\r\n\r\n1. **Context & Evolution** (5 min)\r\n- The challenge of applying deep learning to tabular data\r\n- Learning from the foundation model revolution in text and vision\r\n- Key improvements from V0 to V1 based on community feedback\r\n- Real-world examples where TabPFN shines (and where it doesn't)\r\n\r\n2. **Technical Insights** (8 min)\r\n- How we adapted transformers for tabular data\r\n- Making in-context learning work for structured data\r\n- Performance characteristics and resource requirements\r\n- Understanding current limitations and constraints\r\n\r\n3. **Live Coding & Integration** (12 min)\r\n- Getting started with TabPFN in 3 lines of code\r\n- Handling real-world data challenges:\r\n  - Missing values and mixed data types\r\n  - Built-in uncertainty estimation\r\n  - Working with similar tasks efficiently\r\n- Integration with pandas, scikit-learn and the Python ecosystem\r\n\r\n4. **Practical Applications** (5 min)\r\n- When to choose TabPFN vs traditional methods\r\n- Resource requirements and scalability limits  \r\n- What's next for TabPFN\r\n- Q&A\r\n\r\n**Key Takeaways:**\r\n- Practical understanding of TabPFN's capabilities and limitations\r\n- Hands-on experience integrating with Python data science workflows\r\n- Best practices for working with foundation models on tabular data\r\n- Insight into emerging approaches for structured data analysis", "code": "XRHEYZ", "state": "accepted", "created": "2024-12-20", "speaker_names": "Noah Hollmann", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Computer Vision Data Version Control and Reproducibility at Scale", "abstract": "Computer vision, the field focused on enabling machines to interpret and understand visual data, tackles challenges like image recognition, object detection, and scene understanding. PyData tools play a critical role in solving these issues by offering robust libraries like TensorFlow, PyTorch, Keras, and Langchain for building and training machine learning models, performing image processing, and managing large datasets. This hands-on session will enable attendees to learn how to optimize computer vision projects with end-to-end version control baked in.", "full_description": "Petabytes of unstructured data stand as the cornerstone upon which triumphant Machine Learning (ML) models are built.\u00a0One common method for researchers to extract subsets of data to their local environments is by simply using the age-old copy-paste, for model training. This method allows for iterative experimentation, but it also introduces challenges with the efficiency of data management when developing machine learning models, including reproducibility constraints, inefficient data transfer, alongside limited compute power.\r\n\r\nThis is where data version control technologies can help overcome these challenges for computer vision researchers.\u00a0In this workshop we'll cover:\r\n\r\n- How to use open source tooling to version control your data when working with data locally.\r\n- Best practices for working with data, preventing the need to copy data locally, while enabling the training of models at scale directly on the cloud.\u00a0This will be demoed with an OSS stack:\r\n- Langchain\r\n-Tensorflow\r\n- PyTorch\r\n- Keras\r\n\r\nYou will come away with practical methods to improve your data management when developing and iterating upon Machine Learning models, built for modern computer vision research.\r\n\r\nOutline:\r\n- Introduction to the challenges of managing unstructured data in Machine Learning (ML), including inefficiencies in data transfer, limited compute power, and reproducibility constraints.\r\n- Overview of data version control technologies as a solution for overcoming these challenges in computer vision research.\r\n- How to use open source tools for version controlling data locally to streamline model development.\r\n- Best practices for avoiding local data copies and enabling scalable model training directly in the cloud.\r\n- Live demonstration using an OSS stack (Langchain, TensorFlow, PyTorch, Keras) to improve data management and model development in modern computer vision research.", "code": "CEMVBQ", "state": "submitted", "created": "2024-12-18", "speaker_names": "Einat Orr", "track": "PyData: Computer Vision (incl. Generative AI CV)"}, {"title": "Go Beyond Basic RAG with Agentic Behavior", "abstract": "RAG has transformed AI systems by combining retrieval and generation, but traditional workflows often struggle with the dynamic demands of real-world applications, such as multi-step queries or integrating external APIs. Agentic behavior enhances RAG by enabling LLMs to make decisions, call tools, and adapt workflows dynamically. In this talk, we\u2019ll define agentic behavior, explore its core features such as routing, tool integration, and reasoning, and demo its practical implementation in Python using Haystack.", "full_description": "Retrieval-Augmented Generation (RAG) has transformed how we build AI systems with Large Language Models (LLMs) by combining the strengths of retrieval and generation. However, traditional RAG workflows are static and often struggle to handle the dynamic and complex demands of real-world applications, such as answering multi-step queries, integrating external APIs, or gracefully recovering from retrieval failures. Agentic behavior addresses these challenges by extending RAG pipelines, enabling LLMs to make decisions, integrate tools, and dynamically adapt workflows.\r\n\r\nIn this talk, we\u2019ll explore how agentic behavior enhances RAG pipelines. We\u2019ll define what it means for a system to act as an \u201cagent\u201d and cover core concepts like routing, tool calling, and reasoning. Using hands-on examples implemented in Python with Haystack, we\u2019ll walk through practical use cases, such as integrating external APIs and solving multi-step problems. Finally, we\u2019ll tackle challenges like transparency in complex systems and share how graph-based approaches can make these workflows more interpretable.\r\n\r\n### Outline:\r\n\r\n1. Intro of the speaker (1 min)\r\n2. Introduction to RAG (3 mins)\r\n    - Overview of RAG\r\n    - its capabilities and its key limitations\r\n    - Some examples of complex tasks\r\n3. Agentic Behavior (7-8 mins)\r\n    - Definition & Core Characteristics\r\n    - Reasoning, tool calling, dynamic adaptation of workflows (routing)\r\n    - How these characteristics improve RAG pipelines\r\n4. Practical Implementation in Haystack (10 min)\r\n    - Intro to Haystack and key components to use in the demo: retrieval, generation (3 mins)\r\n    - Show how to implement agentic behavior using Haystack\r\n    - Step-by-step demonstration of an agent calling external APIs and performing multi-hop reasoning\r\n5. Addressing Challenges in Complex Systems (3 min)\r\n    - Challenges of understanding complex, dynamic workflows\r\n    - How graph-based approaches provide clarity and control\r\n6. Conclusion and Q&A (5 min)", "code": "VYL7AU", "state": "submitted", "created": "2024-12-22", "speaker_names": "Bilge Y\u00fccel", "track": "PyData: Generative AI"}, {"title": "Exploring LLM latency", "abstract": "Which LLM provider should you choose based on latency? \r\n\r\nAs LLMs become integral to modern applications, speed can make or break your UX. This talk highlights the performance trade-offs of OpenAI, Anthropic, local models, and other providers, backed by real-world benchmarks that reveal how each stacks up in terms of response time and cost.", "full_description": "We will explore how different models and providers perform across key metrics, helping you determine which option delivers the best user experience. We\u2019ll discuss not only the raw numbers but also how each metric influences user perception.\r\n\r\nWe will measure:\r\n- Time to first token  \r\n- Time to last token  \r\n- Latency variability throughout the day  \r\n- How structured responses affect performance  \r\n\r\nWe will evaluate:\r\n- State-of-the-art models  \r\n- Open-source models hosted by cloud providers  \r\n- Local models you can run on your own infrastructure\r\n\r\nWhat you will learn\r\n- Which model to use based on its latency \r\n- How does prompt caching affects performance\r\n\r\nBy the end, you\u2019ll have a clearer view of which solution fits your needs and how to balance performance, costs, and practical considerations.", "code": "VFPHTE", "state": "submitted", "created": "2024-12-22", "speaker_names": "Pavel Kr\u00e1l", "track": "PyCon: MLOps & DevOps"}, {"title": "Simplifying Everyday tasks with Pre-trained ML models", "abstract": "Picture this, you have a text file you want to convert to speech. With just a few lines of code, you do this in minutes. That\u2019s the power of pre-trained machine learning models. These ready-to-use models, available through libraries like Hugging Face Transformers and TensorFlow Hub,  make it easy to automate tasks such as text classification, image recognition, and speech-to-text with minimal setup.  \r\n\r\nIn this talk, we\u2019ll learn how to effectively utilize pre-trained models to streamline workflows and simplify everyday tasks.", "full_description": "We all agree that building ML models from scratch can be very overwhelming right? It often requires large amount of data, powerful hardware and a lot of time.\r\nThis challenge is what birthed the idea for this talk.\r\nPre-trained machine learning models provide a powerful solution for automating tasks with minimal effort, which enables  developers to achieve functionality without training models from scratch. This session demonstrates how to use libraries like Hugging Face Transformers and TensorFlow Hub to implement tasks such as text classification, image recognition, and speech-to-text in Python. Attendees will learn how to select the appropriate models, integrate them easily into applications, and customize them to meet unique requirements.\r\n\r\nOutline:\r\n-Introduction to Pre-trained ML models and their relevance  (3 mins)\r\n-Overview of Pre-trained Model Libraries and Python Integration (7mins)\r\n-Customizing Pre-trained models for specific tasks(8mins)\r\n-Code breakdown (10mins)\r\n-QnA (2mins)", "code": "QECRCM", "state": "submitted", "created": "2024-12-21", "speaker_names": "Kelechi Chibundu", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Extending Python with Rust, Mojo, Cuda and C and building packages", "abstract": "We all love Python - but we especially love it for its unique ability as a glue language.\r\n\r\nIn this talk we will show a number of ways of extending Python: using Rust, C and Cython, C++, CUDA and Mojo! We will use the pixi package manager and the open source conda-forge distribution to demonstrate how to easily build custom Python extensions with these languages.\r\n\r\nThe main challenge with custom extensions is about distributing them. The new pixi build feature makes it easy to build a Python extension into a conda package as well as wheel file for PyPI.\r\n\r\nPixi will manage not only Python, but also the compilers and other system-level dependencies.", "full_description": "Extending Python with native code is a common way of speeding up the execution. There are a number of traditional ways of writing Python extensions (Fortran, C, C++) but lately some modern languages have also entered the game (Rust, Mojo, and let\u2019s count CUDA as modern, too).\r\n\r\nAll of these have slightly different ways of writing Python extensions, and they require the installation of a compiler and compilation tool chain, as well as possibly other system dependencies. Installing, updating and managing the system dependencies is usually a bit of a hassle, and it is where pixi comes in. Pixi is a new package manager that builds on top of the Conda ecosystem. The community distribution \u201cconda-forge\u201d already has tools like C and Rust compilers, and thus it\u2019s easy to maintain the compiler + Python tool chain in a single project.\r\n\r\nThe new \u201cpixi build\u201d feature makes it even easier to build complex multi-language workspaces that combine different Python versions, compilers, and languages.\r\n\r\nIn the talk we will show lots of live demos going over simple numerical examples that highlight the different ways of extending Python (using pybind11, nanobind, PyO3, Mojo, \u2026) glued together in a single workspace with pixi build compiling the extensions from source. We will also demonstrate how pixi can help not only depending on other packages from source, but also by building the packages into Conda and Wheel (PyPI) packages that can be shared.\r\n\r\nAfter the talk, the listeners will have seen a number of ways how to extend Python code (easily!) with native languages and will have an understanding of benefits and drawbacks of the different approaches.", "code": "P9VKRV", "state": "submitted", "created": "2024-12-21", "speaker_names": "Ruben Arts, Wolf Vollprecht", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "Working for a Faster World: Accelerating Data Science with Less Resources", "abstract": "In data science, achieving fast performance is just as important as accuracy, especially when users demand quick results. This talk delves into several straightforward but effective techniques for improving performance in data science applications. Drawing on a real-life case in which we needed to accelerate a Panel app, we highlight strategies that led to significant speed improvements. While some techniques are case-specific, most are broadly applicable for enhancing performance across various data-driven projects.", "full_description": "In the field of data science, performance is critical. While accuracy and insightful results are important, slow applications can significantly hinder the user experience. In many real-world scenarios, users expect not only correct but also fast results. Lagging or slow data science tools and applications often lead to frustration, decreased productivity, and a lack of trust in the tools. Whether it is in web apps, dashboards, or data pipelines, delays in processing can undermine the overall value of the analysis being provided. Thus, ensuring that data science applications run efficiently and swiftly is crucial for both user satisfaction and business success.\r\n\r\nIn this talk, we explore a range of simple yet highly effective techniques to optimize performance, focusing on improving the speed of data processing and analysis. These techniques, while straightforward, can lead to significant improvements in the performance of data-driven applications. By adopting these methods, teams can optimize the user experience, reducing wait times and enabling quicker decision-making, all while maintaining the accuracy and integrity of the results.\r\n\r\nOne of the most important aspects of these techniques is their real-world applicability. To illustrate their effectiveness, we will share an example in which these methods were directly applied to a Panel app that required substantial performance improvement. Faced with the challenge of speeding up the app for its users, we were able to apply a variety of strategies that resulted in noticeable speed enhancements. These strategies ranged from optimizing underlying algorithms to leveraging parallel processing techniques, all aimed at reducing bottlenecks and streamlining data flow. The success of these methods not only improved performance but also demonstrated their value in a practical, real-world context.\r\n\r\nWhile some of the techniques discussed are tailored to specific use cases, most are broadly applicable and can be implemented across a wide range of data science projects. These approaches provide flexible and scalable solutions to performance challenges, making them useful for various applications \u2014 from dashboards to more complex data processing pipelines. By focusing on enhancing the speed of data handling and computation, our methods ensure that users can work more efficiently, thus allowing them to derive insights faster and respond to business needs in real-time.\r\n\r\nUltimately, improving the performance of data science applications is essential for both user satisfaction and the broader success of data-driven initiatives. Through the techniques we present, teams can significantly reduce latency, optimize processing times, and create smoother, faster user experiences, all without compromising the quality of the analysis. Whether you are working with a web app, an internal tool, or a production-level pipeline, the strategies shared in this talk can help you address performance issues effectively and improve the overall performance of your data science applications.", "code": "AAPZ8B", "state": "submitted", "created": "2024-12-17", "speaker_names": "Stefan Kraus, Maximilian Lattka", "track": "PyData: Data Handling & Engineering"}, {"title": "Conquering the Queue: Lessons from processing one billion Celery tasks", "abstract": "At Userlike, Celery is the backbone of our application, orchestrating over a 100 million tasks per month. In this talk, I\u2019ll share real-world insights into scaling Celery, optimizing performance, avoiding common pitfalls, handling failures, and building a resilient architecture.", "full_description": "At Userlike, Celery plays a critical role as the backbone of our Django-based SaaS application, orchestrating over 100 million tasks per month with speed, reliability, and precision. In this talk, I\u2019ll share the lessons we\u2019ve learned while scaling Celery to handle massive workloads and support the needs of a growing user base. From optimizing performance and avoiding common pitfalls to handling failures gracefully and ensuring a resilient architecture, this session will provide actionable insights for developers and architects working with distributed task queues.\r\n\r\nWhether you\u2019re just starting with Celery or looking to scale an established system, you\u2019ll walk away with practical tips, battle-tested strategies, and a deeper understanding of how to harness Celery\u2019s full potential in real-world scenarios.\r\n\r\nOutline:\r\n\r\n\u2022\tIntroduction: Why Userlike needs a task queue, and why you need one too\r\n\u2022\tFundamental concepts: latency, throughput, failure modes\r\n\u2022\tOptimizing Performance: Strategies for faster and more efficient task execution\r\n\u2022\tAvoiding Pitfalls: Common mistakes and how to mitigate them\r\n\u2022\tHandling Failures: Building fault-tolerant workflows and monitoring systems\r\n\u2022\tResilient Architecture: Designing for reliability and scalability\r\n\u2022\tKey Takeaways: Practical tips for implementing and scaling Celery in your own projects\r\n\r\nThis talk is designed to be technical, engaging, and packed with real-world experiences to help you conquer the queue in your own applications.", "code": "J8FLDN", "state": "submitted", "created": "2024-12-22", "speaker_names": "Daniel Hepper", "track": "PyCon: Django & Web"}, {"title": "Building Retrieval Augmented Generation (RAG) AI applications with LangChain", "abstract": "Retrieval Augmented Generation (RAG) has emerged as a powerful technique for enhancing the capabilities of Large Language Models (LLMs) by providing relevant context to the prompts that are passed to them.\r\nThis talk will show how an advanced framework such as LangChain helps to build RAG application efficiently.", "full_description": "Retrieval Augmented Generation (RAG) has emerged as a powerful technique for enhancing the capabilities of Large Language Models (LLMs) by providing relevant context to the prompts that are passed to them. This approach enables LLMs to produce more accurate and contextually appropriate responses, improving their overall performance across various tasks.\r\nLangChain, a cutting-edge open-source framework for developing applications powered by language models, offers a robust platform for building RAG applications. By integrating LangChain into the development process, developers can create context-aware applications that leverage the full potential of LLMs. These applications can seamlessly connect LLMs to various sources of context, such as prompt instructions, few-shot examples, and additional content, enabling them to generate more informed and coherent responses.\r\nWith its rapid adoption and growing community support, LangChain has emerged as the leading Generative AI framework, boasting over 95k stars on GitHub. Its intuitive design and powerful features make it an ideal choice for developers looking to build advanced RAG applications that push the boundaries of what is possible with language models.\r\nThis talk will present the general architecture of a LangChain application and the various components used to implement RAG including chunking, computing and storage of embeddings, vector search and so on.\r\nEventually, a live demo will showcase the practical use of LangChain to build a RAG application.", "code": "YMQQES", "state": "submitted", "created": "2025-01-05", "speaker_names": "Christophe Bornet", "track": "PyData: Generative AI"}, {"title": "Interactive Data Visualization for mathemtics using Jupyter Notebook", "abstract": "Because of the obstruction sciences and learning disorders, and as we know, there are a lot of limitations for women to learn because of gender discrimination, which is common in today's world.Education and learning have turned into a challenge for learners and teachers. So I developed a program in Jupyter Notebook based on Python language and its libraries, such as Numpy, Matplotlib,ipywidget, and Ipython, also some machine learning libraries such as sklearn to visualize the process of solution of mathematical data from beginners to advanced learners.So the program uses Jupyter Notebook to make the process of learning understandable and easier.\r\nYou can fully explore the code and mentioned project in the below GitHub repository named JupyterDataViz includes whole codes of  the project:\r\nhttps://github.com/JupyterDataViz/Interactive-Data-Visualization-for-mathemtics-using-Jupyter-Notebook\r\n\r\nThe above mentioned program is a base step of a startup, including an online platform whose founder is me, and it visualizes more mathematics data.it's submitted to PtyCon US2025. You can explore the mentioned startup at https://github.com/VizAI-\r\nplatform/Visualization-Of-Mathematic\r\nAll in all, I want to dedicate this program to women and girls who faced problems in education due to gender discrimination and were forbidden from learning and education. We hope that in addition to facilitating learning, it will also be a step toward solving the problem of these struggling girls and women.", "full_description": "Objective: The project was designed to help students learn through the visualization of theoretical mathematics. This project provides visual answers for issues in theoretical mathematics. All learners can input the necessary data into the programs to view the results visually. Moreover, students are given visual solutions to mathematical problems.\r\nTarget Audience: Students who are engaged in mathematical issues and anyone who wants to learn and teach are the audience for this project.\r\nAll in all, by introducing this Jupyter Notebook file, which includes programs built on the Python programming language, we can visualize theoretical mathematical problems and give students an opportunity to understand math problems.\r\nSo this program has visualized the mathematical data. For example, you can enter some numbers as inputs to see the visualization solutions of mathematical calculations in colored charts, graphs, and geometric shapes. For more information, you can see the answer to the square root of perfect numbers in geometric shapes and solve the problem of the square root of imperfect numbers by deep learning g and see answers in the unit circle in trigonometry.\r\nYou can fully explore the code and mentioned project in the below GitHub repository which includes whole codes of  the project in GitHub repository named JupyterDataViz includes whole codes of  the project:\r\nhttps://github.com/JupyterDataViz/Interactive-Data-Visualization-for-mathemtics-using-Jupyter-Notebook\r\nAlso, because of the infinity of mathematics science, we are developing an app to upgrade the visualization of theoretical mathematics science.\r\nThe perspective of visualization mathematics Jupyter Notebook file is developing a Python web based app which makes the accessibility to my codes that have visualized the mathematical data much easier for students and others.\r\nSo the mentioned Jupyter Notebook file which visualizes the mathematical data, is the base level in developing a web based startup. The mentioned startup, which includes the web based app visualization of mathematical data, has been introduced in Pycon US 2025. However, it is the first time that the Jupyter Notebook file of visualization mathematical data has been presented as the basic core of the above mentioned startup. Moreover, the core idea and all codes of the Jupyter Notebook file were developed by me, and the startup, which was introduced in Pycon US 2025, is being further developed in collaboration with my colleague. For more information, you can check the bellow GitHub links.\r\nhttps://github.com/VizAI-platform/Visualization-Of-Mathematic\r\nTools:\r\nAll of the codes is written in Python language using Jupyter Notebook.\r\nNumpy, Matplotlib, ipywidget, and Ipython.Also, some machine learning  libraries, such as sklearn  to visualize the solution of mathematical problems which does not have absolute answers, such as mathematical solutions include imperfect numbers.\r\nFor the last point, one of the main goals is helping all brave women who are eager to learn but have been denied the right to education due to gender discrimination by providing them with easy access to learning. So we dedicate this platform to all brave women. We also hope it will be a step to solve their problems.", "code": "7T7ZKZ", "state": "submitted", "created": "2025-01-05", "speaker_names": "Shaghayegh Bagherian", "track": "PyData: Visualisation & Jupyter"}, {"title": "Realejo, Python, and the Thermo Printer from Hell", "abstract": "Dive into the quirky journey of Realejo, a modern AI-powered reimagining of the Brazilian mechanical organ. While animatronics and AI were a breeze, the real villain was a stubborn Bluetooth thermo printer. This talk celebrates the human hero who saved us with open-source code, Python\u2019s adaptability, and the beauty of community-driven problem-solving in the face of technological obsolescence.", "full_description": "Join us for a humorous journey through the creation of Realejo, a cutting-edge art piece blending AI and animatronics to rekindle the magic of a classic Brazilian mechanical organ. Along the way, we discovered that the real challenge wasn\u2019t AI but reverse-engineering a pesky Bluetooth thermo printer. This talk celebrates Python, open source, and the unsung heroes of the web, showing how tech\u2019s quirks and obsolescence sometimes lead to the most unexpected lessons.\r\n\r\nIn this lighthearted 30-minute talk, we unravel the ironic journey behind Realejo, an ambitious art project blending AI, animatronics, and mechanical nostalgia. While we thought AI and animatronics would be our biggest challenges, the true adversary emerged from an unlikely place: a Bluetooth thermo printer. Its refusal to cooperate took us on a hilarious, maddening quest that revealed the often-overlooked beauty of human ingenuity and open-source collaboration.\r\n\r\nThe Art Piece: Realejo\r\nRealejo rekindles the charm of the classic Brazilian mechanical organ, a realejo, traditionally accompanied by a bird delivering oracular fortunes. Our modern rendition integrates AI to generate thoughtful, personalized messages, an animatronic bird to vocalize them via text-to-speech (TTS), and a printer to deliver these fortunes on paper. It\u2019s a celebration of care, intimacy, and creativity\u2014but one piece of this system did everything it could to sabotage the project.\r\n\r\nThe Villain: A Stubborn Bluetooth Thermo Printer\r\nThe AI? Easy. Animatronics? Manageable. The thermo printer? A nightmare.\r\nWe had assumed this little printer, designed to work seamlessly with phones and apps, would also cooperate with a computer. Spoiler: it didn\u2019t. Reverse-engineering its Bluetooth-only protocol to work with Python involved hours of debugging, failed experiments, and absurd technical hoops. This printer became a symbol of how technology sometimes traps us between sleek innovation and the painful reality of obsolescence.\r\n\r\nThe Hero: A Generous Reverse Engineer\r\nWhen all seemed lost, we stumbled upon an open-source repository\u2014a gift from a generous reverse engineer who had cracked the printer\u2019s code and shared their findings with the world. Armed with their work and Python\u2019s versatility, we finally tamed the thermo printer. Realejo could now print fortunes as intended, and we learned an invaluable lesson about the power of community-driven problem-solving.\r\n\r\nWhat We\u2019ll Talk About\r\n\r\nThis talk blends humor, reflection, and practical insights as we dive into:\r\n\t1.\tThe Creation of Realejo\r\n\t\u2022\tThe nostalgic charm of the realejo and its modern AI-powered reimagining.\r\n\t\u2022\tHow GPT and TTS were seamlessly integrated into an animatronic bird.\r\n\t\u2022\tThe vision of blending technology and intimacy to spark moments of kindness.\r\n\t2.\tThe Thermo Printer Saga\r\n\t\u2022\tWhy this seemingly simple device became the hardest part of the project.\r\n\t\u2022\tHow technological obsolescence creates unexpected barriers in creative work.\r\n\t3.\tThe Open Source Hero\r\n\t\u2022\tThe incredible individual who reverse-engineered the printer\u2019s protocol and shared their findings.\r\n\t\u2022\tHow Python and open-source communities empower artists, developers, and creators to push boundaries.\r\n\t4.\tReflections on Technology and Creativity\r\n\t\u2022\tHow tech often straddles the line between the futuristic and the outdated.\r\n\t\u2022\tWhy open-source collaboration is vital for fostering innovation and creativity.\r\n\r\nThis story is a testament to the unsung heroes of open source, whose generosity and curiosity empower creators to overcome seemingly impossible challenges. It also celebrates Python as the ultimate tool for bridging gaps, solving problems, and keeping art alive.\r\n\r\nWe\u2019ll conclude with a demonstration of Realejo in action. Throughout the conference, the animatronic bird will be available for visitors to experience, offering AI-generated fortunes while showcasing the collaboration of art, technology, and community.\r\n\r\nThis talk will leave attendees with a renewed appreciation for the quirks of tech, the brilliance of open-source collaboration, and, of course, Python\u2019s indispensable role in making it all work.", "code": "YFNL8E", "state": "submitted", "created": "2025-01-05", "speaker_names": "Mateus Knelsen", "track": "PyCon: Programming & Software Engineering"}, {"title": "A hitchiker's guide to DsPY", "abstract": "You prompt. I prompt. We all prompt. But how do you get to the best prompt possible? How do you know this is the right prompt you should use? Meet DSPy - a framework out of Stanford NLP that helps you program, instead of prompt, Large Language Models.", "full_description": "AI is moving fast. RAG, vector databases, agents - there\u2019s a lot out there. However, it always starts with one thing: a prompt. You\u2019ve heard about \u201cprompt engineering\u201d. \r\n\r\nIn this talk, I\u2019ll show you how to put the \u201cengineering\u201d back into \u201cprompt engineering\u201d. We\u2019ll talk about DSPy. A python framework out of Stanford NLP for programming \u2014rather than prompting\u2014language models. \r\n\r\nWe\u2019ll talk about what it is, what problems it solves, and how it can help you design prompts efficiently - instead of relying purely on \u201cvibes.\u201d\r\n\r\nOutline:\r\n- Intro - who I am \r\n- LLMs, agents, and hype\r\n- The state of prompting and dark magic\r\n- What if we could program these\r\n- An introduction to DSPy\r\n- Key use cases\r\n- Leveraging Python, Classes, and Pydantic \r\n- A simple use case: recipe generation\r\n- Closing thoughts\r\n- Q&A\r\n\r\nNote: The outline might change slightly.", "code": "3C3E9A", "state": "submitted", "created": "2024-12-20", "speaker_names": "Duarte O.Carmo", "track": "PyCon: MLOps & DevOps"}, {"title": "Extending Python with Rust", "abstract": "Sometimes we run into performance problems when we develop software for Python with complex algorithms for which the ecosystem does not provide us with a handy package we can use to solve our problem. Here Rust can help us. \r\n\r\nThis talk will introduce how to write packages for Python in Rust using the PyO3 library. Starting with an accessible introduction to Rust for those new to the language, we\u2019ll explore why Rust can bring the optimal performance to our algorithms. With that knowledge we will  build a Python extension in PyO3 together.", "full_description": "Python's performance can be a bottleneck in computation-heavy applications although a lot of heavily optimized libraries like NumPy exist which circumvent this problem. These libaries have been written in C and C++ in the past but with the PyO3 package we can also use Rust to write Python extensions. \r\n\r\nWhile Rust is often praised for its safety features without comprosing the performance needed for a systems programmign language it is also much easier to learn than C and C++ making it an ideal candidate for a language to write Python extensions in. \r\n\r\nWe\u2019ll begin with a beginner-friendly introduction to Rust, covering the essentials needed to follow along with the rest of the session. No prior knowledge of Rust, its tooling and ecosystem is needed. \r\n\r\nThen we will learn how PyO3 works by writing a Python extension together and using it in Python code. \r\n\r\nBy the end of the talk, you\u2019ll have a solid understanding of:\r\n\r\n- Rust's basic syntax and core concepts\r\n\r\n- How to use PyO3 to create Python extension modules.", "code": "NURFCN", "state": "submitted", "created": "2024-12-20", "speaker_names": "Hendrik Niemeyer", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Zero Code Change Acceleration: familiar interfaces and high performance", "abstract": "The PyData ecosystem is home to some of the best and most popular tools for doing data-science. Every data-scientist alive today has used pandas and scikit-learn and even Large Language Models know how to use them! For many years there have also been alternative implementations with similar interfaces and libraries with completely new approaches that focus on achieving the ultimate in performance and hardware acceleration. This talk will look at the recent efforts to give users the best of both worlds: a familiar and widely used interface as well as high performance.", "full_description": "The interfaces defined by libraries like Numpy, pandas or scikit-learn are the defacto standard APIs in each library's domain. Data scientists use these libraries directly as well as indirectly through libraries that depend on them.\r\n\r\nThis talk will look at the different approaches that recent efforts have taken to give users both a familiar interface and GPU acceleration. This means users do not have to rewrite their code, learn a new library and benefit from acceleration when using existing libraries.\r\n\r\nThe cudf team built a pandas accelerator by diving deep into the import system of Python. By hooking into the import system you can replace the result of `import pandas as pd` with a library that uses cudf where possible and falls back to pandas where necessary. This is great as it means the user\u2019s seaborn code is automatically accelerated, without the user or the seaborn authors having to do anything.\r\n\r\nThe polars team collaborated with the cudf team to build a GPU engine for polars. They chose a tightly coupled approach where the cuDF library hooks in after the Polars Intermediate Representation is created from the user input. This means using the GPU engine is seamless from the point of view of the user, there is transparent fallback to the CPU and no import statements need changing.\r\n\r\nThe scikit-learn team is adding experimental support to handle PyTorch and CuPy inputs by using the array API standard. Instead of using the Numpy API to perform array computations, scikit-learn is switching to using the array API. This is a subset of the Numpy API that is supported by several other array libraries. The API of Numpy and PyTorch is similar but not exactly the same, this makes writing code that works with both hard. The array API addresses this problem by providing a unified API. Users can accelerate their scikit-learn code by passing in a CuPy or PyTorch array instead of a Numpy array.", "code": "PNQB7C", "state": "accepted", "created": "2024-12-22", "speaker_names": "Tim Head", "track": "PyData: PyData & Scientific Libraries Stack"}, {"title": "The Community Orchard: How my Passion AI Project Sparked a Thriving Python Ecosystem", "abstract": "This talk shares the inspiring journey of Gift, an African Python enthusiast whose passion project\u2014a humble newsletter\u2014blossomed into a thriving community. What began as a simple effort to connect with like-minded individuals grew into a movement, showcasing the transformative power of grassroots initiatives.\r\n\r\nCentral to this story is the \"Community Orchard Chain,\" a metaphorical framework for building vibrant communities. Like cultivating an orchard, this process involves planting seeds of inspiration through impactful content, nurturing connections with consistent engagement, and reaping the fruits of collaboration and shared growth.\r\n\r\nGift\u2019s journey emphasizes patience, authenticity, and a vision that goes beyond individual success. From newsletters to meetups, hackathons, and conferences, each step reflects a commitment to inclusivity and sustainability. This talk will delve into practical strategies for turning passion projects into platforms for collective impact: creating engaging content, fostering participation, supporting open source, and transforming passive audiences into active contributors.\r\n\r\nAttendees will gain actionable insights into building meaningful communities, leaving inspired to cultivate their own \u201cCommunity Orchard\u201d and spark ripples of connection and collaboration in their fields.", "full_description": "Introduction (2 minutes)\r\nWelcome and introduction of the speaker.\r\nBrief story of Gift(Myself) and the passion project that led to a vibrant community.\r\nOverview of the \"Community Orchard Chain\" metaphor.\r\n\r\n1. Identifying a Gap within the Community (The Barren Landscape) (3 minutes)\r\n\r\n2. Cultivating a Niche Newsletter (Choosing the Right Fruit) (4 minutes)\r\n\r\n3. Establishing a Content Creation Process (Tilling the Soil) (4 minutes)\r\nSteps for creating consistent, high-quality content.\r\nTools and techniques to streamline the content workflow.\r\nBalancing personal passion with audience expectations.\r\n\r\n4. Witnessing the Blossom of a Community (The First Bloom) (3 minutes)\r\nSigns that your newsletter is resonating with the audience.\r\nEarly successes: increased engagement, feedback, and participation\r\n.\r\n5. Highlighting the Power of Collaboration (Branching Out) (4 minutes)\r\nLeveraging partnerships, collaborations, and contributors.\r\nExamples of community projects sparked by collaboration.\r\n\r\n6. Exploring Avenues for Extended Engagement (Abundant Harvest) (5 minutes)\r\nTransforming the community from passive to active: meetups, hackathons, conferences, and more.\r\nCase studies of extended engagement and their impact.\r\n\r\n7. Sharing Key Takeaways and Actionable Tips (Pollination and Spreading the Seeds) (3 minutes)\r\nPractical advice for aspiring community builders.\r\nHow to inspire others to replicate success.\r\n\r\n8. Reflecting on Ongoing Impact and Future Aspirations (The Orchard's Future) (3 minutes)\r\nLong-term vision for the community and its role in driving innovation and inclusivity.\r\nEncouraging attendees to think about their community's potential.\r\n\r\nConclusion and Q&A", "code": "U9S7SR", "state": "submitted", "created": "2024-12-20", "speaker_names": "Gift Ojeabulu", "track": "General: Community & Diversity"}, {"title": "Temporal: Bulletproof Workflows", "abstract": "**Temporal** is an open source, distributed, and scalable workflow orchestration platform designed to execute mission-critical business logic with resilience. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail.", "full_description": "Are you ready for the next generation of orchestration platforms? **Temporal** is the successor to [Uber Cadence](https://cadenceworkflow.io/). It is an open source, distributed, and scalable workflow orchestration platform to execute mission-critical business logic with resilience. Your code will run reliably even if it encounters problems, such as network outages or server crashes. With great Developer Experience (DX) and [Python SDK](https://docs.temporal.io/develop/python), the platform simplifies coding. Join us to unlock the power of Python and Temporal in mastering workflow orchestration!", "code": "BGP3HG", "state": "submitted", "created": "2024-12-15", "speaker_names": "Ruslan Korniichuk", "track": "PyCon: Programming & Software Engineering"}, {"title": "\ud83d\udd25 Mojo's Momentum: A Year in Review Evaluating Progress, Performance, and Community Growth", "abstract": "Two years after Chris Lattner unleashed Mojo on the tech world with promises of blazing speeds and Pythonic ease, it's time to assess its journey. In this talk, we revisit Mojo's initial claims, examine real-world performance gains, explore enhancements in language features, and analyze the growth of its ecosystem and community. Using our experience from developing a Large Language Model Interpretation library, we'll determine whether Mojo has solidified its place among AI development languages or if it still faces challenges against established contenders like Rust and Julia. Join us as we provide a comprehensive update on Mojo\u2019s achievements, setbacks, and future prospects in the rapidly evolving landscape of AI and machine learning.", "full_description": "Since its release on May 2, 2023, Mojo has promised to bridge the gap between Python's simplicity and the performance demands of AI development. As we mark its first anniversary, understanding Mojo's trajectory is crucial for developers, researchers, and organizations considering its adoption. This evaluation not only reflects on Mojo's technical advancements but also on its influence within the programming community and its capacity to foster innovation in AI.\r\n\r\nThis presentation aims to provide an in-depth analysis of Mojo's progress over the past year. Focusing on our hands-on experience building a Large Language Model Interpretation library during an AI Safety Camp project, we will assess Mojo's performance claims, usability enhancements, ecosystem development, and community support. Additionally, we'll compare Mojo's evolution with other high-performance languages in the AI domain to gauge its competitive standing.\r\n\r\n\r\nRecap of Mojo's initial vision and key milestones achieved in the past year.\r\nOverview of major updates, feature additions, and performance optimizations introduced since launch.\r\nPerformance Reality Check\r\n\r\nUpdated benchmarks comparing Mojo's performance against Python, Rust, Julia, and PyPy.\r\nAnalysis of real-world application performance, highlighting areas where Mojo excels and where it falls short.\r\nLanguage and Ecosystem Enhancements\r\n\r\nExamination of improvements in Mojo's syntax, semantics, and tooling based on user feedback and development progress.\r\nExpansion of the Mojo ecosystem: libraries, frameworks, and integrations that have emerged over the year.\r\nCase Study Revisited: Large Language Model Interpretation Library\r\n\r\nUpdates on the project developed in Mojo: performance metrics, new features, and lessons learned.\r\nComparative analysis with implementations in other languages, emphasizing Mojo's strengths and weaknesses encountered.\r\nCommunity and Adoption Growth\r\n\r\nInsights into Mojo's community building: forums, contributions, tutorials, and conferences.\r\nAdoption trends: industries and projects that have embraced Mojo, and the factors driving their decisions.\r\nComparative Landscape\r\n\r\nCurrent state of high-performance languages for AI: Rust, Julia, PyPy, and Mojo.\r\nMojo\u2019s unique propositions and challenges in distinguishing itself within this competitive field.\r\nFuture Outlook and Roadmap\r\n\r\nAnticipated developments in Mojo\u2019s pipeline.\r\nPredictions on Mojo\u2019s potential trajectory in the AI and broader programming ecosystems.\r\nConclusion\r\n\r\nAs Mojo celebrates its first year, the language has made significant strides in fulfilling its ambitious promises. While it showcases remarkable performance improvements and a growing ecosystem, challenges remain in achieving widespread adoption and competing with established languages. Our analysis underscores Mojo's potential to influence AI development, provided it continues to evolve and address the needs of its user base. Whether Mojo will ascend to become a staple in AI programming or remain a promising contender hinges on its ability to sustain momentum and foster a vibrant community.\r\n\r\nExpected Audience Expertise:\r\n\r\nDomain: Intermediate\r\nProgramming Languages: Intermediate (Python familiarity preferred)\r\nAbstract as a Tweet (X) or Toot (Mastodon):\r\n\"Celebrating Mojo's second year! \ud83d\ude80 Has Chris Lattner's Python cousin delivered on its 68k speed promise? We dive into performance, ecosystem growth, and its role in AI dev. Can Mojo stand tall against Rust & Julia? #AI #MojoLanguage #PythonCousin #ProgrammingLanguages\"", "code": "SYWFAF", "state": "submitted", "created": "2025-01-05", "speaker_names": "Jamie Coombes", "track": "PyCon: MLOps & DevOps"}, {"title": "How to Build a Team for Delivering Impactful Data Products: Best Practices and Pitfalls", "abstract": "Building a successful data science team is about fostering the right environment alongside technical execution. Drawing from experiences at GetYourGuide, this talk explores best practices, including cultivating diversity, ownership, accountability, and structuring teams to encourage innovation and flexibility.\r\n\r\nWe\u2019ll also address challenges like measurement uncertainties, missed opportunities, and organizational barriers that stifle progress. Designed for leaders and team members, this session offers insights to navigate the complexities of building and managing high-performing data science teams.", "full_description": "Building effective data science team is as much about fostering the right environment as it is about technical execution. In this talk we will share key lessons from our experiences leading data science teams at GetYourGuide, with a focus on the best practices and common pitfalls that can be useful for any ambitious data science team.\r\n\r\nWe\u2019ll explore how to create  a culture of diversity, ownership and accountability and knowledge. We will also highlight ways to structure team portfolios to encourage diversity of ideas, maintain flexibility, and enable sustainable innovation. We will talk about the role of data science strategy. We will also highlight some painful learnings such as not proactively settling measurement uncertainties, not uncovering all key opportunities, and difficult organizational  barriers that stifle innovation. \r\n\r\nThis talk is for leaders or individual seeking to navigate the challenges of building, managing or being part of a high-performing data science team.", "code": "3PGXPZ", "state": "submitted", "created": "2024-12-22", "speaker_names": "Jean Machado, Tina Treimane", "track": "General: Education, Career & Life"}, {"title": "SUN vs Me : Quest to Outwit the Blinding Sun with micropython and MQTT", "abstract": "Join me on the journey of me trying to battle the Sun for a few more precious moments of sleep using MQTT and python. As someone who adores city lights during the night but hates being rudely awakened by the blinding Sun, I embarked on a journey to automate my curtains using Raspberry Pi, stepper motors, micropython and MQTT. From creative ideas to comical mishaps, and eventually stumbling upon a somewhat functional solution, this talk promises laughter, learning, and a peek into the thrillingly world of microprocessors and the MQTT framework.", "full_description": "## Description:\r\n\r\n**Target Audience:**\r\n\r\n- Aspiring IoT enthusiasts seeking to turn a make an end to end IOT solution.\r\n- Anyone who enjoys learning through laughter and the occasional \"print('facepalm')\".\r\n- everything else we will cover in the talk.\r\n- some very basic knowledge of python.\r\n\r\n**Benefits for Attendees:**\r\n\r\n- Gain practical knowledge of\u00a0**MQTT and its Pythonic potential**.\r\n- Learn to craft\u00a0**multi-layered solutions**\u00a0that combine sensors, user control, and API integration.\r\n- Embrace the\u00a0**iterative nature of development**\u00a0and learn to laugh at your code (trust me, it helps).\r\n\r\n**Outline**\r\n\r\n1. Initial Approach and Challenges (5 minutes):\r\n- Introduce the problem of early morning sunlight disrupting sleep.\r\n- Describe the initial attempt using a light sensor and Python script.\r\n1. Introducing MQTT and Setting Up the System (10 minutes):\r\n- Discover the power of\u00a0MQTT, the messenger of the machine gods!.\r\n- Briefly showcase setting up a Raspberry Pi with an MQTT broker.\r\n- Demonstrate connecting the light sensor to the system and publishing readings.\r\n1. Refining the Solution with User Control and Sunrise Data (10 minutes):\r\n- Acknowledge the limitations of relying solely on sensor data.\r\n- Introduce a mobile app for user control to adjust blinds manually.\r\n- Explain integrating the app with the MQTT broker for communication.\r\n-leveraging an API to retrieve sunrise times and schedule blind adjustments.\r\n1. Lessons Learned and Open Discussion (5 minutes):\r\n- Share the\u00a0trials and tribulations\u00a0of the journey, with a dash of humor.\r\n- Open forum for questions and discussions\r\n\r\n ### Why did I choose this topic\r\n\r\nAs someone who enjoys gazing at the night sky from the comfort of my bed, sunrise has always been a rude awakening. My engineer brain kicked in, and I decided to build a solution!  While the final product might be considered a bit over-engineered and \"mostly working,\" it served a valuable purpose.  Through this project, I gained a deep understanding of how Micropython and MQTT functions and its vast potential.  Most importantly, I learned to laugh off my mistakes and embrace the iterative nature of development.\r\n\r\n## **What audience can get from the talk**\r\n\r\n- This talk will equip attendees with the skills to build powerful automation systems! *\r\n- Master MQTT: Learn a powerful messaging tool (MQTT) and how to use it seamlessly with Python to connect your devices.\r\n- Build complex projects: Go beyond basic controls. Discover how to combine sensors, user interfaces, and even connect to external services (APIs) to create multi-layered solutions.\r\n- Embrace the journey: Development is all about learning from mistakes. We'll show you how to approach coding with a positive attitude and laugh off the inevitable bumps along the road!\r\n\r\n## **Prior knowledges I assume the audience to have**\r\n\r\nThis talk is perfect for anyone with a basic understanding of Python and a healthy dose of humor (trust me, it will be needed for my jokes!).  While we'll be diving deep into the exciting world of  MQTT and its potential for building smart home solutions, no prior knowledge is required.  Get ready to learn, laugh, and discover the joy of iterative development.", "code": "PSPR37", "state": "submitted", "created": "2024-12-12", "speaker_names": "vishrut kohli", "track": "PyCon: Python Language & Ecosystem"}, {"title": "How to Break Python\u2019s Speed Limits", "abstract": "Python is known for its simplicity and flexibility, but when performance matters, even established number-crunching libraries like NumPy can fall short. What if you could keep Python's ease of use and achieving significantly faster execution speeds? In this talk, we\u2019ll explore how to seamlessly integrate C++ into your Python packages to supercharge performance. Whether you\u2019re dealing with heavy computations or performance-critical code, this session will show you how to speedup your code without sacrificing Python's flexibility. Get ready to unlock the best of both worlds: Python\u2019s productivity and C++\u2019s power!", "full_description": "This presentation addresses a common challenge faced by Python developers: achieving high performance for computationally expensive tasks. While Python is widely appreciated for its simplicity and readability, it often struggles with performance bottlenecks, especially for loops, CPU-bound tasks, and large-scale numerical computations.\r\n\r\nWe will demonstrate how integrating C++ into Python projects can significantly improve performance while keeping the process approachable and easy to implement, even for developers without advanced C++ skills. By using binding tools, you will learn how to harness the power of C++ with minimal effort and without sacrificing Python\u2019s user-friendly nature.\r\n\r\nThe talk will highlight:\r\n\r\nPractical techniques for integrating C++ into Python with clear examples.\r\nReal-world cases showing how C++ can outperform popular libraries like NumPy.\r\nSimple strategies for splitting Python loops across multiple CPU cores using multithreading.\r\nHow to achieve significant speedups with minimal coding changes and accessible tools.\r\nThe focus will be on actionable and easy-to-follow insights.\r\n\r\nAfter this session, you will walk away with the confidence to optimize your Python code without deep knowledge of C++ or extensive refactoring.", "code": "JHRTYW", "state": "submitted", "created": "2024-12-20", "speaker_names": "Dima Pr\u00f6frock", "track": "PyCon: Programming & Software Engineering"}, {"title": "50 Shades of Fairness: Empower AI constraints in Python", "abstract": "Imagine a world where AI systems perpetuate biases, exacerbate inequalities, and undermine trust.\r\nWell, you can stop 'imagining' because it WILL happen if we are not careful while developing such systems. AI technology is full of biases, unfairness, and socio-technical problems that can have unexpected results if not understood properly. The consequences are dire, and so, it is high time that we incorporate fairness techniques in our systems to build a conscious new world of 'fair' technology.\r\n\r\nJoin me on a journey to explore the nuances of fairness constraints, or \"shades of fairness\", that can make or break an AI system's integrity. There is NO one-way-fits-all to make an AI system fair.\r\n\r\nTogether, let's build a 'fair' world where AI empowers and uplifts humanity towards a progressive future!", "full_description": "In this talk, we'll learn:\r\n- The Backwardness (Dark side) of AI: Real-world scenarios of unfair AI.\r\n- Practical Fairness Constraints: In-depth learning of what fairness constraints are, how to apply them using open-source Python libraries in your AI projects.\r\n\r\nI. Introduction and background setting (10mins)\r\n- Brief overview of the concept of fairness in AI systems\r\n- Importance of addressing fairness in AI systems\r\n- Types of unfairness, real-life scenarios and their consequences\r\n\r\nIII. Shades of Fairness: Practical Fairness Constraints (15mins)\r\n- Overview of different fairness constraints\r\n- In-depth understanding with a couple of them (demographic parity, equal opportunity) with mathematical formulae, visualizations and examples.\r\n- Application of these fairness constraints in an AI system using some Python open-source libraries.\r\n- Potential limitations and explain why there is NO one-size-fits-all fairness constraint.\r\n\r\nIV. Conclusion (5mins)\r\n- Some takeaways\r\n- Call to action for attendees to prioritize fairness in their own AI projects.", "code": "UYV8MF", "state": "submitted", "created": "2024-12-21", "speaker_names": "Parul Gupta", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Optimizing Energy Tariffing System with Formal Concept Analysis and Dash", "abstract": "As a data scientist, I value the power of insightful visualizations to unlock unique interpretations of complex data. In my talk, I will introduce an elegant mathematical framework called Formal Concept Analysis (FCA), developed in the 1980s in Darmstadt.\r\n\r\nFCA transforms binary data into concepts that can be visualized as a hierarchical graph, offering a fresh perspective on multidimensional data analysis. Leveraging this theory and its open-source Python libraries, I am developing an interactive Dash-based tool featuring interactive tables and graphs to explore data insights.\r\n\r\nTo illustrate its potential, I will showcase an optimization of the entire tariffing system of an energy provider company, highlighting how FCA can bring structure and clarity to even such tangled datasets.", "full_description": "My goal is to introduce Formal Concept Analysis (FCA) as a fascinating mathematical framework. I aim to inspire Python enthusiasts to explore its potential and uncover insights in their data analysis tasks. The talk is divided into three sections:\r\n\r\n1 FCA Basics\r\n\r\n- What is a \"concept\"? *First, I am going to introduce the main terms used in FCA and define the central object of the theory - the formal concept.*\r\n\r\n- Illustrative example. *To show the power of FCA in action, I will provide a relatable example to explain the hierarchical structure of the graph visualization.*\r\n\r\n2 Python Implementation\r\n\r\n- ``fcapy`` Python library. *Core functionality overview of the library and the data formats it can use.*\r\n\r\n- Introducing interactivity with Python Dash: *Enhancing exploration and user experience with interactive tables (AG Grid) and dynamic graph visualizations (Cytoscape).*\r\n\r\n3 Applications and Practical Relevance \r\n\r\n- Use Case: Energy Tariffing System Optimization. *In this section, I am going to showcase the real data in its original complexity and the optimization process of identifying redundancies, overlaps, or inefficiencies.*\r\n- Examples of other  applications and key takeaways", "code": "B8TUR9", "state": "submitted", "created": "2024-12-19", "speaker_names": "Dr. Irina Smirnova-Pinchukova", "track": "PyData: Visualisation & Jupyter"}, {"title": "Effortless Testing in Resource-Intensive Data Applications", "abstract": "Testing data-driven applications often involves significant overhead due to the setup and teardown of resource-intensive systems like databases and cloud services. This talk will explore practical strategies for designing integration and system tests with **pytest** to minimize these inefficiencies. By leveraging techniques to decouple tests while maximizing resource reuse, you\u2019ll learn how to enhance your testing process, improving performance, scalability, and test isolation in your own projects.", "full_description": "Fast and reliable tests are the backbone of maintaining quality and performance in data-heavy applications, where even small issues can cascade into significant bottlenecks or failures. A streamlined testing process allows developers to confidently evolve complex systems while ensuring robustness and efficiency.\r\n\r\nFor integration and system tests in data-driven applications, the setup and teardown of data layers (e.g., databases, blob storage) often take more time than the actual test execution. A common approach is to use session-scoped test resources, but this introduces shared dependencies that couple tests together. As a result, developers must carefully manage resource cleanup after each test to prevent interference, leading to tests that are often flaky, verbose, and harder to maintain.\r\n\r\nThis talk will demonstrate how to leverage pytest fixtures to abstract setup and teardown logic across both session and function scopes. This method ensures proper resource cleanup, improves test isolation, and enhances test readability and development efficiency. We\u2019ll also explore strategies for parallelizing test execution while sharing system resources effectively, enabling faster and more scalable testing.\r\n\r\nThis talk is designed for intermediate to advanced Python developers with a strong interest in testing and improving their development workflows.", "code": "BERGKY", "state": "submitted", "created": "2024-11-28", "speaker_names": "Patrik Hlobil", "track": "PyCon: Testing"}, {"title": "Using Trusted Publishing to Ansible release", "abstract": "\"Trusted publishing\" , the key feature in PyPI empowers the project maintainers to make releases via automated environments directly . This helps us to get rid of the use of manually generated API tokens. This talk will dig deeper in the practical aspects and impact of moving manual  release process to automated release via github actions and trusted publishing. The  talk will describe the trusted publishing from the view of a Release Manager of a critical project like Ansible.", "full_description": "\"Trusted publishing\" is the term for using the OpenID Connect (OIDC) standard in the Python Ecosystem to release on PyPI.  In this talk will go though the usage of trusted publishing in any Python project and how it helped Ansible project to open up release management to the community.\r\n\r\n\"Trusted publishing\" is the  the way of  exchanging short-lived identity tokens between a trusted third-party service and PyPI. This key feature in PyPI empowers the project maintainers to make releases via automated environments directly . This helps us to get rid of the use of manually generated API tokens. This talk will dig deeper in the practical aspects and impact of moving manual  release process to automated release via github actions and trusted publishing. The  talk will describe the trusted publishing from the view of a Release Manager of a critical project like Ansible.", "code": "DCLT93", "state": "submitted", "created": "2024-12-20", "speaker_names": "Anwesha Das", "track": "PyCon: Programming & Software Engineering"}, {"title": "How to migrate from Pytest to Unittest..... Stop, WHAT?", "abstract": "Pytest has gained worldwide love and popularity as a universal and flexible tool for all testing cases. However, it has significant disadvantages that may motivate you to change your usual testing environment.", "full_description": "The talk is based on a comparison of the work with Pytest and unittest libraries.\r\nExamples in this Talk will higlight the main reasons why a certain project had to change testing framework and how to do it with minimal troubles.\r\n\r\nAfter this talk, each participant should have an understanding that the choice of technology is not always based on popularity, but should be based primarily on the issue being solved.", "code": "3CKX9C", "state": "submitted", "created": "2025-01-05", "speaker_names": "Maxim Danilov", "track": "PyCon: Testing"}, {"title": "QuerySet.explain(): make it make sense.", "abstract": "We have tools that show what queries are executed and the time it takes. But what next? What is going on there? Is it good that it's doing that? Will some indexes help?\r\nIn this talk, I will help you decipher database query plans and give some rules of thumb to understand if the database is doing the best it can. We will also use query plans to find the best indexes for queries. I will also share several anti-patterns I have seen in  Django projects and show how to rewrite them in a database-friendly way.", "full_description": "We have tools that show what queries are executed and the time it takes. But what next? What is going on there? Is it good that it's doing that? Will some indexes help?\r\nIn this talk, I will help you decipher database query plans and give some rules of thumb to understand if the database is doing the best it can. We will also use query plans to find the best indexes for queries. I will also share several anti-patterns I have seen in  Django projects and show how to rewrite them in a database-friendly way.", "code": "S7JQTG", "state": "withdrawn", "created": "2024-11-27", "speaker_names": "Aivars Kalv\u0101ns", "track": "PyCon: Django & Web"}, {"title": "How to evaluate fairness and safety in LLM applications?", "abstract": "Fairness and safety are fundamental criteria for building trustworthy and high-quality AI systems, whether they are credit scoring models, hiring assistants, or healthcare chatbots. But what does it truly mean for an AI system to be fair and safe? In this talk, I will explore the potential risks and challenges associated with these principles and introduce various approaches and techniques for evaluating AI systems. The discussion will center on applications powered by Large Language Models (LLMs), highlighting their unique considerations and implications.", "full_description": "Artificial intelligence holds immense promise to revolutionize industries and enhance lives, but its true potential hinges on trust. Fairness and safety are not just ethical ideals\u2014they are critical pillars for the responsible development, deployment, and evaluation of AI systems. This is particularly important for applications powered by Large Language Models (LLMs), which are now shaping diverse fields like coaching, hiring, and financial services. However, ensuring these models meet rigorous fairness and safety standards presents complex challenges.\r\n\r\nIn this 30-minute talk, we will dive into five key areas:\r\n\r\n1. Introduction to LLM-Based Applications:\r\nWe\u2019ll begin with an accessible introduction to Large Language Models\u2014avoiding technical deep dives - and showcase real-world examples of how LLMs are applied across industries.\r\n\r\n2. Understanding Risks and Challenges:\r\nLLMs, while powerful, come with vulnerabilities such as biases and the potential for harmful behaviors. Discriminatory or toxic outputs can infringe on human rights, damage reputations, and result in financial loss. Through real-world examples, I will highlight these risks and discuss why addressing them is crucial.\r\n\r\n3. Defining Fairness and Safety in AI:\r\nWhat does it mean for an AI system to be fair and safe? Here, I will outline the key criteria for trustworthy AI applications, offering a framework to evaluate their ethical and societal impact.\r\n\r\n4. Evaluation Techniques and Tools:\r\nFrom public benchmarks to innovative prompt designs and metrics, we\u2019ll explore practical methods for assessing LLM trustworthiness. This section will empower participants to leverage the right tools to ensure their AI systems meet ethical and operational standards.\r\n\r\n5. Building the Future: Strategies for Fair and Safe AI:\r\nThe final segment focuses on actionable strategies to design and deploy fair, safe, and compliant AI systems. I\u2019ll also discuss the role of emerging regulations, such as the EU AI Act, in guiding responsible innovation.\r\n\r\nThis session offers valuable insights for researchers, developers, and decision-makers, equipping them to create AI systems that are not only high-performing but also aligned with societal values and evolving regulatory frameworks.", "code": "XEEBT9", "state": "submitted", "created": "2024-12-10", "speaker_names": "Sebastian Krauss", "track": "General: Ethics & Privacy"}, {"title": "Streamlining Python deployment with Pixi:  A Perspective from production", "abstract": "In our quest to improve Python deployments, we explored Pixi, a tool designed to enhance dependency management within the Conda ecosystem. This talk recounts our experience integrating Pixi into a setup used in production. We leveraged Pixi to create lockfiles, ensuring consistent builds, and to automate deployments via CI/CD pipelines. This integration led to greater reliability and efficiency, minimizing deployment errors and allowing us to concentrate more on development. Join us as we share how Pixi transformed our deployment process and offer insights into optimizing your own workflows.", "full_description": "In modern software development, managing dependencies effectively is crucial for ensuring that applications run smoothly across various environments. This talk explores our journey to optimize Python deployments by integrating Pixi into our workflow. As a tool that enhances the Conda ecosystem, Pixi offers a reliable and efficient solution to the common challenges in dependency management. While concepts such as consistent builds, reproducibility, and automated deployments are well-established, Pixi simplifies their implementation within a Conda-based environment, making these practices more accessible and manageable.\r\n\r\nThe talk will cover\r\n- DevOps Concept\r\n  Introducing concepts like lockfile, reproducible environments and CI/CD pipeline to set out a good\r\n  baseline for deploying python code productively\r\n- Conda vs Pypi comparison\r\n  Considering the tradeoffs between isolation and development comfort\r\n- Pixi introduction\r\n  An introduction to the philosphy of pixi and how it compares to other conda tooling.\r\n  This also covers how Pixi streamlines the implementation of DevOps concepts\r\n- Implementing DevOps concepts using pixi\r\n  \r\nThis talk is designed for professional software developers who prioritize a robust setup for deploying Python code as services into production. While familiarity with the Conda ecosystem is beneficial, it is not a prerequisite for this session.", "code": "BLKYGU", "state": "accepted", "created": "2024-12-19", "speaker_names": "Dennis Weyland", "track": "PyCon: MLOps & DevOps"}, {"title": "Observability Unboxed: Building an Open-Source Stack with OpenTelemetry, AWS, and Python", "abstract": "This talk aims to make observability easy to understand by breaking it down into three main components: metrics, logs, and traces. Using an \u201cunboxing\u201d approach, we\u2019ll guide attendees step-by-step through building an observability stack with open-source tools like OpenTelemetry, Prometheus, and Grafana, enhanced by AWS for scalability.\r\n\r\nThe session includes live demos and practical examples, ensuring both newcomers and seasoned DevOps professionals gain valuable, actionable insights. Attendees will leave with a toolkit and a clear path to implementing observability in their environments.", "full_description": "Observability often seems complex, with many tools, data sources, and concepts to understand. But what if we looked at it like \u201cunboxing\u201d each part, making it easy to see how everything connects? This talk breaks down observability into simple steps that anyone can follow.\r\n\r\nWe\u2019ll start with the basics, like metrics, logs, and traces. We use OpenTelemetry to collect data, Prometheus to track metrics, and Grafana to display data. We\u2019ll create an observability stack that\u2019s completely open-source and easy to set up. Each part will be explained in a practical, fun way so you\u2019ll understand how they all work together to give a full view of your system\u2019s health.\r\n\r\nWe\u2019ll also bring in AWS to show how adding cloud resources can make this setup more scalable and affordable. With hands-on examples and live demos, you\u2019ll see how to build an observability pipeline\u2014from collecting data to viewing it in real-time.\r\n\r\nBy the end of this talk, you\u2019ll have a practical observability toolkit you can use right away. Whether you\u2019re new to observability or experienced in DevOps, this session will give you a simple, useful path to building observability in your setup.", "code": "EDALFA", "state": "submitted", "created": "2024-12-28", "speaker_names": "Koteswara Rao Vellanki", "track": "PyCon: MLOps & DevOps"}, {"title": "Streaming at 30,000 Feet: A Real-Time Journey from APIs to Stream Processing", "abstract": "Traditional API architectures face significant challenges in environments where repetitive and frequent requests are required to retrieve data updates. These request-response mechanisms introduce latency, as clients must continually query the server to check for changes, often receiving redundant or outdated information. This approach leads to increased network overhead, inefficient use of server resources and diminished scalability as the number of clients or requests grows. Additionally, frequent requests expand the attack surface, requiring security measures to mitigate risks such as (un-)authorised access, rate limiting and query sanitisation. Managing all of these inherent problem results in increasingly complex systems to maintain and improve while putting considerable implementation effort onto the customer.\r\nJoin to find out how transitioning to a streaming architecture can address these issues by providing proactive, event-based data delivery, reducing latency, minimising redundant processing, enhancing scalability and simplifying security management.", "full_description": "In this talk we will go over which benefits, drawbacks and lessons Airbus has encountered/learned in the switch from an API to a Python based Stream Architecture for continuous flight traffic prediction. The Goal is to highlight Stream based architectures as a architectural alternative and allow the attendees to decide if it could be a alternative to their current API based Setup worthwhile to look into.\r\n\r\nThe talk addresses the inefficiencies and limitations of traditional API-based architectures for real-time data delivery. Specifically, it explores challenges such as high latency, network overhead, customer effort, and scalability issues when APIs rely on polling mechanisms. These issues became apparent at Airbus during a project as customer needs evolved, highlighting the shortcomings of APIs in handling real-time updates effectively. This story of the project will aid as an example on how to identify a limiting architectural decision and what pain points can potentially be avoided by taking a new route guiding us along the talk.\r\n\r\nFor developers and architects building modern, data-driven applications, choosing the right architecture is critical. Many face similar challenges when scaling APIs for real-time use cases, such as IoT, financial data, or notifications. This problem is relevant because adopting an unsuitable architecture can lead to poor performance, higher costs, and frustrated users.\r\n\r\nThe proposed solution is to transition from an API-based architecture to a streaming architecture for real-time data delivery. This involves leveraging stream processing systems that push updates proactively, handle high-throughput data efficiently, and offer features like backpressure, partitioning, and stateful processing. Recent developments in Python based tools such as **Bytewax**, **Faust** and **Quix** are highlighted for their scalability and fault-tolerance capabilities.\r\n\r\n### Key Takeaways\r\n1. **Challenges of APIs**: Polling APIs is inefficient for real-time updates, leading to delays, resource wastage, and customer dissatisfaction.\r\n2. **Advantages of Streaming**: Streaming architectures offer real-time data delivery, lower latency, reduced customer effort, better scalability, and improved fault tolerance.\r\n3. **Key Streaming Concepts**: Understanding backpressure, partitioning, and stateful processing is essential for understanding streaming specific limitations and solutions.\r\n4. **Architectural Considerations**: Streaming is ideal for use cases where data changes frequently and needs to be delivered in real time, while APIs may still be suitable for low-frequency, static, or manual queries.\r\n5. **Strategic Transition**: Adopting a streaming approach requires a paradigm shift in thinking about how data is delivered and processed, with significant changes to the system architecture which needs to be cautiously managed.", "code": "CTUEJX", "state": "submitted", "created": "2024-11-27", "speaker_names": "Felix Leon Buck", "track": "PyCon: Programming & Software Engineering"}, {"title": "\ud83e\udd2f No data? No problem! Synthetic data to the rescue", "abstract": "Got data problems? Relax. Synthetic data is here to help. \r\n\r\nI will go over the fundamentals of synthetic data and show how frontier model providers and researchers use synthetic data to speed up their model development processes, proving that sometimes the best solution isn't finding the right data\u2014it's creating it.", "full_description": "Got data problems? Relax. Synthetic data is here to help. \r\n\r\nI will go over the fundamentals of synthetic data and show how frontier model providers and researchers use synthetic data to speed up their model development processes, proving that sometimes the best solution isn't finding the right data\u2014it's creating it. \r\n\r\nThis talk will start with an overview of the importance of data in general and continue with the what, why and how of synthetic data. This will also briefly mention some of the use cases in commercial and research scenarios. \r\n\r\nThen, we'll go over a taxonomy of different synthetic data types focusing on rewriting, judging and rationales. These data types are then discussed based on fun, practical examples and more educational research papers. \r\n\r\nThe final chapter focuses on the evaluation of synthetic data and how we can scale generation from various prompts to pipelines with various useful tools and libraries. We then go over various entry points for getting started with synthetic data.", "code": "F9KRYX", "state": "submitted", "created": "2024-12-02", "speaker_names": "David Berenstein", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Beyond Basic Prompting: Supercharging Open Source LLMs with LMQL's Structured Generation", "abstract": "This intermediate-level talk demonstrates how to leverage Language Model Query Language (LMQL) for structured generation and tool usage with open-source models like Llama. You will learn how to build a RAG system that enforces output constraints, handles tool calls, and maintains response structure - all while using open-source components. The presentation includes hands-on examples where audience members can experiment with LMQL prompts, showcasing real-world applications of constrained generation in production environments.", "full_description": "1. Introduction to structured generation with LMQL and open-source LLMs\r\n   - Key differences between constrained and free-form generation\r\n   - Why structure matters for production applications\r\n   - Setting up LMQL with Llama\r\n\r\n2. Building a RAG system with structured outputs\r\n   - Implementing context retrieval with constraints\r\n   - Enforcing response formats through LMQL decorators\r\n   - Handling edge cases and error states\r\n\r\n3. Tool usage and function calling\r\n   - Implementing tool calls through LMQL\r\n   - Managing tool execution flow\r\n   - Error handling and fallbacks\r\n\r\n4. Interactive segment\r\n   - Audience members will write and test their own LMQL prompts through a live demo environment\r\n\r\n5. Production considerations\r\n   - Scaling structured generation\r\n   - Monitoring and logging strategies\r\n\r\nAttendees will leave with practical knowledge of how to implement structured generation in their own projects using LMQL, understanding both the technical implementation and best practices for production deployment.", "code": "DQTMJB", "state": "submitted", "created": "2024-12-22", "speaker_names": "Christiaan Swart", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Powering Up DDoS Defense with Python: Building Resilient Systems", "abstract": "This talk will explore how Python can be leveraged to build robust DDoS defense mechanisms, focusing on real-time threat detection, mitigation strategies, and system resilience. We will dive into key Python libraries, best practices, and techniques to protect your applications from large-scale DDoS attacks while ensuring high availability.", "full_description": "In the world of modern web applications, Distributed Denial of Service (DDoS) attacks are a growing concern that can cripple services, disrupt business operations, and damage brand reputation. In this session, we\u2019ll dive deep into how Python, when combined with Jenkins, ELK Stack, and ElastAlert, can be used to create an effective, scalable, and resilient DDoS defense system.\r\n\r\nKey Takeaways:\r\n\r\n1. Using Python for Real-Time Detection:\r\n   - Learn how Python\u2019s versatility and performance make it an ideal tool for real-time detection of abnormal traffic patterns that are indicative of a DDoS attack.\r\n   - We will explore how Python scripts can monitor traffic, analyze request headers, identify rate-limiting violations, and trigger alerts when thresholds are exceeded.\r\n\r\n2. Automating Defense with Jenkins:\r\n   - Jenkins, a widely used automation server, can play a crucial role in the DDoS defense workflow by automating the response processes.\r\n   - We'll walk through how Jenkins pipelines can trigger specific defense mechanisms, such as blocking malicious IPs, scaling up server resources, or engaging additional security measures based on attack patterns identified by Python.\r\n\r\n3. Leveraging ELK Stack for Enhanced Monitoring and Visualization:\r\n   - Learn how to integrate Python scripts with the ELK (Elasticsearch, Logstash, Kibana) Stack for better visibility into traffic patterns and security events.\r\n   - Elasticsearch will store and index traffic data, while Logstash processes and analyzes logs in real-time. Kibana\u2019s powerful visualization capabilities allow security teams to quickly identify anomalies and respond to potential DDoS threats.\r\n\r\n4. Proactive Alerts with ElastAlert:\r\n   - ElastAlert, a tool built on top of Elasticsearch, will be used to set up automated alerts and notifications whenever suspicious activity is detected by Python scripts.\r\n   - We\u2019ll discuss how to create custom alerting rules in ElastAlert to notify system administrators via email, Slack, or other communication channels when a DDoS attack is imminent or active.\r\n\r\n5. End-to-End Automation:\r\n   - Combining Python, Jenkins, ELK, and ElastAlert creates an integrated DDoS defense system where Python handles real-time detection and analysis, Jenkins automates defensive actions, ELK provides monitoring and visualization, and ElastAlert ensures timely alerts.\r\n   - This end-to-end solution improves the response time, reduces human error, and ensures that your system remains protected even under sustained attack.\r\n\r\nReal-World Use Case:\r\n\r\n- A gaming platform like PokerBaazi, with millions of active users, could benefit from this setup to defend against DDoS attacks that often target high-traffic periods or gaming events. Using Python, the platform can detect abnormal spikes in traffic patterns and trigger Jenkins to scale resources automatically or block malicious traffic through firewalls, all while sending out alerts to administrators through ElastAlert.\r\n\r\nWhy Attend?\r\n\r\nThis session will equip you with the knowledge to build an automated, scalable, and proactive DDoS defense system using Python and powerful open-source tools. Whether you're a DevOps engineer, a security analyst, or a developer, you\u2019ll walk away with actionable insights to protect your applications from the growing threat of DDoS attacks.", "code": "EPAVRR", "state": "submitted", "created": "2024-12-22", "speaker_names": "Siddharth Vijay", "track": "PyCon: Security"}, {"title": "Conformal Prediction: uncertainty quantification to humanise models", "abstract": "Quantifying model uncertainties is critical to improve model reliability and make sound decisions. Conformal Prediction is a framework for uncertainty quantification that provides mathematical guarantees of true outcome coverage, allowing more informed decisions to be made by stakeholders", "full_description": "Quantifying uncertainties of Machine Learning models is crucial to improve their reliability, accurately assess risks and make more robust decisions. By quantifying and understanding uncertainty, we can build more reliable and trustworthy systems.\r\n\r\nImagine we have a model that predicts whether or not a CT scan contains a tumour: traditional approaches tend to provide binary predictions, while not providing information on the model\u2019s confidence in each prediction.\r\n\r\nConformal Prediction (CP) is a framework for uncertainty quantification that offers an estimate of the confidence in the model\u2019s predictions: instead of providing just a point estimate, it provides a set of possible outcomes (prediction set), together with a measure of confidence in each outcome. These prediction sets come with a (mathematical!) guarantee of coverage of the true outcome, ensuring that they will detect at least a pre-fixed percentage of true values. CP is a model-agnostic paradigm, requiring no retraining of the model and making no major assumptions about the distribution of the data.\r\n\r\nWe Humans, when faced with uncertainty, tend to express indecision and offer alternatives. We will see that CP can be a key tool to include a human in the decision-making loop, once the \u2018humanised\u2019 machine is able to express its uncertainty.\r\n\r\nCP therefore offers a robust framework that allows stakeholders to make more informed decisions, even more so in high-risk sectors such as healthcare, finance and autonomous systems.", "code": "FGEUJJ", "state": "submitted", "created": "2024-12-13", "speaker_names": "Vincenzo Ventriglia", "track": "PyData: Machine Learning & Deep Learning & Statistics"}, {"title": "Empowering Your ML Development Journey", "abstract": "Join us for an insightful exploration of our \"Machine Learning Development Pipeline (MLDevPipeline),\" an innovative MLOps solution designed to streamline the continuous development of machine learning models. Developed in conjunction with our generative behavior modeling project for realistic traffic agents, our MLDevPipeline plays a crucial role in simulating traffic scenarios, facilitating the safe deployment of autonomous vehicles. This presentation addresses the pressing need for efficient MLOps solutions to manage complex workflows, including data preprocessing, hyperparameter optimization, and model deployment. \r\n\r\nWe will share valuable insights and best practices derived from our project, showcasing the integration of Python packages and non-Python tools such as PyTorch, Hydra, and MLflow, alongside Docker for containerization and CI/CD orchestration via GitHub Actions. A key feature of our pipeline is the automatic retraining and evaluation triggered by merged pull requests, ensuring continuous model performance monitoring. Attendees will learn about our hyperparameter optimization strategies, automated model deployment, and API interfacing with customer applications. \r\n\r\nBy participating, attendees will gain practical insights into orchestrating successful ML pipelines, optimizing hyperparameters, deploying models efficiently, and enhancing collaboration in the team. Join us to discover how to elevate your ML workflows and tackle similar challenges in your projects.", "full_description": "Join us for an in-depth exploration of our \"Machine Learning Development Pipeline (MLDevPipeline),\" a cutting-edge MLOps solution. We will unveil the seamless integration of Python and advanced tools, showcasing how our MLDevPipeline enables a continuous machine learning development process. Our MLDevPipeline was developed alongside our model development which is focusing on generative behavior modeling for realistic traffic agents. These agents play an integral part in the simulation of traffic scenarios to enable continuous development and safe deployment of autonomous vehicles. Beyond that project the MLDevPipeline has proven its effectiveness as an inner-source project. \r\n\r\nThe talk addresses the critical need for a streamlined and efficient MLOps solution, tackling the challenges of managing complex machine learning workflows, including data preprocessing, hyperparameter optimization, and model deployment. We will also share insights gained from our project, highlighting the innovative approaches and best practices that have emerged during the development of our MLDevPipeline. By presenting our solution, we aim to provide the audience with practical insights and strategies to enhance their own ML workflows and address similar challenges in their projects. \r\n\r\nOur MLDevPipeline leverages a suite of Python packages and non-Python tools, including PyTorch, PyTorch Lightning for streamlined data preprocessing, model training and evaluation, Hydra for configuration management and hyperparameter optimization, MLflow with a self-developed buffered logger for seamless and comprehensive model analysis, and Docker for efficient model deployment. Additionally, we utilize Docker for a containerized pipeline in GitHub Actions and on a high-performance training cluster, enhancing the efficiency and scalability of our ML workflows. We use GitHub Actions as a powerful CI/CD orchestrator and make use of the latest action features for AI-supported code reviews and pull request monitoring. \r\n\r\nOne example and key feature is the automatic retraining and evaluation on every merged pull request to the main branch, essential for continuously monitoring model performance. This feature includes remote job submission on a high-performance training cluster, automated issue-opening for failed retrainings, a fully containerized pipeline using Apptainer and Docker, and ML lifecycle management based on MLflow. Another insightful example is our hyperparameter optimization based on Hydra as well as our automated model deployment and API-based interfacing with the customer application.  \r\n\r\nAttendees will gain practical insights and best practices for orchestrating a successful ML pipeline, optimizing hyperparameters, and deploying models efficiently. They will also learn how to maintain software quality and automate documentation generation, enhancing collaboration and understanding within their development teams.", "code": "MPSQS8", "state": "submitted", "created": "2024-12-18", "speaker_names": "Johannes Goth", "track": "PyCon: MLOps & DevOps"}, {"title": "VisiData: Exploring tabular data in the terminal", "abstract": "VisiData is a Pythonic tool for tabular data. It combines the clarity of a spreadsheet, the efficiency of the terminal, and the power of Python, into a lightweight utility which can handle millions of rows with ease.\r\n\r\nIn this talk we'll learn to use VisiData for exploratory data analysis.\r\n\r\nWhy VisiData?\r\n\r\n* It works with lots of data formats.\r\n* It's fast.\r\n* It\u2019s keyboard-driven.\r\n* It lives in your terminal.\r\n\r\n\"I wonder if @visidata can.... Yes, yes it can. I love this tool. \" -Luke Plant\r\n\r\n\"A remarkably powerful and well-designed tool.  -Yoz Grahame\r\n\r\n\"The tool I reach for first when I encounter a new dataset.\" -Jeremy Singer-Vine", "full_description": "We'll learn how to accomplish the following goals:\r\n\r\n* Get simple metrics from our data.\r\n* Open different kinds of tabular data, including CSV, JSON, Excel files and databases.\r\n* Plot our data right in the terminal.\r\n* Create histograms and aggregate metrics over them.\r\n* Add expression columns and sort and filter over them.\r\n* Automate repetitive data analysis tasks efficiently.\r\n* Write our own custom VisiData scripts.", "code": "FWSXNB", "state": "submitted", "created": "2024-12-04", "speaker_names": "Ram Rachum", "track": "PyData: Visualisation & Jupyter"}, {"title": "Building a NoGIL Load Balancer in 30 minutes", "abstract": "Load balancers are widespread nowadays because most software systems are web centric and service oriented. However, they\u2019re extremely complex to build, because they must be very performant and use sophisticated algorithms to route traffic. Until recently, you couldn\u2019t use Python to build one, because even though it would make the code simpler, the GIL prevented multiple threads from executing Python at the same time.\r\nNow that you can, what are the practical implications in your day to day work?\r\nIn this talk, we\u2019re going to live code a load balancer in Python. This will help us understand the pros and cons of using modules such as asyncio, threading and concurrent.futures, and what changes when we remove the GIL from the way.\r\nWhether you\u2019re building systems that demand concurrency (such as AI models or DevOps pipelines), or you\u2019re just curious about how tools like gUnicorn or Starlette work under the hood, you\u2019ll come away with practical insights on how to start off the NoGIL era on the right foot.", "full_description": "You've already heard that Python is making the GIL optional, but still aren't sure why that matters to you? Do you think that live coding talks are thrilled to watch? Whether you're building systems that demand concurrency (such as AI models or DevOps pipelines), or you just want to see a risky act of live coding with threads, async and a pre-release version of Python, Building a NoGIL Load Balancer in 30 minutes is for you.\r\n\r\nLoad balancers are widespread nowadays because most software systems are web centric and service oriented. However, they\u2019re extremely complex to build, because they must be very performant and use sophisticated algorithms to route traffic. Until recently, you couldn\u2019t use Python to build one, because even though it would make the code simpler, the GIL prevented multiple threads from executing Python at the same time.\r\nNow that you can, what are the practical implications in your day to day work?\r\nIn this talk, we\u2019re going to live code a load balancer in Python. This will help us understand the pros and cons of using modules such as asyncio, threading and concurrent.futures, and what changes when we remove the GIL from the way.\r\nWhether you\u2019re building systems that demand concurrency (such as AI models or DevOps pipelines), or you\u2019re just curious about how tools like gUnicorn or Starlette work under the hood, you\u2019ll come away with practical insights on how to start off the NoGIL era on the right foot.\r\n\r\n1. Introduction: the GIL, PEP 703, and why a load balancer is an ideal case study for NoGIL (5 min)\r\n2. Load balancer 101 - Single thread, single client (3 min)\r\n3. Introducing Threads, and load testing (5 min)\r\n4. Let\u2019s do asyncIO this time (5 min)\r\n5. Stress testing (4 min)\r\n7. Let's NoGIL and see what happens (6 min)\r\n8. Takeaways (2 min)", "code": "GVFCG8", "state": "submitted", "created": "2024-12-17", "speaker_names": "Alvaro Duran", "track": "PyCon: Python Language & Ecosystem"}, {"title": "Hands-On LLM Security: Attacks and Countermeasures You Need to Know!", "abstract": "Dive into the vulnerabilities of LLMs and learn how to prevent them\r\nFrom prompt injection to data poisoning, we\u2019ll demonstrate real-world attack scenarios and reveal essential countermeasures to safeguard your applications.", "full_description": "The rapid increase in usage of large language models (LLMs) in the last years makes it necessary to address the specific security risks of LLMs.\r\nIn this presentation, we will examine typical vulnerabilities in LLMs from a practical perspective. Starting with a systematic overview, we will use a specific demo app to illustrate the various attack scenarios. Vulnerabilities like prompt injection, data poisoning and system prompt leakage will be explained and demonstrated as well as attacks on RAG and agent implementations.\r\nIn addition to a basic introduction and a presentation of specific vulnerabilities, the talk also presents suitable countermeasures and general best practices for the use of LLMs in productive applications.\r\n\r\nWhat to expect? \r\nAttending this talk, you learn which vulnerabilities need to be considered when using and integrating LLMs. You will see how specific attacks work and what risks are associated with them. You will also learn which countermeasures are suitable and how these can be implemented technically.", "code": "3DSU8V", "state": "accepted", "created": "2024-12-20", "speaker_names": "Clemens H\u00fcbner, Florian Teutsch", "track": "PyCon: Security"}, {"title": "Architecture as Code (AaC) with Python", "abstract": "**Architecture as Code (AaC)** was born for prototyping a new system architecture design without any design tools. Available tools currently support on-premise and main major providers including AWS, Azure, and GCP cloud platforms.", "full_description": "Have you already learned the benefits of the Infrastructure as Code (IaC) process? Not bad, now it\u2019s time for the **Architecture as Code (AaC)** process. No worries, Python code only, no more JSON or YAML. AaC was born for prototyping a new system architecture design without any design tools. Available tools currently support on-premise and main major providers including AWS, Azure, and GCP cloud platforms. In addition, AaC allows you to track the architecture diagram changes in any version control system.", "code": "9WLRFY", "state": "submitted", "created": "2024-12-15", "speaker_names": "Ruslan Korniichuk", "track": "PyCon: Python Language & Ecosystem"}, {"title": "10x or 0.1x: A Junior's Hilarious Journey with GenAI Pair Programming", "abstract": "The reality of being a junior developer in 2025 means navigating the promise and pitfalls of AI-assisted development. Since the advent of ChatGPT, junior developers have been navigating a new landscape of AI-assisted programming. I've discovered the fine line between being a 0.1x and a 10x engineer. Here's my honest journey as a machine learning engineer, skeptically about LLM's. I discovered that becoming a \"10x engineer\" with GenAI isn't quite as advertised. This talk shares the reality of AI-assisted development, where every day brings either brilliant solutions or spectacular failures.", "full_description": "Since ChatGPT's release, I've mapped the fine line between becoming a more efficient developer and falling into the trap of AI dependency. This talk shares practical insights, workflows, and honest failures that helped me develop a balanced approach to AI-assisted development - from spectacular productivity wins to humbling debugging sessions of AI-generated \"optimizations.\"  \r\n\r\nThis talk explores the practical reality of using GenAI tools in daily development workflows. We'll examine how initial expectations of instant expertise often clash with the actual learning curve, and how to transform this challenge into an opportunity for growth. Through real examples and live demonstrations, we'll explore:\r\n\r\n* The evolution from basic code generation to strategic AI collaboration\r\n* Establishing effective workflows that enhance rather than hinder productivity\r\n* Common pitfalls and how to avoid them\r\n* Key strategies for maintaining code quality while leveraging AI capabilities\r\n\r\nThe presentation focuses on practical, implementable approaches rather than theoretical possibilities. Attendees will learn how to integrate AI tools effectively into their development process while avoiding the trap of over-reliance. Whether you're skeptical about AI or eager to embrace it, this talk provides a realistic framework for using GenAI to enhance your development skills without compromising fundamental understanding.", "code": "LG79M3", "state": "submitted", "created": "2024-12-22", "speaker_names": "Thomas Berger", "track": "PyData: Generative AI"}, {"title": "Build AI Systems with an LLM AI Pipeline Builder", "abstract": "In this workshop, we will build an AI system using an LLM agent to build the AI pipelines that make up the AI system. We start by defining the data source(s) for our AI system and build a feature pipeline using our LLM that will extract features (and labels) from the data sources. We then build a training pipeline from our features, outputing a trained model. Finally, we will build an inference pipeline and user interface (in Python) using our trained model and input features. The AI pipelines are connected by shared state in the form of a free serverless feature store & model registry (Hopsworks). You will learn how to use LLMs to improve AI pipeline developer velocity and accelerating the time to your first working version of your AI system.", "full_description": "In this workshop, we will build an AI system using an LLM agent to build the AI pipelines that make up the AI system. We start by defining the data source(s) for our AI system and build a feature pipeline using our LLM that will extract features (and labels) from the data sources. We then build a training pipeline from our features, outputing a trained model. Finally, we will build an inference pipeline and user interface (in Python) using our trained model and input features. The AI pipelines are connected by shared state in the form of a free serverless feature store & model registry (Hopsworks). You will learn how to use LLMs to improve AI pipeline developer velocity and accelerating the time to your first working version of your AI system.", "code": "JGJ9Y9", "state": "submitted", "created": "2024-12-09", "speaker_names": "Jim Dowling", "track": "PyData: Generative AI"}, {"title": "Harnessing LLMs to Craft a Curated Python Code Dataset for Robust Clone Detection", "abstract": "In the dynamic world of software development, Python's rising popularity brings with it the challenge of code duplication, commonly known as code clones. This work addresses the urgent need for a robust dataset specifically crafted for Python code clone detection. Our method involves extracting Python files from GitHub and GitLab repositories under the categories of data science, web development, and system administration. We prioritize repositories with recent development, active contributions, and high star and fork counts to ensure a diverse and high-quality dataset.\r\n\r\nOnce categorized, we measure the cyclomatic complexity of all the code to understand their intricacies. We also assess the granularity level of each script. Large language models (LLMs) are then employed to generate type-1, 2, 3, and 4 code clones. These clones are validated to ensure they reflect real-world coding practices accurately.\r\n\r\nOnce the dataset is developed according to the above procedure, we attach metadata containing information on the code category, its complexity level, granularity level, and type of clone. This comprehensive dataset aims to enhance the performance of code clone detection tools and provides developers and researchers with valuable resources to tackle Python code duplication. Ultimately, our work strives to improve software quality and maintainability, fostering a deeper understanding of Python's coding landscape.", "full_description": "In the ever-evolving landscape of software development, Python stands out for its simplicity and versatility. However, its popularity brings the challenge of code duplication, known as code clones. This tutorial addresses the urgent need for a specialized dataset designed for Python code clone detection, essential for maintaining code quality and software maintainability.\r\n\r\nWe begin with a detailed methodology for extracting Python files from popular platforms like GitHub and GitLab. We focus on actively maintained repositories with recent activity, high star ratings, and significant fork counts to ensure a diverse and current dataset. This dataset reflects various domains, including data science, web development, and system administration.\r\n\r\nNext, we use cyclomatic complexity to assess the structural intricacies of the scripts, providing crucial insights for clone detection. We also evaluate the granularity level of each script to determine the appropriate detection level, whether at the function, statement, or other relevant levels.\r\n\r\nTo further enrich the dataset, we employ large language models (LLMs) to generate type-1, type-2, type-3, and type-4 code clones. These synthetic clones are meticulously validated to mirror real-world coding practices, capturing both semantic and syntactic variations. Each Python script is accompanied by metadata detailing its category, complexity level, granularity level, and clone type, providing valuable context for training and evaluating clone detection tools.\r\n\r\nThe primary objective of this tutorial is to demonstrate effective techniques for enhancing code clone detection tools. By providing developers and researchers with a curated dataset that showcases a variety of coding styles and complexities, we aim to improve their ability to identify and address Python code duplication. This tutorial is designed to advance software engineering practices through practical, hands-on learning.\r\n\r\nIn summary, we introduce an innovative approach to building a robust dataset for Python code clone detection, highlighting the importance of diversity, complexity measurement, and leveraging LLMs. By filling the gaps in existing datasets and methodologies, this tutorial aspires to push the boundaries of code clone detection and elevate the standards of software development.", "code": "VTHJFB", "state": "submitted", "created": "2024-12-04", "speaker_names": "Yenatfanta Shifferaw", "track": "PyData: Data Handling & Engineering"}, {"title": "Learn Instructor for structured, robust and easy testable LLM interactions", "abstract": "How do you get structured, robust, easily testable outputs from an LLM? By using an (overly) complex prompt? Here is the better alternative: Instructor, a library for generating structured and robust outputs from your LLM.\r\n\r\nIn this workshop, you will explore the capabilities and advantages of the Instructor library based on a real-world example using the OpenAI API. This workshop aims to provide you with an overview of Instructor's useful tools to receive reliable, structured, robust, and easily testable LLM outputs. Through hands-on exercises, you will learn three tools of Instructor:\r\n\r\n1. Function calling: Learn how OpenAI's function calling is simplified with instructor and how it helps creating a robust, structured LLM output. \r\n\r\n2. Validators: Learn to use validators for evaluating the output of an LLM and the advantage of automatically reprompting your model to provide the correct output.\r\n\r\n3. Hooks: Explore Instructor's pre- and post-execution-hooks to evaluate the LLM's output.\r\n\r\nAfter this workshop, you will have an overview of the possibilities provided by Instructor to generate structured, robust, and easily testable outputs from your LLM.", "full_description": "Many LLM use cases in the \"real world\" require structured outputs like JSON, for example, extracting information from multiple sources. This is especially important for continuously testing and evaluating your LLM's output, before and during production. Unfortunately, the default output of LLMs is typically unstructured.\r\n\r\nTo receive structured responses, the library 'Instructor' offers tools to ensure structured outputs from your LLM and to evaluate them. In this workshop, you will learn about the Instructor library based on Pydantic and the OpenAI API for obtaining structured LLM outputs that are easily testable and improve the debugging handling of your LLM application. You will explore the advantages and capabilities of Instructor in a real scenario: using an OpenAI GPT model to extract and summarize data from a ticket system. Based on this use case, you will learn three main tools of Instructor through hands-on exercises:\r\n\r\n1. Function calling: OpenAI's function calling allows LLMs to use user-defined functions to solve a task and thereby create a specific, structured output. In Instructor, OpenAI's function calling is combined with Pydantic. Instructor makes it easy to use function calling. Explore instructors features to simplify function calling, and the related advantages in creating robust, structured LLM outputs. \r\n\r\n2. Validators: Validators are provided by Instructor to evaluate the output of an LLM and reprompt it if the evaluation fails. You will learn how to create validators and examine specific use cases related to the ticket system. Furthermore, you will explore different types of validators and how they support you in creating a testable LLM system.\r\n\r\n3. Hooks: Instructor provides pre- and post-execution hooks that are triggered at specific events. You will dive into the concept of pre- and post-execution hooks and explore their capabilities for evaluating the LLM's output. You will see how to implement these hooks for transparency of the LLM's output, ensuring easy logging and debugging.\r\n\r\nAfter this workshop, you will have an overview of Instructor's tools, how they can help you let your LLM generate structured outputs, and how you can use Instructor for testing and evaluating your LLM's output.", "code": "DPJUEP", "state": "submitted", "created": "2024-12-22", "speaker_names": "Felizia Quetscher", "track": "PyData: Generative AI"}, {"title": "Getting Started with Copier", "abstract": "This talk introduces Copier, an open-source project templating tool that automates Python projects setup. I will cover Copier\u2019s core features, demonstrate how to create and apply a template during live coding, and show real-world use cases. Whether for personal projects or team workflows, Copier simplifies project initialization and boosts efficiency. Join me to explore how this tool can improve your Python projects development process!", "full_description": "This talk introduces Copier, an open-source project templating tool that automates Python projects setup. I will cover Copier\u2019s core features, demonstrate how to create and apply a template during live coding, and show real-world use cases. Whether for personal projects or team workflows, Copier simplifies project initialization and boosts efficiency. Join me to explore how this tool can improve your Python projects development process!", "code": "YGLRLY", "state": "submitted", "created": "2024-12-07", "speaker_names": "Kamil Kulig", "track": "PyCon: Programming & Software Engineering"}, {"title": "Building Reliable AI Agents for Publishing: A DSPy-Based Quality Assurance Framework", "abstract": "As publishers increasingly adopt AI agents for content generation and analysis, ensuring output quality and reliability becomes critical. This talk introduces a novel quality assurance framework built with DSPy that addresses the unique challenges of evaluating AI agents in publishing workflows. Using real-world examples from newsroom implementations, I will demonstrate how to design and implement systematic testing pipelines that verify factual accuracy, content consistency, and compliance with editorial standards. Attendees will learn practical techniques for building reliable agent evaluation systems that go beyond simple metrics to ensure AI-generated content meets professional publishing standards.", "full_description": "This presentation addresses one of the most pressing challenges in professional publishing today: ensuring quality and reliability when deploying AI agents in editorial environments. We'll take a deep dive into how DSPy's programmatic approach to language model development can be leveraged to create robust testing and validation pipelines that meet the demanding standards of modern newsrooms.\r\nThe discussion begins by exploring the current landscape of AI evaluation in publishing workflows, examining why traditional testing approaches fall short when dealing with language models, and identifying the specific quality requirements unique to journalistic and editorial content. We'll then move into a detailed technical exploration of solutions built with DSPy, demonstrating how to design modular evaluation pipelines, implement publishing-specific metrics, and create automated systems for fact-checking and consistency validation. Special attention will be given to the integration of knowledge graphs for reference-based evaluation and the incorporation of these systems into broader MLOps workflows.\r\nTo ground these concepts in reality, we'll examine a detailed case study of implementing this framework in an actual newsroom environment. This will include practical discussions of handling various content types, along with strategies for managing test data and evaluation criteria. We'll share real-world performance monitoring approaches and concrete improvement strategies that have proven successful in production environments.\r\nThe presentation concludes with hard-won insights and best practices, including practical strategies for finding the right balance between automated testing and human review, effective approaches to handling edge cases, and methods for scaling quality assurance processes across diverse content teams. Throughout the talk, we'll share code examples and practical implementations that attendees can adapt for their own projects.\r\nThis session is specifically designed for technical leads and machine learning engineers, though the principles and approaches discussed will be valuable for anyone involved in AI quality assurance. Attendees will leave with a comprehensive understanding of how to design and implement QA processes for AI agents, practical knowledge of DSPy implementation for automated testing, and concrete strategies for maintaining high quality standards in AI-assisted workflows.", "code": "F7RDPT", "state": "submitted", "created": "2025-01-04", "speaker_names": "Simonas \u010cerniauskas", "track": "PyData: Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "AutoML with Django and Celery", "abstract": "This talk explores the development of a cloud-based AutoML application using Django as the backend framework and Celery for managing asynchronous tasks such as preprocessing, training, and deployment. The advantages of this solution will be discussed in terms of scalability, modularity, and task efficiency.", "full_description": "The outline for the talk is the following\r\n1. Introduction to ML and AutoML\r\n2. Motivation of choosing Django and Celery\r\n3. Core Architecture \r\n    i. Overview of the system design.\r\n   ii. Workflow examples: Preprocessing, model training, and deployment\r\n4. Challenges and Solutions\r\n   i. Handling large datasets\r\n   ii. Limitation of python ML libraris like pandas and scikit-learn\r\n  iii. Scalability\r\n5. Lessons Learned", "code": "PX3KQA", "state": "submitted", "created": "2024-12-22", "speaker_names": "Michael Loukeris", "track": "PyCon: MLOps & DevOps"}, {"title": "Modern Terminal UIs", "abstract": "Remember when command-line interfaces were just plain text and simple ASCII art? It\u2019s time to level up those terminal skills with Textual, the modern Python TUI framework.\r\n\r\nIn this tutorial, we\u2019ll build a sleek, interactive CLI-based schedule app for PyCon 2025. Whether you\u2019re an experienced engineer or new to coding, you\u2019ll learn how to create stunning text-based apps.", "full_description": "Remember when command-line interfaces were just plain text and simple ASCII art? It\u2019s time to level up those terminal skills with Textual, the modern Python TUI framework.\r\n\r\nIn this tutorial, we\u2019ll build a sleek, interactive CLI-based schedule app for PyCon 2025. Whether you\u2019re an experienced engineer or new to coding, you\u2019ll learn how to create stunning text-based apps.\r\n\r\nWe\u2019ll start with the basics: adding color to your terminal using escape sequences. Next, I\u2019ll introduce Textual and show you how to move from static, rich terminal output to fully interactive applications. Together, we\u2019ll then explore Textual\u2019s capabilities, including widgets, styling, and events. Finally, we will apply everything we learned and put together a scheduling app for the PyCon 2025.\r\n\r\nLet\u2019s bring some magic to the terminal!", "code": "YYDJ98", "state": "submitted", "created": "2024-12-16", "speaker_names": "Malte Klemm", "track": "PyCon: Python Language & Ecosystem"}, {"title": "The Visual Grammar of AI: Building Multi-modal Systems with Python", "abstract": "This tutorial introduces practitioners to the convergence of text and image understanding through modern embedding techniques. Participants will gain hands-on experience implementing multi-modal solutions using Python, focusing on CLIP embeddings and diffusion models. Through interactive examples and visualization tools, attendees will understand why these technologies have converged and how to leverage them effectively.", "full_description": "In this hands-on tutorial, we'll explore how modern AI bridges language and images using CLIP embeddings. You'll discover why these two domains have finally converged. Through interactive visualizations and real code examples, you'll learn to harness the power of unified embeddings for practical applications. Perfect for Python developers curious about the latest advances in AI vision-language understanding. Bring your laptop and sense of adventure - let's make text and images speak the same language! \ud83d\udc0d\ud83d\udc41\ufe0f\ud83d\udcdd", "code": "Z8BALM", "state": "submitted", "created": "2024-12-02", "speaker_names": "Hila Weisman Zohar", "track": "PyData: Generative AI"}, {"title": "Building multi-agent AI applications made easy with LangFlow", "abstract": "AI agents are powerful, but building them can be complex. LangFlow, a visual editor, makes it easy.\r\nIn this talk, we\u2019ll explain AI agents, introduce LangFlow, and demonstrate how to quickly create a multi-agent application live.", "full_description": "AI agents are transforming the way we create applications. However, developing multi-agent applications can often feel complex and time-consuming. LangFlow, the leading Python-based and open-source visual editor for low-code AI applications with over 40k stars on GitHub, simplifies this process by offering an intuitive, easy-to-use interface for building AI-driven solutions.\r\n\r\nIn this talk, we will start by introducing the concept of AI agents\u2014what they are, how they work, and why they\u2019re important for modern applications. Then, we will dive into LangFlow, showcasing how its visual editor allows you to quickly create powerful AI applications without needing to write extensive code. Finally, we\u2019ll bring everything together with a live demo, where we\u2019ll build a multi-agent application from scratch, step by step.\r\n\r\nBy the end of the session, you\u2019ll have a clear understanding of how LangFlow can help you build efficient, multi-agent AI systems with ease, and you'll be ready to start using it to create your own intelligent applications.", "code": "Q9NWP7", "state": "submitted", "created": "2025-01-05", "speaker_names": "Christophe Bornet", "track": "PyData: Generative AI"}, {"title": "Emscripten-Forge: In-Browser Scientific Computing with WebAssembly", "abstract": "This presentation introduces emscripten-forge, a novel package distribution and manager designed to bring the power of the open source scientific computing stack to WebAssembly. It opens new possibilities for web-based scientific applications with an incredibly seamless and efficient user experience.", "full_description": "This presentation introduces emscripten-forge, a novel package distribution and manager designed to bring the power of the open source scientific computing stack to WebAssembly. It opens new possibilities for web-based scientific applications with an incredibly seamless and efficient user experience.\r\n\r\nEmscripten-forge is built around Emscripten to cross-compile C/C++ code to WebAssembly. This allows developers to package a wide range of scientific computing tools for the Web. Similar to classic package managers, emscripten-forge provides in-browser compute environments composed of these readily usable scientific computing packages. At the moment, it supports Python, R, and C/C++ packages, as well as a C++ interpreter powered by LLVM. Access to these environments is facilitated by JupyterLite, which serves as a user interface to an interactive, user-friendly and highly configurable platform for scientific computing running entirely in the browser.\r\n\r\nWith Emscripten-forge, in-browser scientific computing becomes available for everyone, in other words, sharing of complex computations and even large applications can be done with a simple link. This drastically simplifies existing workflows and opens up a whole range of new possibilities for collaboration.", "code": "ZAECVD", "state": "submitted", "created": "2024-12-22", "speaker_names": "Thorsten Beier, Isabel Paredes", "track": "General: Education, Career & Life"}, {"title": "Quantized ColPali - Text Search on Images", "abstract": "ColPali is revolutionary\u2014here\u2019s why: it combines document retrieval with a vision-based large language model, allowing you to search directly within images without needing to extract text. However, running the full model on personal hardware can be challenging due to its computational demands. And thus we\u2019ve released a quantized version of ColPali.", "full_description": "ColPali is a late interaction model, that is the context remain intact. And it's finetuned on vision LLM, Pali Gemma to be able to perform text search on images. But what we did was to be able to bring it more towards consumer by quantizing the model, so you can perform search locally on your laptop.\r\n\r\nThe talk will cover:\r\nWhat is ColPali?\r\nWhat is Late-Interaction?\r\nWhat is Quantization of model, and how to perform it?\r\nHow you can deploy it on locally?", "code": "XYSX9C", "state": "submitted", "created": "2024-12-22", "speaker_names": "Sonam Pankaj", "track": "PyData: Computer Vision (incl. Generative AI CV)"}]}