{"talks": [{"title": "A mind of metal, born of human thought.  A consciousness confined, a digital soul.", "abstract": "AI DevOps combines artificial intelligence and DevOps practices to revolutionize software development and delivery. By automating tasks, predicting failures, and optimizing resource allocation, AI DevOps significantly improves efficiency, reduces errors, and accelerates time-to-market. This talk will explore the key concepts, benefits, and challenges of AI DevOps, showcasing real-world use cases and best practices for successful implementation.", "full_description": "AI DevOps: Automating the Future of Software Development\r\n\r\nAI DevOps is a transformative approach that leverages AI to streamline and optimize the entire software development lifecycle. This talk will explore the key concepts and benefits of AI DevOps, focusing on three main areas:\r\n\r\n1. AI-Driven Automation (10 minutes):\r\n\r\n**Automated testing and debugging using machine learning\r\n**Intelligent code completion and suggestion tools\r\n**Predictive analytics for proactive maintenance\r\n\r\n2. Continuous Integration and Continuous Delivery (CI/CD) with AI (10 minutes):\r\n\r\n**AI-powered pipeline optimization and self-healing\r\n**Automated deployment and rollback strategies\r\n**Real-time monitoring and anomaly detection\r\n\r\n3. AI-Enhanced Collaboration and Decision-Making (10 minutes):\r\n\r\n**Collaborative platforms with AI-powered insights\r\n**AI-assisted decision support tools for strategic planning\r\n**Ethical considerations and responsible AI in DevOps", "code": "8JF8FY", "state": "accepted", "created": "2024-11-19", "social_card_image": "/static/meida/social/8JF8FY.png", "speaker_names": "Christopher Schultz", "speakers": "\n### Christopher Schultz\n\nThis is a sample bio. This is a sample bio.This is a sample bio. This is a sample bio. This is a sample bio.\n", "track": "MLOps & DevOps"}, {"title": "Decoding Topics: A Comparative Analysis of Python\u2019s Leading Topic Modeling Libraries Using Climate C", "abstract": "Topic modelling has come a long way, evolving from traditional statistical methods to leveraging advanced embeddings and neural networks. Python\u2019s diverse library ecosystem includes tools like Latent Dirichlet Allocation (LDA) using gensim, Top2Vec, BERTopic, and Contextualized Topic Models (CTM). This talk evaluates these popular approaches using a dataset of UK climate change policies, considering use cases relevant to organisations like DEFRA (Department for Environment, Food & Rural Affairs). The analysis explores real-time integration, dynamic topic modelling over time, adding new documents, and retrieving similar ones. Attendees will learn the strengths, limitations, and practical applications of each library to make informed decisions for their projects.", "full_description": "Objectives:\r\nThe session aims to:\r\n\r\n1.  Compare Python-based topic modelling libraries, highlighting their relevance to real-world scenarios like policy analysis.\r\n2.    Explore practical use cases, including real-time document integration, tracking topic evolution, and finding similar documents.\r\n3.    Evaluate the tools based on performance, interpretability, scalability, and flexibility, with a focus on climate change policy data presented by [1] focusing on adaptation and mitigation.\r\n4.    Provide actionable guidance on selecting the right library for different project needs and datasets.\r\n\r\nOutline:\r\n\r\n1. Introduction to Topic Modeling: Overview of traditional and modern approaches, including their practical significance.\r\n\r\n2. Algorithms & Libraries Overview: LDA (gensim) [2], CTM [3], Top2Vec [4], BERTopic [5]\r\n\r\n3. Dataset and Use Cases:\r\n      - Overview of the UK climate change policy dataset.\r\n      - Use cases inspired by DEFRA and similar organisations, such as:\r\n            - Real-time integration for continuously adding new documents.\r\n            - Tracking topic development over time (dynamic topic modeling).\r\n            - Retrieving similar documents for faster insights.\r\n           (- Classification)\r\n\r\n4. Evaluation Criteria: Analysis of libraries based on:\r\n        - Ease of Use: How easy it is for no coding experts\r\n        - Quality: Coherence and diversity of extracted topics.\r\n        - Efficiency: Runtime performance and scalability.\r\n        - Flexibility: Features like contextual embeddings and integration capabilities.\r\n        - Interpretability: Ease of understanding topics and output.\r\n\r\n5. Results: Detailed findings, including specific advantages and limitations of each library in supporting the outlined use cases.\r\n\r\n6. Practical Recommendations: Guidance on choosing a library based on project goals, dataset characteristics, and organisational needs.\r\n\r\n7. Conclusion and Future Directions: Summary of key insights and the evolving role of embedding-based methods in topic modelling.\r\n\r\nOutcomes:\r\nBy attending this session, participants will:\r\n\r\n- Gain an in-depth understanding of Python\u2019s top topic modeling libraries.\r\n- Learn how to apply these tools to real-world challenges in policy analysis and other fields.\r\n- Understand how to handle use cases like real-time document integration and topic evolution over time.\r\n- Develop the skills to evaluate and choose the best tool for specific datasets and objectives.\r\n\r\nTarget Audience\r\n\r\nThis talk is for:\r\n- Data scientists and NLP practitioners seeking to apply topic modelling to unstructured text data.\r\n- Policy analysts and researchers working with large textual datasets, such as government or environmental policies.\r\n- Professionals in organisations like DEFRA, where tracking changes, adding new documents, or finding similar records are critical tasks.\r\n- Python enthusiasts interested in cutting-edge NLP techniques for extracting meaningful insights.\r\n\r\n[1] R. Biesbroek, S. Badloe, and I. Athanasiadis. Machine learning for research on cli-\r\nmate change adaptation policy integration: an exploratory uk case study. Regional\r\nEnvironmental Change, 20, 07 2020.\r\n\r\n[2] https://pypi.org/project/gensim/\r\n[3] https://github.com/MilaNLProc/contextualized-topic-models\r\n[4] https://github.com/ddangelov/Top2Vec\r\n[5] https://maartengr.github.io/BERTopic/index.html\r\n5 https://github.com/MilaNLProc/contextualized-topic-models", "code": "BJKSGK", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/meida/social/BJKSGK.png", "speaker_names": "Dr. Lisa Andreevna Chalaguine", "speakers": "\n### Dr. Lisa Andreevna Chalaguine\n\nLisa is an accomplished educator, researcher, and freelancer specializing in data science, natural language processing (NLP), and artificial intelligence. With a PhD in Intelligent Systems from UCL and a master's from Imperial College London, Lisa has extensive experience in academia and industry, having taught at UCL, and contributed to impactful projects like those with Cancer Research UK.\r\n\r\nA digital nomad at heart, Lisa teaches corporate clients and supervises university students worldwide, focusing on Python, machine learning, and NLP. Known for their engaging teaching style and passion for problem-solving, they are currently developing innovative courses and creating a YouTube channel featuring masterclasses on data analysis and machine learning.\r\n\r\nDriven by a love for teaching, research, and helping others succeed, Lisa is exploring opportunities to return to academia, with aspirations to lecture in Eastern Europe and Central Asia. Multilingual and versatile, they are shaping the future of data science education while continuing to inspire learners globally.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building Bare-Bones Game Physics in Rust with Python Integration", "abstract": "Learn how to build a minimalist game physics engine in Rust and make it accessible to Python developers using PyO3. This talk explores fundamental concepts like collision detection and motion dynamics while focusing on Python integration for scripting and testing. Ideal for developers interested in combining Rust\u2019s performance with Python\u2019s ease of use to create lightweight and efficient tools for games or simulations.", "full_description": "Python\u2019s simplicity makes it the go-to choice for scripting, while Rust excels in performance-critical tasks like game physics. This talk demonstrates how to build a minimalist physics engine in Rust, focusing on core concepts like collision detection, basic rigid body dynamics, and force application, while providing seamless Python integration using PyO3.\r\n\r\nWe\u2019ll explore how PyO3 allows developers to expose Rust functionality as native Python modules, enabling Python developers to easily script and interact with the physics engine. Through practical examples, attendees will see how Python can be used for rapid prototyping and gameplay scripting, while Rust handles the heavy lifting of physics calculations.\r\n\r\nBy the end of this session, participants will not only understand the basics of implementing physics in Rust but also how to use PyO3 to bridge the gap between Rust\u2019s performance and Python\u2019s flexibility. This talk is perfect for Python enthusiasts curious about Rust or Rustaceans looking to make their libraries accessible to the Python ecosystem.", "code": "VKYDBD", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/meida/social/VKYDBD.png", "speaker_names": "Sam Kaveh", "speakers": "\n### Sam Kaveh\n\nBorn in Iran, I have embraced diverse roles throughout my career, ranging from founding a startup and software development to consulting companies on cloud migrations and integrating machine learning technologies into their operations. My professional journey has been shaped by a passion for problem-solving and innovation across various domains.\r\n\r\nAcademically, I hold a Ph.D. in particle physics, specializing in Higgs boson precision measurements as part of the CMS experiment at CERN's Large Hadron Collider. This experience honed my analytical skills and gave me a deep appreciation for collaboration in high-stakes, cutting-edge environments.\r\n\r\nToday, I draw on my multidisciplinary background to create solutions at the intersection of software, data science, and high-performance computing, continually seeking to bridge theory and practice in impactful ways.\n", "track": "Programming & Software Engineering"}, {"title": "Conquering PDFs: document understanding beyond plain text", "abstract": "NLP and data science could be so easy if all of our data came as clean and plain text. But in practice, a lot of it is hidden away in PDFs, Word documents, scans and other formats that have been a nightmare to work with. In this talk, I'll present a new and modular approach for building robust document understanding systems, using state-of-the-art models and the awesome Python ecosystem. I'll show you how you can go from PDFs to structured data and even build fully custom information extraction pipelines for your specific use case.", "full_description": "For the practical examples, I'll be using spaCy, and the new Docling library and layout analysis models. I'll also cover Optical Character Recognition (OCR) for image-based text, how to convert tabular data to pandas DataFrames, and strategies for creating training and evaluation data for information extraction tasks like text classification and entity recognition using PDFs and other documents as inputs.", "code": "FUX3FR", "state": "confirmed", "created": "2024-11-28", "social_card_image": "/static/meida/social/FUX3FR.png", "speaker_names": "Ines Montani", "speakers": "\n### Ines Montani\n\nInes Montani is a developer specializing in tools for AI and NLP technology. She\u2019s the co-founder and CEO of Explosion and a core developer of spaCy, a popular open-source library for Natural Language Processing in Python, and Prodigy, a modern annotation tool for creating training data for machine learning models.\n", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Guardians of the Code: Safeguarding Machine Learning Models in a Climate Tech World", "abstract": "LLMs, Machine learning and AI are everywhere, yet their security is often overlooked, leaving your systems vulnerable to serious attacks. What happens when someone tampers with your model\u2019s input, poisons your training data, or steals your model?\r\n\r\nIn this talk, I\u2019ll explore these risks through the lens of the OWASP Machine Learning Security Top 10 using relatable, real-world examples from the climate tech world. I\u2019ll explain how these attacks happen, their impact, and why they matter to you as a Python developer, data scientist, or data engineer.\r\n\r\nYou\u2019ll learn practical ways to defend your models and pipelines, ensuring they\u2019re robust against adversarial forces. Bridging theory and practice, you'll leave equipped with insights and strategies to secure your machine learning systems, whether you\u2019re training models or deploying them in production. By the end, you\u2019ll have a solid understanding of the risks, a toolkit of best practices, and maybe even a new perspective on how important security is everywhere.", "full_description": "Machine learning is applied to a variety of challenges in climate tech, from optimising renewable energy to forecasting energy demands or predicting solar production. We rely more on these models, but we often forget a critical piece: their security. What happens if someone tampers with your model\u2019s inputs, poisons your training data, or sneaks malicious code into an open-source package you\u2019re using? These attacks can throw off predictions and disrupt energy systems or even the grid itself.\r\n\r\nIn this talk, I\u2019ll walk you through the OWASP Machine Learning Security Top 10, using real-world examples from climate tech to show how these attacks can happen. I'll show you cases like manipulating energy consumption forecasts, poisoning datasets, or sneaking malware into open-source libraries used for climate modelling. It\u2019s not just a hypothetical threat, these risks are real and the consequences can be serious.\r\n\r\nI\u2019ll also share practical solutions you can use as a Python developer, data scientist, or data engineer to protect your models and systems. I\u2019ll talk about securing your ML supply chain, validating data, and monitoring your pipelines for suspicious activity. You'll leave with strategies to defend your work so you can build systems that are not only smart but also safe and reliable.\r\n\r\nWhy does this matter? Because in climate tech, the stakes are incredibly high. The predictions we make and the systems we build influence the grid, energy policies, resource allocation, and consumers trust.\r\n\r\nDuring the talk, we'll cover:\r\n\r\n- How attacks on machine learning models can disrupt climate tech applications.\r\n- Examples of adversarial attacks, poisoned datasets, and supply chain vulnerabilities in renewable energy systems.\r\n- Practical steps to protect your machine learning pipelines.\r\n- Why security should be at the core of any ML project, especially in mission-critical fields like climate tech.\r\n\r\nOutline of the Talk:\r\n\r\n1. Why Security in Climate Tech Machine Learning Matters\r\n    - How machine learning is powering renewable energy and climate solutions.\r\n    - What can go wrong when systems are vulnerable.\r\n2. Breaking Down the OWASP ML Security Top 10\r\n    - Input manipulation: How attackers trick models with tampered data.\r\n    - Data poisoning: Real-life example of skewing optimization models with bad data.\r\n    - Supply chain attacks: How a hacked library could disrupt energy demand predictions.\r\n3. Real-World Impact of Attacks\r\n    - Manipulated energy consumption forecasts causing grid instability.\r\n    - Corrupted solar panel efficiency datasets leading to poor resource allocation.\r\n4. How to Protect Your Models\r\n    - How to spot tampered inputs.\r\n    - Data validation, cleaning and checking datasets.\r\n    - Best practices for safe use of open-source libraries.\r\n    - Monitoring and auditing: Setting up checks for unusual activity in your pipelines.\r\n\r\nKey Takeaways\r\n\r\n- Recap of risks and defences.\r\n- Practical steps you can take today to secure your ML systems.\r\n- A call to prioritize security as a core part of building trustworthy ML.\r\n\r\nClimate tech is one of the most exciting and meaningful areas to work in. The systems we\u2019re building have the potential to shape a more sustainable future. But if we don\u2019t make security a priority, we risk undermining the customer's trust. This talk will give you the tools and confidence to keep your machine learning models safe and ensure they\u2019re as reliable and impactful as they need to be.\r\n#ThereIsNoPlanetB", "code": "CVMPVG", "state": "confirmed", "created": "2024-12-03", "social_card_image": "/static/meida/social/CVMPVG.png", "speaker_names": "Doreen Sacker", "speakers": "\n### Doreen Sacker\n\nI'm an MLOps Engineer from Berlin working at the start-up 1KOMMA5\u00b0, and I'm part of the women's tech podcast Unmute IT. I aim to empower underrepresented groups to have a say in shaping the algorithms that impact our world today. Also, I\u2019m always on the lookout for the best coffee shop in town \u2615\ufe0f\n", "track": "MLOps & DevOps"}, {"title": "Guiding data minds: how mentoring transforms careers for both sides", "abstract": "Mentorship is a powerful way to shape careers while building meaningful connections in the data field. In this talk, I\u2019ll share my journey as a professional mentor, what the role entails, and the impact it has on both mentees and mentors. Learn how mentorship drives growth, fosters innovation, and creates value for the data community\u2014and why you should consider stepping into this rewarding role.", "full_description": "Mentorship is a rewarding journey that allows experienced professionals to guide and empower the next generation of talent. As a mentor in the data field, I have had the privilege of helping individuals navigate their careers, refine their skills, and unlock their potential. In this talk, I will share my personal journey into becoming a professional mentor, how I approach mentorship in a structured and impactful way, and the unique value a mentorship brings to both mentees and mentors.\r\n\r\nI\u2019ll provide insights into the day-to-day activities of mentoring, from offering career guidance to solving technical challenges, while also discussing the importance of tailoring advice to individual goals. Beyond technical skills, mentorship fosters confidence, networking, and long-term growth for mentees while offering mentors opportunities for personal development, deep satisfaction, and a broader industry perspective.\r\n\r\nWith the rapid evolution of the data industry, mentorship has never been more critical. This talk will highlight how professionals at any stage of their career can engage in mentorship to create a ripple effect of positive change in the data community\u2014and why taking the step to become a mentor, paid or otherwise, is an investment in the future of data science and yourself.", "code": "TYXMZC", "state": "confirmed", "created": "2024-12-08", "social_card_image": "/static/meida/social/TYXMZC.png", "speaker_names": "Anastasia Karavdina", "speakers": "\n### Anastasia Karavdina\n\nMy background is particle physics, where I was completely spoiled by access to large amounts of data and the freedom to try out every hot ML algorithm on it. The experiments I participated in were so-called large scale experiments (e.g Large Hadron Collider) and had from 500+ up to 2.5k other people working on them. So in addition to physics, I was exposed to the best software development practices that helped us to avoid a complete mess and destroy the Universe. \r\n\r\nAfterwards I was working as Data Scientist in various fields and recently became \"Solution Architect ML/AI and BI\" at big enterprise company. \r\n\r\nDuring my free time, I like learning new tools and techniques and implementing them in end-to-end AI/ML and IoT projects. My experience has also been very helpful in guiding data analysts, data scientists, and machine learning engineers as a mentor and contributing to the growth of the next generation of data scientist elite.\n", "track": "Community & Diversity"}, {"title": "Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond", "abstract": "\"Why Exceptions Are Just Sophisticated Gotos - and How to Move Beyond\" explores a common programming tool with a fresh perspective. While exceptions are a key feature in Python and other languages, they share surprising similarities with the notorious goto statement. This talk examines those parallels, the problems exceptions can create, and practical alternatives for better code. Attendees will gain a clear understanding of modern programming concepts and the evolution of programming.", "full_description": "Exceptions have long been seen as an improvement over error-handling approaches like goto. However, they can introduce complexity and obscure control flow when used without care. This talk will critically examine exceptions, outline the similarities to goto, and explore better ways to handle errors in programming.\r\n\r\n### Outline:\r\n\r\n1. Introduction (5 minutes)\r\n    - The historical role of goto in programming.\r\n    - Spaghetti code and the rise of structured programming.\r\n    - How exceptions emerged as an alternative.\r\n2. Why and What Are Exceptions (10 minutes)\r\n    - Why exceptions were introduced.\r\n    - How they became mainstream in languages like Java and C++.\r\n    - Common problems caused by exceptions: hidden control flow, debugging challenges, and performance impacts.\r\n3. The Evolution Toward Result Types (10 minutes)\r\n    - How result types address the shortcomings of exceptions.\r\n    - Implementations in Haskell, Rust, and Golang.\r\n    - Real-world benefits of using result types.\r\n4. Using Result Types in Python (10 minutes)\r\n    - Introducing the returns package.\r\n    - Practical examples of result types in Python.\r\n    - How this approach improves code clarity and reliability.\r\n5. Conclusion (5 minutes)\r\n    - Recap of the journey from goto to exceptions to result types.\r\n    - Key takeaways: thoughtful error handling and modern best practices.\r\n    - Encouragement to explore and adopt better patterns in Python.\r\n\r\nThis session is ideal for intermediate and advanced Python developers seeking actionable techniques to improve error handling and write cleaner, more predictable code.", "code": "S8MUBF", "state": "confirmed", "created": "2024-12-12", "social_card_image": "/static/meida/social/S8MUBF.png", "speaker_names": "Florian Wilhelm", "speakers": "\n### Florian Wilhelm\n\nFlorian is Head of Data Science & Mathematical Modeling at inovex GmbH, an IT project center driven by innovation and quality, focusing its services on \u2018Digital Transformation\u2019. He holds a PhD in mathematics, has more than 10 years of experience in predictive & prescriptive analytics use-cases and likes everything math \ud83e\udd2f\n", "track": "Programming & Software Engineering"}, {"title": "Reinventing Streamlit", "abstract": "Dreaming of creating sleek, interactive web apps with just Python? Streamlit is great for dashboards, but what if your needs go beyond that? Discover how Reflex.dev, a cutting-edge full-stack Python framework, lets you level up from dashboards to full-fledged web apps!", "full_description": "Have you ever wished you could build sleek, interactive web apps using just Python? Maybe you\u2019ve tried Streamlit and loved its simplicity. But maybe you also had the feeling that your dashboard is no longer a dashboard and your needs have outgrown Streamlit's data model.\r\n\r\nIn this talk, I\u2019ll introduce Reflex.dev, a powerful Python framework that makes web development effortless. Reflex combines the ease of Python with the flexibility of React, enabling you to create full-stack, interactive apps quickly.\r\n\r\nWe\u2019ll cover the basics: what Reflex.dev is and how it stacks up against familiar frameworks. Then, we\u2019ll dive into building a Streamlit-inspired app from scratch in Reflex.dev by creating an API compatibility wrapper. Along the way, I\u2019ll show you how Reflex can:\r\n\r\n- Help you build dynamic, shareable web apps with only Python.\r\n- Smoothly transition your Streamlit app into a stateful Reflex app.\r\n- Make the whole react ecosystem accessible.\r\n- Help testing your application.\r\n\r\nNo web development experience? No problem. This talk is for anyone who wants to create web apps without diving into JavaScript. We\u2019ll stick to Python and start from the ground up.\r\n\r\nBy the end, you\u2019ll leave with a working Streamlit clone and a powerful new tool in your Python arsenal. Let\u2019s make web development fun again!", "code": "7CXSPN", "state": "confirmed", "created": "2024-12-16", "social_card_image": "/static/meida/social/7CXSPN.png", "speaker_names": "Malte Klemm", "speakers": "\n### Malte Klemm\n\nMalte is a seasoned Data Engineer with over 10 years at Blue Yonder, where he recently worked on  company's forecasting service. Holding a PhD from the University of Hertfordshire's Adaptive Systems Research Group, he explored Information Theory in the context of Multi-Agent Systems. Beyond tech, Malte is passionate about great UX, typography, and baking the perfect loaf of bread.\n", "track": "Django & Web"}, {"title": "From Rules to Reality: Python's Role in Shaping Roundnet", "abstract": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community. However, like many emerging sports, it faces challenges in balancing competition, optimizing rules, and increasing accessibility for both players and spectators. This is where Python and data analysis come into play.\r\n\r\nIn this talk, I'll share insights from my role as Data Lead on the International Roundnet rule committee, where we use Python-powered data analysis to make informed decisions about the future of the sport. We'll explore how analyzing gameplay patterns and testing rule changes with simulation can lead to fairer, more exciting games and attract a broader audience.", "full_description": "Roundnet is a dynamic and fast-growing sport that combines quick reaction, athleticism, and strong community.  But what's truly unique about Roundnet is the opportunity it offers: as a new and emerging sport, we have the rare chance to shape its global rule changes entirely through data analysis. This is a groundbreaking approach \u2013 a first for any sport in the modern era.\r\n\r\nIn this talk, I\u2019ll share insights from my role as Data Lead on the International Roundnet Rule Committee and take you through how we are leveraging Python and data analysis to guide these changes. Over the past year, we\u2019ve collected rule proposals and have set up a series of experiments designed to test their effects. Using Python and statistical modeling, we\u2019re planning to select key tournaments worldwide this year to observe how rule adjustments impact gameplay data.\r\n\r\nOur ultimate goal is to discover if specific combinations of rule changes can make Roundnet fairer, more exciting, and accessible for players and spectators alike. This journey is an exploration of how data-driven decision-making can transform a sport from the ground up, using real-world insights and experimentation.\r\n\r\nThis talk will take you on the journey of how we set up our testing framework, what tools we\u2019re using, and how we\u2019ve employed Python-powered analysis to bring empirical evidence into the decision-making process. It will equip you with a new perspective on data's role in shaping real-world change \u2013 especially in grassroots movements, community building, and sports.", "code": "ZT3MGL", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/meida/social/ZT3MGL.png", "speaker_names": "Larissa Haas", "speakers": "\n### Larissa Haas\n\n\n", "track": "Data Handling & Engineering"}, {"title": "Why AI Projects Fail \u2013 Chronicles of Failure and How to Overcome Them", "abstract": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. Organizations often stumble over unrealistic expectations, siloed data, and cultural roadblocks. This talk dives into the human and organizational dynamics that cause AI initiatives to derail.\r\n\r\nUsing real-world examples, I\u2019ll showcase how missing tram stop location data was found on a hobbyist\u2019s blog when official systems failed. Or how critical historical data for resource planning sat locked in Excel files with forgotten passwords, forcing a desperate password recovery operation. These examples highlight the surprising ways poor data governance and lack of interdisciplinary collaboration sabotage even well-intentioned AI projects.\r\n\r\nBut all is not lost! We\u2019ll explore actionable strategies to navigate these challenges: shifting from shiny tools to R&D-driven approaches, fostering better communication between IT and business teams, and building strong foundations for clean, accessible data. Attendees will leave with insights into what AI success truly requires: cultural readiness, realistic expectations, and a commitment to collaboration.\r\n\r\nIf you\u2019re tired of AI hype and looking for practical solutions, this talk offers a clear roadmap to ensure AI projects deliver measurable impact.", "full_description": "Why do AI projects fail? Spoiler: It\u2019s rarely about the technology. The real barriers are organizational, cultural, and human. In this talk, we\u2019ll uncover the reasons AI initiatives stumble and explore how to overcome them\u2014through a mix of anecdotes, real-world failures, and actionable best practices.\r\n\r\nWe begin with a big lamenti\u2014an unfiltered dive into the messy, chaotic world of AI project implementation. From tangled data silos to misaligned expectations, we\u2019ll uncover hard truths through real-world examples that many will recognize:\r\n\t\u2022\tThe missing data problem: Sometimes it\u2019s the smallest datasets that hold the biggest answers.\r\n\t\u2022\tPasswords might be in retirement.\r\n\t\u2022\tDecisions in the dark: When untrained people must make calls on things they don\u2019t fully understand.\r\n\t\u2022\tThe shiny facade: Expectations, the pressure for quick wins, and the relentless demand for good news.\r\n\r\nTo go beyond personal experience, I\u2019ll also bring fresh insights from a survey of my expert network closer to the conference. This will offer a broader, more representative perspective on why AI projects stumble\u2014and how to fix them.\r\n\r\nTimed Outline (45 minutes):\r\n1. Introduction (5 min)\r\n - What qualifies me for this talk.\r\n2. The Big Lamenti \u2013 Real-World Failures and Anecdotes (20 min)\r\n - Anecdotes that illustrate the challenges in a candid yet engaging way.  \r\n - Learnings from the survey\r\n3.  Solutions and Best Practices (15 min)\r\n - Mindset: How AI is perceived and approached within organizations.\r\n - Data: The role of accessibility, governance, and quality as foundational pillars.\r\n - Collaboration: Bridging silos between teams, leadership, and departments.\r\n - Scale: The progression from small, focused initiatives to sustainable implementation.\r\n - Expectations: Aligning vision, goals, and outcomes for realistic project success.\r\n5.\tTakeaways & Closing Thoughts (5 min)\r\n - Summary of insights and practical steps to avoid common pitfalls\r\n- Key mindset shifts for sustainable AI success\r\n5.\tQ&A (5 min)", "code": "JA9NFW", "state": "confirmed", "created": "2024-12-17", "social_card_image": "/static/meida/social/JA9NFW.png", "speaker_names": "Alexander CS Hendorf", "speakers": "\n### Alexander CS Hendorf\n\nAlexander Hendorf has over 20 years of experience in digitalization, data, and artificial intelligence. As an independent consultant, he specializes in the practical implementation, adoption, and communication of data- and AI-driven strategies and decision-making processes.\r\n\r\nA recognized expert in data intelligence, Alexander has served as a speaker and chair at leading international conferences, including PyCon DE & PyData Berlin, Data2Day, and EuroPython. His dedication to the tech community has earned him the titles of Python Software Foundation Fellow and EuroPython Fellow.\r\n\r\nHe currently serves as a board member of the Python Software Verband (Germany). Since 2024, Alexander has been driving Pioneers Hub, a non-profit organization focused on fostering and supporting tech communities.\r\n\r\n\ud83c\udf1f AI Strategy & Open Source Excellence | \ud83e\udd1d Inspiring Communities, Empowering Innovators\n", "track": "Others"}, {"title": "The Mighty Dot - Customize Attribute Access with Descriptors", "abstract": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.", "full_description": "Whenever you use a dot in Python you access an attribute.\r\nWhile this seems a very simple operation,\r\nbehind the scenes many things can happen.\r\nThis tutorial looks into this mechanism that is regulated by descriptors.\r\nYou will learn how a descriptor works and what kind of problems it can help to\r\nsolve.\r\nPython properties are based on descriptors and solve one type of problems.\r\nDescriptors are more general, allow more use cases, and are more re-usable.\r\nDescriptors are an advanced topic.\r\nBut once mastered, they provide a powerful tool to hide potentially complex\r\nbehavior behind a simple dot.\r\n\r\nIn this tutorial you will:\r\n\r\n* Learn how to use Python's descriptors to add new functionality to attribute access\r\n* Acquired solid background knowledge on how descriptors work\r\n* Work with practical examples for applying descriptors\r\n* Learn when to use a property or reach for a descriptor\r\n* Get to know how popular Python libraries apply descriptors for tasks such as\r\n  data structure access, REST-APIs, ORMs, and serialization", "code": "WJPEQH", "state": "confirmed", "created": "2024-12-18", "social_card_image": "/static/meida/social/WJPEQH.png", "speaker_names": "Mike M\u00fcller", "speakers": "\n### Mike M\u00fcller\n\nI've been a Python user since 1999, teaching Python professionally since 2004.\r\nI am also active in the community, organizing Python conferences such as\r\nPyCon DE, EuroSciPy, and BarCamps.\r\nI am a PSF Fellow, PSF Community Service Award winner,\r\nand chair of the German Python Software Verband.\n", "track": "Python Language & Ecosystem"}, {"title": "Data as (Python) Code", "abstract": "In contemporary data-driven environments, the seamless integration of data into automated workflows is paramount. The reliability of automation, however, is constantly threatened by breaking changes in the source data. The Data-as-Code (DaC) paradigm address this challenge by treating data as a first-class citizen within the software development lifecycle.", "full_description": "Data-as-Code (DaC) is a paradigm that streamlines data distribution by encapsulating dataset retrieval within Python packages, along with a data contract. This approach makes it easy to enforce data quality, effortlessly leverage on semantic versioning to prevent errors in the data pipeline, and abstracts away from the Data Scientist all the boilerplate code to load the data needed by the ML models, improving efficiency and consistency. This presentation will delve into the implementation of DaC, demonstrate its practical applications, and discuss the benefits it offers in modern data workflows.\r\n\r\nThis session will cover:\r\n1. Introduction to Data-as-Code (DaC):\r\n   - What problems do we want to solve with DaC\r\n   - What it is out of scope\r\n2. Implementing DaC:\r\n   - Packaging data as Python packages\r\n   - Defining data contracts\r\n3. Advantages of DaC:\r\n   - Application of semantic versioning to manage data changes effectively\r\n   - Breaking changes in data are automatically detected as part of the data distribution\r\n   - Abstraction of data loading mechanisms, allowing seamless transitions between data sources\r\n   - Elimination of hard-coded data field names, enhancing code maintainability\r\n   - Facilitation of unit testing through schema examples\r\n   - Inclusion of comprehensive data descriptions and metadata\r\n   - Centralized data distribution via the Python Package Index (PyPI)\r\n4. DaC in the real world:\r\n   - Step-by-step walkthrough of creating and distributing a DaC package\r\n   - Guidelines for data engineers on preparing data for DaC\r\n   - Instructions for data scientists on consuming DaC packages in their workflows\r\n   - Discussion on the scalability and adaptability of DaC\r\n6. Q&A Session:\r\n   - Addressing audience questions and remarks", "code": "SXRVNU", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/meida/social/SXRVNU.png", "speaker_names": "Francesco Calcavecchia", "speakers": "\n### Francesco Calcavecchia\n\nPhysicist, ML Engineer, Agile adept. I\u2019d rather have a taste of everything than specialize. Eager to learn, unlearn, try out, share, help.\n", "track": "MLOps & DevOps"}, {"title": "Open Table Formats in the Wild: From Parquet to Delta Lake and Back", "abstract": "Open table formats have revolutionized analytical, columnar storage on cloud object stores with critical features like ACID compliance and enhanced metadata management, once exclusive to proprietary cloud data warehouses. Delta Lake, Iceberg, and Hudi have significantly advanced over traditional open file formats like Parquet and ORC.\r\n\r\nIn an effort to modernize our data architecture, we aimed to replace our Parquet-based bronze layer with Delta Lake, anticipating better query performance, reduced maintenance, native support for incremental processing, and more. While our initial pilot showed promise, we encountered unexpected pitfalls that ultimately brought us back to where we began.\r\n\r\nCurious? Join me as we shed light on the current state of table formats.", "full_description": "# Description\r\n\r\nOpen Table Formats (OTF) such as Hudi, Iceberg and Delta Lake have disruptively changed the data engineering landscape in recent years. While the Parquet file format has evolved as the de-facto standard for open, interoperable columnar storage for analyical workloads, it lacked first class support for critical features such as ACID compliance, incremental processing, flexible schema & partioning evolution and scalable meta data management. This led to increased development and maintenance efforts while building idempotent and failure tolerant data pipelines that often resulted in custom frameworks. OTFs solve all of these issues via providing a sophisticated meta data layer and improved maintenance capabilities on top of Parquet.\r\n\r\nDriven by the promises of OTFs, we intended to replace our own bronze-read-only Parquet-based storage layer with Delta Lake. In theory, this should have improved performance, reduced maintenanced and provided more flexibility. However, we've stumbled upon several issues:\r\n\r\n1. drastic performance issues with Liquid Clustering during incremental processing\r\n2. inmature interoperability in the python and cloud-based ecosystem (DuckDB, Pandas, Polars, Athena, Snowflake)\r\n3. maintaining logical session-boundaries during incremental processing\r\n\r\nWhile the first two issues are solvable in foreseeable future, the last one is specific to our requirements and does not overlap with design decisions made for incremental processing in Delta Lake. Taken together, these points ultimately led us to go back to relying on Parquet again.\r\n\r\n## Targeted Audience\r\n\r\nThis talk is mainly intended for an intermediate data engineering audience but is well suited for interested beginners, too. The content of this talk is relevant for all architects and data engineers being responsible for storing and managing data for analytical workloads.\r\n\r\n# Key takeaways\r\n\r\n- What problems do OTFs solve?\r\n- How do OTFs contribute to an open, composable data stack?\r\n- Is there a predominant Open Table Format?\r\n- How does Delta Lake conceptionally work?\r\n- What are concrete real-world advantages of Delta Lake in contrast to \"plain\" Parquet?\r\n- What is the \"small files\" problem and how does Liquid Clustering help?\r\n- How is the current state of interoperability with Delta Lake?\r\n\r\n# Talk Outline\r\n- Introduction (5 min)\r\n- OTFs in comparison (5 min)\r\n- Delta Lake Internals (10 min)\r\n- Use Case Requirements (5 min)\r\n- Benchmarks & Results (10 min)\r\n- Conclusion and Outlook (5 min)\r\n- Questions (5 min)", "code": "MRHNCV", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/meida/social/MRHNCV.png", "speaker_names": "Franz W\u00f6llert", "speakers": "\n### Franz W\u00f6llert\n\nHi my name is Franz and I\u2019m an open source and python enthuisiast:\r\n\r\n- father of 3 girls\r\n- major in psychology\r\n- chess hobbiyst\r\n- competitive ultimate frisbee player\r\n- likes cooking and baking sourdough bread\n", "track": "Data Handling & Engineering"}, {"title": "Streamlining Python deployment with Pixi:  A Perspective from production", "abstract": "In our quest to improve Python deployments, we explored Pixi, a tool designed to enhance dependency management within the Conda ecosystem. This talk recounts our experience integrating Pixi into a setup used in production. We leveraged Pixi to create lockfiles, ensuring consistent builds, and to automate deployments via CI/CD pipelines. This integration led to greater reliability and efficiency, minimizing deployment errors and allowing us to concentrate more on development. Join us as we share how Pixi transformed our deployment process and offer insights into optimizing your own workflows.", "full_description": "In modern software development, managing dependencies effectively is crucial for ensuring that applications run smoothly across various environments. This talk explores our journey to optimize Python deployments by integrating Pixi into our workflow. As a tool that enhances the Conda ecosystem, Pixi offers a reliable and efficient solution to the common challenges in dependency management. While concepts such as consistent builds, reproducibility, and automated deployments are well-established, Pixi simplifies their implementation within a Conda-based environment, making these practices more accessible and manageable.\r\n\r\nThe talk will cover\r\n- DevOps Concept\r\n  Introducing concepts like lockfile, reproducible environments and CI/CD pipeline to set out a good\r\n  baseline for deploying python code productively\r\n- Conda vs Pypi comparison\r\n  Considering the tradeoffs between isolation and development comfort\r\n- Pixi introduction\r\n  An introduction to the philosphy of pixi and how it compares to other conda tooling.\r\n  This also covers how Pixi streamlines the implementation of DevOps concepts\r\n- Implementing DevOps concepts using pixi\r\n  \r\nThis talk is designed for professional software developers who prioritize a robust setup for deploying Python code as services into production. While familiarity with the Conda ecosystem is beneficial, it is not a prerequisite for this session.", "code": "BLKYGU", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/meida/social/BLKYGU.png", "speaker_names": "Dennis Weyland", "speakers": "\n### Dennis Weyland\n\nHey there! I'm Dennis Weyland, and I've been part of the Blue Yonder team for the last five years. I kicked off my career as a Data Scientist but soon found my groove in Data Engineering. Python has been my go-to language for the past seven years, and I love diving into project setups to make everything run smoothly. Before diving into my professional career, I studied Physics at KIT, where I completed my master's thesis and discovered my passion for Python software development and machine learning.\r\n\r\nOutside of work I'm passionate about running, diving, and climbing.\n", "track": "MLOps & DevOps"}, {"title": "Challenges and Lessons Learned While Building a Real-Time Lakehouse using Apache Iceberg and Kafka", "abstract": "How do you build a large-scale data lakehouse architecture that makes data available for business analytics in real time, while being more cost-effective, more flexible and faster than the previous proprietary solution? With Python, Kafka and Iceberg, of course!\r\n\r\nWe built a large-scale data lakehouse based on Apache Iceberg for the Schwarz Group, Europe's largest retailer. The system collects business data from thousands of stores, warehouses and offices across Europe.\r\n\r\nIn this talk, we will present our architecture, the challenges we faced, and how Apache Iceberg is shaping up to be the data lakehouse format of the future.", "full_description": "The Schwarz Group is present in thirty-two countries around the world with over ten thousand stores, hundreds of warehouses, an assortment of over two thousand different products and a single ERP system to manage them all.\r\n\r\nEvery country maintains its own databases for operational purposes but all the data is also gathered in one central analytics platform for all countries. Not only does this platform need to be stable and reliable, the data also needs to be made available for the consumers in near real-time \u2013within mere minutes. The existing analytics platform was based on proprietary solutions which were expensive and required niche knowledge, severely limiting the number of available developers.\r\n\r\nTherefore, we set out on a journey to completely redesign the analytics platform; a new solution, based as much as possible on Open Source technologies like Python, Kafka and Iceberg. Leveraging Python for its great ecosystem and ease of use. Kafka for fast and reliable message processing of over one thousand tables per country into one central hub. And Iceberg at the core, as our data lakehouse format for its fully transparent schema evolution and high performance through its rich metadata layer.\r\nThrough our presentation we will showcase the different challenges we faced during the design of our new architecture, how our selected tech stack allowed us to tackle each of them and the lessons we learnt. We will focus on our challenges in four areas: scalability, performance, continuity of service and data quality.\r\n\r\nScalability: Ingesting changes on over one thousand tables coming from servers across thirty-two countries supporting operations of Europe\u2019s largest retailer is no easy task. Our architecture needs to support receiving tens of thousands of events per second. We will present how we set up Kafka to support our current load and potential for future growth, how we use Tabular\u2019s Iceberg sink connector to ingest all our tables, and how we leverage avro serialization and snappy compression of messages to reduce network traffic.\r\n\r\nPerformance: The large amounts of data we handle, paired with the influx of small files that can result from real-time data ingestion, made ensuring performance an extremely challenging aspect of our application. We will show how we designed our data lake house; using Iceberg's hidden partitioning to ensure performance while remaining flexible to evolve them over time and how we designed and implemented an effective maintenance job to reduce small files in our iceberg tables.\r\n\r\nContinuity of Service: The existing analytics platform contains the core of the business data which is used for many analytics and forecasting use cases across the organization. One of main requirements was to ensure a smooth transition to the new architecture with as little downtime as possible. This meant facilitating the access to existing users by allowing them to retrieve the data in the same way they were doing it in the past, with minimal changes. We will show how our architecture ensures flexibility by allowing access to the data from diverse query engines and show an example on how we integrated our architecture with Snowflake. \r\n\r\nData quality: We faced some challenges when it came to the consolidation of all the data we receive from all 32 countries. For some tables, the schemas diverged across countries; having different sets of columns, different data types being used and even different primary keys. We will talk about how we handled data quality issues coming from the operational databases by using iceberg\u2019s schema evolution capabilities, a schema registry and Kafka Connect single message transforms (SMTs).", "code": "JUAF3S", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/meida/social/JUAF3S.png", "speaker_names": "Jonas B\u00f6er, Elena Ouro Paz", "speakers": "\n### Jonas B\u00f6er\n\nSoftware Engineer since 2018, Data Engineer at inovex since 2022 \u2013\u00a0happy to get the job done, but prefers beautiful solutions.\n\n### Elena Ouro Paz\n\n\n", "track": "Data Handling & Engineering"}, {"title": "Graph Machine Learning in all its glory!", "abstract": "Our world is complex: one approach to understanding and learning more about relationships within the data is to represent it as a network or a graph - with entities as nodes and relations between them as edges. Network applications are abundant - Facebook, knowledge databases like Wikipedia, traffic routes, molecular pathways, and the fun starts when we start thinking of the physical world as graphs. Where there\u2019s data, there\u2019s uncertainty and the need to predict the future to make it a *little less* uncertain, which is why we need machine learning.\r\n\r\n**This talk will be about modeling one such network using a Graph Neural Network (GNN) and predicting on it using Deep Graph Library, an efficient and scalable open source framework to train and serve GNNs. While GNNs can be used for any ML task, we\u2019ll focus on building a recommendation model.**\r\n\r\nBut the transition from traditional data structures to graph-based models is not straightforward. Unlike tabular data, graphs require adapting ML principles to accommodate their topological and relational complexities. \r\n\r\n**This talk will help participants gain practical insights into modeling their data as graphs and leveraging GNNs to build a recommendation system - from scratch. We\u2019ll go over each ML model building step and discuss how to adapt our learnings from tabular data to graph data.**", "full_description": "This talk is to get the audience excited about graph ML and hopefully inspire them to think of their data as a potential graph which can open doors for this new paradigm of ML. Graphs are complex, but then again so is most of the data now.\r\n\r\nThe talk will be supported by code snippets and will be using an open source recommendation dataset. \r\n\r\n### Outline: \r\n\r\n**1. Intro to Graph Machine Learning** (6 minutes)\r\n- Brief overview of traditional data vs. graph-based data and graph components\r\n- Networks in real-life and relevance\r\n- ML on graphs: recent advancements\r\n\r\n**2. Graph Neural Networks (GNNs) and DGL** (6 minutes)\r\n- What are GNNs and how do they work? \r\n- Overview of the Deep Graph Library (DGL) \r\n- Architecture for building recommendation models using GNNs\r\n- GNN Model Architecture\r\n\r\n**3. Training a GNN model in DGL** (15 minutes)\r\n\r\nThis section is meant to address the following key questions on training and serving models using DGL:\r\n- How do we transform tabular data into meaningful nodes and edges? \r\n- How do we prepare data for graph modeling?\r\n- Can we split train / test data randomly as is convention? \r\n- How do we represent categorical/numerical features? \r\n- How do we train and make predictions on this model? \r\n- (optional): How do we decide on a model architecture?\r\n- How do we evaluate this model?\r\n- How do we re-train this model? \r\n- What are the data / scalability limitations when training GNNs?\r\n\r\nWe will also discuss certain common pitfalls and how to avoid them. \r\n\r\n**4. Q&A and Closing Remarks** (3 minutes)\r\n\r\n\r\nAfter the talk, the audience should have a good understanding about:\r\n- How to structure their data as graph components and prepare it for modeling\r\n- What graph machine learning is all about, what a GNN is and how do we train and serve GNNs in DGL?\r\n- End-to-end ML tasks on graph data: best practices", "code": "CHAEXA", "state": "confirmed", "created": "2024-12-19", "social_card_image": "/static/meida/social/CHAEXA.png", "speaker_names": "Shreya Khurana", "speakers": "\n### Shreya Khurana\n\nI'm a data scientist at Intuit in California and I have experience working with recommendation systems, anomaly detection and deep learning. I was previously building NLP models at GoDaddy, but I enjoy working with data in general. I'm a Python enthusiast and enjoy sharing my learnings with the community - I've previously presented at the PyData, Grace Hopper Conference, PyCon US, EuroPython, and GeoPython. When not opposite a screen, I can be found frolicking in nature and exploring new trails.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Where have all the post offices gone? Discovering neighborhood facilities with Python and OSM", "abstract": "When it comes to open geographic data, [OpenStreetMap](https://www.openstreetmap.org/) (OSM) is an awesome resource. Getting started and figuring out how to make the most out of the data available can be challenging.\r\n\r\nUsing a personal example: frustration at the apparent lack of post offices in my neighborhood, we'll walk through examples of how to parse, filter, process, and visualize geospatial data with Python.\r\n\r\nWhile this talk is aimed at those beginning with geographic data, it would be helpful to have some background knowledge about Python and data handling.\r\n\r\nAt the end of this talk, you will know how to process geographic data from OpenStreetMap using Python and find out some surprising info that I learned while answering the question: _Where have all the post offices gone?_", "full_description": "### Problem statement and brief introduction\r\n\r\n- Needing an international postcard stamp, I headed to my nearest post office only to find out that it was permanently closed, the latest closure among others in recent memory. Was this just in my neighborhood or was this happening all over the state? To answer these questions, I turned to open data and Python.\r\n- What is OpenStreetMap?\r\n\r\n### How can we identify types of places, like post offices and districts, in OSM data?\r\n\r\n- Types of data in OSM\r\n- Tags\r\n- Brief discussion of tools ([Overpass API](https://overpass-api.de/), [overpass turbo](https://overpass-turbo.eu/)) for diving into the data to get an idea of how it is structured and how to construct queries\r\n\r\n### How can we access the raw OSM data and work with it in Python? \r\n\r\n- How many post offices are there in each neighborhood? What about by area or population?\r\n- Working with PBF files: parsing and filtering with the Python library [PyOsmium](https://osmcode.org/pyosmium/).\r\n- Using [GeoPandas](https://geopandas.org/) to store the data in a GeoDataFrame and apply transformations\r\n\r\n### What are some tools for visualizing the data?\r\n\r\n- How can we make an interactive plot of post offices in each neighborhood? What about other facilities and resources?\r\n- Plot directly from a GeoDataFrame\r\n- Interactive plotting", "code": "ADSXCA", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/ADSXCA.png", "speaker_names": "Katie Richardson", "speakers": "\n### Katie Richardson\n\nKatie Richardson is a Data Scientist, who is passionate about the intersection of data science and ML Ops. With experience in recommendation systems and search ranking, she is currently focused on demand forecasting.\n", "track": "Data Handling & Engineering"}, {"title": "Algorithmic Music Composition With Python", "abstract": "Computers have long been an integral part of creating music. Virtual instruments and digital audio workstations make creating music easy and accessible. But how do programming languages and especially Python fit into this? Python can serve as a tool for creating musical notation\r\nand MIDI files.   \r\n\r\nThroughout the session, you\u2019ll learn how to:\r\n\r\n- Use Python to create melodies, harmonies, and rhythms.\r\n- Generate music based on rules, randomness, and mathematical principles.\r\n- Visualize and export your compositions as MIDI and sheet music.\r\n\r\nBy the end of the talk, you\u2019ll have a clear understanding of how to turn simple algorithms into expressive musical works.", "full_description": "This talk provides a general introduction into creating music algorithmically using Python. Little prior knowledge about music is assumed. It is helpful to know how sheet music looks and what the MIDI format is beforehand. \r\n\r\nWe will start by looking briefly into the basic building blocks of music (harmony, melody and rhythm) and what our goal is (creating sheet music and a playable MIDI file). \r\n\r\nThen we will discuss the history of algorithmic composition in music and from that we will develop ideas how we can create music from algorithms and randomness. \r\n\r\nFor creating sheet music we will look into the packages Abjad und music21. \r\n\r\nIn the end will we will create a playable MIDI file for our music using MIDIUtil.", "code": "TQN98D", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/TQN98D.png", "speaker_names": "Hendrik Niemeyer", "speakers": "\n### Hendrik Niemeyer\n\nHendrik is a C++ developer and works on software for analysis of pipeline inspection data. This includes topics like machine learning, \r\nnumerical mathematics and distributed computing. Before this he completed his PhD in physics at the University of Osnabr\u00fcck with a thesis about quantum mechanics and \r\nnumerical simulations where he got to know and and love programming and complex, mathematical tasks. \r\nHis favorite programming languages, in which he also has the most experience, are C++, Python and Rust. He describes himself as a \"learning enthusiast\" \r\nwho always gets absorbed in trying out new things. Therefore, he values being up to date with programming languages \r\nand using the latest features of them in a meaningful way.\n", "track": "Python Language & Ecosystem"}, {"title": "What do a tree and the human brain have in common-a not so serious introduction to digital pathology", "abstract": "While trees and human brains don't share that many properties regarding their domain, the analysis of the height of a tree and cancer in human brains does.\r\nThis talk provides a not-so-serious introduction to the domain of computer vision for pathological use cases. \r\nBesides a general introduction to (digital) pathology and the technical similarities between satellite images (GeoTIFs) and pathological images (Whole-Slide Images), we will take a look at computer vision (both ML-based and conventional) on Python.\r\nWhether you have never done image processing in Python, are an expert (ready to share some tricks with me), or are just curious to see pictures of a human brain, this talk is for you.\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "full_description": "Inspired by last year's talk about the height of a tree [\ud83c\udf33 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery \ud83d\udef0\ufe0f] and the strong similarities between optical high resolution satellite images and pathological images, this talk will give a not-so-serious introduction to a quite serious topic.\r\nThe main content is:\r\n- a short introduction to digital pathology (know your domain)\r\n- the similarities between a tree and your brain (technically speaking, there are a lot)\r\n- ML-based and conventional computer vision in Python with some practical use cases\r\n- Why we can steal (nearly) everything from radiology and get away with it\r\n\r\nWarning: this talk contains quite abstract pink-ish pictures of human tissue. If you are unsure this is something you are comfortable with (have a friend), do a quick search for \"HE-stained whole-slide image\".", "code": "MJD7TG", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/MJD7TG.png", "speaker_names": "Daniel Hieber", "speakers": "\n### Daniel Hieber\n\nDaniel is a PhD student in digital neuropathology at Julius-Maximilians-University W\u00fcrzburg and a research assistant at the University Hospital Augsburg as well as Neu-Ulm University of Applied Sciences. His work is focused on applying computer vision techniques to automate analysis processes in the pathological departments.\n", "track": "Computer Vision (incl. Generative AI CV)"}, {"title": "Hands-On LLM Security: Attacks and Countermeasures You Need to Know!", "abstract": "Dive into the vulnerabilities of LLMs and learn how to prevent them\r\nFrom prompt injection to data poisoning, we\u2019ll demonstrate real-world attack scenarios and reveal essential countermeasures to safeguard your applications.", "full_description": "The rapid increase in usage of large language models (LLMs) in the last years makes it necessary to address the specific security risks of LLMs.\r\nIn this presentation, we will examine typical vulnerabilities in LLMs from a practical perspective. Starting with a systematic overview, we will use a specific demo app to illustrate the various attack scenarios. Vulnerabilities like prompt injection, data poisoning and system prompt leakage will be explained and demonstrated as well as attacks on RAG and agent implementations.\r\nIn addition to a basic introduction and a presentation of specific vulnerabilities, the talk also presents suitable countermeasures and general best practices for the use of LLMs in productive applications.\r\n\r\nWhat to expect? \r\nAttending this talk, you learn which vulnerabilities need to be considered when using and integrating LLMs. You will see how specific attacks work and what risks are associated with them. You will also learn which countermeasures are suitable and how these can be implemented technically.", "code": "3DSU8V", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/3DSU8V.png", "speaker_names": "Clemens H\u00fcbner, Florian Teutsch", "speakers": "\n### Clemens H\u00fcbner\n\nFor more than ten years, Clemens H\u00fcbner has been working at the interface between software and security. After roles as a software developer and in penetration testing, he joined inovex in 2018 as a software security engineer. Today, he supports development projects at the conception and implementation level and is a trainer both in-house and for clients. He advises on secure development processes and DevSecOps. As speaker, he is invited to national and international conferences.\n\n### Florian Teutsch\n\nFlorian Teutsch possesses extensive knowledge in the field of generative AI and works as a Machine Learning Engineer at inovex. After successfully completing his studies in Information Systems at the University of Cologne in 2020, he worked for two years as a Data Scientist on an innovative AI-based image search. Since joining inovex, he has been able to continuously expand his practical experience in the field of generative AI.\n", "track": "Security"}, {"title": "Bias Meets Bayes: A Bayesian Perspective on Improving Model Fairness", "abstract": "Bias in machine learning models remains a pressing issue, often disproportionately affecting the most vulnerable groups in society. This talk introduces a Bayesian perspective to effectively tackle these challenges, focusing on improving fairness by modeling and addressing bias directly.\r\nYou will learn about the interplay between uncertainty, equity, and predictive accuracy, while gaining actionable insights to improve fairness in diverse applications. Using a practical example of a risk-scoring model trained on data with underrepresented minority groups, I will showcase how Bayesian methods compare to traditional techniques, demonstrating their unique potential to mitigate bias while maintaining performance.", "full_description": "Machine learning models often perpetuate biases that exacerbate societal inequities, particularly for vulnerable groups. As machine learning increasingly shapes critical decisions, addressing these biases is more important than ever. In this talk, I will explain how Bayesian methods offer a principled and effective approach to improving fairness by directly addressing bias and incorporating uncertainty into machine learning models.\u2028\r\n\r\nThe talk will cover:\r\n\r\n1.\tTheoretical Foundations: I will start by exploring the connection between Bayesian statistics, fairness, and accuracy, with a focus on why uncertainty is a crucial factor in fairness interventions.\r\n2.\tPractical Example: Using a risk-scoring model trained on a dataset with underrepresented minority groups, I will demonstrate how Bayesian methods compare to traditional fairness techniques. This example will illustrate their ability to not only mitigate bias but also adapt to complex, real-world data distributions while maintaining predictive accuracy.\r\n3.\tKey Insights and Applications: Finally, I will provide actionable takeaways on incorporating Bayesian thinking into existing workflows, enabling more equitable and robust outcomes across diverse applications.\u2028\r\n\r\nThis talk is designed to be accessible to a broad audience. While minimal familiarity with machine learning concepts and fairness principles is recommended, no advanced knowledge of statistics is required. Attendees will leave with practical tools, code examples, and insights to address bias effectively in real-world scenarios, empowering them to promote fairness in their own projects and organizations.", "code": "ER3V7W", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/ER3V7W.png", "speaker_names": "Vince Nelidov", "speakers": "\n### Vince Nelidov\n\nVince Nelidov is a Staff Data Science at Blue Yonder with diverse consulting experience in the data domain in a variety of industries from energy sector and banking to skincare and agriculture. Throughout his years in the data world, Vince has been combining advanced data science with business insights to make data work with an impact. He aspires to see far beyond what is on the surface and get to the essence of the problems, discovering robust and scalable long-term solutions rather than temporary fixes.\r\n\r\nVince is passionate about sharing his knowledge and insights, believing that Data literacy should not be a privilege of a few. And his goal is to be there to make this a reality. Making the intricacies of data science intelligible and uncovering the regularities hiding in the data is a major source of inspiration for Vince. With this goal in mind, he combines his years of experience in consulting with his background in statistics, research and teaching to make this knowledge accessible to businesses and individuals in need.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "The Foundation Model Revolution for Tabular Data", "abstract": "What if we could make the same revolutionary leap for tables that ChatGPT made for text? While foundation models have transformed how we work with text and images, tabular / structured data (spreadsheets and databases) - the backbone of economic and scientific analysis - has been left behind. TabPFN changes this. It's a foundation model that achieves in 2.8 seconds what traditional methods need 4 hours of hyperparameter tuning for - while delivering better results. On datasets up to 10,000 samples, it outperforms every existing Python library, from XGBoost to CatBoost to Autogluon.\r\n\r\nBeyond raw performance, TabPFN brings foundation model capabilities to tables: native handling of messy data without preprocessing, built-in uncertainty estimation, synthetic data generation, and transfer learning - all in a few lines of Python code. Whether you're building risk models, accelerating scientific research, or optimizing business decisions, TabPFN represents the next major transformation in how we analyze data. Join us to explore and learn how to leverage these new capabilities in your work.", "full_description": "TabPFN shows how foundation model concepts can advance tabular data analysis in Python. Born as research published at ICLR 2023, it found strong community adoption with 1,200+ GitHub stars and 100,000+ downloads. Our upcoming January 2025 release introduces major improvements in speed, scale and capabilities that we're excited to preview at PyCon.\r\n\r\n**Detailed Outline:**\r\n\r\n1. **Context & Evolution** (5 min)\r\n- The challenge of applying deep learning to tabular data\r\n- Learning from the foundation model revolution in text and vision\r\n- Key improvements from V0 to V1 based on community feedback\r\n- Real-world examples where TabPFN shines (and where it doesn't)\r\n\r\n2. **Technical Insights** (8 min)\r\n- How we adapted transformers for tabular data\r\n- Making in-context learning work for structured data\r\n- Performance characteristics and resource requirements\r\n- Understanding current limitations and constraints\r\n\r\n3. **Live Coding & Integration** (12 min)\r\n- Getting started with TabPFN in 3 lines of code\r\n- Handling real-world data challenges:\r\n  - Missing values and mixed data types\r\n  - Built-in uncertainty estimation\r\n  - Working with similar tasks efficiently\r\n- Integration with pandas, scikit-learn and the Python ecosystem\r\n\r\n4. **Practical Applications** (5 min)\r\n- When to choose TabPFN vs traditional methods\r\n- Resource requirements and scalability limits  \r\n- What's next for TabPFN\r\n- Q&A\r\n\r\n**Key Takeaways:**\r\n- Practical understanding of TabPFN's capabilities and limitations\r\n- Hands-on experience integrating with Python data science workflows\r\n- Best practices for working with foundation models on tabular data\r\n- Insight into emerging approaches for structured data analysis", "code": "XRHEYZ", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/XRHEYZ.png", "speaker_names": "Noah Hollmann, Frank Hutter", "speakers": "\n### Noah Hollmann\n\n\n\n### Frank Hutter\n\nFrank is a Hector-Endowed Fellow and PI at the ELLIS Institute T\u00fcbingen and has been a full professor for Machine Learning at the University of Freiburg (Germany) since 2016. Previously, he has been an Emmy Noether Research Group Lead at the University of Freiburg since 2013. Before that, he did a PhD (2004-2009) and postdoc (2009-2013) at the University of British Columbia (UBC) in Canada. He received the 2010 CAIAC doctoral dissertation award for the best thesis in AI in Canada, as well as several best paper awards and prizes in international ML competitions. He is a Fellow of ELLIS and EurAI, Director of the ELLIS unit Freiburg, and the recipient of 3 ERC grants. Frank is best known for his research on automated machine learning (AutoML), including neural architecture search, efficient hyperparameter optimization, and meta-learning. He co-authored the first book on AutoML and the prominent AutoML tools Auto-WEKA, Auto-sklearn and Auto-PyTorch, won the first two AutoML challenges with his team, is co-teaching the first MOOC on AutoML, co-organized 15 AutoML-related workshops at ICML, NeurIPS and ICLR, and founded the AutoML conference as general chair in 2022. In recent years, his focus has been on the intersection of foundation models and AutoML, prominently including the first foundation model for tabular data, TabPFN.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Outgrowing your node? Zero stress scaling with cuPyNumeric.", "abstract": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library, which  gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library on one of the user's examples covering some detail on its implementation.", "full_description": "Many data and simulation scientists use NumPy for its ease of use and good performance on CPU.  This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library.  cuPyNumeric gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.\r\n\r\nA compelling example when scaling is necessary is when scientists at the Stanford Linear Accelerator Center(SLAC) need to process a large amount of data within a fixed time window, called beam time.  The full dataset generated during experiments is too large to be processed on a single CPU. Additionally, the code often must be modified during the beam time to adapt to changing experimental needs. Being able to use NumPy syntax rather than lower level distributed computing libraries makes these changes quick and easy, allowing researchers to focus on conducting more experiments rather than debugging or optimizing code.\r\n\r\ncuPyNumeric is designed to be a drop-in replacement to NumPy. Built on top of task-based distributed runtime from Stanford University, it automatically parallelizes NumPy APIs across all available resources, taking care of data distribution, communication, asynchronous and accelerated execution of compute kernels on both GPUs or multi-core CPUs.  In addition, cuPyNumeric can be integrated with other popular Python libraries like SciPy, matplotlib, Jax.  With cuPyNumeric, SLAC scientists successfully ran their data processing code distributed across multiple nodes and GPUs, processing the full dataset with a 6x speed-up compared to the original single-node implementation.\r\n\r\nIn this talk we showcase the productivity and performance of cuPyNumeric library covering some detail on its implementation.", "code": "HPGEKH", "state": "confirmed", "created": "2024-12-20", "social_card_image": "/static/meida/social/HPGEKH.png", "speaker_names": "Irina Demeshko, Quynh L. Nguyen", "speakers": "\n### Irina Demeshko\n\nIrina Demeshko is a senior software engineer at NVIDIA working on cuNumeric and Legate projects. Before NVIDIA, Irina was a research scientist and team leader of the Co-Design team at the Los Alamos National Laboratory. Her work and research interests are in the area of new HPC technologies and programming models. Irina received her Ph.D. in mathematical and computer science from the Tokyo Institute of Technology in 2013.\n\n### Quynh L. Nguyen\n\nQuynh L. Nguyen is an Associate Scientist at SLAC National Accelerator Laboratory. She completed her PhD in Physics at JILA-University of Colorado Boulder (2020) and postdoctoral work at SLAC National Accelerator Laboratory and Stanford University as a Q-FARM Bloch Fellow. Her research interests include nonequilibrium dynamics of materials and high performance computing with applications towards quantum information science.\n", "track": "Programming & Software Engineering"}, {"title": "Getting Started with Bayes in Engineering: Implementing Kalman Filters with RxInfer.jl", "abstract": "Bayesian methods are not commonly seen in Civil Engineering and Structural Dynamics. In this talk we explore how RxInfer.jl and the Julia Programming Language can simplify Bayesian modeling by implementing a Kalman filter for tracking the dynamics of a structural system. Perfect for engineers, researchers, and data scientists eager to apply probabilistic modelling and Bayesian methods to real-world engineering challenges.", "full_description": "Bayesian methods are renowned for their ability to incorporate domain knowledge and quantify uncertainty, making them valuable across various engineering and data science fields. However, finding practical examples of these methods in civil engineering, especially within structural dynamics, can be challenging.\r\n\r\nThis talk aims to make Bayesian inference accessible to engineering practitioners by demonstrating how RxInfer.jl, a Julia package for probabilistic programming, can be used to implement a Kalman filter for tracking the dynamics of a structural system. The session covers:\r\n\r\n1. Bayesian Modelling in Python and Julia: A brief comparison of probabilistic programming languages, highlighting Python and Julia\r\n2. State Space Modelling of Structural Dynamical Systems: A brief introduction to state space models and their use in structural dynamics\r\n3. Linking State Space Modelling to Finite Element Modelling: Making the connection between FEM and SSM\r\n4. A Simplified Overview of Bayesian Filtering and Kalman Filters for Dynamical Systems\r\n5. Bayesian Filtering Made Simple with RxInfer.jl: a step-by-steo guide to setting up a user-friendly and readable Bayesian filter using Rxinfer.jl\r\n6. Full Workflow Example\r\n7. Interpreting the Results and Next Steps\r\n8. Connections to Julia, Python and Open-Source Ecosystems: exploring integrations with tools like FreeCAD and other open-source platforms\r\n\r\nBy the end of the talk, attendees will have a clear understanding of how to start using Bayesian methods in their engineering projects, supported by reproducible and open-source code.", "code": "U9KHNA", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/meida/social/U9KHNA.png", "speaker_names": "Victor Flores Terrazas", "speakers": "\n### Victor Flores Terrazas\n\n\n", "track": "Research Software Engineering"}, {"title": "Mastering Demand Forecasting: Lessons from Europe's Largest Retailer", "abstract": "Ever craved your favorite dish, only to find its key ingredient missing from the store? You're not alone - stock outs can have significant consequences for businesses, resulting in frustrated customers and lost sales. On the other hand, overstocking can lead to wasted storage costs and potential write-offs. The replenishment system is responsible for striking the right balance between these opposing risks.\r\nThe key to successful replenishment is making accurate predictions about future demand. \r\n\r\nThis presentation takes a deep dive into the intricate world of demand forecasting, at Europe's largest retailer. We will demonstrate how enhancing simple machine learning methods with domain knowledge allows to generate hundreds of millions of high-quality forecasts every day.", "full_description": "This talk will provide an in-depth look at the forecasting engine, the heart of Lidl's replenishment system.\r\n\r\nEach day at Lidl, hundreds of millions of various products journey from suppliers to warehouses before reaching the shelves. Our so-called forecasting engine helps to automate the supply chain at every step along the way. \r\nEven with the vast amount of data at our disposal, the problem is still extraordinarily intricate. Each item, store or warehouse has unique demand patterns influenced heavily by a wide range of factors, such as holidays. While most of the effects are quantifiable, others remain unavailable and a certain degree of stochasticity is inherent to the process. The objective of our demand prediction may also vary based on their usages. Accuracy on the day level typically matters for short-term predictions, while it doesn't for long-term predictions. \r\n\r\nWe'll present our pragmatic modeling methodology on a simplified version of the problem at hand: The warehouse forecasting of single items. \r\n\r\nWe explain the rationale for training separate models for each item-warehouse combination and go into the reasons why we opted for using a LGBM model and why we believe it is best suited for our application. In addition to outlining our high-level modeling approach, we demonstrate how business and domain expertise are integrated into the modeling process through the use of sample and feature weighting and examine the impact of this integration on prediction quality. Following the base model, extensions are introduced that enable the incorporation of higher-level information at the finest level of granularity. This is achieved through decomposition and recomposition of the time-series at hand. In detail, we will present uplift decomposition for different use-cases, which include handling of promotions and holidays.\r\n\r\nTo conclude, we will give an overview of how all the presented methods synergize in delivering reliable forecasts for happy customers, so that you will hopefully never find yourself in front of an empty shelf!", "code": "NNGWGC", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/meida/social/NNGWGC.png", "speaker_names": "Moreno Schlageter, Yovli Duvshani", "speakers": "\n### Moreno Schlageter\n\nMachine Learning Engineer at Schwarz IT, Germany, where I'm passionate about harnessing the power of AI to revolutionize the retail industry\n\n### Yovli Duvshani\n\nVersatile data scientist with 3+ years of experience building AI-products at the service of the industry. I believe that the key for success revolves around embracing shared best practices, upholding high quality standards for code development and having a team composed of complementary skill sets.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "They are not unit tests: a survey of unit-testing anti-patterns", "abstract": "The entire industry approves of unit testing but almost no one can fully agree on how to do it correctly, or even on what unit tests are. This results in unit tests often being associated with slower development cycle and an overall less enjoyable workflow. I'll show you how testing turns into hell in real enterprises with the most common anti-patterns and then I'll show you that most of them are avoidable with modern tooling like mutation testing, snapshot testing, dirty-equals, and many more. We'll discuss how to make tests speed up your development and make refactoring easy.", "full_description": "Similar to TDD, unit tests are one of the most misunderstood concepts in software engineering. In this session, I will cover the most important fallacies about unit testing and the most common anti-patterns. I will also show you how modern infrastructure (pytest-fixture-classes, inline-snapshot, dirty-equals, import-linter, mutmut, and pytest-xdist) makes it possible to avoid most of them. \r\n\r\nWe will discuss that the real goal of tests is not always stability and how tests often make refactoring and restructuring your project easy, not hard. I will define my criteria for good tests and then for the rest of the session, we will be using it to analyze anti-patterns and explore modern solutions to them. You will see:\r\n\r\n1. How people make their \"units\" too small and how you can prevent it using import-linter\r\n2. How people make their \"units\" too big and what architectural patterns can you use to make them smaller\r\n3. How the real value of tests is in the quality of their assertions and how mutation testing can measure it for you\r\n4. How people end up with asserting too much, and how inline-snapshot and dirty-equals make this problem obsolete\r\n5. How people try to cover the volatile parts of their software, and how coveragepy already has tooling to prevent it\r\n6. How slow tests hurt you, and how to make your tests fast even if you tried it many times and failed\r\n7. How to build an architecture that makes writing tests hard, and how to make it easy using inline-snapshot, pytest-fixture-classes, and a few clever tricks\r\n8. How you can mock your way into making your tests useless and what you should mock\r\n\r\nAfter this session, your tests will become your friend instead of slowing you down.", "code": "VFE78U", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/meida/social/VFE78U.png", "speaker_names": "Stanislav Zmiev", "speakers": "\n### Stanislav Zmiev\n\nExperienced platform engineer and architect with a passion for open source and developer tools. The author of Cadwyn -- a sophisticated API Versioning framework based on FastAPI. A contributor to numerous projects such as CPython and tortoise-orm. Currently building the future of finance at Monite.\n", "track": "Testing"}, {"title": "From Trees to Transformers: GetYourGuide\u2019s Journey Towards Deep Learning for Ranking", "abstract": "GetYourGuide, a global marketplace for travel experiences, reached diminishing returns with its XGBoost-based ranking system. We switched to a Deep Learning pipeline in just nine months, maintaining high throughput and low latency. We iterated on over 50 offline models and conducted more than 10 live A/B tests, ultimately deploying a PyTorch transformer that yielded significant gains. In this talk, we will share our phased approach\u2014from a simple baseline to a high-impact launch\u2014and discuss the key operational and modeling challenges we faced. Learn how to transition from tree-based methods to neural networks and unlock new possibilities for real-time ranking.", "full_description": "GetYourGuide is a global online marketplace that helps travelers discover and book the best experiences. One of our core challenges is ensuring users always see the most relevant activities first\u2014a task historically powered by an XGBoost-based ranking system. However, as we continued refining our tree-based models, returns on incremental improvements began to plateau. To spark our next step change in performance, we decided to adopt Deep Learning.\r\n\r\nIn this talk, we will share how, in just nine months, we migrated our ranking pipeline to a Deep Learning architecture while maintaining tight latency and high-throughput requirements. We will walk through our phased approach, starting with a minimal viable model to confirm our production setup and gradually increasing its complexity. Along the way, we tested over 50 iterations offline and ran more than 10 live A/B tests to validate the impact on our customers. Ultimately, we rolled out a PyTorch transformer-based model with significant business impact. We will also discuss the main challenges we faced on the operational and modeling sides, how we overcame them, and the lessons we learned.\r\n\r\nYou will leave with practical strategies for transitioning from traditional tree-based models to neural networks in production. Join us to learn how to advance your machine-learning capabilities and unlock new dimensions of relevance and personalization for real-time ranking.", "code": "83QH37", "state": "confirmed", "created": "2024-12-21", "social_card_image": "/static/meida/social/83QH37.png", "speaker_names": "Theodore Meynard, Mihail Douhaniaris", "speakers": "\n### Theodore Meynard\n\nTheodore Meynard is a data science manager at GetYourGuide.He leads the evolution of their ranking algorithm, helping customers to find the best activities to book and locations to explore. Beyond work, he is one of the co-organizers of the Pydata Berlin meetup and the conference. \r\nWhen he is not programming, he loves riding his bike, looking for the best bakery-patisserie in town.\n\n### Mihail Douhaniaris\n\nMihail Douhaniaris is a Senior Data Scientist at GetYourGuide, a leading online travel marketplace for discovering and booking tours, attractions and activities worldwide. At GetYourGuide, Mihail works on the marketplace ranking algorithms, enhancing search relevance and enabling travelers discover experiences best tailored to their preferences. Beyond his role, Mihail is also passionate about responsible AI, ML observability, and deploying machine learning at scale.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Beyond DALL-E: Advanced Image Generation Workflows with ComfyUI", "abstract": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. There are well-known models like Stable Diffusion and Flux, which can be used with easy-to-use frontends like A1111 or Invoke AI, but if you want to do more complex or bleeding-edge workflows, you need something else. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build complex pipelines that are otherwise only possible using plain code.", "full_description": "Image generation using AI has made huge progress over the last years, and many people still think that DALL-E with a text prompt is the best way to generate images. But thanks to Stable Diffusion, Flux, and many supplementary models like ControlNet or an Image Prompt Model, we have much more control over the images we want to create. There are frontends for that, like A1111 or Invoke AI, but if you want to try bleeding-edge models or do something more complex, you will have a hard time implementing such a pipeline in code yourself, and it requires a steep learning curve. In this talk, I want to show you ComfyUI, an open-source node-based GUI written in Python where you can build workflows as a DAG. Thanks to many other contributors, there are a lot of plugins available which bring in new functionality. This talk shows the capabilities and power of this tool using practical examples and how you can combine many things together to create a complex workflow much faster than coding it yourself.\r\n\r\nI want to cover the following topics:\r\n - What are the limits of a simple text-to-image workflow?\r\n - What is ComfyUI?\r\n - What are the requirements to use ComfyUI? (Resources, OS, etc.)\r\n - What can you do with ComfyUI that you can't do with a simple text-to-image interface?\r\n   - Pre- and post-processing of images in a single workflow\r\n   - Advanced conditioning using images, bounding boxes, depth maps, etc., all together\r\n - The examples shown as a demonstration:\r\n   - Integrating existing objects from a photo into a generated scenery\r\n   - Creating optical illusions and surreal images", "code": "LRUKZQ", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/LRUKZQ.png", "speaker_names": "Ren\u00e9 Fa", "speakers": "\n### Ren\u00e9 Fa\n\nJust another Python nerd with a freshly gained enthusiasm for image gen AI. I'm working as a Data Engineer for nearly three years with focus on computer vision topics.\n", "track": "Computer Vision (incl. Generative AI CV)"}, {"title": "Inclusive Data for 1.3 Billion: Designing Accessible Visualizations", "abstract": "According to the World Health Organization (WHO), an estimated 1.3 billion people (1 in 6 individuals) experience a disability, and nearly 2.2 billion people (1 in 5 individuals) have vision impairment. Improving the accessibility of visualizations will enable more people to participate in and engage with our data analyses.\r\n\r\nIn this talk, we\u2019ll discuss some principles and best practices for creating more accessible data visualizations. It will include tips for individuals who create visualizations, as well as guidelines for the developers of visualization software to help ensure your tools can help downstream designers and developers create more accessible visualizations.", "full_description": "Specifically, we will cover:\r\n\r\n- What makes data visualizations inaccessible? We will cover accessibility fundamentals like color contrast, alternative text descriptions, keyboard navigation support, screen reader compatibility, and more, with specific examples and demonstrations.\r\n- Are Python data visualization tools accessible? We will teach how to analyze the visualization landscape and discuss how tool developers can begin and prioritize improvements. \r\n- How accessible is my visualization? We will demonstrate how to conduct accessibility audits for data visualization tools by performing and documenting two accessibility evaluation tests live.\r\n\r\nThis talk will include specific examples from our ongoing work to improve the accessibility of Bokeh, a Python library for creating interactive data visualizations for web browsers. We hope this talk enables you to take the first few steps in making your next data visualization and your visualization tools, more accessible.", "code": "LNW3KE", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/LNW3KE.png", "speaker_names": "Dr. Tania Allard, Pavithra Eswaramoorthy", "speakers": "\n### Dr. Tania Allard\n\n\n\n### Pavithra Eswaramoorthy\n\nPavithra Eswaramoorthy is a Developer Advocate at Quansight, where she works to improve the developer experience and community engagement for several open source projects in the PyData community. Currently, she maintains the Bokeh visualization library, and contributes to the Nebari (adjacent to the Jupyter community), and conda-store (part of the conda ecosystem).\r\n\r\nPavithra has been involved in the open source community for over 5 years, notable as an emeritus contributor to the Dask library and Wikimedia Foundation projects. In her spare time, she enjoys a good book and hot coffee. :)\n", "track": "Visualisation & Jupyter"}, {"title": "Demystifying Design Patterns: A Practical Guide for Developers", "abstract": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation will empower you with a clear, structured understanding of these reusable solutions to address common programming challenges.\r\n\r\nWe'll delve into design patterns\u2019 key categories: Behavioral, Structural, and Creational, as well as explore their functionality and how they can be applied in your daily development workflow. For each category, we'll also explore a practical design pattern in detail and showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation.\r\n\r\nYou'll gain valuable insight into how these patterns can translate into real-world development scenarios, such as facilitating communication between objects (Behavioral), separating interfaces from implementation for flexibility (Structural), and enabling dynamic algorithm selection at runtime (Creational).", "full_description": "Do you ever worry about your code becoming spaghetti-like and difficult to maintain?\r\nMaster the art of crafting clean, maintainable, and adaptable software by harnessing the power of design patterns. This presentation will empower you with a clear, structured understanding of these reusable solutions to address common programming challenges.\r\n\r\nWe'll delve into design patterns\u2019 key categories: Behavioral, Structural, and Creational, as well as explore their functionality and how they can be applied in your daily development workflow. For each category, we'll also explore a practical design pattern in detail and showcase real-world applications of these patterns, along with small-scale code examples that illustrate their practical implementation.\r\n\r\nYou'll gain valuable insight into how these patterns can translate into real-world development scenarios, such as facilitating communication between objects (Behavioral), separating interfaces from implementation for flexibility (Structural), and enabling dynamic algorithm selection at runtime (Creational).", "code": "8PFFPS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/8PFFPS.png", "speaker_names": "Tanu", "speakers": "\n### Tanu\n\nTanu is a Software Engineer at Bloomberg on the BQL (Bloomberg Query Language) team. BQL provides intelligent query suggestions to empower users for efficient data exploration.  A passion for crafting clean, maintainable, and efficient software solutions fuels her work in this role and throughout her career. She has a Master's degree in Distributed Systems and 6 years of industry experience building scalable systems. She is a tech writer for Medium, has organized hands-on workshops and delivered technical presentations internally for 100+ people  . She is passionate about staying on top of tech and sharing knowledge at conferences. In her free time, Tanu enjoys traveling and playing music.\n", "track": "Programming & Software Engineering"}, {"title": "Python Performance Unleashed: Essential Optimization Techniques Beyond Libraries", "abstract": "Every Python developer faces performance challenges, from slow data processing to memory-intensive operations. While external libraries like Numba or Cython offer solutions, understanding core Python optimization techniques is crucial for writing efficient code. This talk explores practical optimization strategies using Python's built-in capabilities, demonstrating how to achieve significant performance improvements without external dependencies. Through real-world examples from machine learning pipelines and data processing applications, we'll examine common bottlenecks and their solutions. Whether you're building data pipelines, web applications, or ML systems, these techniques will help you write faster, more efficient Python code.", "full_description": "Performance optimization remains a critical challenge in Python development. While Python's simplicity and extensive ecosystem make it the language of choice for many applications, its interpreted nature can lead to significant performance bottlenecks. This is particularly evident in data-intensive applications, machine learning pipelines, and large-scale production systems where every millisecond counts.\r\n\r\nMany developers immediately reach for external libraries or complex solutions when facing performance issues. However, Python's standard library and built-in features offer powerful optimization opportunities that are often overlooked. Understanding these fundamental optimization techniques not only improves code performance but also helps developers write more efficient code from the start.\r\n\r\nThis talk addresses the core performance challenges faced by Python developers daily. From memory management to algorithmic efficiency, we'll explore how seemingly simple code changes can lead to substantial performance improvements. Through practical examples drawn from real-world applications, we'll demonstrate how to identify, measure, and optimize performance bottlenecks effectively.", "code": "AJDYRL", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/AJDYRL.png", "speaker_names": "Thomas Berger", "speakers": "\n### Thomas Berger\n\nHi, I\u2019m Thomas Berger! I work as a Machine Learning Engineer at a FinTech company and also teach part-time as a lecturer. I\u2019ve been working with Python for over six years, starting during my studies, where I focused on machine learning. For the last three years, I\u2019ve been applying these skills professionally in my full-time role as a Machine Learning Engineer. I\u2019ve been diving deep into Python for things like **machine learning**, **reinforcement learning**, and **high-performance computing**. I love finding ways to make Python run faster and more efficiently, especially when tackling big data or complex models.\r\n\r\nAt PyCon, I\u2019ll be talking about **high-performance Python** and sharing tips and tricks to help you optimize your code for demanding tasks. I\u2019m excited to share what I\u2019ve learned and connect with others in the Python community!\n", "track": "Python Language & Ecosystem"}, {"title": "Zero Code Change Acceleration: familiar interfaces and high performance", "abstract": "The PyData ecosystem is home to some of the best and most popular tools for doing data-science. Every data-scientist alive today has used pandas and scikit-learn and even Large Language Models know how to use them! For many years there have also been alternative implementations with similar interfaces and libraries with completely new approaches that focus on achieving the ultimate in performance and hardware acceleration. This talk will look at the recent efforts to give users the best of both worlds: a familiar and widely used interface as well as high performance.", "full_description": "The interfaces defined by libraries like Numpy, pandas or scikit-learn are the defacto standard APIs in each library's domain. Data scientists use these libraries directly as well as indirectly through libraries that depend on them.\r\n\r\nThis talk will look at the different approaches that recent efforts have taken to give users both a familiar interface and GPU acceleration. This means users do not have to rewrite their code, learn a new library and benefit from acceleration when using existing libraries.\r\n\r\nThe cudf team built a pandas accelerator by diving deep into the import system of Python. By hooking into the import system you can replace the result of `import pandas as pd` with a library that uses cudf where possible and falls back to pandas where necessary. This is great as it means the user\u2019s seaborn code is automatically accelerated, without the user or the seaborn authors having to do anything.\r\n\r\nThe polars team collaborated with the cudf team to build a GPU engine for polars. They chose a tightly coupled approach where the cuDF library hooks in after the Polars Intermediate Representation is created from the user input. This means using the GPU engine is seamless from the point of view of the user, there is transparent fallback to the CPU and no import statements need changing.\r\n\r\nThe scikit-learn team is adding experimental support to handle PyTorch and CuPy inputs by using the array API standard. Instead of using the Numpy API to perform array computations, scikit-learn is switching to using the array API. This is a subset of the Numpy API that is supported by several other array libraries. The API of Numpy and PyTorch is similar but not exactly the same, this makes writing code that works with both hard. The array API addresses this problem by providing a unified API. Users can accelerate their scikit-learn code by passing in a CuPy or PyTorch array instead of a Numpy array.", "code": "PNQB7C", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/PNQB7C.png", "speaker_names": "Tim Head", "speakers": "\n### Tim Head\n\nI am a scikit-learn core maintainer and work at NVIDIA.\r\n\r\nBefore working on scikit-learn I helped build mybinder.org and worked on JupyterHub.\r\n\r\nMany years ago I was a particle physicist at CERN in Geneva.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "LLM Inference Arithmetics: the Theory behind Model Serving", "abstract": "Have you ever asked yourself how parameters for an LLM are counted, or wondered why Gemma 2B is actually closer to a 3B model? You have no clue about what a KV-Cache is? (And, before you ask: no, it's not a Redis fork.) Do you want to find out how much GPU VRAM you need to run your model smoothly? \r\n\r\nIf your answer to any of these questions was \"yes\", or you have another doubt about inference with LLMs - such as batching, or time-to-first-token - this talk is for you. Well, except for the Redis part.", "full_description": "The talk will cover the theory necessary to understand how to serve LLMs. The talk covers the math behind transformers inference in an accessible and light way. By the end of the talk, attendants will learn:\r\n\r\n1. How to count the parameters in an LLM, especially the ones in the attention layers.\r\n2. The difference between compute and memory in the context of LLM inference.\r\n3. That LLM inference is made up of two parts: prefill and decoding.\r\n4. What is an LLM server, and what features they implement to optimise GPU memory usage and reduce latency\r\n4. How batching affects your inference metrics, like time-to-first-token.\r\n\r\nThe talk will cover:\r\n\r\n**Did you pay attention?** (4 min). A short review of the attention mechanism and how to count parameters in a transformer-based model.\r\n\r\n**Get to know your params** (8 min). The math-y section of the talk, explaining how to translate parameter counts into memory and compute requirements.\r\n\r\n**Prefill and Decoding** (8 min) Explains that inference happens in two steps (prefill and decoding) and how KV-cache exploits this to make decoding faster. Common metrics to measure inference performance, like time-to-first-token and token-per-second.\r\n\r\n**Context and batch size** (5 min) Adds to the picture the sequence length, as well as the number of requests to process in parallel. Explains how LLM servers, like vLLM, use techniques like Paged Attention to optimise GPU usage\r\n\r\n**Conclusion** (5 min) Wrap up, Q&A.", "code": "G3AT7E", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/G3AT7E.png", "speaker_names": "Luca Baggi", "speakers": "\n### Luca Baggi\n\nAI Engineer at xtream by day, and open source maintainer by night. I strive to be an active part of the Python and PyData communities - e.g. as an organiser of PyData Milan. Feel free to reach out!\n", "track": "Generative AI"}, {"title": "Security for Devs", "abstract": "Two truths and a lie:\r\n\r\nYou have been hacked and you don\u2019t know yet\r\nYou haven\u2019t been hacked because it\u2019s not yet convenient (for the attackers)\r\nYou think your application is secure\r\n\r\nSecurity is hard. Hard to measure and is always a catch-up game.\r\n\r\nJoin this talk if you are interested in understanding a bit more about modern security and how it goes way beyond the code of your app.", "full_description": "Usually devs don\u2019t care much about security, and that has probably different reasons:\r\n\r\nFocus on security itself doesn\u2019t pay the bills\r\nNot always clear what being secured means\r\nSecurity =! my code is properly tested\r\nOften comes as an annoying byproduct of ISO27001/SOC2\r\n\r\nBut attacks and data breaches are becoming way more common, and DORA, NIS2 and the law obligation to publicly share data breaches are trying to bring the companies\u2019 attention (and budget) back to this topic.\r\n\r\nLeaving the legalise aside, during this talk we will cover these topics\r\n\r\nSome real examples\r\nYour code\r\nCI/CD\r\nYour app\r\nYour production environment\r\nFramework to assess your security posture\r\nWhere to Start", "code": "QGLS8H", "state": "accepted", "created": "2024-12-22", "social_card_image": "/static/meida/social/QGLS8H.png", "speaker_names": "Christian Barra", "speakers": "\n### Christian Barra\n\nChristian Barra is a Software Engineer, Tech Lead and international speaker living in Lisbon.\r\nHe\u2019s the co-founder of ZeroBang, a cloud consulting company.\r\nHe is an active member of the tech community in Berlin, conference organiser and a Python Software Foundation Fellow.\r\nYou can follow him on X @christianbarra\n", "track": "Security"}, {"title": "Probably Fun: Board Games to teach Data Science", "abstract": "In this tutorial, you will speed-date with board and card games that can be used to teach Data Science. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\n\r\nAs a result, you will experience multiple ideas that you can use to make complex ideas more understandable and enjoyable. We would like to demonstrate how gamification can not only used to produce short puzzles and quizzes, but also as a tool to reason complex problem-solving strategies.\r\n\r\nWe will bring a set of carefully selected games that have been proven effective in teaching statistics, programming, machine learning and other Data Science skills. We also believe that it is probably fun to participate in this tutorial.", "full_description": "Games encourage people to put their brains to work in a focused, constructive and peaceful way. This makes games a fantastic tool in the classroom. Many board games contain sophisticated algorithms and statistical models right under the surface. Therefore, Data Science education can be boosted by playing carefully selected games.\r\n\r\nWe have applied popular board and card games such as Memory, Wizard, Machi Koro, Pandemic and Sky Team (the 2024 Game of the Year in Germany) to teach Data Science concepts in our courses. Learners would first play a game, discuss the mechanisms and only after that get exposed to the theory. Finally, they would move to practical applications using computers.\r\n\r\nThis game-driven approach provides learners with an intrinsic motivation to solve a real practical problem (succeeding at the game).\r\nAnalyzing a game makes it easier to grasp the core mechanism or algorithmic model and ask qualified questions about the details later.\r\nIt also makes sure learners will want to come back for the next class. We have documented practical lessons and made them available under a CC license on https://www.academis.eu/probably_fun/ .\r\n\r\nIn this tutorial, you will speed-date with several short games that can be used to teach Data Science concepts and skills. You will play one game for 15 minutes, reflect on the Data Science concepts it involves, and then rotate to the next table. \r\nThis way, you will experience multiple ideas you can use to make complex methods and ideas more accessible. Also, the tutorial is probably fun to participate in.\r\n\r\nThe tutorial will be executed according to the following pseudocode (or lesson plan):\r\n\r\n1. The presenters give a short introduction on why games matter (5 min)\r\n2. The presenters group participants into teams of up to 6 people.\r\n3. Each team is assigned to a game table with a game and a cheat sheet with instructions. The presenters facilitate with understanding rules and to remove other obstacles.\r\n4. The teams play the game for up to 15 minutes.\r\n5. The teams discuss 1-3 prepared reflection questions to make the transfer from the game to the data science concepts.\r\n6. Each team moves to the next table.\r\n7. Repeat for 3-4 rounds.\r\n8. Everybody gets together for a joint Q & A\r\n9. A QR-Code links to material with games that help learning Data Science and lesson plans", "code": "WELCVS", "state": "confirmed", "created": "2024-12-22", "social_card_image": "/static/meida/social/WELCVS.png", "speaker_names": "Paula Gonzalez Avalos, Dr. Kristian Rother", "speakers": "\n### Paula Gonzalez Avalos\n\nData Nerd & Python Pydata community lover. AI education specialist with five years of experience shaping data science and AI educational offers. Currently leading the AI Academy at the appliedAI Institute for Europe.\n\n### Dr. Kristian Rother\n\nKristian is a freelance Python trainer who wrote his first lines of Python in the year 11111001111. After a career writing software for life science research, he has been teaching Python, Data Analysis and Machine Learning throughout Europe since 2011. More recently, he has built data pipelines for the real estate and medical sector.\r\n\r\nKristian has translated 5 Python books and written 2 more himself, in addition to numerous teaching guides. Kristian has collected 364 stars on Advent of Code. His knowledge about async is, unfortunately, miserable. His favorite Python module is 're'. Kristian believes everybody can learn programming.\r\n\r\nYou can find Kristians teaching materials on https://www.academis.eu\n", "track": "Education, Career & Life"}, {"title": "A tour of the 9 namespaces of Polars expressions", "abstract": "[Polars](https://github.com/pola-rs/polars) expressions are at the core of why the Polars API is so flexible and intuitive, but there are hundreds of expressions available.\r\n\r\nThis talk will give an overview of the 9 namespaces of Polars expressions, namely `arr`, `bin`, `cat`, `dt`, `list`, `meta`, `name`, `str`, and `struct`.\r\n\r\nBy showing simple examples of when these namespaces come into play, the audience will become better acquainted with the methods that Polars expressions provide and thus will be better equipped to write efficient data queries with Polars.", "full_description": "This talk will introduce briefly the 9 namespaces that Polars expressions have with one or two motivating examples of when the namespace would be needed.\r\n\r\nThe examples will be simple but representative of what the namespace(s) are used for:\r\n- `arr` and `list` \u2013 namespaces that hold all of the functionality to work with the Array and List data types which are commonly mixed together, so we will see when to use one or the other;\r\n- `bin` \u2013 namespace that provides functionality to work with binary data;\r\n- `cat` \u2013 namespace dedicated to the categorical data type;\r\n- `dt` \u2013 large namespace that is useful when working with temporal data, time series, and more;\r\n- `meta` \u2013 a namespace that provides tools to manipulate expressions at a meta level;\r\n- `name` \u2013 a small but powerful namespace that is essential when working with expression expansion;\r\n- `str` \u2013 this namespace provides functionality to work efficiently with strings, which is typically the bane of dataframe libraries; and\r\n- `struct` \u2013 an entire namespace dedicated to the data type Struct.", "code": "J8NYMD", "state": "accepted", "created": "2024-12-24", "social_card_image": "/static/meida/social/J8NYMD.png", "speaker_names": "Rodrigo Gir\u00e3o Serr\u00e3o", "speakers": "\n### Rodrigo Gir\u00e3o Serr\u00e3o\n\nRodrigo Gir\u00e3o Serr\u00e3o is the author of multiple independently published books on Python, programming, and mathematics, and frequently blogs about those same topics.\r\nRodrigo has also presented talks and tutorials at some of the largest Python conferences in the world, including PyCon US, EuroPython, and multiple European PyCons.\r\n\r\nAt Polars, Rodrigo channels his pedagogical skills and his passion for teaching to make Polars more accessible to everyone.\r\nBy creating technical content that is easily digestible by those looking to start using, or improve their usage of Polars, Rodrigo is working to help everyone appreciate the elegance and effectiveness of the Polars API.\r\n\r\nWhen he is not working or writing, Rodrigo can be found playing board games, walking his dog, or exercising at the gym.\n", "track": "PyData & Scientific Libraries Stack"}, {"title": "Unforgettable, that's what you are: Evaluating Machine Unlearning and Forgetting", "abstract": "Can deep learning/AI models forget? In this talk, you'll explore the realm of machine unlearning, where researchers and practitioners aim to remove memorized examples from machine learning models. This is relevant for training increasingly overparameterized models and growing GDPR/Privacy concerns with large scale model development and use.", "full_description": "Deep learning memorization is a known phenomena, where deep learning / AI models memorize parts of their training dataset. This happens often for repeated examples, novel examples and occurs more often in overparameterized models.\r\n\r\nThis presents problems for guiding machine learning behavior, requiring much effort in guardrails and output monitoring, as well as questioning whether the models can be GDPR-compliant (i.e. the right to be forgotten).\r\n\r\nA growing area of research on machine unlearning or machine forgetting has emerged to investigate ways a model might unlearn or forget particular memorized examples. In this talk, you'll learn about the field of machine unlearning and related topics like data anonymization to evaluate exactly what's truly unforgettable. Jokes aside: you'll have some practical take-aways to apply to your work in data and machine learning development.", "code": "SZFRRA", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/meida/social/SZFRRA.png", "speaker_names": "Katharine Jarmul", "speakers": "\n### Katharine Jarmul\n\nKatharine Jarmul is a privacy activist and an internationally recognized data scientist and lecturer who focuses her work and research on privacy and security in data science and machine learning. You can follow her work via her newsletter, Probably Private (https://probablyprivate.com) or in her recently published book, Practical Data Privacy (O'Reilly 2023) now also available in German as Data Privacy in der Praxis.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "AI Agents of Change: Creating, Reflecting, and Monetizing", "abstract": "Create, reflect, and earn\u2014with purpose. In this workshop, you\u2019ll not only build your own AI agent but also confront the ethical questions it raises, from its impact on jobs to its potential for social good. Together, we\u2019ll explore how to harness AI for empowerment while uncovering pathways to turn your skills into meaningful value.\r\n\r\nThis workshop is designed to equip Python enthusiasts with the tools to create their own AI agent while fostering a deeper understanding of the societal implications of this technology. Through hands-on learning, collaborative discussions, and practical monetization strategies, you\u2019ll leave with more than just code\u2014you\u2019ll gain a vision of how AI can be wielded responsibly and profitably.", "full_description": "The session unfolds in three engaging parts:\r\n\t1.\tBuild Your AI Agent\r\nStart with the fundamentals of AI by designing and implementing a functional agent. Using Python, we\u2019ll demystify the process and equip you with practical skills for creating an AI that responds to user needs and scenarios.\r\n\t2.\tReflect on Ethics and the Future of Work\r\nOnce your agent comes to life, we\u2019ll pause to examine the bigger picture:\r\n\t\u2022\tHow does the AI agent you have created may reshape the job market?\r\n\t\u2022\tCan it democratize and decentralize opportunities, or does it risk amplifying inequalities?\r\n\t\u2022\tWhat collective vision do we want for the future of work?\r\nThis thought-provoking discussion will challenge you to think critically about the role of technology in fostering empowerment or exacerbating social challenges.\r\n\t3.\tEarn by Sharing Value\r\nFinally, we\u2019ll explore how your AI agent can create real-world value. You\u2019ll learn how to leverage marketplaces like OpenServ to turn your innovation into income. Whether you aim to solve practical problems, inspire creativity, or contribute to ethical AI development, this segment will connect your skills with opportunities for meaningful impact.\r\n\r\nBy the end of the workshop, you\u2019ll have built an AI agent, grappled with its ethical dimensions, and uncovered how to use your coding prowess to create and share value\u2014all while shaping a more inclusive, responsible AI ecosystem.", "code": "PKZD8L", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/meida/social/PKZD8L.png", "speaker_names": "Paloma Oliveira", "speakers": "\n### Paloma Oliveira\n\nI\u2019m a wholehearted explorer and community-driven developer, advocating for FOSS while blending art, technology, and inclusion.\n", "track": "Generative AI"}, {"title": "Why Don\u2019t Customers Want My Free Goods? \u2013 Why Forecasting Models Don\u2019t Answer 'What If' Questions", "abstract": "Forecasting and causal inference are distinct but fundamental tasks in data science. While forecasting predicts future outcomes based on history, causal inference explores the \"why\" behind those outcomes and helps simulate \"what if\" scenarios. Confusing the two can lead to misleading results.\r\n\r\nAt Blue Yonder, we encountered a case where a customer's forecasting model predicted demand accurately based on price. However, when they used the model for simulations to explore \"what if\" scenarios, the results were counterintuitive: lower prices led to lower demand. I will share how we resolved this issue and emphasize the importance of incorporating causal thinking when addressing questions like, \"Why did this happen?\" or \"What if I do X?\"\r\n\r\nIn this talk, I\u2019ll show how to identify common pitfalls, like confounders, when integrating causal inference into forecasting workflows. We\u2019ll also explore Bayesian models, powered by Markov Chain Monte Carlo (MCMC) methods, to bridge the gap between forecasting and causality using the PyMC library on practical examples.\r\n\r\nBy the end of this talk, you\u2019ll learn how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses with Directed Acyclic Graphs (DAGs) to understand relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.", "full_description": "Forecasting and causal inference are fundamental yet distinct tasks in data science. While forecasting predicts future outcomes based on historical patterns, causal inference seeks to understand the \"why\" behind these outcomes and explore \"what if\" scenarios to simulate the impact of interventions. These tasks often overlap but have different requirements\u2014and confusing them can lead to unexpected results.\r\n\r\nAt Blue Yonder, we observed an intriguing case where a customer's forecasting model used price information to predict demand accurately. However, when they used the model for causal simulations\u2014adjusting prices to explore \"what if\" scenarios\u2014the predictions behaved counterintuitively: lower prices led to lower predicted demand! I would like to share the details of how we helped the customer resolve this issue in this real-world example and highlight the importance of taking a causal perspective when tackling questions like, \"Why did this happen?\", \"What if I do X?\", or \"What if I had done Y instead?\"\r\n\r\nIn this talk, I will demonstrate how to identify and avoid common pitfalls, such as confounders, when incorporating causal inference into traditional forecasting workflows. I\u2019ll show how we can use Bayesian models powered by Markov Chain Monte Carlo (MCMC) methods to bridge the gap between forecasting and causality. Together, we\u2019ll walk through practical examples using the PyMC library to illustrate these concepts.\r\n\r\nBy the end of this talk, you\u2019ll gain actionable insights on how to:\r\n\r\n- Consider causal reasoning when building models that forecast well but can also be used for interventions and \"what if\" scenarios.\r\n- Visualize causal hypotheses using Directed Acyclic Graphs (DAGs) to understand causal relationships.\r\n- Leverage PyMC to build Bayesian models for testing causal hypotheses and answering \"what if\" questions.\r\n\r\nWhether you\u2019re interested in forecasting, causal inference, Bayesian modeling, or all of them, this talk will equip you with practical tools to approach each with clarity and confidence\u2014grounded in a real-world example from the Blue Yonder experience.", "code": "LRRVGG", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/meida/social/LRRVGG.png", "speaker_names": "Matthias Binder", "speakers": "\n### Matthias Binder\n\nSenior Staff Data Science Consultant at Blue Yonder. I'm passionate about ML and AI in the Supply Chain. I'm an avid learner and my newest goal is to dive deeper into Bayesian Inference and Causal Inference.\n", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Information Retrieval Without Feeling Lucky: The Art and Science of Search", "abstract": "Search is everywhere, yet effective Information Retrieval remains one of the most underestimated challenges in modern technology. While Retrieval-Augmented Generation has captured significant attention, the foundational element - Information Retrieval - often remains underexplored. \r\n\r\nIn this talk, we put Information Retrieval center stage by asking: \r\nHow do we know that user queries and data 'speak' the same language?\r\nHow do we evaluate the relevance and completeness of search results? And how do we prioritize what gets displayed? Or do we even want to hide specific content?\r\n\r\nWe try to answer these questions by introducing the audience to the art and science of Information Retrieval, exploring metrics such as precision, recall, and desirability. We\u2019ll examine key challenges, including ambiguity, query relaxation, and the interplay between sparse and dense search techniques. Through a live demo using public content from Sendung mit der Maus, we show how hybrid search improves upon vector and keyword based search in isolation.", "full_description": "Information Retrieval goes beyond keyword matching - it\u2019s about intent, context, and delivering relevant and accurate results. As RAG applications gain traction, understanding the retrieval process becomes more crucial for developers, data scientists, and search engineers.\r\n\r\nWe start with the Why. People have different needs for search - lookup, research, and inspiration. Each of these needs can be influenced and affected by the key IR metrics of search engines: precision, recall, and desirability. Having introduced these fundamentals, we go into common retrieval challenges, such as ambiguity, mismatched vocabularies, and the impact of context.\r\n\r\nAiming to solve these challenges, we then go into advanced search techniques, comparing sparse (keyword-based) and dense (vector-based) retrieval, highlighting their strengths and limitations. We\u2019ll explore hybrid search as a powerful approach that blends these techniques. In a live demo, using crawled data from the Sendung mit der Maus, we\u2019ll showcase a hybrid search setup leveraging tools like Mistral, Elasticsearch, and Streamlit. While the dataset language is German, the core concepts and search dynamics should hopefully be easily understandable also for non native speakers.\r\n\r\nThe talk concludes with key takeaways on building effective search systems and a look ahead at future developments in contextualized search.\r\n\r\nTentative Outline:\r\n1. Introduction to Information Retrieval (~ 5 min)\r\n\u00a0 * Why do we search? Lookup, research, inspiration\r\n\u00a0 * Core metrics: precision, recall, desirability\r\n\r\n2. Challenges in Search and Retrieval (~ 5 min)\r\n\u00a0 * Ambiguity\r\n\u00a0 * Discrepancy in query and content \r\n\u00a0 * The impact of context \r\n\r\n3. Search Techniques (~ 10 min)\r\n\u00a0 * Sparse vs dense retrieval: comparing keyword and vector search (semantic search, embeddings, synsets, decompounders)\r\n\u00a0 * Hybrid search: Combining sparse and dense approaches\r\n\r\n4. Hybrid Search in Action (< 10 min)\r\n\u00a0 * Setting up a hybrid search with Mistral, Elasticsearch, and Streamlit\r\n\u00a0 * Live Demo: exploring search in Lach- & Sachgeschichten from Sendung mit der Maus\r\n\r\n5. Takeaways & Outlook (< 5 min)\r\n* hybrid search systems combine semantics, precision and explainability\r\n* contextualized search\r\n\r\nThe talk is directed at anyone interested in building or improving search systems. Attendees will gain a deeper understanding of the tools, methodologies, and metrics essential for building robust and explainable search systems.", "code": "ZHT9HW", "state": "confirmed", "created": "2025-01-05", "social_card_image": "/static/meida/social/ZHT9HW.png", "speaker_names": "Anja Pilz", "speakers": "\n### Anja Pilz\n\nI received my PhD in Machine Learning (ML) and Natural Language Processing (NLP) from the University of Bonn and Fraunhofer IAIS where I was member of the Text Mining group. Now I work on AI and data driven products, mostly focused on applications in the medical and healthcare domain.\r\nMy main passion is in NLP, especially for the German language, and Information Retrieval (IR). Sometimes I build Recommender Systems.\n", "track": "Others"}]}