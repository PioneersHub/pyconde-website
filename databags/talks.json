{"talks": [{"title": "In Praise of Documentation: Tools, Tips & Techniques for Literate Programming in the AI Age", "abstract": "This talk has one simple message: *please document your code*. If you attend my talk, you'll hear me explain why I praise documentation, and why you should too. \r\n\r\nWhile writing documentation is generally acknowledged to be a \"good thing\", most engineers do not document their work. I'll offer my optionated lament on the life and death of literate programming. A lament is a poetic discourse, expressing sadness, or feeling sorry about something. I'll give some examples of the *bad things* that can happen when people don't write documentation.\r\n\r\nThen, after making you feel bad, I'll give examples of how you can *feel good*. I'll explain why writing documentation is a \"good\" edifying activity, which helps you to be a better person, and make a better world.\r\n\r\nI'll review types of open source documentation (Python and Unix), documentation frameworks (Di\u00e1taxis), and Python tools (Sphinx, Jupyter, Quarto) you can try out as soon as my talk is finished.\r\n\r\nThen, I'll get \"cool n' futuristic\" by talking about AI. I'll emphasise the importance of text to AI-assisted coding and agentic workflows for \"spec-driven development\" (e.g. Agent-OS with Claude Code), before tempering your excitement by giving you some old-fashioned advice on \"good\" writing style by George Orwell.\r\n\r\nIn summary, if you come to my talk, you might experience an unusual mixture of sadness combined with hope. To conclude, I'll tell you to \"please document your code\". You'll laugh, go to the next talk, and forget my advice.", "full_description": "# Introduction\r\n\r\n# In Praise of Documentation\r\n\r\n- The Promise of \"Literate Programming\"\r\n- A Lamentation on the Death of Literate Programming\r\n- Bad things that happen when you don't document\r\n\r\n# Why You Should Document\r\n\r\n- Code is Communication\r\n- Accessible, Maintainable, Sustainable Code\r\n- Version Control (e.g. GitHub)\r\n\r\n# Examples\r\n\r\n## Examples of Open Source Documentation:\r\n\r\n- Python `help`\r\n- Docstrings\r\n- Unix `man` pages\r\n- `README.md`\r\n- `Readthedocs.com`\r\n\r\n## Documentation Framework Example: \r\n\r\n- Di\u00e1taxis\r\n\r\n## Python Documentation Tool Examples: \r\n\r\n- Sphinx\r\n- `cookiecutter`\r\n\r\n## Scientific Publishing Tool Examples:\r\n\r\n- Jupyter\r\n- Quarto\r\n\r\n# The Bit About AI (yes, I know, and I'm sorry)\r\n\r\n- The importance of text for AI Code Assistant and Agentic Coding workflows\r\n\r\n## Documentation in \"Spec-Driven\" Development:\r\n\r\n- `AGENTS.md`\r\n- Agent-OS with Claude Code\r\n\r\n# Calls to Action: \r\n\r\n- Writing Tips by George Orwell\r\n- Please Document Your Code!", "code": "37AESH", "state": "confirmed", "created": "2025-11-20", "social_card_image": "/static/media/social/talks/37AESH.png", "speaker_names": "Stephen", "speakers": "\n### Stephen\n\nPsychologist turned full-stack polyglot Data Scientist with an established career in data analytics, scientific psychology, and project leadership. Driven by values of care, compassion and privacy.\n", "domain_expertise": "None", "python_skill": "Novice", "track": "Education, Career & Life"}, {"title": "From Ticket to Draft: How Munich Automates Citizen Inquiries with AI", "abstract": "The City of Munich is modernizing its communication: With the transition to the Zammad ticketing system, there is a unique opportunity to not only manage citizen inquiries but to proactively process them using Artificial Intelligence. The Zammad-AI project utilizes a two-stage process consisting of intelligent classification and RAG-based (Retrieval-Augmented Generation) response drafting to significantly reduce the workload of administrative staff.\r\nIn this talk, we demonstrate how we integrated Zammad-AI via an internal Kafka message bus to process tickets in real-time. We explore the technical workflow\u2014from thematic context analysis to the generation of valid response drafts based on a department-specific knowledge base.", "full_description": "## 3. Session Outline (30 Minutes)\r\n\r\n### I. Context & The Pre-Study | **5 min**\r\n* **The Shift:** Transitioning from legacy email communication to **Zammad** within Munich's city administration.\r\n* **Proving the Case:** Utilizing LLMs to analyze historical ticket data to calculate automation potential and project significant time savings before development began.\r\n\r\n### II. Architecture: Integration & Pipeline | **6 min**\r\n* **Event-Driven Design:** Connecting to Zammad via the city-internal **Kafka** message bus.\r\n* **Real-time Processing:** How new tickets are captured and routed to the AI component seamlessly.\r\n\r\n### III. The Two-Stage Process | **12 min**\r\n* **Step 1: Classification & Extraction:** Analyzing thematic context through rule-based logic and LLM-powered information extraction.\r\n* **Step 2: Response Generation:** A **RAG (Retrieval-Augmented Generation)** approach leveraging a knowledge base maintained by subject matter experts.\r\n* **Human-in-the-Loop:** Integrating response drafts into the agent UI for review vs. automated **\"dark processing\"** for high-confidence categories.\r\n\r\n### IV. Scaling & Lessons Learned | **4 min**\r\n* **Multi-Tenant Capability:** Designing for configurability and deployment across various city departments.\r\n* **Key Benefits:** Efficiency gains, response consistency, and establishing a \"Single Voice of the City.\"\r\n\r\n### V. Q&A | **3 min**\r\n* Open discussion on technical tooling, model selection, and legal/privacy frameworks.\r\n\r\n---\r\n\r\n## 4. Key Takeaways for Attendees\r\n\r\n* **Validating Automation:** Techniques for using LLMs to audit historical data and justify development through projected time savings.\r\n* **Practical AI Integration:** How to integrate AI services into existing enterprise infrastructures like Zammad and Kafka.\r\n* **Modular Workflow:** The importance of separating classification from generation for higher system reliability.\r\n* **Operational Insights:** Lessons from scaling AI solutions across diverse governmental branches.", "code": "39MHWT", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/39MHWT.png", "speaker_names": "Leon Lukas", "speakers": "\n### Leon Lukas\n\nHere is the translation of the biography, keeping the tone professional and suitable for a conference or website profile:\r\n\r\nLeon Lukas has been the team lead of the AI Competence Center for two years and has played a key role in the development and implementation of AI solutions within the city administration. While he initially trained models and built systems himself, he is now responsible for the architecture and projects at it@m, the city\u2019s IT service provider. For more information on AI in the City of Munich, visit: ki.muenchen.de.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building Secure Environments for CLI Code Agents", "abstract": "AI code agents like Claude Code are powerful but require careful isolation. Learn how to run them in secure containers with persistent credentials, API logging, and complete filesystem isolation\u2014protecting your host system while maintaining full functionality.", "full_description": "AI-powered code agents like Claude Code can autonomously edit files, run commands, and interact with your development environment. This power comes with risks: unrestricted filesystem access, exposed credentials, and unmonitored API usage. How do you harness this capability safely?\r\n\r\nThis talk presents a practical containerization approach for running CLI code agents in complete isolation from your host system. You'll learn how to build secure environments that maintain persistent authentication, enable workspace access through volume mounts, and provide full API request logging, all while keeping the agent sandboxed.\r\n\r\nI'll demonstrate a production-ready setup using Docker containers that includes credential management, an API proxy for request logging and monitoring, and Datasette integration for analyzing API usage patterns. You'll see how to structure volumes for security, implement network isolation, and maintain developer productivity while enforcing safety boundaries.", "code": "3BYLZU", "state": "confirmed", "created": "2026-01-05", "social_card_image": "/static/media/social/talks/3BYLZU.png", "speaker_names": "Harald Nezbeda", "speakers": "\n### Harald Nezbeda\n\nHi, my name is Harald and I'm a passionate Python developer interested in development, DevOps and AI. I'm currently located in Austria working as a Senior Software Developer and Python Technical Leader for [Anexia](https://anexia.com/).\r\n\r\nI also work on [Open Source Projects](https://nezhar.com/pages/projects) and write Articles and Tutorials on my [blog](https://nezhar.com/).\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Is my AI Recruiting biased? - How to evaluate these systems", "abstract": "AI recruiting systems are increasingly used to filter, rank, and select applicants at scale. Yet their deployment raises essential questions: How reliable are these models in real hiring environments, and how do we ensure fairness and safety across diverse applicant profiles? This talk presents a structured approach to testing and validating AI-driven recruiting pipelines. It highlights the role of synthetic test data, data augmentation, and fairness metrics in uncovering systemic risks and mitigating bias. Attendees will walk through a complete evaluation workflow. The session also incorporates insights from real-world testing practices, demonstrating how rigorous validation can increase trust and transparency in recruitment AI.", "full_description": "AI recruiting systems are rapidly reshaping talent acquisition by automating candidate filtering, ranking, and selection. However, their growing influence raises critical concerns around fairness, robustness, and decision transparency. This talk introduces a practical testing methodology for evaluating AI recruiting pipelines beyond traditional accuracy metrics.\r\n\r\nWe will examine how synthetic data and augmentation techniques can expose hidden weaknesses, improve coverage, and stress-test edge cases. The talk will address the role of proxy variables, why they matter, and how they can help uncover unintended model behavior. We will also explore fairness measurement strategies, including individual and group fairness metrics, and discuss how these approaches reveal structural bias in ranking and scoring outcomes.\r\n\r\nBecause parts of the evaluation process can be automated, the session will demonstrate how Python-based agents and LLM \u201creferees\u201d can assist in generating and augmenting CVs and certificates, validating predictions, and assessing explanation quality. This automation can accelerate workflows, increase reproducibility, and reduce human error.\r\n\r\nParticipants will walk through a complete testing pipeline, supported by insights from real-world projects that illustrate how different tools and strategies expose systemic risks and guide mitigation. Attendees will leave with practical techniques to make recruiting systems more reliable, transparent, and trustworthy in real deployment contexts.", "code": "3C9P9V", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/3C9P9V.png", "speaker_names": "Sebastian Krauss", "speakers": "\n### Sebastian Krauss\n\nMy name is Sebastian and I work as an AI Test Engineer at Validaitor. With a background in Mechatronics and Autonomous Systems, and hands-on experience at Bosch, Fraunhofer, and in international research settings, I focus on the intersection of AI trustworthiness and real-world deployment. My current work involves developing methods to test AI models for vulnerabilities, safety risks, and secure behavior - ensuring AI systems perform reliably and ethically. I like to share my experience with other techies all around the world. When I don't look into a screen, I like bouldering and books. :)\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Ethics & Privacy"}, {"title": "Catch the LLM if you Can: Watermarking LLMs", "abstract": "With Large Language Models (LLMs), generating high-quality text and images is easy and so is\r\nmisusing it. As AI-generated content becomes harder to distinguish from human generated content,\r\ndevelopers are increasingly asking: How can we verify whether a piece of text comes from an LLM?\r\nWe\u2019ll explore Python\u2019s simplicity and rich ecosystem of libraries to solve this problem.\r\n\r\nThis talk introduces the foundations of LLM watermarking and shows how developers can implement\r\nthese techniques entirely in Python. We\u2019ll discuss two core approaches, EXP sampling method and\r\nKGW method. We will walk through the implementation of the KGW method using simple,\r\ntransparent code, and compare it with the EXP approach. There's no need for a large model or a GPU\r\ncluster to understand how these systems work and the core ideas can be implemented in pure\r\nPython using simple code. The code repositories, which includes both methods will be provided so\r\nthat the attendees can follow along.\r\n\r\nAlong the way, we\u2019ll discuss the trade-offs and the limitations of current research. And for those\r\nwondering, \u201cDo I have to implement all this myself?\u201d, the talk concludes with a demo of MarkLLM, an\r\nexisting open-source toolkit that provides a unified Python interface for experimenting with\r\nwatermarking algorithms.\r\n\r\nAttendees will leave with a clear understanding of how watermarking works, when it\u2019s useful, and\r\nhow to integrate these techniques into real-world Python projects.", "full_description": "During the talk we will cover:\r\n1. Why Watermarking Matters?\r\n     - What can go wrong when AI-generated content becomes indistinguishable from human writing\r\n     -  Why provenance and transparency are becoming essential to trust and safety.\r\n2. How LLM Watermarking Works?\r\n     - What is a watermark and what isn't\r\n     - The core idea behind statistical watermarking\r\n3. Two Key Algorithms implemented using Python's established frameworks\r\n     - EXP Watermark: modifying logits with pseudo-random perturbations.\r\n     - KGW Green-List Watermark: partitioning tokens into \u201cgreen\u201d and \u201cred\u201d lists to bias sampling.\r\n     - Python code walkthrough of implementing the KGW method and comparing it with the EXP method.\r\n4.  How you can use MarkLLM (open-source toolkit)\r\n     - Generate and detect watermarked text.\r\n     - How to use the toolkit for experiments in your own workflows.\r\n5. Real-World Challenges and Limitations\r\n     - How robust and evasive are the current algorithms\r\n\r\nKey Takeaways:\r\n     - Watermarking is a promising tool for provenance.\r\n     - Developers can implement and test watermarking fully in Python.\r\n     - Understanding these methods helps build more transparent and trustworthy AI systems.\r\nThis talk is for people who:  \r\n   - Care about ethics and privacy in AI and want to understand what watermarking can (and cannot)  solve.\r\n   - Build applications using LLMs and want mechanisms for verifying or auditing generated text.\r\n   - Are ML researchers or hobbyists interested in how watermarking algorithms function at a technical level.\r\n   - Work in AI safety, trust & transparency, or responsible AI and need practical tools for content provenance.\r\n\r\nNote: No prior experience with LLM architecture is required, basic familiarity with probability is recommended; no advanced math needed.", "code": "3JLSEF", "state": "confirmed", "created": "2025-11-19", "social_card_image": "/static/media/social/talks/3JLSEF.png", "speaker_names": "Subhosri Basu", "speakers": "\n### Subhosri Basu\n\nI am a GenAI researcher at Fraunhofer Institute, Germany. Born in India, I decided to move to Germany in search of new challenges. My professional journey has been shaped by a passion to solve problems in various domains. Academically, I have graduated with a Master's degree from the department of electrical and computer science. My focus has always been around statistics. I have been able to work on projects related to artificial intelligence and deep learning, especially in the field of signal processing and imaging. With my experience, I want to guide the growth of next generation of ML researcher. When I am not working, you will find me exploring Europe.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Ethics & Privacy"}, {"title": "Embedding Data Science in IoT devices with MicroPython and emlearn", "abstract": "Python is the standard solution for many machine learning and data science applications,\r\nfrom large cloud systems, to workstations, and even on larger embedded or robotics systems.\r\nBut as we move down into more constrained environments regular (C)Python starts to be a less good fit.\r\nThe MicroPython project provides a Python implementation that is tailored for such environments,\r\nand this makes it possible scale down to microcontrollers with just a few megabytes of RAM (or less!).\r\nAs a bonus, MicroPython with WebAssembly also makes lightweight browser applications possible.\r\nIn this talk, we will discuss how to combine Internet of Things (IoT) hardware, MicroPython and browser to build stand-alone smart sensor systems and laboratory gear for physical data science.", "full_description": "Typical Internet of Things devices send off most of the data to an external cloud service for analysis.\r\nThis causes challenges both in terms privacy, poor reliability under poor connectivity, and loss-of-availability when the service is discontinued.\r\n\r\nWe would like to show that it is possible to achieve the majority of functionality using a local-first approach, including machine-learning based sensor-data analysis.\r\nAnd that this can done on low-cost microcontrollers such as ESP32.\r\n\r\nThis talk will cover how to build stand-alone devices for measuring and analying physical sensor data, using MicroPython. This includes these aspects:\r\n\r\n- Measuring the surroundings using sensors\r\n- Connectivity using WiFi\r\n- Data storage using on-board filesystem\r\n- Serving a webui for configuration/control, using Microdot\r\n- Automated data processing/analysis using DSP and ML, with emlearn-micropython\r\n- Enabling interactive data analysis via webui\r\n- Managing concurrency on microcontroller, using asyncio\r\n- Optional integration. Pull using HTTP, and/or push using Webhooks/MQTT\r\n\r\nThe sensor data will either be accelerometer, sound or images/video (To be Decided).\r\n\r\n### About MicroPython\r\n\r\nMicroPython is an implementation of Python that runs on practically all microcontrollers with 128kB+ RAM. It provides access to the microcontroller hardware, functions for interacting with sensors and external pheripherals, as well as connectivity options such as WiFi, Ethernet, Bluetooth Low Energy, etc.\r\n\r\nWhile MicroPython can target a very wide range of hardware, we will focus on the Espressif ESP32 family of devices. These are very powerful and affordable, with good WiFi+BLE connectivity support, good open-source toolchains, are very popular both among hobbyist and companies, and have many good ready-to-use hardware development kits.\r\n\r\n### About emlearn-micropython\r\n\r\nemlearn-micropython is Machine Learning and Digital Signal Processing package for MicroPython, built on top of the emlearn C library. It provides convenient and efficient MicroPython modules, and enables application developers to run efficient Machine Learning models on microcontroller, without having to touch any C code. Compared to pure-Python approaches, the emlearn-micropython models are typically 10-100x faster and smaller.\r\n\r\n### Intended audience and expected background\r\n\r\nIntended audience: Any developer or data scientist curious about sensor data processing, IoT, and how Python scales down to the smallest of devices.\r\n\r\nThe audience is expected to have a basic literacy in Python and proficiency in programming. \r\nFamiliarity with microcontrollers and embedded systems is of course an advantage, but the talk should be approachable to those who are new to this area. Familiarity with basic networking and web/browser concepts is an advantage.", "code": "3U3BZH", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/3U3BZH.png", "speaker_names": "Jon Nordby", "speakers": "\n### Jon Nordby\n\nJon is a Machine Learning Engineer specialized in IoT systems, with a Master in Data Science and a Bachelor in Electronics Engineering. He has been contributing to open-source software since 2010.\r\n\r\nThese days Jon is Head of Data Science at Soundsensing, a provider of monitoring solutions for HVAC systems in commercial buildings. He is also the maintainer of emlearn, an open-source Machine Learning library for microcontrollers.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Embedded Systems & Robotics"}, {"title": "It Works on My Machine: Why LLM Apps Fail Users (Not Tests)", "abstract": "LLM applications frequently pass tests but fail users in production. This talk examines the gap between evaluation metrics and user experience through three lenses: **Expectations** (what \"working\" means to users), **Functional** (system-level vs. component-level success), and **Operational** (real-world reliability).\r\n\r\nDrawing from production experience, we'll share scenarios of expectation mismatches, silent failures, and undetected drift\u2014plus practical strategies for bridging the gap. The core message: evaluation should answer whether your system serves users, not whether it passes tests.", "full_description": "You've deployed an LLM application. Your tests show that it's working. The metrics look good. Then a user says **it's broken.**\r\n\r\nThis happens more often than you would expect.\r\n\r\nIn this talk, we'll share our experience of building and maintaining LLM applications, and discuss what we've learned about the discrepancy between evaluation results and user experience.\r\n\r\nWe will explore three dimensions of evaluation through the lens of user experience:\r\n\r\n## Expectations: What does 'working' actually mean to your users?\r\n\r\nSometimes the gap between tests and reality comes down to expectations. Questions that seem obviously hard to users turn out to be easy for the LLM\u2014and vice versa. Understanding this mismatch is the first step to building systems that users actually trust.\r\n\r\n## Functional: Does the system do what it's supposed to do?\r\n\r\nWhen you're working with LLMs, individual components might pass tests while the whole system fails. With prompts, model parameters, evaluation criteria, metadata, and ever-growing datasets all interacting, the complexity compounds quickly.\r\n\r\n## Operational: Does it remain reliable in real-world conditions?\r\n\r\nIn this section, we'll share practical lessons from operating LLM applications in production: how we use observability tools like Opik to monitor model behavior, how telemetry helps us understand actual usage patterns, and how dedicated validation endpoints allow us to detect issues in on-premises deployments before users do.\r\n\r\nWe'll discuss real-life scenarios we've encountered, such as when users expected different results to those delivered by our system, when external changes affected the system silently, and when performance drifted in ways that our metrics didn't detect.\r\n\r\nThis isn't a talk about frameworks or tools (even though we'll mention a few). It's about the human element of evaluation: **ensuring that the system we built serves the people using it.**\r\n\r\nWhether you're just starting out with LLM applications or running them at scale, you'll probably recognize these scenarios. We'll share the strategies and patterns that we've developed, not as prescriptive rules, but as a starting point for your own approach.\r\n\r\n## Outline\r\n\r\n1. Why users report the LLM application is broken while it passes every test\r\n2. Three dimensions of the problem\r\n    * Expectations\r\n    * Functional\r\n    * Operational\r\n3. Real-life scenarios\r\n4. Our current strategies and patterns\r\n5. Evaluation = understanding if the system serves users, not proving it's good", "code": "3UHPZB", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/3UHPZB.png", "speaker_names": "Thomas Prexl, Frank Rust", "speakers": "\n### Thomas Prexl\n\nThomas builds LLM applications that create business impact. He co-founded neunzehn innovations GmbH to bring generative AI into companies that need it.\r\n\r\nBefore that, he ran startup support in Heidelberg\u2014designing accelerators, connecting founders with money and know-how, and launching events like Neurons & Neckar, Sensors & Data Hackathon, and Startup Weekend Rhein-Neckar. Earlier: marketing and business development in electrical engineering and diagnostics.\r\n\r\nHe studied at Mannheim, got his doctorate at Basel, teaches at both Heidelberg and Mannheim, and talks about AI when someone asks him to.\n\n### Frank Rust\n\nFrank is deeply passionate about technological advancements and a co-founder of neunzehn innovations, a company specializing in AI solutions. His professional background combines entrepreneurial experience\u2014having established an innovation and strategy consultancy focused on strategy and deep tech\u2014with several years at a major software corporation. Throughout his tenure in the software industry, he contributed to multiple product and service launches, working across various teams to bring new offerings to market. Outside the office, he enjoys discovering new horizons in the camper van.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "From Research Models to SLAs: Operationalizing TSFMs with Python", "abstract": "Time series foundation models (TSFMs) such as Chronos, Lag-Llama, TimesFM, and Siemens\u2019 own GTT have shown strong generalization capabilities across diverse forecasting tasks. However, integrating these models into a large organization is primarily a software engineering and MLOps challenge rather than a modeling one.\r\n\r\nIn this talk, we present a real-world case study based on Siemens KPI Forecast, a Python-based forecasting platform that operationalizes multiple TSFMs as reusable, production-grade services. The platform integrates both open research models and Siemens-developed models behind a unified API, supporting zero-shot inference, fine-tuning jobs, and fine-tuned inference depending on user needs and operational constraints.\r\n\r\nWe focus on how Python is used to compose heterogeneous components including open and closed-source models, internal data products, APIs, and orchestration layers into a consistent time series specialist user experience. The session also covers operating such services with clear SLAs in a B2B environment, including monitoring, versioning, and governance.\r\n\r\nAttendees will gain practical insights into turning TSFMs into reliable Python services that scale across teams and use cases.", "full_description": "### Motivation\r\n\r\nTime series foundation models promise rapid prototyping and strong performance across domains, but many teams struggle to move beyond notebooks and benchmarks. In practice, the hardest problems are not model accuracy or architecture, but integration, operability, and developer experience.\r\n\r\nThis talk addresses a common but under-discussed question:\r\n\r\nHow do you operationalize time series foundation models inside a large organization with real users, real constraints, and real SLAs?\r\n\r\n### Case study context\r\n\r\nThe talk is based on hands-on experience building and operating Siemens KPI Forecast, a Python-based forecasting platform that exposes multiple TSFMs through stable APIs. The platform integrates:\r\n\r\n- Chronos ([https://arxiv.org/abs/2308.16103](https://arxiv.org/abs/2403.07815))\r\n- Lag-Llama ([https://arxiv.org/abs/2310.08268](https://arxiv.org/abs/2310.08278))\r\n- TimesFM ([https://arxiv.org/abs/2310.10688](https://arxiv.org/pdf/2310.10688))\r\n- GTT, a Siemens-developed large-scale time series model (https://arxiv.org/pdf/2402.07570.pdf)\r\n\r\nChronos, Lag-Llama, and TimesFM are open-source research models, while GTT is a proprietary Siemens model. The platform is designed to treat both open and closed-source models uniformly from a developer and user perspective.\r\n\r\n### Topics covered\r\n\r\n- Why TSFMs are easy to prototype but hard to operationalize\r\n- Designing Python APIs that unify multiple foundation models\r\n- Supporting zero-shot inference, fine-tuning jobs, and fine-tuned inference in one system\r\n- Integrating open-source and proprietary models consistently\r\n- Making forecasting services accessible to different user personas\r\n- Operating ML services with SLAs in a B2B environment\r\n- Monitoring, versioning, and governance considerations\r\n\r\n### What attendees will learn\r\n\r\n- How to structure Python services around foundation models\r\n- How to avoid fragmentation when supporting multiple models and workflows\r\n- Practical MLOps patterns for operating ML services beyond notebooks\r\n- Lessons learned from running TSFMs at organizational scale\r\n\r\nThis session focuses on engineering and operational lessons that are broadly applicable to teams building Python-based ML platforms in both enterprise and open-source contexts. Model references are included for transparency; the talk focuses on system design and operational patterns rather than proprietary details.", "code": "3XDMXS", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/3XDMXS.png", "speaker_names": "Jeyashree Krishnan, Catarina Filipe", "speakers": "\n### Jeyashree Krishnan\n\nJeyashree Krishnan is a Senior Machine Learning Engineer at Siemens AG. Her work focuses on building and operationalizing scalable machine learning services, with an emphasis on foundation models and time series forecasting. She is also a Visiting Researcher at the Center for Computational Life Sciences, RWTH Aachen University.\n\n### Catarina Filipe\n\n\n", "domain_expertise": "Advanced", "python_skill": "Intermediate", "track": "MLOps & DevOps"}, {"title": "Building a Digital Self with Telegram Chat Fine-Tuning", "abstract": "Fine-tuning an LLM on five years of Telegram chat shows how real-world conversations challenge models: messy context, sarcasm, unbalanced data, and tokenization pitfalls. This talk covers dataset design, LoRA training, surprising behaviors, and what it means to build a \u201cdigital self\u201d responsibly.", "full_description": "Telegram has been my personal playground for experimenting with NLP and conversational AI. Over the past five years, I accumulated a private group chat with recurring inside jokes, emotional context, sarcastic replies, short one-liners, and messy multilingual threads \u2014 the exact opposite of ideal training data. This talk documents how I fine-tuned a Large Language Model on that chat history to create a chatbot that speaks in my style.\r\n\r\nInstead of focusing on already well-documented LoRA fine-tuning tutorials, this session shows the real challenges behind turning unstructured human communication into a usable dataset. I\u2019ll walk through how raw Telegram exports become prompt-response training pairs, why Instruct models refused to learn my dark humor, how tokenization mistakes destroyed training stability, and why a 30k dataset of private interactions is worse than it looks.\r\n\r\nAttendees will learn what works, what breaks, and what to avoid when training personality-driven LLMs. We\u2019ll look at surprising insights: the model reproducing one-word time messages, the loss explosion caused by inappropriate humor threads, and how shorter context windows produced more coherent imitation of my tone.\r\n\r\nThis talk is an honest exploration of the technical, ethical, and psychological implications of building a digital version of yourself.\r\n\r\nOutline:\r\n\r\nMotivation \u2014 Why attempt a digital self\r\nFrom early Telegram bots and failed Seq2Seq models to attention-based LLMs; the project driven by curiosity, not necessity.\r\n\r\nData extraction: Telegram \u2260 dataset\r\nHTML exports converted to JSONL; group chat chaos, multi-turn threads, and prompt\u2013response formatting to teach conversational flow.\r\n\r\nDataset engineering: real mistakes\r\nSarcasm, toxicity, and dark humor breaking models; overrepresented patterns (like time replies); why \u201cinclude everything\u201d distorts personality.\r\n\r\nModel selection\r\nTinyLlama hallucinations vs. Mistral\u2019s nuance; why Instruct models reject certain humor; switching to Mistral-7B-v0.1 to unlock tone.\r\n\r\nTraining strategy with LoRA\r\nQLoRA setup, tuning r/alpha/dropout, small batches with accumulation, avoiding catastrophic forgetting, and real GPU costs (L4 vs A100).\r\n\r\nTokenization: easiest way to fail\r\nPrompt+response concatenation with prompt masking; how token length and masking errors ruined runs; respecting max_length.\r\n\r\nInference and reality check\r\nTinyLlama\u2019s broken tone vs. Mistral capturing inside jokes; temperature shaping sarcasm; absurd, funhouse-mirror outputs.\r\n\r\nUnexpected lessons\r\nHumor spikes loss, time replies dominate, shorter threads learn better tone; imitation is sometimes distorted but compelling.\r\n\r\nEthics and psychological complexity\r\nPrivate data and consent, style vs identity, when the model sounds \u201ctoo close,\u201d and why curation is a moral decision.", "code": "7GAQTN", "state": "confirmed", "created": "2025-12-01", "social_card_image": "/static/media/social/talks/7GAQTN.png", "speaker_names": "Alessandro Romano", "speakers": "\n### Alessandro Romano\n\nI'm a data scientist with a strong foundation in software engineering and statistics. I work at the intersection of data and business, solving complex problems that don't always have a clear path \u2014 and that's exactly what I enjoy most.\r\n\r\nI focus on creating solutions that matter, always with the goal of bringing real value to the people who use them. Whether it's building models, writing clean code, or exploring new tools, I like to stay hands-on and close to the problem.\r\n\r\nOutside of work, I speak at conferences, teach, and advocate for better data practices. I enjoy sharing what I learn and helping others grow, just as much as I enjoy digging into a tough technical challenge.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "How to mix conda and pip without causing \u201cenvironmental\u201d damage", "abstract": "Ever mixed conda and pip and ended up with a broken conda environment, yet, swear it worked before? This talk explains why! Learn the difference between pip and conda, what happens when you mix them and how to combine them safely using the latest community developed tools and updates in conda.", "full_description": "Users frequently run `pip` inside their `conda` environments, sometimes successfully, sometimes with unintentional consequences. Confusing errors and broken environments often lead users to ask: when is it safe to use `pip` in a `conda` environment, and when is it not?\r\n\r\nIn this presentation I will answer this question. \r\n\r\nI will begin by discussing the differences between `pip` and `conda` (a question conda maintainers get asked a lot!), starting with the specific use-cases of both tools. \r\nThis will include an \u201cenlightenment\u201d moment: `pip` and `conda` solve slightly different problems, one is a Python package installer, the other is a language agnostic package and environment manager.\r\n\r\nI will then explain the differences between `.conda` packages, tarballs, and Python wheels, revealing how these format differences make interoperability difficult and mixing tools unreliable.\r\nUsers end up mixing `pip` and `conda` because sometimes the packaging ecosystem leaves them no other choice. Users often report, \"I tried installing a package  with `conda`, but it didn't work, so I ran `pip install` instead and it worked\u201d. This mixing, sadly, has consequences, which I refer to as \u201cenvironmental damage\u201d. \r\nI will highlight this damage in my talk. \r\n\r\n`pip` and `conda` are two separate ecosystems but over time many community efforts (most recent being `conda-pypi`),  have tried to improve interoperability. I will explain how the latest updates in `conda` along with the features in `conda-pypi` have now made it possible to `conda install` Python wheels from PyPI directly into `conda` environments. Thereby bringing us a step closer to better interoperability. \r\n\r\nI will conclude the presentation with best-practice recommendations for using `pip` and `conda` together. \r\nBy the end of this presentation, users will have learned when to use `pip`, when to use `conda`, why they are different and how to combine them safely.\r\n\r\n\r\nHere is a link to the conda-pypi repository on GitHub: https://github.com/conda-incubator/conda-pypi \r\n\r\nTime outline of the presentation:\r\n3 mins- self introduction and introduction to the topic (what to expect)\r\n5 mins- difference between pip and conda and their use cases\r\n10 mins- different package formats, problems with mixing pip and conda\r\n5 mins- wheels support feature in conda-pypi and updates in conda\r\n5 mins- how it helps users and best practices \r\n2 mins- closing remarks", "code": "7H9DF8", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/7H9DF8.png", "speaker_names": "Mahe Iram Khan", "speakers": "\n### Mahe Iram Khan\n\nI am a Software Engineer working at Anaconda. I have been working on the conda project for more than 3 years. My hobbies are crocheting, writing and cooking.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Programming & Software Engineering & Testing"}, {"title": "Offline Fallback for a Mobile LoRaWAN Gateway", "abstract": "LoRaWAN gateways typically depend on cloud-based network servers, creating a vulnerability during internet outages. This talk presents a hybrid solution: a Raspberry Pi-based mobile gateway that operates on The Things Stack Sandbox while simultaneously decoding all device messages locally.\r\n\r\nThe system leverages existing network infrastructure for broad coverage during normal operation, while maintaining full local data access when connectivity fails. This is particularly valuable for emergency response scenarios and remote monitoring where sensor data must remain available regardless of network conditions.\r\n\r\nThe implementation uses Python for gateway orchestration and API integration, while incorporating existing JavaScript libraries (`lora-packet` and device decoders) for LoRaWAN decryption and payload decoding. Data is stored locally in SQLite for reliability and easy access.", "full_description": "LoRaWAN (Long Range Wide Area Network) is widely used for IoT sensor deployments due to its long range and low power consumption. Operating at 868MHz across Europe, it's ideal for remote monitoring applications\u2014from water level sensors to asset tracking and personnel location systems. However, traditional LoRaWAN deployments rely on cloud-based network servers, making them vulnerable to internet outages.\r\n\r\n**The Challenge**\r\n\r\nWhile networks like The Things Stack provide good geographic coverage, gaps remain\u2014particularly in remote areas where emergency response units operate. A mobile gateway can close these gaps, but standard configurations still require internet connectivity. You could deploy a completely local network with your own network server, but this sacrifices the existing infrastructure's coverage. \r\n\r\n**The Solution**\r\n\r\nThis talk presents a hybrid architecture that combines cloud-based operation with local resilience. The system primarily operates through The Things Stack Sandbox, leveraging its network coverage. Simultaneously, a Raspberry Pi-based mobile gateway decodes all messages from your devices locally in parallel. During normal operation, you benefit from cloud features. When internet connectivity fails, your sensor data remains accessible locally on the gateway.\r\n\r\n**Technical Implementation**\r\n\r\nThe solution consists of:\r\n\r\n1. **Raspberry Pi Gateway**: Configured as a mobile LoRaWAN gateway for The Things Stack Sandbox, suitable for vehicle deployment\r\n2. **Session Key Management**: Python service retrieving session keys for your devices via The Things Stack API\r\n3. **Local Message Processing**: Real-time decryption and decoding of LoRaWAN messages without internet dependency\r\n4. **Data Storage**: SQLite-based local storage for reliable data persistence\r\n\r\n**Python and JavaScript Integration**\r\n\r\nThe core implementation uses Python for gateway orchestration, API integration, and data management. For LoRaWAN encryption/decryption and payload decoding, the system leverages existing JavaScript libraries\u2014specifically `lora-packet` and community-maintained device decoders. This talk demonstrates practical patterns for Python/JavaScript interoperability.\r\n\r\n**Real-World Context**\r\n\r\nDrawing from volunteer emergency response experience, this solution addresses operational requirements where sensor data must remain available regardless of infrastructure status. The system ensures continuity of critical information during incidents.\r\n\r\n**What You'll Learn**\r\n\r\n- Designing resilient edge computing architectures for IoT\r\n- Integrating Python with JavaScript libraries\r\n- LoRaWAN security fundamentals (session keys, encryption)\r\n- Building offline-first systems with SQLite\r\n- API integration with The Things Stack\r\n\r\n**Open Source**\r\n\r\nComplete implementation available on GitHub, providing a reproducible setup valuable for volunteer organizations, research projects, and scenarios requiring IoT infrastructure that remains operational during connectivity disruptions.\r\n\r\n**Target Audience**\r\n\r\nPython developers interested in IoT and edge computing. No prior LoRaWAN experience required.", "code": "7JXYKH", "state": "confirmed", "created": "2025-11-19", "social_card_image": "/static/media/social/talks/7JXYKH.png", "speaker_names": "Jannis L\u00fcbbe", "speakers": "\n### Jannis L\u00fcbbe\n\n2008 \r\nM.Sc. Physics and Computer Science at Osnabr\u00fcck University\r\n\r\n2012 \r\nPhD in Physics at Osnabr\u00fcck University\r\n\r\n2013 - now \r\nSensor Developer at ROSEN Group\r\n\r\n2020 - now \r\nVolunteer operative in the Federal Agency for Technical Relief (THW, Germany)\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Infrastructure - Hardware & Cloud"}, {"title": "To nest, or not to nest? Nested data types in Polars with big data", "abstract": "Do you find yourself weighing up the pros and cons of using nested types in the Polars library - pondering whether you should encode your variables in structures using lists, arrays or opt for a flat format without complex hierarchy? This talk focuses on the crucial design choices available, the performance implications, and how this impacts the logic of your queries, as well as code readability, when deciding how to implement your big data pipeline in Polars. The methods available for nested types in Polars have seen some significant additions over the last year, with powerful functionality, such as filtering and aggregation, released in the latest versions of the library. These provide much-needed shortcuts for queries interrogating complex nested structures that previously required sophisticated user-defined functions. It makes the use of nested types much easier and intuitive, but does this mean you should nest your data? Through practical examples you\u2019ll learn some guidelines to help you decide.", "full_description": "If you\u2019ve ever designed or used SQL databases in your data science projects perhaps you\u2019ve cringed at the lack of relational structure and data duplication in the design of big data storage and processing. On the other hand, if you\u2019ve spent any considerable time getting dirty with Polars\u2019 vectorized and columnar processing, you\u2019ll also know that this can be somewhat of a moot point. So why bother?\r\n\r\nOutline of the talk:\r\n\r\n5 minutes: Introduction & origin story. What are Polars nested types? How do they work? Why do they matter?\r\n5 minutes: Back to the future. Advanced queries on nested types, past & present.\r\n5 minutes: Query structure - \u201cGroup by\u201d forever baby, versus element-wise.\r\n5 minutes: Storage comparison and the gigabyte scrooge - how a miser decides on a nested Polars structure.\r\n5 minutes: Time is money \u2013 How performance stacks up.\r\n5 minutes: Q&A\r\n\r\nBy the end of the talk, participants will have seen several straightforward examples, as well more advanced illustrations of nested structures in Polars using real-world data. They will be able to identify some key considerations informing their use of nested structures, including query logic, storage and performance.", "code": "7PNT37", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/7PNT37.png", "speaker_names": "Daniel Finnan", "speakers": "\n### Daniel Finnan\n\nDaniel Finnan is a 2nd year PhD candidate at the Lirsa laboratory, Conservatoire national des arts et m\u00e9tiers (CNAM), in Paris. His thesis focuses on decentralized finance, specifically decentralized exchanges, applying a quantitative methodology using blockchain data, techniques in data science, and time series econometrics. He codes in Python, R, and occasionally Rust and JavaScript, specifically using Python to manage data pipelines. He has a professional certification in full-stack development and holds a Master\u2019s degree in Economics, with a specialization in Economic, Digital and Data strategies from CNAM\u2019s department of Economics, Finance, Insurance and Banking.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Sentinel Values in Python: Semantics, Double Dispatch, and the Limits of Typing", "abstract": "Python relies heavily on special values such as `None`, `NotImplemented`, `Ellipsis`, and `dataclasses.MISSING`. These values are not incidental: they encode language semantics, enable control flow between objects, and shape API design.\r\n\r\nThis talk examines sentinel values as a first-class concept in Python. We will look at why None is often the wrong representation for absence, how NotImplemented enables double dispatch in rich comparisons, and where sentinel values appear throughout the standard library.\r\n\r\nA central focus is typing. While sentinel values are ubiquitous at runtime, Python currently has no standardized way to express them precisely in type hints. We will examine why Optional, overloads, and Literal fall short, what limited narrowing is possible today, and why creating a \u201creal\u201d custom sentinel with reliable type narrowing is still unsolved.\r\n\r\nFinally, we will discuss [PEP 661](https://peps.python.org/pep-0661/), i.e., the deferred proposal to standardize sentinel values and their typing semantics, and what its deferral means in practice. Using real-world examples, including Pydantic\u2019s experimental missing concept, this talk provides a clear mental model for sentinel values and realistic guidance for using them in typed Python codebases today.", "full_description": "Sentinel values are a fundamental but under-documented part of Python\u2019s design. They are used to represent absence, unsupported operations, incomplete state, and to coordinate control flow between objects. Yet, they are often treated as ad-hoc implementation details.\r\n\r\nThis talk starts by clarifying what sentinel values are and why None is frequently semantically overloaded and incorrect for modelling \u201cmissing\u201d or \u201cunset\u201d values. We then examine built-in sentinels such as `NotImplemented`, `Ellipsis`, and `dataclasses.MISSING`, with a detailed look at how `NotImplemented` enables double dispatch in equality and ordering operations.\r\n\r\nThe second half of the talk focuses on typing, where sentinel values expose fundamental tensions between Python\u2019s dynamic semantics and static type systems. We will discuss:\r\n\r\n* why Optional[T] does not mean \u201cunset\u201d\r\n* why Literal appears attractive for sentinels but rarely works in practice\r\n* what limited type narrowing is possible today and under which assumptions\r\n* why a fully reliable, user-defined sentinel with correct narrowing is currently not achievable in a portable way\r\n\r\nTo ground this in practice, we will look at real-world patterns used in production code, including Pydantic\u2019s experimental missing concept, and explain the trade-offs these designs make.\r\n\r\nFinally, we will examine [PEP 661](https://peps.python.org/pep-0661/), the proposal to standardize sentinel values and their typing semantics. We will explain what it would solve, why it was deferred, and what that deferral means for library and API authors today.\r\n\r\nThe talk concludes with concrete, honest guidelines: when sentinel values are the right tool, how to design APIs around them, and how to communicate absence clearly in typed Python code without pretending the type system can do more than it currently can.", "code": "88TTRY", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/88TTRY.png", "speaker_names": "Florian Wilhelm", "speakers": "\n### Florian Wilhelm\n\nFlorian is Head of Data Science & Mathematical Modeling at inovex GmbH, an IT project center driven by innovation and quality, focusing its services on \u2018Digital Transformation\u2019. He holds a PhD in mathematics, has more than 10 years of experience in predictive & prescriptive analytics use-cases and likes everything math \ud83e\udd2f\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Octopus AutoML: Extracting Signal from Small and High-Dimensional Data", "abstract": "Many machine learning tools assume abundant, independent data, rely on a single data split plus cross-validation, and leave test-set separation to the user.\r\n\r\nIn application-driven domains such as industrial materials science and pharmaceutical development, data are scarce, high-dimensional, and often correlated, creating conditions under which standard ML pipelines frequently fail. Small datasets are highly sensitive to the random seed used for splitting, and common pitfalls such as feature selection before splitting or distributing correlated samples across train and test sets cause data leakage and inflated performance metrics.\r\n\r\nOctopus is an open-source Python AutoML library explicitly designed for small-data, high-dimensional regime. It enforces strict nested cross-validation for model and hyperparameter selection, quantifies performance variability across multiple splits, and tightly controls data leakage. Its modular architecture embeds an internal ML engine, several feature selection methods (e.g., MRMR, Boruta), and external AutoML solutions such as AutoGluon into a unified, rigorous validation framework, enabling systematic and fair comparison of methods on limited data. In addition, Octopus supports survival analysis, addressing time-to-event problems common in healthcare and materials science. This talk will use realistic small-scale datasets to illustrate how conventional pipelines can be misleading and how to obtain more reliable models when every sample matters.", "full_description": "Many machine learning tools are based on the quiet assumption that data is plentiful, independent, and identically distributed, and that a random training/testing split, plus a little cross-validation, is \u201cgood enough\u201d. In application-driven domains such as pharmaceutical development and industrial materials science, however, this is often not the case. Synthesizing a new compound can take months and early phase clinical trials are small, so we often work with fewer than 1,000 samples and several thousands of features. In this context, standard AutoML practice can be dangerously optimistic.\r\n\r\nOn small datasets, performance can vary significantly depending on the random seed used for splitting the data. Working with a single split exposes us to this randomness: with an unlucky seed we might prematurely abandon promising experiments, while a particularly favorable seed can lead to overestimating the true performance. Another major risk is data leakage, such as performing feature selection before splitting the data, or distributing correlated samples (e.g., repeated measurements from the same patient or material batch) across both training and test sets. Such leakage inflates evaluation metrics and produces models that fail to generalize to new data.\r\n\r\nOctopus is an open-source Python AutoML library designed specifically for small and high-dimensional datasets. Its core idea is simple: make statistically honest evaluation the default. Octopus enforces strict nested cross-validation, with an inner loop for model and hyperparameter selection and an outer loop that provides generalization performance estimates. Thanks to this nested setup, users also obtain an estimate of how much performance varies across multiple data splits; low variation increases trust in the reported results. Furthermore, because Octopus handles the entire data-splitting process and is carefully designed to avoid information leakage, the reported metrics are far less likely to be inflated.\r\n\r\nOur library provides a robust drop-in replacement for existing machine learning workflows, ensuring a principled implementation of nested cross-validation while leveraging advanced machine learning techniques in the background. Adopting a modular architecture, the library offers a dedicated, internally developed ML module, seamless integration of several feature selection methods (e.g., MRMR, Boruta), and support for external ML solutions such as AutoGluon. This modular design makes Octopus a powerful platform for benchmarking different methods and solutions on specific datasets and use cases, helping users systematically compare and select the most suitable approach for their problem\r\n\r\nOctopus also supports time-to-event (survival) problems, which are common healthcare (e.g. time to progression or death) and in materials science (e.g. time to failure or degradation). Survival models are evaluated using appropriate metrics within the same nested cross-validation framework. \r\n\r\nThis talk will demonstrate, using realistic small-scale datasets, how standard AutoML pipelines can report deceptively strong performance and how these metrics change when proper nested cross-validation and domain-aware splits are applied. Attendees will learn where typical mistakes originate and how Octopus establishes practical safeguards against them. The goal is straightforward: to produce better models and more reliable conclusions when data are scarce and every sample matters.", "code": "8YTYEN", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/8YTYEN.png", "speaker_names": "Nils Haase, Andreas Wurl", "speakers": "\n### Nils Haase\n\nNils is Lead Data Scientist at Merck KGaA, Darmstadt, Germany, where he builds and productionizes machine learning solutions in Python. He earned his PhD in Physics from Universit\u00e4t Augsburg and has his background in R&D and material development. This path allows him to bridge domain-heavy lab and engineering problems with modern ML tooling, turning complex industrial data into robust, deployable systems.\n\n### Andreas Wurl\n\nLead Data Scientist at Merck Healthcare KGaA \r\nClinical Measurement Sciences, Biomarker development\r\n\r\n\r\nsee Linkedin\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Ship Data with Confidence: Declarative Validation for PySpark & Pandas", "abstract": "Tired of data quality issues crashing your PySpark and Pandas pipelines? This talk introduces [dataframe-expectations](https://github.com/getyourguide/dataframe-expectations), a lightweight, open-source library for declarative data validation. We will dive into the library's design and demonstrate how to easily define and apply data quality expectations to catch errors early, reduce debugging time, and ship more reliable data products, faster. Learn to build more robust data pipelines and move from reactive problem-solving to proactive data validation.", "full_description": "This session introduces a practical, open-source solution to a critical challenge facing data engineers and scientists: how to proactively guarantee data quality. In today's fast-paced development cycles, data pipelines are increasingly complex and reliant on numerous upstream sources, elevating the risk of data quality issues that have the potential to cause production failures. While monitoring and alerting systems are essential for flagging these failures, they are fundamentally reactive; their value is entirely dependent on the quality and coverage of the underlying validation logic that engineers must build and maintain. The true goal is to shift from reactive clean-up to proactive prevention. This talk demonstrates a more effective approach: stopping bad data from ever reaching production by embedding clear, declarative validation directly into your data pipelines. This provides immediate visibility into errors, allowing you to catch and fix data quality issues at the earliest possible stage of development.\r\n\r\n[dataframe-expectations](https://github.com/getyourguide/dataframe-expectations) is an attempt to address this problem through a lightweight, open-source Python library designed for declarative data validation in both PySpark and Pandas. This session will explore the key design choices behind its implementation and architecture, including its lightweight nature, which ensures the library doesn't become a bottleneck by impacting CI/CD run times or bloating container image sizes, making it ideal for data pipelines, unit tests and end-to-end tests alike. Through examples, we will walk through its fluent, chainable API and showcase its extensive list of reusable, parameterized expectations. We will then dive into advanced features, including powerful decorator-based validation that seamlessly integrates quality checks into your existing code, and a flexible tag-based filtering system that allows you to dynamically decide which expectations to run at runtime.\r\n\r\nAttendees will leave with a clear, actionable strategy for integrating declarative data quality checks into their pipelines, understanding how a simple, extensible tool can dramatically increase the reliability of their data products and, ultimately, their development velocity.", "code": "93SXWY", "state": "confirmed", "created": "2025-12-08", "social_card_image": "/static/media/social/talks/93SXWY.png", "speaker_names": "Ryan Sequeira", "speakers": "\n### Ryan Sequeira\n\nAs a Data Scientist on the Traveler Data Products team at GetYourGuide, I have spent the last 4 years developing and refining the ranking and relevance systems that power one of the world's leading travel experience platforms. My work is focused on enhancing the traveler's journey, helping millions discover and book their ideal experiences through data-driven solutions.\r\n\r\nMy path to data science is built on a foundation of diverse technical experience. I began my career in 2013 as a backend developer in Pune, India, before pursuing a Master's in Computer Science at the Indian Institute of Technology Patna, where I specialised in Network Science. Following my studies, I continued at the institute for two years as a research assistant, further honing my expertise in Network Science, which paved my way into the field of data science.\r\n\r\nIn 2021, I relocated to Berlin to join GetYourGuide, where I apply my software engineering background and machine learning skills to solve real-world problems at scale. This blend of backend development experience, academic research, and industry application gives me a unique perspective on building robust, production-ready data solutions.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "When Space Weather Breaks Your GPS: Building an Explainable Early Warning System", "abstract": "Have you ever happened to use GPS and realised that it is not working properly? The Sun could be responsible.\r\n\r\nIn this talk, I present a **real-world machine learning forecasting system** designed to predict a Space Weather phenomenon affecting GNSS accuracy and radio communications. The system is based on **CatBoost** and integrates data from space- and ground-based observations. **SHAP** is used to debug model behaviour and to build trust in model outputs. The talk focuses on **model design and evaluation choices**, showing how interpretability and uncertainty-aware forecasting can be combined in a real-time operational pipeline.", "full_description": "**Space Weather** doesn\u2019t just produce beautiful auroras: it can silently disrupt navigation systems, radio links, and satellite-based technologies we rely on every day.\r\n\r\nTravelling Ionospheric Disturbances (TIDs) are wave-like structures in the ionosphere that affect GNSS accuracy and HF communications. From an ML perspective, forecasting TIDs is a challenging rare-event prediction problem involving imbalanced data and heterogeneous physical inputs.\r\n\r\nIn this talk, I will present an operational machine learning approach developed within the T-FORS project to forecast TID occurrence over Europe. The model is built using **CatBoost** and integrates data from space- and ground-based observations.\r\n\r\nThe talk focuses on **model design and evaluation choices**. In particular, I will show how **SHAP** can be used to debug model behaviour, validate feature relevance, and build trust in predictions in a high-risk operational context.\r\n\r\nAlong the way, I\u2019ll share practical engineering lessons on:\r\n- handling class imbalance,\r\n- incorporating domain knowledge into ML pipelines,\r\n- producing **uncertainty-aware outputs** via **Conformal Prediction**, and\r\n- running **interpretable models in real-time forecasting systems**.\r\n\r\nThe talk is aimed at data scientists and ML practitioners interested in applied forecasting, interpretable models, uncertainty quantification and ML at the boundary between data and physics.\r\n\r\n---\r\n\r\n**Talk outline**\r\n- 0-4: What is Space Weather and why should we care\r\n- 4-7: Framing TID forecasting as an ML problem\r\n- 7-10: Model design with CatBoost\r\n- 10-13: Explainability with SHAP\r\n- 13-18: Uncertainty quantification with Conformal Prediction\r\n- 18-22: Cost-sensitive learning and real-time operations\r\n- 22-25: Lessons learned\r\n- 25-30: Q&A", "code": "99UMEL", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/99UMEL.png", "speaker_names": "Vincenzo Ventriglia", "speakers": "\n### Vincenzo Ventriglia\n\nA results-driven data professional, focused on hype-free solutions tailored to business needs.\r\n\r\nI currently create value at the **National Institute of Geophysics and Volcanology**, where I develop machine learning models in the **Space Weather** domain. My work is complemented by finding the hidden stories in data and make them accessible to stakeholders. I studied Physics in Italy (Napoli) and Germany (Frankfurt am Main), previously worked in Analytics within the strategic division of the world's largest professional services network, as well as in the Data Science department of Italy\u2019s leading publishing group.\r\n\r\nI am also an organiser of **PyData Roma Capitale**, actively involved in building the local Python and data science community. Outside of work, I enjoy theatre, discussing finance, and learning new languages.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Empowering Data Scientists with Zero Platform Friction: Deploying Streamlit & Friends in 3 Minutes", "abstract": "A data scientist builds a Streamlit or Dash prototype, the business wants to validate it, and the hard parts begin: getting access to live data, making the app available company-wide, and ensuring every user only sees what they are allowed to see. Following \"best practices\" turn a simple demo into weeks of platform work, leaving data scientists frustrated and blocking them from shipping apps to end users.\r\n\r\nIn this talk we will **live-demo** Merck's self-service app service we have developed and hardened over multiple years. It lets **teams deploy Streamlit (and friends) in 3 minutes** while meeting best practices like SSO, CI/CD, and governed data access control. The platform has become essential for Merck to ship data apps at scale: in 2025 it powered **750+ active apps** reaching **8,000+ unique end users**.\r\n\r\n**Under the hood, we show:** how a use-case based access model enables scoped resource permissions so apps can safely access data on-behalf of the user. We also show starter templates that generate a deployable Git repo with example pages (e.g. Snowflake access or internal LLM chatbot). Finally, we cover the guardrails needed to operate this safely.\r\n\r\n**What you will learn:** a cost-effective reference architecture based on AWS that you can adapt to your hyperscaler or platform, practical patterns for balancing the trade-off between central control and decentral freedom, and how templates and CI/CD help teams iterate quickly without compromising security or reliability.", "full_description": "This session is for anyone who has built a Streamlit (or Dash, R Shiny, FastAPI, React) prototype and then hit the wall when it needed to be shared with real users: access to live data, SSO, permissioning, deployment, and operational guardrails.\r\n\r\nWe will present the workflow and the architecture from both sides: as a data scientist shipping an app, and as a platform admin operating the service safely at scale.\r\n\r\n## What we will demo\r\nWe will demo the end-to-end workflow from zero to a running app using our internal app service. The platform includes a web console for self-service provisioning and configuration and the deployment runtime managing the state of the application.\r\n\r\n- Using the web console to create and configure a new app from a framework template (Streamlit, Dash, R Shiny, FastAPI, React).\r\n- How a Git repository is created and the first version is deployed behind the scenes, including a working starter app with example pages.\r\n\r\n## Key design decisions (the parts that are usually hard)\r\n- Identity propagation: the app receives the signed-in user identity from SSO and uses it for downstream authorization.\r\n- Authorization at the data layer: dataset permissions are scoped to use-case resource, making sure tokens can not be exploited.\r\n- Safe multi-tenancy: per-app isolation plus resource limits to prevent noisy-neighbor problems.\r\n- Repeatable delivery: templates plus CI/CD conventions so a new app starts from a working, deployable baseline.\r\n- Day-2 operations: guardrails like quotas, rate limiting, and idle shutdown to keep the platform reliable and cheap.\r\n\r\n## Running at scale\r\n- Production usage: 750+ active apps and 8k+ unique end users (2025).\r\n- Infrastructure run rate under 10k USD per month (excluding engineering time).\r\n\r\n## Who should attend\r\n- Data scientists and analysts who want to ship apps beyond a demo.\r\n- Data platform and DevOps engineers building self-service tooling for governed environments.\r\n- Teams standardizing how internal data & AI products are delivered to business users.\r\n\r\n## Takeaways\r\n- For data scientists: what a good internal app hosting platform should provide, and which requirements you should ask your platform team for (governed on-behalf of data access, templates, CI/CD, guardrails).\r\n- For platform teams: a blueprint you can adapt beyond AWS, including the architecture and tradeoffs necessary to operate fine-grained authorization and a multi-tenant runtime at scale.\r\n\r\n**If you do not have such an app platform in your company yet, use this talk as a checklist to start the conversation with your IT or platform teams. :-)**", "code": "9MUDUY", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/9MUDUY.png", "speaker_names": "Bernhard Sch\u00e4fer, Nicolas Renkamp", "speakers": "\n### Bernhard Sch\u00e4fer\n\nBernhard is a Senior Data Scientist at Merck with a PhD in deep learning and over 7 years of experience in applying data science and data engineering within different industries. For more information you can connect with him on LinkedIn. \ud83d\ude42\n\n### Nicolas Renkamp\n\nAs the Global Head of Platform Products Portfolio, Nicolas leads high performing teams that design, implement and maintain Merck's global data, analytics and AI ecosystem UPTIMIZE.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "MLOps & DevOps"}, {"title": "Securing AI Agentic Systems: Enforcing Safety Constraints in AI Agent", "abstract": "AI agents are increasingly deployed with autonomy: calling tools, accessing data, modifying systems, and making decisions without human supervision. While prompts and guardrails are often presented as safety solutions, they break down quickly in real-world agentic systems.\r\n\r\nIn this talk, we explore how to enforce safety constraints in AI agents beyond prompting, using engineering techniques familiar to Python developers and data engineers. We will examine common failure modes in agentic systems such as tool misuse, goal drift, and over-permissioning and show how to mitigate them using policy layers, capability boundaries, and execution-time validation.", "full_description": "AI agents are increasingly used as autonomous systems that can call tools, access data, and take actions in real environments. As these systems gain more autonomy, ensuring their safe and predictable behavior becomes an engineering challenge rather than a prompting problem.\r\n\r\nThis talk examines how safety constraints can be explicitly enforced in agentic AI systems, instead of relying solely on natural language instructions or model alignment. We will discuss typical safety and security issues that arise in agent based architectures, including over permissioned tools, unintended action chains, goal drift, and unsafe retries.\r\n\r\nUsing practical Python examples, the talk introduces architectural patterns for constraining agent behavior, such as policy layers, capability based tool access, action budgets, and runtime validation of agent decisions before execution. We will also explore how human in the loop checkpoints and audit logging can be integrated into agent workflows to support safer operation in production environments.\r\n\r\nThe focus of this session is on practical design and implementation techniques that help developers build AI agents with clearly defined boundaries, making their behavior more controllable, observable, and secure\r\n\r\nThrough practical Python examples, we will demonstrate how to:\r\n\r\n- Design constrained agent architectures\r\n- Enforce tool level permissions and action budgets\r\n- Validate and block unsafe agent actions at runtime\r\n- Combine human-in-the-loop checkpoints with automated controls", "code": "9PBYAP", "state": "confirmed", "created": "2026-01-02", "social_card_image": "/static/media/social/talks/9PBYAP.png", "speaker_names": "John Robert", "speakers": "\n### John Robert\n\nJohn Robert leads data and cloud projects at Sunnic Lighthouse (Enerparc AG), where he works on building and operating data-intensive workflows in production. He has over eight years of experience with Python, machine learning, and AI, and began his career working on autonomous driving systems at Daimler (Mercedes-Benz).\r\n\r\nJohn has spoken at conferences across Europe, the United States, and other regions, sharing practical insights on building, deploying, and operating AI systems in real-world environments. His current focus is on AI safety and AI security, particularly how agentic and autonomous systems can be designed with clear boundaries and controls.\r\n\r\nHe is the founder of Don\u2019t Fear AI, an initiative aimed at helping people understand how to use AI responsibly and how to build reliable AI systems without hype or unnecessary complexity. John believes in a future where humans and AI systems work together safely and effectively.\r\n\r\nOutside of technology, John enjoys traveling and has visited nearly 50 countries.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Security"}, {"title": "A minimalist introduction to Ansible", "abstract": "[Ansible](https://docs.ansible.com/) is a popular [infrastructure as code](https://en.wikipedia.org/wiki/Infrastructure_as_code) tool for server configuration and software deployment. This tutorial will cover things that I wish the first day that I started using Ansible to manage the projects at my work.", "full_description": "[Ansible](https://docs.ansible.com/) is a popular Python package for declarative configuration of servers that includes batteries (for example, encrypted vault for secrets and Jinja template engine). As a Swiss Army knife, Ansible is capable of solving my problems but come with many features that novices will not know how to use. This tutorial is hands-on and will guide attendees to learn the core features of Ansible. Attendees must have Podman or Docker installed in the machine they will use during the tutorial.", "code": "9ZKYRD", "state": "confirmed", "created": "2025-12-10", "social_card_image": "/static/media/social/talks/9ZKYRD.png", "speaker_names": "Raniere Silva", "speakers": "\n### Raniere Silva\n\nI'm a Research Software Engineer helping social scientists to have their work reproducible. I'm a former The Carpentries instructor and content creator.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "MLOps & DevOps"}, {"title": "Simplicity Scales: Rewriting to a Django Monolith and Monorepo", "abstract": "Simplicity scales better than complexity. In this talk we share what we learned from a year-long refactor of our Python-based infrastructure where we majorly improved developer velocity and overall developer happiness with two choices: moving everything into a monorepo and replacing our microservices architecture with a Django monolith. \r\nInstead of going deep on any single technology, we offer a holistic view of how these decisions enabled a multi-disciplinary team to move faster on a shared codebase. We'll introduce a blueprint for a uv-based Python monorepo, discuss why we chose \"boring\" tools over custom solutions, and share the metrics we used to measure success. The metrics dashboard will be open-sourced as part of this talk.", "full_description": "When working on older codebases most developers encounter the question of \"Should we fix this or rewrite it completely?\". Weeklong releases, multi-day bug hunts and a very obvious impact of tech debt on developer happiness and development velocity led us to ask that question in a very general way. We were wondering how a better approach to a Python-based infrastructure could look when working in a multi-disciplinary startup environment. We believe that many developers have been in a similar situation and we would like to introduce our holistic take on a when and how to refactor older codebases.\r\n\r\nOur solution to these problems consists of two main changes: moving the entire code of various teams in a uv-based monorepo and questioning a lot of technical decisions of the past under the paradigm of \"Can this be done simpler or can we rephrase the problem to solve it with an existing technical solution?\".  We will share our insights into how a multi-language monorepo approach can work at a startup where full-stack and ML practitioners work on the same code base. This includes going over standardized procedures (e.g. code quality) that are shared between all teams.\r\n\r\nFurthermore we will discuss some of the high-level decisions and introduce our reasoning within the available options of the python ecosystem. This includes for example why for us a monolithic approach based on Django works better than a Flask based microservice solution.\r\n\r\nThe goal of this talk is to give the listener an introduction to our solutions and make it easy to draw parallels to their own situation or problems. Attendees will leave with an insight into our decision framework and we will show what metrics we used to validate the success of our refactoring and technical choices. A simple list of metrics and a dashboard will be open-sourced as part of this talk to give interested listeners a starting point with their own \"refactoring\" journey.", "code": "AAY8KQ", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/AAY8KQ.png", "speaker_names": "Bruno Vollmer", "speakers": "\n### Bruno Vollmer\n\nHey I\u2019m Bruno, I\u2019m an experienced Senior Software Engineer passionate about building innovative solutions in fast-paced and evolving startup environments.\r\n\r\nI consider myself a generalist when it comes to software engineering and I enjoy working at the intersection of engineering leadership and software development. Currently I\u2019m working as a team lead in a startup in Lausanne. My team is mainly responsible for building and scaling the main platform of our solution. Next to this I\u2019m responsible for the overall software architecture of our solution.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Programming & Software Engineering & Testing"}, {"title": "SQL is Dead, Long Live SQL: Engineering reliable analytics agent from scratch", "abstract": "Is it still worth learning SQL in 2026, or can we just \"chat\" with our data? This hands-on tutorial explores that exact question by pushing Text-to-SQL to its absolute limits. This won't be just happy paths; we will deliberately expose where LLMs fail : ambiguity, hallucinations, and \"dirty\" data...and build the engineering stack required to fix them!\r\n\r\nYou will build a local data Agent from scratch using DuckDB, MCP and a minimalist semantic layer. By the end, you will understand the hard boundaries of AI reasoning, how a semantic layer acts as a safety net, and why knowing SQL is still (since 1974) the most critical skill for building reliable analytics agents.", "full_description": "This session is a \"reality check\" for AI analytics. We combine theory with engineering to answer one question: Where are the limits of Text-to-SQL? Participants will experience the frustration of a hallucinating LLMs and the satisfaction of fixing it with a realistic minimalist local setup.\r\n\r\nLearning objectives:\r\n1. Map the limits: Identify exactly where LLMs break (e.g., complex joins, specific business logic, non-standard schemas).\r\n2. Bridge the gap: Learn how a semantic layer translates fuzzy English into deterministic SQL.\r\n3. Modern architecture: Overview and hands-on on DuckDB Model Context Protocol (MCP) to give agents standard, safe tools to do analytics.\r\n4. The verdict: Understand why SQL is becoming the \"Assembly Language\" of the AI era, and why you still need to be fluent in it and what is still missing to just \"chat with our data\".\r\n\r\nPrerequisites:\r\n- Laptop with Python 3.10+.\r\n- Beginner SQL knowledge (joins, aggregations).\r\n- No prior AI/LLM experience required.", "code": "AF9DNH", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/AF9DNH.png", "speaker_names": "Mehdi Ouazza", "speakers": "\n### Mehdi Ouazza\n\nI started my career in data 10+ years ago as a data engineer, working in large corporates like AXA setting up on-prem Spark clusters (yes, that old!) to tech unicorns building data platforms in the cloud at Klarna, Back Market, and Trade Republic.\r\n\r\nOver the years, I found a passion for sharing what I learned and teaching others. It became my full-time job when I joined as the first DevRel at MotherDuck (DuckDB in the cloud) in 2023.\r\n\r\nI believe learning should be fun. I enjoy making complex topics more approachable through storytelling and creativity.\r\n\r\nI want to keep teaching curious students (in-person and online) and help the next generation learn not just data, but software engineering in this post-AI world.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Data Handling & Data Engineering"}, {"title": "Array-Oriented Programming in Python: Libraries, Techniques, and Trade-offs", "abstract": "Python has become the dominant language for scientific computing and data science, largely due to powerful array libraries that enable high-performance numerical computation. This tutorial introduces array-oriented programming as a paradigm and surveys the modern Python array ecosystem.\r\n\r\nWe'll explore when and how to use different array libraries: NumPy for general-purpose array operations, JAX for automatic differentiation, just-in-time compilation of array-oriented code, and GPU acceleration, Numba for just-in-time compilation of imperative code, and Awkward Array for nested and irregular data structures. Through live demos, we'll show how to think in arrays, discuss the limitations of array-oriented programming, and demonstrate how JIT compilation addresses these challenges.\r\n\r\nWhether you're analyzing data, building machine learning models, or doing scientific simulations, understanding the strengths and trade-offs of each library will help you choose the right tool for your problem.", "full_description": "## Overview\r\n\r\nPython's dominance in scientific computing and data science stems from its powerful array libraries that enable high-performance numerical computation. This 90-minute tutorial introduces array-oriented programming as a paradigm and surveys the modern Python array ecosystem, helping you understand which tools to use and when.\r\n\r\n## What is Array-Oriented Programming?\r\n\r\nArray-oriented programming is a paradigm that separates problems into lightweight Python bookkeeping and heavy numerical computation handled by vectorized operations in fast, precompiled libraries. We'll demonstrate how this approach combines Python's ease of use with near-compiled-language performance.\r\n\r\nThrough live examples, you'll see how array operations can be orders of magnitude faster than explicit loops. This mindset shift\u2014thinking about operations on entire arrays rather than individual elements\u2014is fundamental to effective scientific Python programming.\r\n\r\n## The Array Library Landscape\r\n\r\nWe'll survey the modern Python array ecosystem and when to use each tool:\r\n\r\n- **NumPy**: The foundation for general-purpose array operations\r\n- **Numba & JAX**: JIT compilation approaches\u2014when and why to use each\r\n- **Awkward Array**: Handling nested and ragged data structures\r\n- **Large dataset tools**: Brief overview of Dask, Xarray, Zarr, and Blosc2 for distributed computing, labeled arrays, and compression\r\n\r\nWe'll demonstrate the strengths and limitations of each through live coding examples, showing trade-offs between different approaches.\r\n\r\n## Understanding Limitations and Trade-offs\r\n\r\nA critical part of choosing the right tool is understanding when array-oriented programming has limitations. We'll discuss challenges like intermediate array overhead and algorithms that don't naturally vectorize, and show how different libraries address these problems.\r\n\r\n## What You'll Learn\r\n\r\nBy the end of this tutorial, you will:\r\n\r\n1. **Understand array-oriented programming** as a paradigm and how it differs from imperative programming\r\n2. **Know which library to choose** for different problems: NumPy vs. Numba vs. JAX vs. specialized tools\r\n3. **Recognize when array-oriented approaches have limitations** and how to address them with JIT compilation\r\n4. **Handle non-rectilinear data** using libraries like Awkward Array\r\n5. **Work with large datasets** using chunking, compression, and labeled arrays\r\n6. **Write more performant Python code** by applying array-oriented thinking to your own problems\r\n\r\n## Prerequisites\r\n\r\nFamiliarity with Python (loops, functions, if statements) and basic NumPy exposure (what an array is and how to use it). No deep expertise required.\r\n\r\n## Target Audience\r\n\r\nData scientists, researchers, and engineers who want to write more efficient Python code, understand the modern array ecosystem, or choose the right tools for their problems.", "code": "AGYLTV", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/AGYLTV.png", "speaker_names": "Iason Krommydas", "speakers": "\n### Iason Krommydas\n\nI'm a PhD student in the Department of Physics and Astronomy at Rice University, conducting research in high-energy physics as a member of the CMS experiment at the Large Hadron Collider at CERN. My work focuses on studying Higgs boson decays into two photons, analyzing data collected by the CMS detector, and contributing to software development for large-scale scientific analyses. I'm passionate about scientific computing and open-source tools that enable reproducible and efficient research. I\u2019m maintainer of Awkward Array, an array library for nested, variable-sized data, using NumPy-like idioms, and an author and maintainer of Coffea, a toolkit designed to simplify data analysis in particle physics. With experience in the scientific Python ecosystem, I enjoy building tools that drive insight and accelerate scientific discovery.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "From Pixel to Payouts: A Multi-Agent System for Real-Time Insurance Claims Processing", "abstract": "The traditional process for auto damage evaluation is relatively slow, subjective, and prone to fraud. With this presentation, the goal is to show a Multi-Agent System designed for the automation and standardization in real-time of the car damage evaluation, disrupting the initial claims workflow. The system is built around an Orchestrator Agent with the role to coordinate specialized AI agents: a Vision Agent (powered by OpenAI GPT-5.2) for damage analysis and severity classification, two Cost Estimation Agents (powered by Perplexity's sonar-pro) to provide comparative quotes (OEM vs. Aftermarket), and a Shop Finder Agent for local repair options. The system produces a report that includes a description of the damage, severity, comparative repair costs in local currency, and recommended repair shops, all embedded into a Gradio interface. The task of this approach is to reduce the processing time, improve transparency for customers, and provide insurers with objective data to enable faster claims resolution.", "full_description": "**Project Goal and Business Impact**\r\nImagine filing an auto insurance claim. Instead of waiting days for a damage evaluation, photograph the car with your phone and, within minutes, receive a detailed assessment.\r\nThe primary objective of this project is to drastically improve the efficiency and objectivity of the initial auto insurance claim process. Current methods rely heavily on human adjusters and manual estimates, resulting in delays and potential cost inflation. By deploying a sophisticated Multi-Agent System, the aim is to provide a fastly, data-driven assessment that benefits both the insurer and the customer.\r\n**The Multi-Agent Architecture**\r\nAt the heart of this solution, there is an orchestrated system of specialized AI agents, each with a distinct role. The architecture follows a sketch where an Orchestrator Agent works as the brain, creating execution plans, managing agent lifecycle, coordinating the execution, and aggregating results into coherent outputs.\r\nThe Vision Agent, powered by OpenAI GPT-5.2, acts as the system's eyes. It analyzes uploaded damage photos with technical precision, identifying specific damaged parts (bumpers, panels, headlights, etc.), classifying severity levels (minor, moderate, severe), categorizing damage types (collision, scratch, dent, paint damage), and generating detailed technical assessments. \r\nTwo specialized Cost Estimation Agents run, representing different repair philosophies. The OEM (Original Equipment Manufacturer) Agent focuses on premium repairs using manufacturer-certified parts from authorized dealers, while the Aftermarket Agent explores cost-effective alternatives using quality certified aftermarket parts from independent shops. Both agents are powered by Perplexity's sonar-pro model, which provides access to current market data and pricing information. \r\nThe Shop Finder Agent searches for repair facilities near the user's location, provides contact information, ratings, and availability, and adapts its search strategy based on the information retrieved.\r\n**Technical Highlights**\r\nThe system is built in Python, leveraging several key technologies. The Gradio framework provides an intuitive web interface for image upload, location input, and real-time results display. OpenAI's GPT-5.2 handles computer vision tasks. Perplexity's sonar-pro model accesses current market data for repair costs and local business information. \r\nA sophisticated state management system provides each agent with memory of past interactions, confidence scores to assess decision quality, performance tracking to optimize the system, and autonomous decision-making based on context. \r\nThe application provides comprehensive, auditable car damage analysis, with every step explicitly recorded.\r\n**Generative AI vs. Manual/Traditional Tools**\r\nWhile traditional automated tools rely on rigid, rule-based computer vision and static databases, this Multi-Agent System introduces a modular reasoning layer that bridges the gap between raw data and decision-making. According to the industry research from McKinsey (2025) the agentic workflows reduce claim cycle times from days to seconds with consistency in claim evaluations.\r\nTraditional tools are often \"black boxes\" or monolithic scripts, instead this modular architecture give the opportunity to develop in the future every task as a swappable module for an hybrid framework where every single agent can be replaced by a non Generative AI tool, for flexible, custom and scalable solution.  \r\n**The Future of Insurance Claims**\r\nThis multi-agent architecture is a robust, scalable blueprint for automating complex decision-making business processes, such as insurance claims. It leverages the strengths of several large language models (LLMs) and specialized agents to deliver a fast, transparent, and comprehensive output that far exceeds the capabilities of a single model. The project demonstrates practical, real-world applications of multi-agent systems in production environments.", "code": "APWGQB", "state": "confirmed", "created": "2025-12-14", "social_card_image": "/static/media/social/talks/APWGQB.png", "speaker_names": "Claudio Giorgio Giancaterino", "speakers": "\n### Claudio Giorgio Giancaterino\n\nI have Statistics & Actuarial background\r\nI'm an Actuary during the day\r\nand AI Scientist in the free time\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Vibe NLP for Applied NLP", "abstract": "One of the hardest parts of applied NLP has always been breaking down complex business problems into machine learning components. It's so hard because it requires domain expertise and reasoning about the specific use case, and it's the one thing technology couldn't fix. But what if we could take some of the learnings from AI-powered coding assistants and apply them to solving real-world NLP problems? In this talk, I'll show how we've built powerful assistants and tools to help developers solve NLP tasks using open-source software, and create modular solutions that are small, fast and fully data-private.", "full_description": "At the core of it is an often overlooked idea: using LLMs to\u00a0*build systems*\u00a0instead of\u00a0*as systems*. AI-powered coding assistants have transformed the way we build software \u2013 and they can be even more impactful for AI development itself and bridge the experience gap that's often holding teams back and causing projects to fail. In the talk, I will show you a new way of using generative models for AI development, and some practical examples of how to make \"Vibe NLP\" work for real-world problems.", "code": "AWMRFD", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/AWMRFD.png", "speaker_names": "Ines Montani", "speakers": "\n### Ines Montani\n\nInes Montani is a developer specializing in tools for AI and NLP technology. She\u2019s the co-founder and CEO of [Explosion](https://explosion.ai) and a core developer of [spaCy](https://spacy.io), a popular open-source library for Natural Language Processing in Python, and [Prodigy](https://prodi.gy), a modern annotation tool for creating training data for machine learning models.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Small Language Models for Tool Calling Are Better Than You Think", "abstract": "Large language models have been widely used in tool-calling workflows thanks to their strong performance in generating appropriate function calls. However, due to their size and cost, they are inaccessible to small-scale builders, and server-side computing makes data privacy challenging. Small language models (SLMs) are a promising, affordable alternative that can run on local hardware, ensuring higher privacy.\r\n\r\nUnfortunately, SLMs struggle with this task - they pass wrong arguments when calling functions with many parameters, and make mistakes when the conversation spans multiple turns. On the other hand, for production applications with specific API sets, we often don't need general-purpose LLMs\u2014we need reliable, specialized models.\r\n\r\nThis talk demonstrates how to increase the accuracy SLMs (under 8B parameters) for custom tool calling tasks. We will share how leveraging knowledge distillation helps to get the most out of SLMs in low-data settings - they can even outperform LLMs! We will present the whole pipeline from data generation, fine-tuning, and local deployment.", "full_description": "Large language models have been widely used in tool-calling workflows thanks to their strong performance in generating appropriate function calls. However, due to their size and cost, they are inaccessible to small-scale builders, and server-side computing makes data privacy challenging. Small language models (SLMs) are a promising, affordable alternative that can run on local hardware, ensuring higher privacy.\r\n\r\nUnfortunately, SLMs struggle with this task - they pass wrong arguments when calling functions with many parameters, and make mistakes when the conversation spans multiple turns. On the other hand, for production applications with specific API sets, we often don't need general-purpose LLMs\u2014we need reliable, specialized models.\r\n\r\nThis talk demonstrates how to increase the accuracy SLMs (under 8B parameters) for custom tool calling tasks. We will share how leveraging knowledge distillation helps to get the most out of SLMs in low-data settings - they can even outperform LLMs! We will present the whole pipeline from data generation, fine-tuning, and local deployment.\r\n\r\n**What you'll learn:**\r\n\r\n1. **Tool calling:** Different tool calling settings (single and multi-turn)\r\n2. **Distillation**: Using large models as teachers to train specialized, compact models that maintain reliability with lower computational cost.\r\n3. **Tool calling data generation:** Challenges in generating diverse tool calling data.", "code": "AZ7GD3", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/AZ7GD3.png", "speaker_names": "Gabi Kadlecova", "speakers": "\n### Gabi Kadlecova\n\nI am a Machine Learning Researcher at Distil Labs.\r\n\r\nI have a PhD in Neural Architecture Search from Charles University in Prague. I also did an internship at Amazon Alexa in Turin. During my PhD, I focused on Neural Architecture Search and surrogate models. \r\nNow I shifted focus to small language models - I like that they save us a lot of compute & as local models, they are privacy-friendly.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Compilerless Immediate-Mode Shader Generation in Pure Python", "abstract": "Discover how to build a GPU shader generator in pure Python, without having to write a compiler.\r\n\r\nWe start by discussing how Pythonic embedded domain-specific languages (EDSLs) can help address the common challenges of shader programming.\r\n\r\nWe then examine the architectural decisions shared by popular frameworks like Warp and Taichi and outline their limitations. In particular, their reliance on introspection means supporting only a subset of Python - a language within a language - while compiler-like backends necessitate complex implementations in languages like C++.\r\n\r\nThe talk introduces an alternative architecture making it possible to overcome these limitations. Instead of introspection, we capture the program's logic by tracing execution with proxy objects at Python runtime, similar to JAX and PyTorch. Instead of building an IR, we emit target code eagerly, line-by-line, similar to how PyTorch Eager Mode launches computations. And because we don't implement a compiler, the implementation remains 100% Python.\r\n\r\nAttendees will leave with a toolbox of Python metaprogramming patterns empowering them to write a code generator in Python without having to implement a compiler.", "full_description": "The area of shader programming offers many tough problems to solve. The range of target platforms is vast: from CPU path-tracers to mobile GPUs - served by a zoo of incompatible languages: from GLSL to HLSL, from OSL to WGSL.\r\n\r\n Common challenges include portability, managing specializations, and a lack of abstraction mechanisms. The solutions for these include the archaic C Preprocessor, templates/generics, visual graph frameworks, transpilers and, finally, embedded domain-specific languages (EDSLs).\r\n\r\nPython is an ideal host for Embedded Domain-Specific Languages (EDSLs). Warp, Taichi, Numba, and Triton evolved to target GPU compute. All of them share common architectural decisions. They capture the program's logic by inspecting the Python source code, generate an internal representation and compile that IR to the target format.\r\n\r\nThe above approach comes with significant disadvantages. Only a subset of Python is supported, debugging with standard tools is impossible, integration with external Python code is limited, metaprogramming requires special syntax, and heavy compiler infrastructure needs to be implemented in a language like C++.\r\n\r\nThis talk proposes an alternative architecture. Instead of introspection, we capture the program's logic by tracing execution with proxy objects at Python runtime, similar to JAX and PyTorch. Instead of building an IR, we emit target code eagerly, line-by-line, similar to how PyTorch Eager Mode launches computations. And because we don't implement a compiler, the implementation remains 100% Python.\r\n\r\nWe discuss in detail how core elements of Python syntax can be overloaded to implement such an architecture:\r\n* Operator overloading to capture expressions.\r\n* Context managers to simulate C-like scopes.\r\n* `__setattr__`/`__getattr__` to capture variable names.\r\n* Function decorators to capture function signatures.\r\n\r\nAttendees will leave with a toolbox of Python mataprogramming patterns empowering them to write a code generator in Python without having to implement a compiler.\r\n\r\n# Outline\r\n## Introduction - why write shaders in Python? (5 min)\r\n\r\n* Platforms targeted by shaders\r\n* The variety of shading languages\r\n* Common challenges:\r\n   * Portability\r\n   * Permutation explosion\r\n   * Low level of abstraction\r\n* Existing solutions:\r\n   * C Preprocessor\r\n   * Templates/generics\r\n   * Visual graph frameworks\r\n   * Transpilers\r\n   * Embedded domain-specific languages (EDSLs) -> that's where we fit in.\r\n\r\n## Pythonic GPU EDSLs (5 min)\r\n\r\n* Warp, Taichi, Numba, and Triton\r\n* How do they work?\r\n    * Introspection: AST or byte code\r\n    * IR -> compilation\r\n* Limitations\r\n    * A subset of Python!\r\n    * Debugging\r\n    * Integration with other Python code\r\n    * Need to implement a compiler, meaning C++\r\n\r\n* Alternative architecture\r\n   * Tracing instead of introspection\r\n      * PyTorch and JAX analogy\r\n   * Immediate code emission\r\n      * PyTorch Eager analogy\r\n      * How correctness is maintained - the semantic model\r\n\r\n## Pythonic metaprogramming patterns making it all work (15 min)\r\n\r\n* Operator overloading to capture expressions.\r\n   * Tracer objects record math operations instead of performing them.\r\n   * Tracer objects are strongly typed and can enforce semantics better than the target language.\r\n* Context managers to simulate C-like scopes.\r\n   * The differences between the two and how to work around them.\r\n* How can capture variable names without introspection?\r\n   * `__setattr__`/`__getattr__` to on generator objects.\r\n   * How this helps composability with arbitrary Python code.\r\n* Function decorators to capture function signatures.\r\n\r\n## Proof-of-concept demo (5 min)\r\n\r\n* What actual generated shaders look like \r\n* A demo 3D app with glTF PBR shaders implemented with our architecture", "code": "B8GQ9Z", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/B8GQ9Z.png", "speaker_names": "Pavlo Penenko", "speakers": "\n### Pavlo Penenko\n\nI was born and raised in Kyiv, Ukraine.\r\n\r\nIn 2000, I received a BSc in Computer Science from Taras Shevchenko National University of Kyiv.\r\n\r\nI started my career in game dev in Kyiv in the early 2000s, and continued it in Canada, moving to Vancouver in 2008 to render zombies at Capcom.\r\n\r\nLater, I worked on VR at AMD, content pipelines at Toonbox, Houdini Engine at SideFX, the Maya viewport at Autodesk and Redshift RT at Maxon.\r\n\r\nCurrently, I'm Principal Software Developer at Autodesk, working on material and shading workflows in MaterialX and Hydra applications.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "Accuracy Is Overrated: Ship Stable Forecasts (Without Lying to Yourself)", "abstract": "Forecasting talks love a clean ending: \u201cand then we improved WMAPE by 3.7%.\u201d\r\nNice. Now put that model into production without suffering from instability.\r\n\r\nYou retrain your model on a few new weeks of data and suddenly the one-year forecast jumps 15\u201320%. Planning teams redo decisions, trust erodes, and your \u201caccurate\u201d model becomes unusable. This talk is about forecast stability: how much forecasts change when you add new data and rerun the same pipeline.\r\n\r\nWe run a simple experiment: train a model, forecast one year ahead, add recent data, retrain, and measure forecast-to-forecast change. We repeat this across common forecasting approaches including ETS/ARIMA, Prophet, XGBoost with lag features, AutoGluon ensembles, neural/global models, and TimeGPT-style APIs.\r\n\r\nYou will see that high accuracy does not guarantee usable forecasts, and that some models are systematically more volatile than others. We then cover practical ways to stabilise forecasts without freezing them, focusing on reconciliation and ensembling (including origin ensembling).\r\n\r\nThis talk is for forecasting practitioners who want models users actually trust, not just good metrics.", "full_description": "Forecasting talks love a clean ending: \u201cand then we improved WMAPE by 3.7%.\u201d\r\nNice. Now put that model into production without suffering from instability.\r\n\r\nBecause here is what users actually see: the forecast changes every week. The \u201cone-year view\u201d jumps 15 to 20 percent because you retrained on three extra Mondays. Planning teams redo decisions. Operations loses trust. Your model becomes an expensive random-number generator with excellent dashboards.\r\n\r\nThis talk is about forecast stability: how much your future forecast moves when you add a small amount of new data, retrain, and run the same pipeline again. Not error versus actuals. Forecast versus forecast.\r\n\r\nYou will see a simple but uncomfortable experiment: \r\n\r\n- Taking a demand-style time series dataset with seasonality, promotions, and noise (Kaggle competition style).\r\n- Training a model and produce a one-year-ahead forecast.\r\n- Adding a few recent weeks of data, retrain, forecast again.\r\n- Measuring how much the overlapping horizon changed.\r\n\r\nWe repeat this across model families people actually use:\r\n\r\n- Statistical baselines like ETS and ARIMA\r\n- Prophet\r\n- Feature-based ML with lag features such as XGBoost\r\n- AutoML and ensembles with AutoGluon TimeSeries\r\n- Neural and global models where relevant\r\n- And yes, what happens when you add an API model like TimeGPT into the mix (no hype, just behaviour under updates)\r\n\r\nYou will see something totally \"unexpected\": a model can be \u201caccurate\u201d and still be operationally useless because its forecast revisions are chaotic. And you will see the opposite too: models with slightly worse headline accuracy that people actually trust, because next year does not get rewritten every week.\r\n\r\nThis is not a philosophical debate. It is a measurable property of forecasting systems that most teams never track.\r\n\r\nSo what do we do about it?\r\nWe focus on techniques that improve stability without turning forecasts into fossils:\r\n\r\n1) Reconciliation\r\nHierarchical and temporal reconciliation as a stabiliser, not just a coherence tool. If SKU-level forecasts panic while higher-level signals stay calm, reconciliation can prevent nonsense from propagating into decisions.\r\n\r\n2) Ensembling and origin ensembling\r\nCombining models is not only about accuracy. Averaging forecasts across models and across forecast origins dampens noise and makes forecast updates behave like signals instead of mood swings.\r\n\r\nWho this talk is for:\r\n\r\nForecasting practitioners, data scientists working on demand forecasting, and anyone who has ever heard: \u201cYour model looks good, but I don\u2019t trust it.\u201d\r\n\r\nWhat you\u2019ll take away:\r\n\r\n- A methodology to measure forecast stability using forecast-to-forecast change.\r\n- A mental model for when forecast revisions are useful and when they are just noise.\r\n- Practical patterns you can implement immediately in Python to make forecasts calmer without hiding real change.\r\n\r\nIf you optimise only accuracy metrics, you are grading homework.\r\nIf you care about stability, you are building a forecasting product.", "code": "B8KVNJ", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/B8KVNJ.png", "speaker_names": "Illia Babounikau", "speakers": "\n### Illia Babounikau\n\nDr. Illia Babounikau is an accomplished data scientist with extensive expertise in machine learning and forecasting. He holds a Ph.D. in Physics from Hamburg University and initially pursued an academic career, focusing on large-scale data analysis and machine learning applications. His contributions have been instrumental in international scientific collaborations, including the CMS experiment at CERN\u2019s Large Hadron Collider and the COMET project at J-PARC.\r\n\r\nFor the past five years, Dr. Babounikau has been a Data Scientist at Blue Yonder and VOIDS, specializing in developing and fine-tuning advanced forecasting models for retail planning and inventory management. He leads the design and implementation of tailored machine-learning solutions, addressing complex challenges within supply chains across diverse industries.\r\n\r\nDr. Babounikau is passionate about bridging the gap between data science and business strategy, ensuring machine learning models are aligned with business objectives to drive data-informed decision-making.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Agent-Based Hyperparameter Optimization for Gradient Boosted Trees", "abstract": "### Agent-Based Hyperparameter Optimization for Gradient Boosted Trees\r\n\r\nHyperparameter optimization for gradient boosted tree models is a repetitive yet cognitively demanding task. Practitioners must combine statistical intuition with detailed, library-specific knowledge\u2014often buried across hundreds of pages of documentation for tools such as XGBoost, LightGBM, or CatBoost. As models and configurations grow in complexity, traditional approaches like grid search, random search, or even Bayesian optimization struggle to incorporate semantic understanding of model behavior.\r\n\r\nUsing XGBoost as a concrete case study, I demonstrate how RAG-powered agents, orchestrated in a structured workflow, can analyze model behavior via SHAP values, diagnose failure modes (e.g. overfitting, feature dominance, interaction leakage), and propose targeted hyperparameter adjustments grounded in both theory and library-specific constraints.\r\n\r\nThe system combines open-source tools including XGBoost, SHAP, Optuna, and LangGraph/LangChain, where agents specialize in tasks such as model diagnostics, documentation-aware parameter reasoning, and experiment orchestration. Rather than replacing existing optimization frameworks, agents operate on top of them, injecting domain knowledge and interpretability signals into the optimization loop.", "full_description": "### Why This Problem Matters in Practice\r\n\r\nIn every day work as data scientist & ML engineer, hyperparameter tuning often consumes a disproportionate amount of experimentation time, yet many tuning failures stem from recurring structural issues rather than random chance. These issues are typically identifiable by experienced practitioners but remain inaccessible to automated optimization systems due to their reliance on scalar objective functions alone.\r\n\r\n### What Is New or Different\r\n\r\nThis work reframes hyperparameter optimization as an iterative reasoning process rather than a pure search problem. The key insight is that intermediate explanation artifacts\u2014specifically SHAP value distributions\u2014can be treated as first-class signals that guide subsequent optimization decisions. Encoding this reasoning explicitly via agents enables systematic reuse of expert heuristics that are otherwise applied informally.\r\n\r\n### System Architecture and Workflow\r\n\r\nThe proposed system decomposes the optimization process into agent roles with clearly defined responsibilities, such as diagnostic reasoning, parameter constraint validation, and experiment coordination. These agents interact through a controlled workflow that preserves reproducibility and auditability while leveraging Retrieval-Augmented Generation to reason over model documentation and prior experiment context.\r\n\r\n### Scope and Limitations\r\n\r\nThe case study focuses on gradient boosted tree models, with XGBoost used as the primary example. While the approach generalizes conceptually, it is most effective in settings where model interpretability and parameter interactions dominate performance outcomes. The talk explicitly discusses scenarios where agent-based optimization adds limited value or introduces unnecessary complexity.\r\n\r\n### Audience Takeaways\r\n\r\nAttendees will gain:\r\n- Practical heuristics for translating explanation outputs into tuning actions  \r\n- Design patterns for building agent-based optimization workflows  \r\n- Guidance on integrating agent reasoning into existing experimentation and MLOps setups  \r\n- A realistic understanding of the trade-offs and failure modes of agent-driven systems  \r\n\r\n### Format and Reproducibility\r\n\r\nThe presentation is a technical case study supported by architecture diagrams and experiment traces. All code, configurations, and artifacts will be made available as open source to ensure reproducibility and facilitate adaptation.", "code": "BAXEXY", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/BAXEXY.png", "speaker_names": "Huijo Kim", "speakers": "\n### Huijo Kim\n\nI am a machine learning practitioner and former founder working across predictive modeling, computer vision, MLOps, and autonomous systems. After studying mechanical engineering, I worked in the electric vehicle development sector at **Hyundai Motor Group**, contributing to large-scale, safety-critical automotive systems.\r\n\r\nI later founded and scaled an [**agtech startup**](http://hexafarms.com) from zero to a six-figure ARR business. This experience shaped my focus on building technology that delivers measurable, real-world value rather than chasing technical hype. After exiting, I transitioned into the **e-commerce domain**, applying machine learning to large-scale experimentation and operational optimization.\r\n\r\nMy background includes graduate research in robotics, published work in applied machine learning, and hands-on experience deploying end-to-end ML systems. I am particularly interested in explainability-driven optimization, agent-based workflows, and cross-disciplinary system design. I believe polymath practitioners\u2014those who can bridge domains\u2014will be especially valuable in the era of AI.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "From Scratch to Scale: How Far Python Takes You in Building LLMs", "abstract": "Python has been at the center of my work in machine learning and AI for more than a decade. It is where I start from scratch, experiment with ideas, and build systems that help me understand how large language models really work.\r\n\r\nIn this keynote, we will explore how Python enables this entire journey, from defining model architectures and training loops to scaling data and computation across devices. I will also reflect on how Python continues to support both the large models of today and the evolving systems of tomorrow, even as new backends take over the heavy lifting.", "full_description": "Python has been at the center of my work in machine learning and AI for more than a decade. It is where I start from scratch, experiment with ideas, and build systems that help me understand how large language models really work.\r\n\r\nIn this keynote, we will explore how Python enables this entire journey, from defining model architectures and training loops to scaling data and computation across devices. I will also reflect on how Python continues to support both the large models of today and the evolving systems of tomorrow, even as new backends take over the heavy lifting.", "code": "BFL7MQ", "state": "confirmed", "created": "2026-01-26", "social_card_image": "/static/media/social/talks/BFL7MQ.png", "speaker_names": "Sebastian Raschka", "speakers": "\n### Sebastian Raschka\n\n\n", "domain_expertise": "None", "python_skill": "None"}, {"title": "Build a web coding platform with Python, run in WebAssembly", "abstract": "Ever wanted to build a website that can run python, but you're worried about running user submitted code on your server?\r\nIn this talk I'll show how Holoviz Panel can create an interactive coding environment where students can write functions, solve exercises, and experiment safely, all while their code runs locally via WebAssembly.", "full_description": "### The Problem\r\nBuilding interactive Python learning platforms traditionally requires server infrastructure to execute user code, creating security risks and operational overhead. What if we could run Python entirely in the browser?\r\n\r\n### The Solution\r\nThis talk presents a production-ready coding platform built with Holoviz Panel that executes Python through WebAssembly via Pyodide. The entire application \u2013 UI and code execution \u2013 runs client-side, eliminating backend complexity while providing safe, isolated Python execution.\r\n\r\n### Architecture Overview\r\nThe platform combines three key technologies:\r\n- **Holoviz Panel** for building the interactive interface with its built-in code editor component\r\n- **Pyodide** for secure Python execution via WebAssembly in the browser\r\n- **LocalStorage** for persisting student progress without a database\r\n\r\n### Key Features\r\nThe platform supports multiple learning modalities:\r\n- Coding exercises validated against pre-defined test cases \u2013 from simple variable assignments to complete functions with return values or print statement\r\n- Interactive playground that evaluates expressions and captures output\r\n- Single and multiple-choice questions for concept checks\r\n\r\n### What You'll Learn\r\nThis talk covers the technical integration between Panel's UI framework and Pyodide's execution environment \u2013 the critical piece that makes browser-based Python coding work. Attendees will understand:\r\n- How to architect client-side Python applications\r\n- Running Panel components with Pyodide\r\n- Trade-offs between client-side and server-side execution\r\n- Handling code execution, output capture, and state management\r\n\r\n### Target Audience\r\nData scientists, educators, and developers interested in building interactive Python tools without server infrastructure. Basic familiarity with Python web frameworks is helpful but not required.\r\n\r\n### Background\r\nThis work originated from my bachelor thesis exploring Python education. The resulting platform demonstrates that WebAssembly enables entirely new architectures for Python applications \u2013 shifting from traditional server models to fully client-side execution.\r\n\r\nThe takeaway: You can build sophisticated Python applications that run anywhere there's a browser, with no backend server setup, no security concerns about arbitrary code execution, and no additional infrastructure costs.", "code": "BFYYQG", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/BFYYQG.png", "speaker_names": "Maris Nieuwenhuis", "speakers": "\n### Maris Nieuwenhuis\n\nJunior Software Developer\r\nBachelor of Science in Media Informatics (Medieninformatik B. Sc.)\r\nGraduated from Berliner Hochschule f\u00fcr Technik in 2025\r\nPassionate about Python and programming in general, coding challenges and tutoring/education\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Python Language & Ecosystem"}, {"title": "Django-Q: Async Tasks Made Simple", "abstract": "Managing asynchronous task queues in Django with tools like Celery can be overkill for many projects. Django-Q is a lightweight alternative that integrates natively with the Django admin. In this talk, you will learn how to streamline your background tasks and cron jobs, featuring a practical demo to get you started immediately.", "full_description": "Handling asynchronous tasks and cron jobs in Django is essential for features like sending emails or generating periodic reports. However, the industry standard Celery often comes with significant configuration overhead and infrastructure dependencies like Redis or RabbitMQ.\r\n\r\nIf you have ever struggled with that complexity or looked for a more intuitive way to manage background processes, Django-Q is the answer. It is a lightweight solution that leverages your existing database, eliminating the need for complex brokers. Its native integration makes it perfect for small to medium-sized projects that need to move fast.\r\n\r\nThis talk will guide you through integrating Django-Q to simplify your workflow:\r\n\r\n- Problem Solving: We will look at how to use Django-Q to solve real-world task management issues.\r\n\r\n- Feature Deep Dive: We will explore key features, such as using the database as a backend and monitoring tasks directly from the Django Admin interface.\r\n\r\n- Live Demo: We will configure Django-Q from scratch to handle asynchronous email sending and schedule a recurring maintenance job", "code": "BHJERV", "state": "confirmed", "created": "2025-11-27", "social_card_image": "/static/media/social/talks/BHJERV.png", "speaker_names": "Moin Uddin", "speakers": "\n### Moin Uddin\n\nPassionate about technology, innovation, and continuous learning. I like to automate thing and I Love Python and K8s. \r\nBeyond the technical world, I am an avid traveller and explorer, always seeking new perspectives and inspiration from around the globe.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Django & Web"}, {"title": "Demystifying Containers with Python: Building a Minimal Engine from Scratch", "abstract": "Containers are a fundamental part of the modern developer's toolkit, yet they are frequently misunderstood and described as \"lightweight virtual machines.\" This talk demystifies containerization by building a functional, minimal engine from scratch using only the python standard library. We will step away from high-level tools like docker to explore how the linux kernel provides isolation through features like `chroot`. Using a hands-on approach, we will demonstrate how to set up a sandboxed environment, isolate a filesystem, and execute processes within it. This session is designed for developers who use containers daily but haven't yet had the opportunity to look under the hood or explore the underlying operating system principles. By implementing a simplified version of these tools, you will gain a clearer, more practical understanding of the core mechanics that make containerization possible.", "full_description": "In modern software development, containers have become a standard tool for deploying code. However, they are frequently misunderstood and described as \"lightweight virtual machines.\" For many developers - especially those transitioning from academia, like myself - the layer between their python code and the operating system kernel is often overlooked. This talk is based on the idea that the best way to understand a concept is to implement it in its simplest form. By bypassing the complexity of modern container orchestrators, we can focus on the fundamental system calls that make isolation possible.\r\n\r\nDuring the session, we will demonstrate the core mechanics of containerization by building a minimal engine in python. We will begin by preparing a root filesystem to show what a container image actually is at its most basic level. We will implement isolation using the `os.chroot()` function to trap a process in a specific directory and will talk about linux namespaces, which isolate what a process can see, and `cgroups`, which limit how much of the hardware resources a process can use.\r\n\r\nThe main takeaways of this talk include a clear technical distinction between virtual machines and containers and the realization that a container is essentially a process with a restricted view of the host system. You will gain practical knowledge of the `os` module for system-level tasks and the confidence to explore low-level computer science concepts by implementing them in python. By the end of this session, you will have a practical understanding of the basic principles that make containerization possible.", "code": "BLC7FS", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/BLC7FS.png", "speaker_names": "Alexander Zaytsev", "speakers": "\n### Alexander Zaytsev\n\nI am a data engineer at Blue Yonder, where I build infrastructure for large-scale demand forecasting solutions. My career spans nearly a decade in academia, high-tech R&D, and industry, during which python has been a constant tool across a wide variety of environments. During my PhD in physics, I used python to analyze time-series data from some of the world\u2019s most precise quantum sensors in the search for dark matter. Earlier in my career, I applied python to data analysis and modeling in high-precision laser gyroscope R&D, and today I continue to use it to develop robust, production-grade machine learning systems.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Beyond Stateless: Why Your Web Service Architecture is Fighting Against Performance", "abstract": "We've been told for years that stateless services are the holy grail of scalable web architectures. But what if this foundational principle is actually hurting both development velocity and runtime performance? This talk challenges the dominant paradigm by demonstrating how stateful, object-oriented programming can automatically scale to millions of users without the typical infrastructure complexity.\r\n\r\nI'll show how keeping objects with their state in distributed memory eliminates the need for explicit caching strategies, reduces database bottlenecks, and dramatically simplifies your codebase. You'll see how a simple Python class can transparently scale across multiple servers, handling millions of concurrent users without implementing REST endpoints, message queues, or cache invalidation logic.\r\n\r\nWe'll examine why the historical evolution of web services created the myth that \"stateless is good\" and demonstrate an alternative where Python objects live persistently in a cluster, maintaining their state while the framework handles distribution, persistence, and failover automatically. Learn how to write Python code that scales from prototype to production using the same simple object-oriented patterns throughout.", "full_description": "## The Problem Many Face\r\n\r\nEvery developer of a successful web service knows this progression: You start with a simple FastAPI or Django app. It works great locally. Then you deploy it, traffic grows, and suddenly you're working primarily on infrastructure complexity. Load balancers, cache layers, database replicas, message queues, and before you know it, your simple microservice based business logic has become a complex distributed system mesh including careful cache invalidation logic.\r\n\r\nBut what if this complexity isn't inevitable? What if it's actually the result of a historical mistake that became \"best practice\"?\r\n\r\n## Challenging the Stateless Dogma\r\n\r\nThis talk challenges a fundamental assumption of modern web architecture: that stateless services are superior for scalability. I'll demonstrate that this belief, born from the constraints of early web servers, is now actively harmful to both performance and developer productivity. The truth is: separating logic from state (the core of stateless architecture) creates most of the complexity we fight daily. Every database query, every cache lookup, every message queue: they're all workarounds for the fact that we threw away our object's state after each request.\r\n\r\n## Key Takeaways\r\n\r\n- Stateless isn't a virtue, it's a workaround: modern systems can and should maintain state efficiently across requests.\r\n- Your objects can be the cache: when objects persist in distributed memory, explicit caching becomes redundant.\r\n- Scale by writing normal Python code: the same object-oriented patterns work from prototype to web-scale.\r\n- Performance through simplicity: eliminating layers of infrastructure translation improves both latency and throughput.\r\n- Focus on business logic, not plumbing: let the framework handle distribution, persistence, and failover.\r\n\r\n## Who Should Attend\r\n\r\nPython developers who:\r\n- are building or maintaining web services,\r\n- have experienced the pain of cache invalidation,\r\n- want to scale without changing their programming model,\r\n- are curious about alternatives to microservices.\r\n\r\n## A Paradigm Shift\r\n\r\nJust as we moved from manual memory management to garbage collection, it's time to move on from manual state management. Your Python objects should live as long as they're needed, not just for the duration of a request. This isn't theoretical. Systems using this approach power gaming platforms with millions of concurrent users, financial systems requiring microsecond latency, and IoT platforms managing billions of devices. The technology exists. We just need to unlearn the \"stateless is good\" mantra.", "code": "BQYTVM", "state": "confirmed", "created": "2025-12-06", "social_card_image": "/static/media/social/talks/BQYTVM.png", "speaker_names": "Heiner Wolf", "speakers": "\n### Heiner Wolf\n\nHeiner Wolf is a physicist and coder. After completing his Master\u2019s degree in particle physics at CERN, he got a PhD in computer science and is now a passionate full stack developer (C#, TypeScript, Python). Heiner has been CTO for many years, in his own startups and those of others. Alongside all sorts of good stories, he enjoys realistic future scenarios and hard science fiction. And when triggered on physics, he\u2019ll gladly rant about how fusion research should really be done.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "(Autism and) The Predictive Brain Theory (in Tech)", "abstract": "New studies showed how the brain is not a passive receiver of stimuli but an active predictor of stimuli. People with autism have more difficulties when the predicted and received stimuli doe not match. How do we create a tech workforce where autistic individuals can work more comfortable due to predictability?", "full_description": "What does new autism, AI and quantum computing research have in common? We need to stop following the well known 'input - process - output' model. Let's escape this model and embrace the predictive brain theory to understand autism and how people on the spectrum interact with technology and at the workplace.\r\n\r\nRecent brain research shows that the brain is not a passive receiver of stimuli, but an active predictor of what will happen next. The brain is constantly building a big model of the world based on past experiences and using this model to predict what will happen next. When we have a big model of the world we know what to expect. If something unexpected happens, the brain receives this as an error and needs to update its model to accommodate the new information. This is not autism or tech specific, this is how the brain works for everyone according to the latest brain research.\r\n\r\nA recent hypothesis suggests that people on the autism spectrum often have a harder time building the big model of the world and the stimuli-response system of people on the spectrum is often more sensitive, adding context blindness and the higher energy cost of executive functioning to the mix, it is harder for people on the spectrum to predict what will happen next and to deal with unexpected situations. This can lead to anxiety, stress and burnout.\r\n\r\nIf your tech job is constantly causing errors in the predictive model of the world, it will be hard to do your job and to be happy at work. In this talk I will explain how the predictive brain theory can help us understand autism and how we can build better technology and workplaces for people on the spectrum.", "code": "BRCNB7", "state": "confirmed", "created": "2025-12-16", "social_card_image": "/static/media/social/talks/BRCNB7.png", "speaker_names": "Dennie Declercq", "speakers": "\n### Dennie Declercq\n\nDennie is Microsoft MVP in AI and Developer Technologies and has experience in accessibility with Microsoft technologies. In daily life Dennie is president and developer at DDSoft, a nonprofit that connects IT to People who are less tech-savvy. Dennie invented technical solutions and systems to help people with disabilities to participate in their daily life. Thanks to his autism he's the right man at the right spot to contribute as a volunteer in function of people with disabilities.\n", "domain_expertise": "None", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "On Interventional Generalisation", "abstract": "If I do X instead of Y, will I get the outcome I want? What about in a new unseen situation? Making predictions alone is pointless, one wants to act in the world. Furthermore one must act in situations that are similar but different to all past experience. The real underlying goal of all decision making is really interventional generalisation: the ability to evaluate hypothetical choices in new unseen situations. Unfortunately data science and statistics has a inordinate focus on observation and statistical significance instead of intervention, counter-factuals and generalisation. Improve your modelling both practically and conceptually with the mental tools presented in this talk.", "full_description": "If I do X instead of Y, will I get the outcome I want (in a novel situation)? Making predictions alone is pointless, one wants to act in the world. Furthermore one must act in situations that are similar but different to all past situations. The real underlying goal of all decision making is interventional generalisation: the ability to evaluate hypothetical choices in new unseen situations.\r\n\r\nThis talk covers this history and problems of null hypothesis significance testing, the benefits (and limitations) of Bayesian reasoning. Introduces the basics of Pearl-ian causality theory and its treatment of interventions and counter-factuals (things that hypothetically could have happened, but didn't), finally we discuss the next step, interventional generalisation, that is being able to compare the value of hypothetical interventions in new unseen situations. Decisively improve your modelling practically and conceptually with the mental tools in this talk.", "code": "BRRHGY", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/BRRHGY.png", "speaker_names": "Andy Kitchen", "speakers": "\n### Andy Kitchen\n\nBorn hacker. Curious human. I've started a couple of companies. I liked AI before it was cool, I swear.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "LLM-Based Recommendation Systems: From Embeddings to Real Personalization", "abstract": "Large Language Models are rapidly changing how we think about recommendation systems. Traditional pipelines based on collaborative filtering or matrix factorization are being complemented and sometimes replaced by embedding-based and LLM-driven approaches.\r\n\r\nIn this talk, we explore how modern recommendation systems can be built using LLM embeddings, vector databases, and hybrid architectures that combine classical ML with generative models. We will discuss practical design patterns for personalization, retrieval, ranking, and user modeling, focusing on real-world constraints such as latency, cost, and evaluation.\r\n\r\nThe session emphasizes hands-on insights from production systems and highlights where LLMs add real value and where they don\u2019t. Attendees will leave with a clear mental model for designing scalable, LLM-powered recommendation systems beyond toy examples.", "full_description": "Recommendation systems are a core component of many data-driven products, yet most practitioners are still navigating how and when to incorporate Large Language Models into these systems effectively.\r\n\r\nThis talk presents a practical, end-to-end view of LLM-based recommendation systems. We start by revisiting classical recommendation architectures and then move into modern approaches built around embeddings, vector similarity search, and retrieval-augmented generation (RAG).\r\n\r\nTopics covered include:\r\nUsing LLM embeddings for user and item representation\r\nHybrid retrieval pipelines combining vector search and traditional ranking models\r\nPrompt-driven personalization and context-aware recommendations\r\nOffline and online evaluation strategies for LLM-based recommenders\r\nTrade-offs around latency, cost, and system complexity\r\n\r\nThe focus is on real-world applicability rather than theoretical novelty. Examples and design patterns are drawn from production-like systems and practical experimentation. This session is aimed at data scientists, ML engineers, and practitioners who want to move beyond hype and build recommendation systems that deliver meaningful personalization using LLMs.", "code": "C7GQQY", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/C7GQQY.png", "speaker_names": "\u00d6zge \u00c7inko", "speakers": "\n### \u00d6zge \u00c7inko\n\nI\u2019m \u00d6zge \u00c7inko, a curious soul with a computer engineering degree and a heart full of ideas.\r\n\r\nI\u2019m currently shaping the future as an AI Research Engineer at Huawei. I work in AI research, but I\u2019m just as passionate about blending creativity with code.\r\n\r\nWhether it\u2019s turning emotions into visuals, building fictional chatbots, or crafting data stories, I love making tech feel personal.\r\n\r\nI write, build, explore, and sometimes get beautifully lost in too many ideas, but always with Python by my side.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "\"Honey, I vibe coded some crypto\" - Security in the age of LLMS", "abstract": "What only a few years ago started out as smart tab completion turned into a way of working in which a growing number of programmers don't even bother to open up an IDE anymore. Let's take a moment to contemplate the changing nature of software engineering as a profession, and to explore chances to avoid looming disaster.", "full_description": "What only a few years ago started out as smart tab completion turned into a way of working in which a growing number of programmers don't even bother to open up an IDE anymore. Let's take a moment to contemplate the changing nature of software engineering as a profession, and to explore chances to avoid looming disaster. \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e \u200e", "code": "CMDHUN", "state": "confirmed", "created": "2026-01-25", "social_card_image": "/static/media/social/talks/CMDHUN.png", "speaker_names": "Gabriela Bogk", "speakers": "\n### Gabriela Bogk\n\n\n", "domain_expertise": "None", "python_skill": "None"}, {"title": "Beyond Vibe-Coding: A Practitioner's Guide to Spec-Driven Development in AI Engineering", "abstract": "AI-assisted coding became the default. Tools like GitHub Copilot, Cursor, and Claude can generate hundreds of lines of Python in seconds.\u00a0However, the real challenge isn\u2019t how fast we generate code \u2014 it\u2019s how we ensure that generated code actually represents our intent, follows best practices, and integrates cleanly into existing systems.\r\n\r\nIn this talk, I share my personal journey adopting *Spec-Driven Development (SDD)*, a way to engineer the context in which AI writes code. Using real examples from my daily work building production-grade RAG systems, I show how specifications can become a practical way to interact with\u00a0 AI coding tools.\r\n\r\nI present an explicitly opinionated comparison of emerging tools such as SpecKit and OpenSpec, focusing on what each tool is good at, where it breaks down, and when I would (or would not) use it.", "full_description": "AI Engineering is fundamentally about system building. It is the transition from  demos to production-grade Python systems that must be scalable, reliable, and testable. In my experience, one way to achieve this consistently with AI-generated code is to stop coding first \u2014and start specifying first. Spec-Driven Development (SDD) is a practical methodology for AI-assisted development. It is not about heavy bureaucracy; it\u2019s about creating a \"Single Source of Truth\" that both humans and AI agents can rely on.\r\n\r\nIn this talk, I will show how I use existing tools like OpenSpec and SpecKit to develop my own AI-assisted workflows that actually fit my way of thinking. I will present an explicitly opinionated comparison of these tools\u2014not to choose a winner, but to understand the trade-offs:\r\n- **SpecKit:** My choice for larger features with many dependencies.\r\n- **OpenSpec:** My choice for smaller features, rapid prototyping, or  bug fixes.\r\n\r\nThe talk is grounded in real-world features I have developed in my daily work as an AI Engineer, particularly in building RAG systems.\r\n\r\n**What You Will Learn:**\r\n- **What is Spec-Driven Development?** A clear breakdown of the spec-first workflow.\r\n- **The Paradigm Shift:** Why \"specifying\" may be the new \"coding\" in a world of Large Language Models.\r\n- **The Subjective Tool Guide:** A practical comparison of SpecKit vs. OpenSpec regarding their technical benefits and trade-offs.\r\n- **Customization:** How to adapt existing tools to fit your personal engineering workflow.", "code": "CVPVPK", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/CVPVPK.png", "speaker_names": "Alina Dallmann", "speakers": "\n### Alina Dallmann\n\nAlina Dallmann is an AI Engineer at scieneers GmbH. As a computer scientist, she combines her passion for classical software engineering with modern, data-driven projects. Most recently, her focus has been on building production-ready Retrieval-Augmented Generation (RAG) systems.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "PathSim: Block Diagram Simulation in Pure Python", "abstract": "When you need to simulate interconnected dynamical systems in Python, scipy.integrate gives you ODE solvers but no structure for managing complex block diagrams, signal routing, or discrete events. MATLAB/Simulink offers this structure but locks you into proprietary tools.\r\n\r\nPathSim is an open-source framework that bridges this gap. Born from real engineering challenges in control systems and physics simulation, it brings block diagram modeling to pure Python while supporting modern workflows: stiff system solvers, event handling for hybrid dynamics, co-simulation through FMI, and integration with the scientific Python stack.\r\n\r\nIn this talk, I'll share the development journey, demonstrate what makes PathSim different from basic ODE solving through live examples (from oscillators to event-driven systems), and show how its architecture enables everything from rapid prototyping to hardware-in-the-loop testing. You'll learn when block diagram simulation is the right tool and how to get started.", "full_description": "## Talk Overview\r\n\r\nThis talk presents PathSim through its development journey: why it was created, what technical challenges it solves, and how it fits into the scientific Python ecosystem. Rather than just another simulation tool, PathSim addresses a specific gap between low-level ODE solvers and expensive proprietary environments.\r\n\r\n## Target Audience\r\n\r\n- Engineers and researchers simulating physical systems\r\n- Python developers curious about simulation frameworks\r\n- Anyone who's hit limitations with scipy.integrate for complex systems\r\n- Those interested in the journey of building scientific software\r\n\r\nNo simulation background required - concepts explained from first principles.\r\n\r\n## Talk Structure (30 minutes)\r\n\r\n**The Origin Story (4 min)**\r\n- Why build PathSim? Real problems that motivated development\r\n- The gap between scipy.integrate and full simulation environments\r\n- Design decisions: decentralized architecture, pure Python, minimal dependencies\r\n\r\n**What Makes Block Diagram Simulation Different? (8 min)**\r\n- Live comparison: same system with scipy.integrate vs PathSim\r\n- Signal routing and interconnections\r\n- The block diagram mental model for complex systems\r\n- When you need structure beyond \"write a function, call solve_ivp\"\r\n\r\n**Core Features Through Examples (12 min)**\r\n\r\n*Example 1: Event Handling*\r\n- Bouncing ball with zero-crossing detection\r\n- Why this is hard with basic ODE solvers\r\n- PathSim's event system in action\r\n\r\n*Example 2: Stiff Systems*\r\n- Van der Pol oscillator demonstration\r\n- Implicit solver integration (ESDIRK, BDF methods)\r\n- Adaptive timestepping visualization\r\n\r\n*Example 3: Hierarchical Modeling*\r\n- Building subsystems and nesting them\r\n- Modularity for large-scale simulations\r\n\r\n**Integration & Ecosystem (4 min)**\r\n- Co-simulation capabilities and FMI support\r\n- Hardware-in-the-loop applications\r\n- Custom block development and extensibility\r\n- Integration with other Python tools (control libraries, ML frameworks)\r\n\r\n**Getting Started (2 min)**\r\n- Installation and documentation\r\n- Example gallery walkthrough\r\n- Where PathSim fits in your workflow\r\n- Contributing and community\r\n\r\n## Key Takeaways\r\n\r\n- Understand when block diagram simulation adds value over basic ODE solving\r\n- See practical examples of stiff systems and event handling\r\n- Learn about PathSim's architecture and extensibility\r\n- Know how to integrate simulation into broader Python workflows\r\n\r\n## Why This Talk Matters\r\n\r\nMany Python users face the \"scipy.integrate isn't enough, but I don't want MATLAB\" problem. This talk shows there's a middle ground: structured simulation that's open source, Pythonic, and extensible. The development story provides insights into building scientific software, while practical demos show immediate applicability.\r\n\r\nThe talk emphasizes technical substance over credentials - showing what problems PathSim solves and how, rather than where it's being used.", "code": "DBDCU9", "state": "confirmed", "created": "2025-11-26", "social_card_image": "/static/media/social/talks/DBDCU9.png", "speaker_names": "Milan Rother", "speakers": "\n### Milan Rother\n\nMilan is a freelance developer specializing in numerical simulation and scientific computing tools. Based in Braunschweig with a master's degree in EE from TU Braunschweig, he builds open-source frameworks that bridge engineering practice and Python scientific computing.\r\n\r\nAs the creator of PathSim, Milan has worked on a range of projects including modeling for biomedical sensors, design automation for integrated circuits, microwave imaging, and most recently nuclear fusion systems. Beyond PathSim, he maintains several scientific computing tools including vectorfitting algorithms, harmonic balance frameworks, and RFIC design tools.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "PyData & Scientific Libraries Stack"}, {"title": "Getting Career Clarity in Uncertain Times", "abstract": "Feeling unsure about your next step in your career?\r\n\r\nThe data & AI field is evolving faster than ever. New tools, new roles, and constant \u201cnext big things\u201d can make even experienced professionals feel unsure about where they are heading, and how to make intentional career decisions in the middle of all this change.\r\n\r\nYou might be doing well, feeling comfortable. Interesting work, steady progress, recognition.\r\nAnd still, there\u2019s that question in the background: Where is this actually going?\r\n\r\nThis interactive workshop helps you explore different future paths, understand trade-offs, and gain clarity about what kind of work and influence you want next.", "full_description": "The data & AI field is evolving faster than ever. New tools, new roles, and constant \u201cnext big things\u201d can make even experienced professionals feel unsure about where they are heading, and how to make intentional career decisions in the middle of all this change.\r\n\r\nYou might be doing well, feeling comfortable. Interesting work, steady progress, recognition.\r\nAnd still, there\u2019s that question in the background: Where is this actually going?\r\n\r\nA lot of career advice in tech assumes there is a clear path to follow. In reality, most data & AI careers don\u2019t work that way. Roles shift, organisations change, and what used to feel like a logical next step often isn\u2019t anymore.\r\n\r\nIn this workshop, we\u2019ll slow things down and focus on direction rather than decisions. The goal is not to figure out \u201cthe next job\u201d, but to get clearer on the kind of work you want to do.\r\n\r\nThis is a practical, hands-on session.\r\n\r\nWe\u2019ll use exercises such as odyssey planning to explore a few possible future paths you could take from here. \r\n\r\nYou\u2019ll work through:\r\n    \u2022    Different ways your career could evolve\r\n    \u2022    The trade-offs each direction comes with\r\n    \u2022    What feels worth exploring further, and what doesn\u2019t\r\n\r\nAnd, just as importantly, how you feel about these plausible futures.\r\n\r\nYou\u2019ll leave with a clearer sense of what matters to you now, and a stronger sense of direction\r\n\r\nBy the end of the session, you will:\r\n    \u2022    Have more clarity about the kind of work and influence you want going forward\r\n    \u2022    See more than one possible future, instead of feeling stuck with a single \u201cright\u201d option\r\n    \u2022    Feel more confident navigating uncertainty without rushing into decisions\r\n\r\nWho this session is for\r\n    \u2022    Data & AI professionals with a few years of experience or more\r\n    \u2022    People who feel \u201cin between\u201d stages, roles, or directions\r\n    \u2022    Individual contributors and leaders alike\r\n    \u2022    Anyone who wants more intentionality in their work", "code": "DDVW3W", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/DDVW3W.png", "speaker_names": "Tereza Iofciu", "speakers": "\n### Tereza Iofciu\n\nTereza Iofciu is a data and AI expert, leadership coach, and PSF Fellow with 15+ years of experience leading data and product teams at neuefische, FREE NOW, and New Work (XING). She helps professionals lead and adapt in the age of AI through her Data Diplomat Framework\u2122, bridging technical depth with human leadership.\n", "domain_expertise": "None", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "State of In-Browser ML: WebAssembly, WebGPU, and the Modern Stack", "abstract": "What if you could run real data/ML workflows right in your browsers - sandboxed, with no installation or sending your data anywhere? Such an approach would have tons of benefits: it is easy to distribute, safer by default, and can scale almost infinitely with virtually no infrastructure costs. \r\n\r\nThis talk is a pragmatic overview of the current in-browser ML stack. We\u2019ll cover what  workflows are realistic today (from training of traditional ML models to on-device LLM inference), how packaging/loading works, and the constraints one should be aware of. By the end of the talk you will have a clear sense of when in-browser ML is a good fit, and when it isn\u2019t.", "full_description": "Over the last few years, the tooling has matured enough to make \"ML in a tab\" worth taking seriously. Today, you can execute Python code in a sandboxed environment, ship interactive demos as a single URL, and even run LLM inference entirely on-device, without installations, servers, or sending data anywhere. In this talk, we will give a practical overview of the current in-browser ML stack, focusing on what is realistically possible today and the practical limits you still have to design around.\r\n\r\nWe will start with interactive environments such as JupyterLite and explain how they work under the hood via Pyodide: what it means to run CPython compiled to WebAssembly, how the filesystem and networking model differ from \"normal\" Python, and what that implies for performance, I/O, and package support. \r\n\r\nWe will then move from notebooks to applications with PyScript, showing how the same building blocks can be used to create shareable browser-based tools. We will also briefly cover the lower-level approach: using Pyodide directly and orchestrating it with JavaScript for granular control over loading, packaging, and data interchange.\r\n\r\nFinally, we will cover in-browser inference workflows for both traditional and deep learning models (via ONNX), and LLMs (via wllama and WebLLM), and discuss how WebGPU can accelerate these pipelines.\r\n\r\nBy the end of the talk, attendees will have a clear overview of the in-browser ML ecosystem and the practical intuition to decide whether it's the right choice for your next project.\r\n\r\n**Target Audience:** \r\nThis talk can be relevant for a broad audience. However, at least intermediate knowledge of ML / familiarity with Python ML ecosystem is required.\r\n\r\n**Outline:**\r\n- Introduction + Motivating examples [4 min]\r\n- Running Python in WebAssembly [6 min]\r\n    - Overview of Pyodide [2 min]\r\n    - Package management [3 min]\r\n    - Runtime and memory constraints [1 min]\r\n- Overview of interactive dev environments / JupyterLite [4 min] \r\n- Building applications with PyScript and direct Pyodide bindings [7 min]\r\n- On-device ML inference using ONNX, WebGPU, WebLLM, and wllama [5 min]\r\n- Q&A [4 min]", "code": "DQDEES", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/DQDEES.png", "speaker_names": "Oleh Kostromin", "speakers": "\n### Oleh Kostromin\n\nI am a Data Scientist primarily focused on Deep Learning and MLOps. In my spare time I contribute to several open-source python libraries.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "Personalized Restaurant Recommendations at Scale combining Transformer with Gradient-Boosted Ranking", "abstract": "Wolt\u2019s Universal Venue Ranker (UVR) is a large-scale, sequence-aware ranking model for personalized restaurant recommendations, deployed across more than 30 countries. UVR replaces three previously independent models\u2014Neural Collaborative Filtering, a second-pass ranker, and a first-time-user model\u2014by combining a transformer with a gradient-boosted decision tree for ranking.\r\n\r\nThe model follows a two-stage design. In the first stage, an encoder-style transformer learns a personalized user state representation from historical restaurant purchase sequences enriched with spatiotemporal signals such as time and location. In the second stage, a CatBoostRanker uses the transformer output as an input feature alongside additional user-, venue-, user\u2013venue-, and delivery-specific features to score and rank candidate venues.\r\n\r\nIn this talk, we present the model and service architecture, the training and evaluation setup, and both offline and online results from a multi-country online A/B test, demonstrating significant improvements in global conversion rate and new venue trial rate. We also share practical lessons from deploying and operating a multi-stage ranking model under strict latency constraints at global scale.", "full_description": "Personalized restaurant ranking is a core machine learning problem in food delivery platforms, requiring models to balance relevance, exploration, latency, and robustness across highly heterogeneous markets. In this talk, we present UVR (Universal Venue Ranker), Wolt\u2019s production ranking model for restaurant recommendations, currently deployed in more than 30 countries.\r\n\r\nUVR unifies the capabilities of three previously separate models\u2014Neural Collaborative Filtering (NCF), a second-pass ranker, and a first-time-user (FTU) model\u2014into a single, sequence-aware ranking approach. Beyond improving recommendation quality, this consolidation significantly reduced model complexity, operational overhead, and long-term maintenance cost.\r\n\r\nThe model follows a two-stage architecture implemented using widely adopted Python-based machine learning technologies, including PyTorch, CatBoost, and Flyte. The first stage is an encoder-style transformer trained with a classification loss on a next-purchase prediction task. It learns a compact user state representation from historical restaurant purchase sequences enriched with spatiotemporal information, such as purchase time and user location. This stage outputs a personalized venue relevance score.\r\n\r\nThe second stage is a CatBoostRanker, trained with a learning-to-rank loss on grouped venue requests. It combines the transformer-derived score with a rich set of additional features, including user-specific attributes, venue metadata, user\u2013venue interaction features, and delivery-related signals. This separation of objectives\u2014classification for representation learning and ranking for final scoring\u2014proved critical for both model performance and training stability.\r\n\r\nWe will walk through the end-to-end training and evaluation pipeline, covering feature construction, offline validation using ranking metrics, and a multi-country online A/B testing setup. UVR delivered significant and substantial improvements in global conversion rate and new venue trial rate, a key driver of long-term user retention. We will discuss how offline improvements translated into online gains.\r\n\r\nA dedicated section of the talk focuses on production and serving architecture, including low-latency inference and orchestration of training and deployment workflows using Flyte. We also share hard-earned lessons from training a multi-stage ranking model, such as preventing data leakage between models trained with different objectives and on different data as well as handling cold-start.\r\n\r\nFinally, we outline our roadmap toward extending UVR into a cross-domain ranking model for both restaurants and stores, enabling knowledge transfer across domains while preserving strong personalization guarantees.", "code": "DVCKHF", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/DVCKHF.png", "speaker_names": "Marcel Kurovski, Steffen Klempau", "speakers": "\n### Marcel Kurovski\n\nSenior Applied Scientist in Wolt's Personalization Team working on Venue and Item Ranking and Recommendation. Show Host of Recsperts - Recommender Systems Experts, the Podcast Show with industry and academia experts in Recommender Systems. Building Recommenders and Personalization Solutions with Python for various industries since 9+ years as well as creator and instructor of Python RecSys Training.\n\n### Steffen Klempau\n\n\n", "domain_expertise": "Advanced", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Reaching the next level of abstraction: meta classes and what they enable", "abstract": "Python is especially powerful due to its deep meta programming capabilities. In this talk, I give an overview of one example: meta classes. I show how you can use them to customize class creation, ensure data integrity, or define your own syntactic sugar for classes.", "full_description": "Python is accessible and easy, but what makes it especially fun and powerful are its deep meta programming capabilities. One salient example are meta classes, which allow us to deeply hook into the class creation process. But, they seem quite complex at first glance, which may have deterred you so far from exploring them. In my talk, I want to alleviate your uncertainty and give you concrete examples of how meta classes work and what they enable you to do. We will look at using them to customize class creation, ensure data integrity by adding custom validators, or defining custom syntactic sugar that reduces boilerplate.\r\n\r\nOutline:\r\n* Programming and meta programming\r\n* Everything is an object\r\n* Higher-order functions\r\n* Meta class basics: customizing class creation and enforcing constraints\r\n* Advanced example: custom syntactic sugar\r\n* With great power comes great responsibility", "code": "EE39VN", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/EE39VN.png", "speaker_names": "Valentin Zieglmeier", "speakers": "\n### Valentin Zieglmeier\n\nI work as a software consultant at TNG Technology Consulting. Previously, I completed a doctorate (Dr. rer. nat.) in Computer Science at the Technical University of Munich (TUM) in the area of software engineering where I taught a course on advanced Python programming.\n", "domain_expertise": "None", "python_skill": "Advanced", "track": "Python Language & Ecosystem"}, {"title": "Hierarchical Models in MMM: Can Structure beat data size?", "abstract": "In every marketing project, teams strive to find more data, a longer timeframe, and more detailed splits, just to fix noisy channel attribution.\r\n\r\nBut what if structure played a bigger role than size and volume? \r\nIn this talk, we try to prove this. Using a simple toolkit like Arviz and PyMC, we show you a simple hierarchical mix model, and how, by applying partial pooling, we can stabilize important KPIS like ROAS estimates across sparse channels- without the need for more data.\r\nWe will go through the code, transformation, and the real-life practices that allow us to get as close to the truth, to be able to have a meaningful impact in the marketing world.\r\nThe approach will be centered around marketing mix models, different transformations, and how useful it will be for the business.", "full_description": "## What we are going to show\r\n\r\n- Country-specific marketing Data that is, unfortunately, never good.\r\n- Function and Python transform like Adstock and saturation (with the tests so that you can see it in action).\r\n- Differentiation between pooled, unpooled, and partial pooling.\r\n- Meaningful diagnostics.\r\n- Wins and losses of hierarchical modeling.\r\n\r\n## Why this is interesting and relevant\r\n\r\nHow do you model marketing effectiveness when you only have 12 months of data per country, some channels are interrupted for weeks, and your manager wants reliable ROAS estimates yesterday?\r\nMost teams think: \"We need more data.\" But getting more data takes time, costs money, and sometimes isn't even possible (or the quality is bad).\r\n\r\nWhat if you could get better estimates by changing how you model the problem?\r\nThis is where hierarchical modeling and partial pooling come in. Instead of treating each market as separate (unpooled) or pretending they're all identical (pooled), we let markets share information through partial pooling. Countries with thin data borrow strength from the group, while markets with strong signals pull away from the mean. You get stability where you need it and flexibility where the data supports it. We show this end-to-end in Python: from building testable transform functions (Adstock, saturation curves, lag effects) to assembling three different model architectures in PyMC, to evaluating which one gives you calibrated intervals and stable ROAS estimates. You'll see the good, the bad, and the ugly.\r\n\r\n## Main challenges\r\n- Making transforms reusable and testable_ Marketing transformations like adstock and saturation are usually hidden in modeling code. It is generally very difficult to imagine how they look, how they change the data. We pull them out as pure Python functions with clear signatures, unit tests (pytest), and property-based checks (hypothesis). This makes them composable, debuggable, and easy to understand and even improve.\r\n- Building fair model comparisons: We construct pooled, unpooled, and hierarchical models with identical priors where appropriate so the comparison isolates the effect of structure, not prior choice. We walk through the PyMC code, show how partial pooling works mathematically, and run short MCMC chains that still demonstrate the key differences.\r\nWe go beyond \"we reached 90% R2\" to actual decision metrics:\r\n     * Posterior predictive checks: Does the model generate realistic data?\r\n     * ROAS stability: how much do channel estimates vary across groups?\r\n\r\nWe use ArviZ throughout to visualize traces, compare models, and compute these metrics. You'll see exactly when hierarchical structure pays off and when it doesn't.\r\n\r\n## Practical lessons and the repo\r\n\r\n**We share what we learned building this:**\r\n- Data checks and control using Pydantic, so you catch errors before MCMC runs for hours\r\n- Test your transforms independently: Yes, for unit tests!\r\n- Use synthetic data with known ground truth to validate the whole pipeline\r\n- Calibration metrics matter more than posterior predictive RMSE alone\r\n\r\n**The repo will include:**\r\n- Typed transform functions (Adstock, saturation, lag) with unit tests\r\n- Three PyMC models with matching priors\r\n- ArviZ evaluation scripts (calibration, PPC)\r\n- A Typer CLI to run everything on a predefined CSV\r\n\r\n**When hierarchical lose (and what to do about it):**\r\nPartial pooling isn't magic. If your groups are genuinely wildly different and you have almost no data per group, hierarchical models can still produce overconfident nonsense. We show a scenario where this happens and discuss alternatives: stronger priors, splitting the hierarchy, or just admitting you don't have enough signal.\r\nThe takeaway: structure beats volume in the right conditions. We help you recognize those conditions and build models that respect them.", "code": "EL7X8C", "state": "confirmed", "created": "2025-11-25", "social_card_image": "/static/media/social/talks/EL7X8C.png", "speaker_names": "Mohamed Amine Jebari", "speakers": "\n### Mohamed Amine Jebari\n\nMohamed Amine Jebari is a Lead Data Scientist based in Berlin, specializing in large-scale machine learning systems, Marketing Mix Modeling, and applied NLP. With extensive hands-on experience in Python and the scientific ecosystem, including pandas, NumPy, scikit-learn, PyMC, transformers, and Hugging Face. Amine builds end-to-end solutions that bridge rigorous statistical modeling with modern LLM-driven workflows.\r\n\r\nWorking at a data-driven consultancy, he leads a team of data scientists while remaining deeply involved in technical development, from Bayesian modeling to production-grade pipelines on AWS. Their work often focuses on solving real-world business problems with interpretable, high-impact models.\r\nCurious to uncover the truth and being a big fan of puzzles, he is now heavily working on causal inference and marketing mix models, pulling one inch at a time, closer the the truth.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Building MCP at the Speed of Hype: Principles That Outlast the Trends", "abstract": "Every week, development in AI brings us another groundbreaking release, another model version, another must-have integration. In this rapidly shifting landscape, how does one build production systems that won\u2019t be obsolete by the time you deploy them?\r\n\r\nWe'll explain how trusting in proven engineering principles from software development and machine learning, like separation of concerns and evaluation practices, became our anchor in an ever-changing landscape of AI development. We share lessons learned from building two MCP applications using FastMCP and PydanticAI. Against these challenges, we found that fundamental engineering principles provided the foundation we needed. \r\n\r\nParticipants in the process of developing AI tools will leave with practical strategies for building AI-powered systems that are flexible enough to adapt, yet stable enough to trust.", "full_description": "Every week, AI brings us another groundbreaking release, another model version, another must-have integration. Among these developments, agentic systems have emerged as a key component. Introduced at the end of 2024, the Model Context Protocol (MCP) has become an important enabler of this change and has established itself as the standard for connecting AI agents with external data sources and tools. In this rapidly shifting landscape, how does one build production systems that won't be obsolete by the time you deploy them?\r\n\r\nThis talk shares practical lessons from building two real-world MCP applications with FastMCP and PydanticAI: JobmonitorMCP, which leverages the jobmonitor.de API to create intelligent regional labor market reports, and a tool for an international non-profit combining multiple agents into a powerful question and answer application. \r\n\r\nDuring development, we faced multiple challenges: MCP clients and models that interpret the same protocol differently, emerging features with limited documentation and trying to evaluate non-deterministic outputs. Stakeholders repeatedly asked \"Why does it behave differently today?\" and \"Are we using the newest model yet?\"\r\n\r\nWhat we learned: The antidote to AI hype isn't avoiding new technology, it's anchoring development in trusted engineering principles. Separation of concerns and focused components helped us design for the protocol rather than specific clients. Rigorous evaluation approaches combined LLM-as-Judge with manual review and user feedback. Transparent communication helped us manage expectations around AI capabilities without undermining confidence.\r\n\r\nThis session targets intermediate Python developers building or planning to build AI-powered applications. You'll leave with concrete strategies for building AI systems that adapt to new models while maintaining production stability, reflection questions for your own projects, and perhaps a little more confidence in your existing knowledge.", "code": "EPASS8", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/EPASS8.png", "speaker_names": "Rahkakavee Baskaran, Friederike Bauer", "speakers": "\n### Rahkakavee Baskaran\n\nRahkakavee Baskaran is the Data Lead at &effect. As a developer, she works in field of Natural Language Processing and Generative AI with experience in software and infrastructure development.  Her work focuses on leveraging data science and software development to create social impact, particularly in projects related to social sciences and the public sector.\n\n### Friederike Bauer\n\nFriederike Bauer (First-time-speaker on technical conference) studied Social Science in Stuttgart and Social and Economic Data Science in Konstanz. She works as a Data Scientist for &effect and develops software solutions as a Frontend-Developer.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Don\u2019t call your LLM too often! How to build your dialog graph with confidence and sleep at night.", "abstract": "Keywords: **Explainable AI, enhanced RAG, GraphRAG, LLMOps, dialog system evaluation.**\r\n\r\nDesigning reliable dialog flows for LLM-based systems remains challenging once conversations require branching, correction, or multi-step reasoning. Dialog graphs often evolve organically and accumulate structural issues: endless correction loops, dead subpaths, redundant validation steps, overly generic catch-all branches, or linear sequences that should be collapsed. Such phenomena raise operational costs, significantly increase TTFT and make the system answer less predictable and explainable.\r\n\r\nMany solutions try to introduce an all-fit generalized RAG retrieval solution. Contrary to this, we present our empirical learnings on how to enhance system speed, lower overall costs and offer a better dialog graph explainability through enhanced LLM call tracing and iterative enhancements for common dialog paths.\r\n\r\nWe also show that more elaborated knowledge retrieval strategies like GraphRAG may drastically enhance overall response quality and shorten the dialog graph. We evaluate several approaches and give recommendations on how to leverage more complex document indexing phases for inference time benefits.\r\n\r\nOverall, the session argues that scalable conversational systems require not only better prompts, but explicit graph structures paired with rigorous tracing and data-driven optimization.", "full_description": "Building reliable dialog flows for LLM-based conversational systems remains difficult once interactions move beyond linear question\u2013answer patterns. While early prototypes often rely on prompt chains, real-world systems quickly require branching, correction, clarification, and multi-step reasoning. At this stage, dialog logic implicitly turns into a graph, yet is still implemented and reasoned about as a sequence. This mismatch leads to structural problems that are hard to detect without explicit modeling and observability.\r\n\r\nComplex document retrieval systems are not born out of theoretical itch. We\u2019ll exemplify practical problems framing them around the following practical use case from the area of electricity/power production.\r\n\r\n*Use Case: Aladdin and the Case of the Almost-Exploding Power Plant*\r\n\r\nRick and Morty are operations engineers at a large electrical power plant. Every single day, they face the same heroic challenge: too many documents, too little clarity.\r\n\r\nThe technical staff produces a constant stream of operational reports: free-text summaries describing the health and performance of steam generators. These reports are rich in knowledge, but poor in structure. Rick\u2019s daily ritual is to read, compare, and summarize them, trying to predict which units will soon need maintenance. If he gets it right, the plant saves money by avoiding unnecessary service routines which are prescribed by regular maintenance guidelines. If he gets it wrong\u2026 well, let\u2019s just say steam generators have a dramatic way of expressing dissatisfaction.\r\n\r\nBut unstructured reports are only one part of the story. Alongside them exists a well-behaved, structured world: databases containing results of regular, non-invasive ultrasonic inspections of pipelines, used to track corrosion development over time. Morty has built a quantitative model that predicts the probability (and timing out of this probability) of a pipeline rupture based on these corrosion measurements.\r\n\r\nNaturally, Rick and Morty want everything. They want one system that can: 1) Understand messy human-written reports, 2) Reason over numerical corrosion models, and 2) Answer simple document questions without investing into unnecessary intelligence.\r\n\r\n\r\nThus, the system Aladdin is born.\r\n\r\nAladdin combines three very different subsystems:\r\n\r\n  - An agentic indexing component, which dynamically builds a search index for a GraphRAG over heterogeneous documents, given a pre-defined graph structure.\r\n  - An autonomous analytical agent, which evaluates pipeline failure probabilities using Morty\u2019s quantitative corrosion model.\r\n  - A lightweight text-based RAG, backed by a vector index, for fast and simple document retrieval.\r\n\r\nBut what is the challenge? Once these components start talking to each other, the dialog graph becomes unpredictable. Execution paths depend heavily on what information is actually present in the documents. And this is something that cannot be fully reasoned about in advance. Loops appear, branches explode, and theoretically \u201cclean\u201d dialog designs fail in practice.\r\n\r\nThis use case illustrates why observability, tracing, and empirical optimization of dialog graphs are essential when building real-world document retrieval systems for industrial environments. Especially when Rick just wants a straight answer and Morty really doesn\u2019t want another pipeline incident on his watch.\r\n\r\nGiven this use case we will exemplify several structural pathologic cases in the dialog graph which we observed in the practice and for which we found curative approaches.\r\n\r\n**Non-ending loops in the dialog graph**\r\nA frequent failure mode is the emergence of endless circular dialog graphs. Typical examples include:\r\n  - correction loops (\u201cPlease rephrase your input\u201d \u2192 user rephrases \u2192 validation fails again \u2192 same prompt),\r\n  - clarification cycles (\u201cWhat do you mean by X?\u201d \u2192 partial answer \u2192 same clarification),\r\n  - fallback loops where a generic catch-all path routes the conversation back to an earlier state without introducing new information.\r\n\r\nSuch cycles are rarely intentional; they arise from local fixes applied over time and are difficult to identify by prompt inspection alone. In production, they manifest as stalled conversations, increased latency, rising token costs, and user frustration.\r\n\r\nBeyond circularity, several other structural pathologies commonly appear in document retrieval systems.\r\n\r\n**Dead subpaths after non-matching branching conditions**\r\n\r\nDialog graphs often include branches guarded by semantic or data-dependent conditions, but changes in document structure, embeddings, or preprocessing can make these conditions unsatisfiable, creating dead subpaths that are never executed. These paths are dangerous because they give a false sense of coverage, increase maintenance and reasoning complexity, and in production often manifest as mysterious fallback behavior where the system always takes a default route instead of a specialized one.\r\n\r\n**Redundant validation and re-validation steps**\r\n\r\nAnother common issue is redundant validation, where the same or equivalent checks are performed multiple times along a single dialog path. This often happens when validation logic is added defensively at multiple layers: once at input parsing, again before retrieval, and again before response generation. While each validation step may seem harmless in isolation, their combination leads to inflated dialog depth, unnecessary latency, and increased cognitive load when analyzing traces. Worse, slight inconsistencies between validation prompts can produce contradictory outcomes, for example, an input being accepted in one step and rejected in the next.\r\n\r\n**Overly generic catch-all branches**\r\n\r\nCatch-all branches are often introduced as a safety mechanism: a \u201cdefault\u201d path that handles unexpected input or retrieval failure. Over time, however, these branches tend to grow in scope and responsibility, eventually becoming overly generic handlers that do everything. Such branches blur the distinction between genuinely exceptional situations and routine cases. As more logic is added to the catch-all path, it becomes harder to reason about what the system is actually responding to. Specialized logic may be silently bypassed, while unrelated scenarios are forced through the same generic response strategy.\r\n\r\n**Linear sequences that should be collapsed**\r\n\r\nMany dialog graphs contain long linear chains of nodes with no branching, no state changes, and no observable side effects between steps. These sequences often originate from iterative prompt development, where small transformations are added one by one (\u201cextract entities\u201d \u2192 \u201cnormalize entities\u201d \u2192 \u201crephrase query\u201d \u2192 \u201ccheck relevance\u201d). While conceptually clean, such linear chains are rarely optimal. They increase token usage, latency, and the number of failure points, without adding expressive power. More importantly, they obscure the true logical structure of the system: what could be a single semantic transformation is spread across multiple opaque steps.\r\n\r\nAn additional aspect of an overcomplicated dialog graph - especially baked by an autonomous agent - are barely predictable costs. Autonomous parts of the system need a very tight observability net to stay under control and not to burst cost prediction by an order of magnitude.\r\n\r\nWorking within a specifically regulated environment of a power plant posts additional restrictions on the explainability of the results. Every fact must be trackable to the source of the information and model hallucinations must be recognized in the very early step. \r\n\r\nAll the above requirements result in a setup which is heavily based on an LLM Operating Platform like Langfuse. \r\n\r\nWhen combined with dialog-oriented orchestration frameworks such as Langflow, experiment tracking extends from single calls to full conversational trajectories. Complete dialog traces expose path stability, node utilization, dead branches, fallback prevalence, and user-facing metrics such as turns to resolution or correction-loop repetition.\r\n\r\nOver time, this empirical evidence replaces design-time assumptions. Dialog paths are merged or removed based on observed execution rather than theoretical intent, with unreachable branches, redundant validations, and unstable loops revealed directly through trace analysis. Dialog graph optimization thus becomes a continuous, reproducible process grounded in measured behavior.\r\n\r\nThis talk proposes an engineering-oriented approach that models conversational logic as explicit dialog graphs and treats execution traces as first-class data. Using Langfuse instrumentation, developers can analyze concrete execution paths\u2014branch frequency, loop formation, latency hotspots\u2014and compare alternative graph designs through aggregated metrics and A/B testing, enabling systematic optimization based on evidence rather than intuition.\r\n\r\n**To sum up:** using concrete production-oriented examples, the talk shows how graph-based dialog design improves multi-step retrieval, explainability, and robustness across languages. Endless correction loops are detected and eliminated, dead branches are pruned, and overly generic catch-all paths are replaced with targeted recovery strategies. The overall message is that scalable conversational systems require not just better prompts or larger models, but explicit dialog graphs combined with rigorous tracing and data-driven optimization.", "code": "EWZMJK", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/EWZMJK.png", "speaker_names": "Evgeniya Ovchinnikova, Andrei Beliankou", "speakers": "\n### Evgeniya Ovchinnikova\n\nAbout\r\nI build solutions that make technology work for people. With experience in AI, data, and automation, I turn real needs into tools that make work faster and smarter.\r\n\r\nTrained as a physicist, I moved into data science and innovation to make a more direct impact on real-world problems. Since then, I\u2019ve worked across telecommunications, energy, e-commerce, and insurance\u2014helping teams create technology that delivers real value.\r\n\r\nOne highlight was helping build a GenAI platform used by more than 48,000 people, saving over 2 million working hours every year. I also contributed to an intelligent system that helps over 20,000 employees share knowledge more easily and work together more effectively.\r\n\r\nI enjoy learning, improving, and working with others who want to make a difference. Let\u2019s connect and explore new ideas together.\n\n### Andrei Beliankou\n\n\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Black Hole Stars: An Astronomical Mystery (Mostly) Solved with NumPyro and JAX", "abstract": "The James Webb Space Telescope has revealed a mysterious population of \"Little Red Dots\": extremely distant objects that have upended our understanding of the early Universe. However, revealing the true nature of these marvels requires computationally-intensive statistical modeling of complex astronomical data. In this talk, we explore how we used JAX and NumPyro to help solve this puzzle. We will introduce these powerful Python tools, demonstrate how they accelerate complex statistical data analysis, and show how they provided evidence that Little Red Dots may in fact be \"Black Hole Stars.\"", "full_description": "The James Webb Space Telescope (JWST) has transformed extragalactic astronomy. In particular, it has uncovering a puzzling population of compact, red objects in the distant universe known as \"Little Red Dots\" (LRDs). These sources exhibit distinctive features not seen in typical galaxies and therefore their nature remains a subject of intense debate. Are they supermassive black holes hiding behind screens of dust? Massive dead galaxies appearing too early in the universe? Or something entirely new? To find out, we need to perform computationally heavy statistical analysis on the astronomical data. However traditional tools have been too slow or made many assumptions to reduce the complexity of the JWST data that can lead to inaccurate results.  \r\n\r\nIn this talk, I will introduce the modern Python stack that now makes this possible: JAX and NumPyro. JAX allows you to write standard Python code that runs on GPUs and automatically computes derivatives, while NumPyro leverages that power for incredibly fast statistical modeling. We will start with the basics, using simple examples to demonstrate how JAX can speed up existing workflows and how NumPyro makes Bayesian inference accessible. \r\n\r\nThen, we will look at the \"Little Red Dot\" mystery as a case study. I will show how we built a custom inference engine (`unite`) to process thousands of JWST observations. By leveraging JAX's speed and NumPyro's flexibility, we were able to efficiently and accurately test complex physical models against the data, uncovering evidence that these unique may in fact be supermassive black holes embedded in dense gas clouds: essentially, stars powered not by fusion but by black holes.\r\n\r\nThis talk is for anyone interested in high-performance Python and especially for (data) scientists interested in modern scientific methods in designing scalable inference pipelines. You will leave with a solid introduction to JAX and NumPyro and an appreciation for how these tools are already helping solve the Universe's greatest mysteries.", "code": "EXXWMV", "state": "confirmed", "created": "2025-12-16", "social_card_image": "/static/media/social/talks/EXXWMV.png", "speaker_names": "Raphael Hviding", "speakers": "\n### Raphael Hviding\n\nDr. Raphael Hviding is Astronomer working at the Max-Planck Institute for Astronomy. He is a member of the Data Science and Galaxies & Cosmology Departments. He works on problems related to complex data analysis from the world's frontier observatories as well as the applications of data science to solving astronomical mysteries. Originally from the USA, he obtained his PhD from Steward Observatory at the University of Arizona working on insights Dust-Obscured Supermassive Black Holes from large Astronomical Surveys. \r\nHe now lives in Heidelberg with his wife and three cats, enjoys cycling, bouldering, and building computers.\n", "domain_expertise": "None", "python_skill": "Novice", "track": "PyData & Scientific Libraries Stack"}, {"title": "Scaling Data Processing for Training Workloads at DeepL Research with Rust", "abstract": "This talk will detail how we used Rust to solve a number of resource utilization inefficiencies while scaling data pre-processing to a petabyte scale and enable next-generation model training at DeepL. Besides other factors, this was done by developing an internal library for interacting with Parquet files in a memory efficient nature.\r\n\r\nTopics include:\r\n\u2022 Convincing you to love Rust for its memory safety\r\n\u2022 Comparing C++ and Rust ecosystems for Python library development\r\n\u2022 Diving into Python-Rust interoperability\r\n\u2022 Convincing you to love Rust for its user-friendly (yes, actually!) language features\r\n\u2022 Providing a high-level overview of the continuously growing impact that Rust is having on the Arrow and data engineering ecosystem", "full_description": "We set out to replace an inefficient internal file format with an industry standard - a seemingly straightforward task. What we got instead was a descent into memory leak hell.\r\n\r\nThis talk will walk you through our journey of scaling DeepL's data preprocessing and model training pipelines to handle petabyte-scale corpora. When open-source C++-based Python libraries proved too unstable and memory-inefficient, we invested time and resources into developing our own Rust-based tooling and, compared to our previous internal file format, decreased memory load by a factor of 10 and latency until first byte read by a factor of 50.\r\n\r\nWhat we'll cover:\r\n\u2022 **Why Rust's memory safety guarantees matter in practice:** We will provide a direct comparison of our results using C++-based vs Rust-based implementations for data processing libraries.\r\n\u2022 **The Rust ecosystem advantage for Python interop:** While C++ offers a fragmented landscape of build systems and tooling choices, Rust provides a canonical path with cargo, maturin, and PyO3\u2014providing a clean interface for everything from GIL management to readable, zero-copy conversions between Rust and Python objects\r\n\u2022 **Rust's surprisingly friendly features:** Despite its reputation for having a steep learning curve, Rust offers language features that make it genuinely pleasant to work with, even for beginners coming from a Python background: from enums to pattern matching, error handling with Result, and cargo's canonical, ergonomic tooling.\r\n\u2022 **Rust's impact on the arrow ecosystem and data engineering with Python in general:** Besides the well-known impact that Rust-based data processing libraries like polars, Daft, and datafusion are having on the engineering ecosystem, we we will show how the Rust implementation of Arrow called arrow-rs is having a growing impact and expanding the data engineering toolkit by powering an increasing number of great and contributor-friendly processing and introspection tools built in Rust.", "code": "F79RG9", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/F79RG9.png", "speaker_names": "Jonas Dedden, Johanna", "speakers": "\n### Jonas Dedden\n\nHi, I'm Jonas Dedden, Staff Research Data Engineer at DeepL SE, Germany. Johanna Goergen and I work at the Research Data Platform team of DeepL Research, where we are responsible for the on-prem & cloud-based k8s compute infrastructure for petabyte scale data processing pipelines. We provide the platform that our Research Data Engineers can use to collect & preprocess all data needed for training the DeepL foundational language models that power our production services.\n\n### Johanna\n\n\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Rust"}, {"title": "Increase productivity of CNC-machining of aerospace engine parts with Python", "abstract": "Increasing unit labour costs and the imperative need to reduce energy consumption raises the necessity to enhance productivity in industrial production. Python is an excellent tool for GKN Aerospace, as the world\u2019s leading tier one aerospace supplier, to address the needs for higher utilization and unmanned operation on the shopfloor on its site in Kongsberg, Norway.\r\n\r\nAs an example, the presentation shares insight into the in-house developed \u201cProduction Execution System\u201d, consisting of a Python backend and a REACT frontend. The application orchestrates all necessary data on cell-level, like NC-programs and additional digital services of the company\u2019s IT environment during unmanned production. Furthermore, it supports the operator with necessary information to ensure highest quality of engine parts in a work environment of increasing digitalization and workload.", "full_description": "Python is not exclusively a powerful tool for datascience and web-development. It gains in importance on the shopfloor in industrial production, too. Increasing unit labour costs and the imperative need to reduce energy consumption raises the need to enhance productivity in industrial production in general.\r\n\r\nFor GKN Aerospace, as the world\u2019s leading tier one aerospace supplier of systems and components, this leads to higher utilization and unmanned operation of a high variety of production processes and CNC-machining tools. On its site in Kongsberg, Norway, GKN produces mainly turbine shafts and casings for civil and military engines, which are used for up to 100.000 flights every day around the globe.\r\n\r\nThe programming language Python enables fast and forward-thinking development of powerful applications which supports the operators and increases the degree of automation in the next years by concurrent assurance to fulfill demanding quality requirements and to cope the workload.\r\n\r\nStandardization of applications is a key factor for increasing robustness of the automated production and reduce maintenance effort. Therefore a standardized interface to NC-controller and PLC\u2019s from the \u201cpre ASCII era\u201d in the 70ies to up-to-date systems had to be found and suitable gateway services had to be developed.\r\n\r\nAs an example for the usage of Python on the GKN shopfloor in Norway, a standardized in-house developed \u201cProduction Execution System\u201d, consisting of a Python backend and a REACT frontend is presented. Connected to more than 25 different machining tools on the shopfloor and additional digital services of the company\u2019s IT environment, the application orchestrates all necessary data on cell-level. It provides data, like NC-programs and part meta-data to the machining tools and additional process information to the operators, necessary to produce a high variety of different engine components.\r\n\r\nIt enables unmanned production by selecting the next part for machining and commanding part changes to the machine to maximize utilization of the means of production. Furthermore, it reports back collected data for usage by other processes downstream in production.\r\n\r\nCloud-based and due to its restart capability in running production, bugfixes and implementation of new features can be carried out \u201con-the-fly\u201d. Thereby automated unit- and integration testing for the core functionalities ensure robustness for the high variety of used machining tools.\r\n\r\nThe development and usage of the \u201cProduction Execution System\u201d on GKN Aerospace\u2019s shopfloor are an excellent example for the increasing importance of the programming language Python to ensures highest quality of engine parts in a work environment of increasing digitalization and workload.", "code": "FJQXEQ", "state": "confirmed", "created": "2025-11-28", "social_card_image": "/static/media/social/talks/FJQXEQ.png", "speaker_names": "Nico Buhl", "speakers": "\n### Nico Buhl\n\nWith a background as mechanical engineer and a PhD in material science, I describe myself as a mechanical engineer who can program and not as a software engineer. Started programming in Python, Perl, PHP and C++ as a pupil for fun in the end of the 90ths, I lost track of programming for some years during mechanical studies just to rediscover Python several years ago as a perfect tool for engineers to solve real-world problems in industrial production. The huge eco-system of Python and the intuitive syntax open new opportunities to me to combine domain knowledge in industrial processes with digitalization solution approaches. I enjoy sharing this with all people interested in improving industrial production in context of digitalization.\n", "domain_expertise": "None", "python_skill": "None", "track": "Programming & Software Engineering & Testing"}, {"title": "Are we free-threaded ready? Looking at where free-threaded Python fails", "abstract": "Free-threaded Python aims to significantly improve performance, allowing multiple native threads to execute Python bytecode concurrently. In this talk, we will explore the current state of Python's free-threading initiative and assess its practical readiness for widespread adoption.", "full_description": "We begin by exploring the background of free-threaded Python, summarising its origins, current status, and the technical differences distinguishing it from standard Python implementations. A key focus will be examining the compatibility landscape, specifically investigating how many popular third-party libraries are currently prepared for free-threading. We will distinguish between generic pure Python wheels and explicitly free-threaded wheels and I\u2019ll explain how the community can contribute to compatibility verification. \r\n\r\nWe then critically discuss free-threaded Python's necessity, weighing the disadvantage of increased thread safety concerns (and verification methods) against the promised advantage of speed (including multithreaded profiling).Will free-threaded Python become a critical future direction for the language? How can you contribute? If and how specific projects can immediately benefit from it? Let\u2019s find out together!", "code": "FP7YN7", "state": "confirmed", "created": "2025-12-05", "social_card_image": "/static/media/social/talks/FP7YN7.png", "speaker_names": "Cheuk Ting Ho", "speakers": "\n### Cheuk Ting Ho\n\nAfter having a career as a Data Scientist and Developer Advocate, Cheuk dedicated her work to the open-source community. Currently, she is working as a developer advocate for JetBrains. She has co-founded Humble Data, a beginner Python workshop that has been happening around the world. Cheuk also started and hosted a Python podcast, PyPodCats, which highlights the achievements of underrepresented members in the community. She has served the EuroPython Society board for two years and is now a fellow and director of the Python Software Foundation.\n", "domain_expertise": "Advanced", "python_skill": "Advanced", "track": "Python Language & Ecosystem"}, {"title": "From Row-Wise to Columnar: Speeding Up PySpark UDFs with Arrow and Polars", "abstract": "Python UDFs often become the slowest part of PySpark pipelines because they run row-by-row and pay a high cost crossing the JVM\u2194Python boundary. Spark\u2019s Arrow-backed execution changes that cost model by moving data in columnar batches, which can reduce overhead and enable efficient, vectorized processing in Python.\r\n\r\nIn this session, we\u2019ll cover practical patterns for writing Arrow-friendly UDF logic and integrating it with fast Python execution engines that operate on Arrow data. We\u2019ll compare common approaches\u2014scalar UDFs, Pandas UDFs, Arrow-native UDFs, and table-shaped Arrow transforms\u2014then translate the results into a decision guide you can apply to production pipelines. Attendees will leave knowing when Arrow helps, when it doesn\u2019t, and how to design UDF-heavy transformations that scale.", "full_description": "Objective\r\nDemonstrate how to accelerate UDF-heavy PySpark workloads by switching from row-wise execution to Arrow-backed columnar execution, using Polars for fast, maintainable column transformations and table transformations.\r\n\r\nKey Takeways\r\n- How Arrow is being used in PySpark for batched, columnar data exchange\r\n- Why Polars helps: a higher-level DataFrame API plus Arrow interoperability that can often reuse Arrow buffers\r\n- How to design fast column transformations (column in \u2192 column out) and fast table transformations (batch/table in \u2192 batch/table out).\r\n- Benchmarks and tradeoffs across scalar UDFs, Pandas UDFs, Arrow-native UDFs, and Polars-based Arrow table transforms on real-world examples.\r\n\r\nAudience\r\n- Data engineers and data scientists working with PySpark at scale\r\n- Engineers seeking concrete strategies to optimize spark pipelines that rely on Python UDFs\r\n\r\nKnowledge Expected\r\n- Familiarity with PySpark DataFrames and UDFs\r\n- Basic understanding of Spark execution helps but is not required\r\n- Exposure to Polars/Arrow is not required but might be beneficial", "code": "FT7V39", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/FT7V39.png", "speaker_names": "Aimilios Tsouvelekakis", "speakers": "\n### Aimilios Tsouvelekakis\n\nAimilios works as a software engineer for Frontiers Media SA. With a passion for solving technical challenges and a commitment to sharing his knowledge in different aspects of computer engineering, including but not limited to ETL pipelines and optimization, improving the in-house tooling, contributing to different architectural decisions, he makes a valuable contribution to his team's objectives. Prior to joining Frontiers, he gained experience working as a Devops engineer at CERN, where he actively contributed in projects related to cloud computing and disaster recovery, automation, observability and databases. He holds a MEng in Electrical and Computer Engineering from National Technical University of Athens.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Surviving AI Fatigue: Staying Sane and Relevant in a Fast Moving Field", "abstract": "In an era where new AI models, benchmarks, and frameworks emerge daily, many of us feel caught in a relentless cycle of catching up, what is called \"AI fatigue\". This talk dives into the causes and consequences of that fatigue, from information overload and social media hype to the constant pressure to stay relevant. Drawing on personal experience and community insights, we explore why chasing every new paper or trend often leads to burnout rather than mastery.\r\n\r\nMore importantly, we share practical, evidence-backed strategies to stay informed without losing balance: curating a focused \u201cinformation diet,\u201d setting clear boundaries, using summarization tools intelligently, maintaining a personal knowledge base, and embracing \u201cJOMO\u201d\u2014the joy of missing out. We also discuss how organizations can combat fatigue structurally by promoting focus, curiosity, and psychological safety.\r\n\r\nThis session is for anyone, from beginners to seasoned professionals, seeking to rediscover genuine curiosity in AI while preserving mental well-being. Attendees will leave with concrete tools, actionable habits, and a renewed sense that it is not only acceptable but healthy to not know everything.", "full_description": "The world of AI and machine learning is moving at breakneck speed, with new papers, models, benchmarks, and frameworks announced daily. If you have ever felt overwhelmed, behind, or simply exhausted trying to keep up, you are not alone. In this talk, we share our own journey grappling with AI fatigue, what it feels like, why it happens, and what we have learned about staying informed without burning out.\r\n\r\nWe will start by defining AI fatigue and reflecting on why it is such a pervasive experience in our community, from social media hype to the sheer pace of real innovation. We highlight some of the common pitfalls, like chasing every trend, consuming too much noise, or neglecting mental health, and show why these approaches are counterproductive.\r\n\r\nThen, we focus on actionable strategies and habits that actually work. We share concrete tips and techniques we personally use to manage our learning and maintain our enthusiasm for the field, including:\r\n\r\n- Crafting an intentional information diet with trusted sources\r\n- Setting clear boundaries and time boxing your learning\r\n- Building a personal knowledge base for long term retention\r\n- Using summarization tools to cut through dense papers and blogs\r\n- Practicing \u201cJOMO,\u201d the joy of missing out, by focusing on depth over breadth\r\n- Learning in public by teaching, blogging, or pairing with others\r\n- Designing small, achievable experiments to stay engaged and motivated\r\n\r\nFinally, we will suggest how organizations and teams can help prevent fatigue at a structural level by fostering focus, psychological safety, and curiosity instead of always on urgency.\r\n\r\nThis talk is for anyone, from beginner to expert, who wants to stay relevant and curious about AI without losing sight of their well being. You will leave with a set of practical tools, a fresh perspective on learning in a chaotic environment, and hopefully the reassurance that it is okay to not know everything.", "code": "GATMPP", "state": "confirmed", "created": "2025-12-14", "social_card_image": "/static/media/social/talks/GATMPP.png", "speaker_names": "Ajay, Jeyashree Krishnan", "speakers": "\n### Ajay\n\nSenior R&D Engineer at Ansys with a PhD in computational science from RWTH Aachen University. Work in the area of simulations, machine learning and AI safety.\n\n### Jeyashree Krishnan\n\nJeyashree Krishnan is a Senior Machine Learning Engineer at Siemens AG. Her work focuses on building and operationalizing scalable machine learning services, with an emphasis on foundation models and time series forecasting. She is also a Visiting Researcher at the Center for Computational Life Sciences, RWTH Aachen University.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Education, Career & Life"}, {"title": "When LLMs Are Too Big: Building Cost-Efficient High-Throughput ML Systems for E-Commerce Cataloging", "abstract": "E-commerce cataloging at idealo operates at extreme scale: 4.5 billion offers from 50,000+ shops across six countries, with peak ingestion rates of 4.8 million offers per minute. While large language models (LLMs) provide strong classification accuracy, they are too slow and costly for billion-scale real-time processing. This talk shows how idealo builds a cost-efficient, high-throughput machine learning system that leverages LLM knowledge without deploying full models in production. \r\n\r\nWe present how knowledge distillation from a large e5 instruction model enables a compact multilingual MiniLM encoder to achieve high accuracy, and how optimized inference runtimes and specialized hardware such as AWS Neuron help meet strict latency and cost requirements. Beyond modeling, we highlight key operational challenges: constructing training datasets from massively imbalanced data, selecting the right encoder architecture from today\u2019s model landscape, and designing a robust MLOps lifecycle with automated data sampling, training, deployment, and monitoring. \r\n\r\nAttendees will learn practical techniques for scaling ML systems under real-world constraints, how to extract value from LLMs when they are too large to serve directly, and how to transition research prototypes into reliable, high-volume production pipelines.", "full_description": "## When LLMs Are Too Big: Building Cost-Efficient High-Throughput Machine Learning for Cataloging in E-Commerce\r\n\r\n \r\n\r\nidealo.de offers a price comparison service for over 5.7 million products from a wide variety of over thousands of categories. It navigates a dynamic, constantly changing billion-scale landscape with **over 4.5 billion offers from 50,000+ shops in 6 countries**. Our central challenge is cataloging this huge amount of offers automatically at scale, with a peak throughput of **processing 4.8 million offers per minute.** \r\n\r\n \r\n\r\nWhile modern large language models (LLMs) excel in such tasks, they do not scale well to huge amounts of data. To fulfill business needs, we need to strike a balance between processing speed and offer cataloging quality. By employing modern machine learning techniques to extract specialist knowledge from downscaled state-of-the-art LLMs and a multitude of performance enhancing techniques we speed up idealo\u2019s processing while massively improving cataloging performance. This talk presents how these solutions find the balance between cost and performance and how they integrate into idealo\u2019s offer cataloging pipelines. \r\n\r\n  \r\n\r\n \r\n\r\n### What makes this approach unique?\r\n\r\nOur solution and practical experiences in the area of high-throughput classification are presented. This includes the operational aspects of our system, in particular the design of a stable and high-performance MLOps lifecycle integrated into our CI/CD and continuous Training pipelines. Where we automate continuous data sampling, model training, model deployments, and monitoring. \r\n\r\nConcrete solutions and best practices are discussed that demonstrate how our model accuracy of the multilingual MiniLM transformer encoder model is improved through knowledge distillation by a large e5 instruction transformer. Additionally, we show how the integration of these models on specialized hardware like AWS Neuron enables strict runtime and latency requirements to be met in a cost-efficient manner. \r\n\r\nIn detail we will discuss the following topics: \r\n\r\n* Machine Learning Operation Lifecyle for a high-throughput category classification system. \r\n* Challenges when creating training and testing datasets from the huge amount of existing massively unbalanced data efficiently. \r\n* Selecting the right model in presence of the current encoder language model zoo. \r\n* Using knowledge distillation via student-teacher models to balance required compute and classification performance. \r\n* Integrating quantization techniques for speed improvements. \r\n* Selecting ideal compute instances for our production environment. \r\n* How to compile the model on custom designed machine learning accelerators using the neuron package. \r\n\r\n \r\n\r\n \r\n\r\n### Key takeaways for attendees:\r\n\r\n* An overview of months of research and exploration for massive throughput environments including their practical integration in live systems. \r\n* Modern machine learning systems in production, especially with billion-scale data, need to carefully balance business needs in terms of cost and quality. \r\n* State-of-the-art LLMs are often not feasible for large-scale tasks. However, new machine learning techniques can extract their knowledge for specific applications. \r\n* How to transition research findings to production. \r\n\r\n \r\n\r\n \r\n\r\nThe talk will be aligned along our tech stack, which includes PyTorch, PyTorch Lightning, Huggingface, AWS Sagemaker, AWS Neuron SDK, Grafana Loki, Docker and GitHub Actions.", "code": "GAUNKM", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/GAUNKM.png", "speaker_names": "Tobias Senst, Bastian Wandt", "speakers": "\n### Tobias Senst\n\nTobias Senst is a Senior Machine Learning Engineer at idealo internet GmbH. Tobias Senst received his PhD in 2019 from the Technische Universit\u00e4t Berlin under the supervision of Prof. Thomas Sikora. He has more than 10 years of experience in Computer Vision and Video Analytics research.\r\n\r\nAt idealo, he switched from the world of images and videos to Natural Language Processing and is responsible for the operation and development of machine learning models in a productive environment.\n\n### Bastian Wandt\n\nBastian is a Senior Machine Learning Research Engineer at idealo Internet GmbH, where he focuses on large-scale offer cataloging and high-throughput machine learning systems. Before joining idealo in 2025, he was an Assistant Professor at Link\u00f6ping University in Sweden, leading a research group in 3D computer vision.\r\n\r\nHe completed his PhD in 2020 at Leibniz University Hannover with a thesis on 3D human pose estimation and subsequently spent two years at the University of British Columbia in Canada as a PostDoc, expanding his research into broader areas of 3D computer vision and teaching related courses.\n", "domain_expertise": "Advanced", "python_skill": "Intermediate", "track": "MLOps & DevOps"}, {"title": "How to create effective data visualizations", "abstract": "What distinguishes a lousy plot from a beautiful chart that communicates insights effectively? This talk will show you the underlying principles of good data visualization, offer lots of practical tips and tricks and give an overview of the data visualization landscape in Python. \r\nAfter the talk, you will be able to create better charts, whether for exploring your own data or for communicating results to others.", "full_description": "In this talk, you will learn about:\r\n\r\n- **Fundamental principles** of data visualization\r\n  - The Grammar of Graphics\r\n  - Visual hierarchy\r\n  - Data storytelling\r\n- **Best practices** regarding:\r\n  - Which colors to use\r\n  - Visual comparability \r\n  - Pros/cons of several chart types\r\n  - Context and audience: Adding text and annotations\r\n- The **data visualization landscape in Python**\r\n  - What libraries exist: matplotlib, plotly, altair etc., including add-ons and lesser-known ones\r\n  - What are their differences and strengths?\r\n  - Which library is suited for which usecase?\r\n\r\n\r\nEquipped with the knowledge presented in this talk, you will understand why certain charts are more aesthetically pleasing and more effective at conveying information than others. Apply the shown principles, take into account best practices and choose the right tools in Python to create more beautiful and impactful data visualizations.", "code": "GBKUNF", "state": "confirmed", "created": "2025-12-15", "social_card_image": "/static/media/social/talks/GBKUNF.png", "speaker_names": "Dominik Haitz", "speakers": "\n### Dominik Haitz\n\nDominik is a Senior Data Scientist with multiple years of experience in various industries. Enthusiastic about data and technology, he creates solutions that deliver real business value.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Visualisation & Notebooks"}, {"title": "pytest tips and tricks for a better testsuite", "abstract": "pytest lets you write simple tests fast - but also scales to very complex scenarios: Beyond the basics of no-boilerplate test functions, this training will show various intermediate/advanced features, as well as gems and tricks.\r\n\r\nTo attend this training, you should already be familiar with the pytest basics (e.g. writing test functions, parametrize, or what a fixture is) and want to learn how to take the next step to improve your test suites.\r\n\r\nIf you're already familiar with things like fixture caching scopes, autouse, or using the built-in `tmp_path`/`monkeypatch`/... fixtures: There will probably be some slides about concepts you already know, but there are also various little hidden tricks and gems I'll be showing.", "full_description": "We'll cover things like:\r\n\r\n- Recommended pytest settings for more strictness\r\n- What's xfail and why is it useful?\r\n- How to mark an entire test file or single parameters\r\n- Ways to deal with parametrize IDs and syntax\r\n- Useful built-in pytest fixtures\r\n- Caching for fixtures\r\n- Using fixtures implicitly\r\n- Advanced fixture and parametrization topics\r\n- How to customize fixtures behavior based on markers or custom CLI arguments\r\n- Patching, mocking, and alternatives\r\n- Various useful plugins, and how to write your own\r\n- Short intro to property-based testing with Hypothesis", "code": "GCGLPN", "state": "confirmed", "created": "2025-11-19", "social_card_image": "/static/media/social/talks/GCGLPN.png", "speaker_names": "Freya Bruhin", "speakers": "\n### Freya Bruhin\n\nFreya Bruhin (\"The Compiler\") is a long-time contributor and maintainer of both the pytest framework and various plugins. Discovering pytest in 2015, Freya has since given talks and conducted workshops about pytest at various conferences and companies. Freya's main project, qutebrowser (a keyboard-focused web browser), has grown from a hobby to a donation-funded part-time job.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "How Open Source makes programming education possible in Namibian schools.", "abstract": "Open-source tools have become essential for making programming education possible in schools that lack modern equipment, stable internet, or licensed software. This talk shares practical insights from teaching Python in under-resourced environments in Namibia, highlighting how tools like Linux, Python, and cloud-based notebooks enable meaningful learning even when only a few devices are available. It also draws on findings from research with teachers across different schools to show the real barriers faced when adopting open-source solutions, and the creative strategies used to overcome them. Attendees will gain a realistic understanding of what digital education looks like in low-resource settings and learn practical approaches for making programming more accessible and inclusive using open-source tools.", "full_description": "Many schools across the world, especially in developing regions, face a basic but often overlooked challenge: learners want to study programming, but the school infrastructure is not designed for modern computing education. Limited computers, unreliable connectivity, and the absence of licensed software create a gap between curriculum expectations and what teachers and learners can realistically achieve.\r\n\r\nOpen-source tools often become the only viable path forward. They provide accessible, reliable, and cost-free ways to teach programming and computational thinking, even in environments where technology resources are scarce. This talk explores how open-source software enables meaningful programming education in such low-resource settings, using practical examples from classroom situations and insights from ongoing research with teachers in Namibia.\r\n\r\nThe session will highlight the specific challenges encountered when teaching Python without enough machines, when internet access is intermittent, and when schools rely heavily on outdated hardware. It will also show how tools such as Linux distributions, Python\u2019s ecosystem, and browser-based or offline notebooks make it possible for learners to write, run, and share code with only minimal resources. These examples demonstrate that high-quality programming education does not always require high-end infrastructure, what matters more is adaptability, tooling choice, and an open ecosystem.\r\n\r\nThe talk will also cover experiences gathered from teachers across different schools, focusing on the barriers they face when adopting open-source tools. Alongside these challenges, it will share creative strategies teachers have developed: planning offline lessons, integrating printed materials, running unplugged computing sessions, and gradually introducing Linux where appropriate.\r\n\r\nThe goal of this session is to introduce the wider Python community to a perspective that is rarely represented in global conferences: how open-source tools directly shape educational opportunities in environments where resources are limited.", "code": "GKH8PY", "state": "confirmed", "created": "2025-11-27", "social_card_image": "/static/media/social/talks/GKH8PY.png", "speaker_names": "Robson Kanhalelo", "speakers": "\n### Robson Kanhalelo\n\nI am a master\u2019s degree student at the University of Namibia, currently studying towards a master\u2019s degree in Educational Technology. Additionally, I hold a certificate in Big Data Technologies from the Namibia University of Science and Technology (NUST). I am passionate about teaching, learning, and software development.\n", "domain_expertise": "None", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "Making bad CLIs fun with Small Language Models", "abstract": "Command Line Interfaces (CLIs) offer an efficient and powerful way to interact with software, but poorly designed interfaces can be incredibly frustrating. Complicated parameter names and unconventional formats can turn using a great tool into a burdensome experience.\r\n\r\nLarge Language Models (LLMs) seem like a great solution to this problem as they can easily add a natural-language interface to any CLI. However, LLMs can introduce their own challenges, such as requiring API keys or high-performance GPUs. In this talk, I'll demonstrate a method for creating natural-language interfaces for any CLI using fine-tuned Small Language Models. These models are lightweight enough to be run directly on laptops or even smartphones.\r\n\r\nWe'll explore the process of generating synthetic data, fine-tuning models, and evaluating their performance using both an in-house CLI and a well-known open-source package as examples.", "full_description": "I've often had to rely on a poorly designed home-grown CLI, leading to frustration due to constantly forgetting argument names and allowable values. While Large Language Models (LLMs) initially appeared to be an ideal fix, their limitations quickly became evident, suggesting the need for a more efficient approach.\r\n\r\nTo begin, we'll have a look at what makes CLIs hard to use and articulate why LLMs fall short in addressing them. Following this, we'll examine the process of generating synthetic data tailored for any CLI, whether it's proprietary or open-source. Then, I'll show you how to use this synthetic dataset to fine-tune a Small Language Model on your laptop or in the cloud. We will use the smallest variant of Google's Gemma 3 models, which boasts a lean 270 million parameters, to transform natural language instructions into actionable CLI commands.\r\n\r\nLastly, I'll share benchmark results to illustrate that these models can operate smoothly on various machines without needing API keys or GPUs, showcasing their robust capability and practical deployment potential.", "code": "GMNE3E", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/GMNE3E.png", "speaker_names": "Moritz Bauer", "speakers": "\n### Moritz Bauer\n\nMoritz Bauer is a Senior Data Scientist at Blue Yonder, where he currently develops software for demand forecasting. In a previous career, he obtained a Ph.D.  in high-energy particle physics and contributed research to the Belle II flavor physics experiment at KEK.\r\n\r\nWhile demand forecasting works very well without language models, he can't escape the fascination of modern AI and is always looking for excuses to spend some time in this domain.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building reliable data pipelines with polars and dataframely", "abstract": "If you have worked with real-world data before, you know that processing it can be challenging. Data often comes scattered across tables, in inconsistent encodings, with duplicated rows and is generally dirty. In this tutorial, you will learn how to process large amounts of data reliably and quickly using `polars` and `dataframely`.\r\n\r\nWhat we love about `polars` is that it's easy to use, fast and elegant \u2014 it allows us to build and compose complex transformations with ease. On this basis, we built `dataframely`: a library for defining and validating contents of polars data frames. With `dataframely`, we can build pipelines without ever getting confused about what's in our data frames. We document and validate our expectations and assumptions clearly, which makes our pipeline code simpler and easier to understand. \"Is this join correct?\", and \"where did this column come from?\" are questions you will not have to worry about anymore.\r\n\r\nIn this tutorial, you will become familiar with `polars` basics by writing a simple pipeline: you will read data, transform it to make it ready for use, and you will learn how to do that fast. With `dataframely` schemas, you will upgrade your code from \"it works\" to \"it's beautiful!\", and along the way, `dataframely` will help you eliminate entire classes of bugs you will never have to think about again. After the tutorial, you will be all set to use these tools in your own work.", "full_description": "In this tutorial, you will become familiar with `polars` basics by writing a simple pipeline: you will read data, transform it to make it ready for use, and you will learn how to do that fast. With `dataframely` schemas, you will upgrade your code from \"it works\" to \"it's beautiful!\", and along the way, `dataframely` will help you eliminate entire classes of bugs you will never have to think about again. After the tutorial, you will be all set to use these tools in your own work.", "code": "GPJGH3", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/GPJGH3.png", "speaker_names": "Andreas Albert, Oliver Borchert", "speakers": "\n### Andreas Albert\n\nI am a software and data engineer working on data pipelines at QuantCo. In a previously life I looked for dark matter in particle collisions.\n\n### Oliver Borchert\n\nFor the past 4 years, I have been working on machine learning and data engineering and QuantCo. Previously, I studied computer science at the Technical University of Munich, focusing on machine and deep learning.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Holistic Optimization: Implementing \"Pipeline-as-a-Trial\" HPO with Ray and Cloud Infra", "abstract": "Most hyperparameter optimization (HPO) stops at the model boundary. But what happens when your system relies on a complex chain of steps, a short-horizon model, a long-horizon model, ensembles, postprocesses etc? Tuning one piece in isolation often leads to sub-optimal global results.\r\n\r\nIn this talk, we explore how we used Ray to move beyond simple model tuning. We\u2019ll dive into a \"Pipeline-as-a-Trial\" architecture where Ray acts as the brain, triggering independent, scalable cloud workflows ( SageMaker Pipelines or Databricks Workflows) for every hyperparameter set.\r\n\r\nWe will discuss:\r\n* The architectural shift from tuning models to tuning pipelines\r\n* How to build the DAG/pipeline on Sagemaker/Databricks using declarative configs\r\n* How to use Ray to orchestrate heavyweight remote jobs without bottlenecks.\r\n\r\nAttendees will learn how to optimize entire pipelines (in a scalable manner on cloud) to minimize global metrics like WAPE, rather than just local model loss.", "full_description": "As a data science team working on forecasting, we rarely rely on a single model to produce optimal results. Instead, we run composite pipelines: short and long-term models followed by ensemble strategies, granularity reconciliation, and post-processing. We found that optimizing the parameters of one model in isolation often degraded the performance of the final ensemble or the post-processed output. We needed to treat the entire pipeline as the function to optimize.\r\n\r\nThis talk details how we implemented a \"Pipeline-as-a-Trial\" architecture utilising Ray with cloud infrastructure (Sagemaker + Databricks + custom solution).\r\n\r\nThe solution architecture consists of 2 pieces:\r\n1. The driver notebook (the entrypoint, where the config of the experiment will be parsed and ray initiated). Ray hyperliner will run here and trigger trials for each set of hyperparameters\r\n2. The dag construction and run: each set of hyperparameters (trial) would create and run either a sagemaker pipeline or a databricks workflow. Which would save the results and return the WAPE of the pipeline to the driver notebook\r\n\r\n**Operational Challenges:**\r\nWe will deep dive into the different trade-offs and hurdles of this implementation:\r\n* Trigger mechanism (how the configs trigger the pipeline?)\r\n* Warm pool support\r\n* Cost\r\n* debugging/monitoring\r\n* Infra limits", "code": "GPV9SM", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/GPV9SM.png", "speaker_names": "Abdullah Taha", "speakers": "\n### Abdullah Taha\n\nData/MLOps Engineer at Zalando. During my career  I always worked along data scientists to build robust ML pipelines. I am very enthusiastic about designing and implementing scalable and robust systems.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "MLOps & DevOps"}, {"title": "From Imports to Innovation: The Dynamics Behind Python\u2019s Evolution", "abstract": "Python\u2019s ecosystem evolves through thousands of developers importing, combining, and reinventing third-party packages. But how predictable is this process? And why do programming ecosystems behave in ways that are hauntingly similar to seemingly unrelated, distant realms of innovation, such as patents?\r\n\r\nIn this talk, we analyse 15 years of Python code snippets from Stack Overflow and uncover a striking pattern: the way Python developers create and recombine libraries follows the same mathematical laws as those governing US patents and scientific publications. New packages enter the ecosystem at a slowing rate, while new combinations of packages appear at a remarkably stable, almost linear pace.\r\nWe show how these regularities emerge naturally from a simple adjacent-possible urn model\u2014a surprisingly powerful tool that captures how innovation unfolds as old ideas are reinforced, and new ones occasionally appear.\r\n\r\nThe obtained quantitative view can explain how Python changes over time, how novelty spreads, and what governs the evolution of the software we use every day.", "full_description": "What can millions of real Python code snippets tell us about how the language evolves? And why do the patterns we observe in Python look uncannily similar to patterns found in patents and scientific research \u2014 systems that seem to have nothing to do with software?\r\n\r\nThis talk begins with a practical challenge: extracting structured signals from the chaotic world of Stack Overflow. We built a pipeline that scanned posts for Python code blocks, identified import statements, normalised package names, filtered noise, and reconstructed a time-ordered stream of collections, each composed of the packages used in that snippet. From this, we derived two simple indicators of innovation:\r\n\t\u2022\tnew packages appearing for the first time, and\r\n\t\u2022\tnew package pairs appearing together for the first time.\r\n\r\nOnce these signals are extracted, a surprisingly coherent picture emerges. The Python ecosystem introduces brand-new packages less and less frequently over time, yet continues to generate new combinations of packages at a remarkably steady pace. Developers reuse familiar tools, but they also explore the space of possible pairings with a precision that looks \u2014 statistically \u2014 almost mechanical.\r\n\r\nTo understand just how surprising this is, we compare Python\u2019s behavior with two very different worlds. The first is the US patent system, where technology codes assigned to inventions can be analyzed the same way we analyze Python imports. A classic 2015 study by Youn et al. showed that while new technology codes appear at a slowing rate, pairs of codes accumulate almost linearly over two centuries of innovation. The second is a corpus of physics publications, which behaves in much the same way when one treats subject classification codes as ingredients.\r\n\r\nAcross all three domains \u2014 software, science, and invention \u2014 the same pattern holds. Distinct components grow sublinearly (Heaps\u2019 law), while distinct combinations grow close to linearly. This parallel is not only unexpected; it suggests that these systems share a deeper underlying mechanism, bound not by specific domain-specific details but by the very foundational patterns of human innovation.\r\n\r\nIn the second half of the talk, we introduce the concept of \"adjacent possible\" and demonstrate its modelling via a simple stochastic model: a P\u00f3lya urn extended with the adjacent possible. The model assumes only two forces: reinforcement of frequently used components and occasional introduction of new ones. Despite its simplicity, it reproduces the empirical behavior of all three systems without requiring domain-specific rules. It shows how a stable exploration\u2013exploitation balance can arise naturally, leading to predictable rates of combinatorial novelty even in rapidly changing ecosystems.\r\n\r\nThe framework offers a new way to think about the ecosystem: not as a chaotic swarm of libraries, but as an innovation system governed by universal constraints. It sheds light on why certain libraries become dominant, why the combination space grows the way it does, and how the community collectively expands the \u201cadjacent possible\u201d of the language.\r\n\r\nAttendees of the talk will learn:\r\n\t\u2022\thow to extract meaningful innovation signals from real Python code at scale,\r\n\t\u2022\thow to measure novelty and combinatorial creativity in software ecosystems,\r\n\t\u2022\twhy Python\u2019s long-term evolution aligns with empirical laws from patents and science,\r\n\t\u2022\tand how simple generative models can help reason about complex developer behavior.\r\n\r\nThe talk connects engineering, data analysis, and innovation theory to reveal an unexpected insight: Python grows the way many creative systems grow \u2014 slowly at the edges, rapidly in combinations, and always under the quiet guidance of reinforcement and the adjacent possible.", "code": "GQAC7F", "state": "confirmed", "created": "2025-12-10", "social_card_image": "/static/media/social/talks/GQAC7F.png", "speaker_names": "Gabor Meszaros", "speakers": "\n### Gabor Meszaros\n\nMathematician turned software engineer turned network scientist. I explore how structure and behavior emerge in complex systems \u2014 from code ecosystems to large graphs. Passionate about Python, powered mostly by coffee, and firmly in the tabs-over-spaces camp.\n", "domain_expertise": "None", "python_skill": "Intermediate", "track": "Others"}, {"title": "Solving Marketplace Cold Start at Scale as part of the ranking system", "abstract": "[Cold start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems) is a critical bottleneck for marketplaces: new items lack behavioral signals and reviews, so ranking models under-expose them, delaying the very signals needed to rank them well. This talk shares practical solutions developed at scale for a travel marketplace, including guaranteed exposure at key positions, efficient real-time re-ranking under latency constraints, and targeted boosting for unactivated items. Attendees will learn how experiment-driven iteration shaped a robust system that accelerates early traction for new items without sacrificing overall marketplace health.", "full_description": "[Cold start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems) cripples two\u2011sided marketplaces: new items lack behavioral signals and social proof, ranking models under\u2011expose them, which delays the very signals needed to rank them well. This talk shares our journey to break down this loop at GetYourGuide, a marketplace for travel experiences. We evolved our exploration/activation framework over the past three years with three complementary interventions: guaranteed exposure at strategic positions, a real\u2011time reranker to allocate that exposure efficiently under tight latency budgets, and guardrail boosting for unactivated items when primary assessment slots are empty. \r\n\r\nThe talk is a pragmatic case study: we\u2019ll show how experiment\u2011led exploration shaped the system over the last 3 years. We will share what worked, what did not, and how we managed trade-offs between short-term revenue and long-term marketplace health. Attendees will leave with a blueprint for safely accelerating early traction in their own marketplaces, combining learning\u2011to\u2011rank with exposure guarantees without sacrificing overall business health.", "code": "GVHZW9", "state": "confirmed", "created": "2025-12-06", "social_card_image": "/static/media/social/talks/GVHZW9.png", "speaker_names": "Theodore Meynard", "speakers": "\n### Theodore Meynard\n\nTheodore Meynard is a data science manager at GetYourGuide.He leads the evolution of their ranking algorithm, helping customers to find the best activities to book and locations to explore. Beyond work, he is one of the co-organizers of the Pydata Berlin meetup and the conference. When he is not programming, he loves riding his bike and looking for the best bakery-patisserie in town.\n", "domain_expertise": "Advanced", "python_skill": "None", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Making Tech Tutorials Accessible: Practical Techniques for Educators", "abstract": "Want to make your tech tutorials accessible but don't know where to start? This talk shares practical techniques anyone can use.\r\nIn June 2025, I started creating tutorials for deaf and hard-of-hearing learners because my partner is hard of hearing. I learned that accessible content helps everyone: international learners, people on noisy trains, junior developers and tired seniors at the end of the day.\r\nIn this talk, I will share practical techniques for creating accessible tech tutorials:\r\n\u2022\tCreating videos with meaningful subtitles (manual timing, simple language)\r\n\u2022\tPrinciples of simple language for technical content\r\n\u2022\tStructuring content so everyone can navigate it easily\r\nI am a content creator who learned these techniques through experimentation while teaching Excel. The talk presents my actual workflow with examples from creating tutorials for deaf/hard-of-hearing learners.\r\nWhether you're creating video tutorials, writing documentation, or teaching workshops, you'll leave with actionable steps to make your content more accessible.\r\n\r\nWhy it matters: Tech education is growing globally. Making our content accessible isn't just good ethics\u2014it makes our teaching better for everyone.", "full_description": "Accessible content isn't just for people with disabilities\u2014it makes tech education better by design for everyone. International learners, people on noisy trains junior developers and tired seniors at the end of the day\u2014they all benefit from subtitles, simple language, and clear structure. Yet most developers who become educators have never learned how to make their content accessible.\r\nThis talk shares practical techniques I use creating tech tutorials for deaf and hard-of-hearing learners. Since June 2025, I've been creating Excel tutorial videos with manual subtitles in simple language for a YouTube community (~400 subscribers). My partner is hard of hearing, which taught me that accessibility isn't optional\u2014it's essential. The techniques could be applied to any tech content: Python tutorials, data science courses, documentation, or workshops.\r\nThe talk follows this structure:\r\n1. Understanding Barriers (5 minutes) Who benefits from accessible content? People with permanent, temporary, and situational limitations. Brief introduction to the Web Content Accessibility Guidelines using the POUR principles (Perceivable, Operable, Understandable, Robust).\r\n2. Creating Accessible Videos (15 minutes) My core workflow: manual subtitles in DaVinci Resolve with timing based on text length, using AI tools to simplify technical language, visual clarity with arrows and highlights, and the insight that you can't be accessible to everyone\u2014focus on your target audience.\r\n3. Clear Structure for any Content (8 minutes) Applying video principles to any format: logical heading hierarchy for navigation, alternative text for images, using built-in accessibility checkers, and simple language techniques.\r\n4. Getting Started (2 minutes) One action to take this week, free tools to use, and resources for continued learning.\r\nI completed the W3C \"Introduction to Web Accessibility\" course and will be conducting a guest lecture on accessible learning materials at MSB Medical School Berlin (January 2026). As a non-native German speaker, I understand language barriers firsthand.\r\nAttendees will leave with practical techniques they can implement immediately and the confidence that accessibility is achievable without being an expert. No prior knowledge on accessibility is required.", "code": "GYBRVN", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/GYBRVN.png", "speaker_names": "Tamara Badikyan", "speakers": "\n### Tamara Badikyan\n\nTamara Badikyan is a Data Analyst currently working at the National Association of Statutory Health Insurance Physicians (KBV) in Berlin. Since June 2025, she has been creating accessible tech tutorials for deaf and hard-of-hearing learners. She runs a YouTube channel focused on making Excel content accessible through manual subtitles and simple language.\r\nTamara holds master's degrees in Migration and Intercultural Relations (Erasmus Mundus Program, University of Oldenburg) and Sociology and Social Anthropology (Central European University, Budapest). She completed the W3C \"Introduction to Web Accessibility\" course and will be conducting a guest lecture on accessible learning materials at MSB Medical School Berlin in January 2026.\r\nAs a non-native German speaker who is also learning German Sign Language, Tamara understands language barriers and accessibility challenges firsthand.\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "Demystifying Parallel Programming in Python: from CPU to quantum processors, including GPU and TPU", "abstract": "This talk provides a beginner-friendly overview of Python\u2019s parallel programming ecosystem. You\u2019ll discover the key libraries and techniques\u2014JIT compilation, multithreading, multiprocessing, distributed computing, HPC/grid computing, and even a first look at quantum programming\u2014to help you write faster, more efficient code, regardless of your hardware.", "full_description": "# Demystifying Parallel Programming in Python\r\n\r\n## Understanding the Hardware Basics\r\n\r\n* A gentle introduction to modern processors: What are CPUs, GPUs, TPUs, and quantum processors?\r\n* Essential terminology explained: cores, hyper-threading, cache memory, multithreading, multiprocessing, multitasking, SIMD, NUMA, and more\u2014no prior knowledge required!\r\n\r\n## Parallel Programming Techniques for Beginners\r\nA practical overview of Python\u2019s parallel programming tools, organized by approach:\r\n\r\n* Just-In-Time (JIT) compilation: Speed up your code without changing your workflow\r\n* Multithreading: Do more at once, and removing the GIL with Python 3.13+\r\n* Multiprocessing: Use all your CPU cores\r\n* Distributed computing: Scale your code across multiple machines\r\n* Quantum programming: A first look at the future of computing\r\n\r\n## Hands-On Examples\r\n\r\n* JIT compilation made easy: PyPy, Numba, and JAX\r\n* The GIL and Python 3.13: What\u2019s changing and why it matters\r\n* Distributed computing for everyone: Celery and Dask on HPC clusters\r\n* GPU computing for beginners: CuPy, cuDF, and Numba\r\n* Your first quantum \u201cHello World\u201d: A taste of the quantum revolution\r\n\r\n## Conclusion\r\n\r\nBy the end of this talk, you\u2019ll have a clear map of Python\u2019s parallel programming landscape.\r\nNo experience needed\u2014just bring your curiosity and let\u2019s explore together!", "code": "HBFL78", "state": "confirmed", "created": "2025-12-25", "social_card_image": "/static/media/social/talks/HBFL78.png", "speaker_names": "Ga\u00ebl Pegliasco", "speakers": "\n### Ga\u00ebl Pegliasco\n\nPython Developer & Trainer\r\nSpecializing in Machine Learning and Parallel Computing with NumPy, Pandas, Scikit-Learn, TensorFlow, PyTorch, MPI, Dask, and more.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "Pair & Share: How formal Mentoring pushed REWE Analytics to a new level", "abstract": "As one of Europe\u2019s largest retail corporations, REWE Group owns and manages prominent supermarket chains such as REWE and PENNY, among many other subsidiaries. In this talk I will give a brief overview of how we introduced a formal mentoring program, Pair & Share, at the central analytics department of REWE Group with its more than 150 data scientists, engineers, analysts and other colleagues. \r\n\r\nBefore Pair and Share, there was no formal process for personal, technical or methodological growth. Although there are plenty of possibilities, further training and education was self-organized and fragmented. To increase growth among our colleagues and build and strengthen inter-team exchange, we introduced the formal mentoring program, Pair & Share. \r\n\r\nThis talk will cover a brief overview of REWE Group and our analytics department followed by a motivation for Pair & Share. Afterwards I will explain how we planned the mentoring program and defined the parameters like the matching process, the time frame and how to recruit participants. I will also share my experiences of the first six months of mentoring, what kind of roadblocks but also pleasant surprises we encountered. The talk will be concluded with an outline of how we plan to continue and improve the program.", "full_description": "Did you ever wonder how to bring your analytics department to the next level? Do you want to help colleagues to network, learn or pass on their knowledge? And did you ever want to start your own mentoring program in a large corporation? Think no more, as I will describe in detail how we set up a mentoring program, Pair & Share, in REWE Group\u2019s analytics department, with its 150 data scientists, data engineers, analysts and other data people. As one of Europe\u2019s largest retail corporations, REWE Group owns and manages prominent supermarket chains such as REWE and PENNY, among many other subsidiaries. However, before Pair and Share there was no formal process for personal, technical or methodological growth within REWE Analytics. Although there are plenty of possibilities, further training and education was self-organized and fragmented. To increase growth among our colleagues and build and strengthen inter-team exchange, we introduced the formal mentoring program, Pair & Share. \r\n\r\nThis talk will cover a brief overview of REWE Group and our analytics department, who we are and what we do. This is followed by a description of why, while we found personal growth and training to be fine, we realized that we could do better with Pair & Share. Afterwards, I will explain how we planned the details of the mentoring program and defined the parameters like the matching process, time frame and how to recruit participants. As the first iteration of mentoring comes to an end in March 2026, I will share my experiences of the first six months of mentoring. This will include the kind of roadblocks we faced, how participants shaped their own mentoring experience and what pleasant surprises we encountered. As we measured participant satisfaction with regular pulse checks as well as many feedback sessions, I will conclude the talk with an overview of what went well and how we plan to do better with the next iteration of mentoring.", "code": "HKFCBM", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/HKFCBM.png", "speaker_names": "Axel Buddendiek", "speakers": "\n### Axel Buddendiek\n\nAxel Buddendiek is an astronomer turned data scientist. After finishing his PhD in 2015, Axel started working in data teams at different companies, continuously learning and building up new skills. In 2022, he joined the Analytics Department at REWE Group as a senior data scientist. When Axel is not at work, he enjoys jogging, reading, and watching football.\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "Beyond Kafka and S3: HTTP-Native Bytestreams for Python Data Pipelines", "abstract": "Real-time bytestreams between systems in different organizations or secured environments, whether for batch dataset delivery or continuous streaming, are surprisingly hard. Traditional solutions fall short: message brokers like Kafka use discrete messages, file storage like S3 works for batch exchange but lacks streaming and coordination, while HTTP client-server approaches require one side to host and expose server endpoints, introducing security and operational overhead.\r\n\r\nThis talk introduces the ZebraStream Protocol: an open, HTTP-based bytestream protocol with coordination mechanisms that let you stream data\u2014Parquet files, compressed archives, encrypted content\u2014directly between decoupled systems using Python's file-like interface. No message framing, no server hosting, no exposed endpoints.\r\n\r\nWe'll explore the design of a bytestream protocol for data sharing and integration that crosses the file-stream boundary, enabling seamless integration with pandas, DuckDB, and any Python library expecting file-like objects, supporting use cases from ETL pipelines to IoT data delivery, cross-org collaboration to home network automation.", "full_description": "Streaming data between systems\u2014whether across organizations, from secured environments, isolated networks, or even home setups\u2014remains a common challenge in modern data engineering and data sharing workflows. This talk introduces the ZebraStream Protocol: an open, HTTP-based bytestream protocol designed specifically for decoupled systems, where both sides act as clients\u2014no server hosting, no exposed endpoints.\r\n\r\n### Talk Outline (45 minutes)\r\n\r\n**1. The Challenge: Data Sharing Between Decoupled Systems (5 min)**\r\n- Real-world scenarios: cross-org data exchange, secured environments, isolated networks, home automation, IoT deployments\r\n- Use cases: ETL pipelines, dataset delivery, continuous monitoring, exploratory data access\r\n- Current solutions and their limitations:\r\n  - Message brokers (Kafka): discrete messages, can't coordinate query-response without external notification\r\n  - File storage (S3/SFTP): batch-oriented, lacks streaming\r\n  - HTTP client-server: requires endpoint hosting, security overhead\r\n  - Webhooks: incomplete solution, still needs server hosting\r\n\r\n**2. ZebraStream Protocol Overview (6 min)**\r\n- Why HTTP? Interoperability, evolution (HTTP/2, HTTP/3), standardized infrastructure, firewall-friendly\r\n- Two-part protocol design:\r\n  - Data API: HTTP-based bytestream transfer (like UNIX pipes over HTTP)\r\n  - Connect API: Built-in coordination for push and pull patterns\r\n- Key properties: client-to-client via relay, zero-trust security model, ephemeral, direct data flow\r\n\r\n**3. Why Bytestreams Matter (8 min)**\r\n- Bytestreams vs. messages: continuous byte flow vs. discrete units\r\n- Native format streaming: Parquet, compressed archives, encrypted content\r\n- Supporting event patterns: JSON-lines, CSV within bytestreams\r\n- Python's file-like interface (`io.IOBase`) as universal abstraction\r\n- Live demo: Streaming Parquet directly into pandas/DuckDB\r\n- Live demo: Log streaming like `tail -f`\r\n\r\n**4. Coordination for Decoupled Systems (7 min)**\r\n- The \"who initiates when?\" problem\r\n- Symmetric push/pull patterns with same API\r\n- Coordination within `open()` call\r\n- Live demo: Event-driven pipeline activation\r\n\r\n**5. Python Integration: File-Like Interface (6 min)**\r\n- Why file-like objects matter: universal Python abstraction\r\n- Two dimensions of simplicity: language-agnostic HTTP + Python-specific interface\r\n- Examples: pandas integration, compression layering, encryption composition\r\n- Stream limitations: seekability and Unix pipe compatibility\r\n\r\n**6. Open Protocol Specification & Security Model (5 min)**\r\n- Open specification: Data API, Connect API, security model\r\n- Security: TLS transport, bearer token auth, ephemeral design, zero-trust with E2EE\r\n- End-to-end encryption patterns (application-layer, protocol-agnostic)\r\n- Comparison with alternatives: Kafka, S3, HTTP client-server (security dimensions)\r\n- Reference implementation: Python client (open source), ZebraStream.io (managed service)\r\n\r\n**7. Real-World Integration Examples (4 min)**\r\n- **Data engineering:** Cross-org Parquet ETL pipelines with token-based access control\r\n- **Privacy-preserving data exchange:** End-to-end encrypted datasets (healthcare, research, GDPR compliance)\r\n- **Operations:** Log streaming and event processing\r\n- **IoT & Home automation:** Raspberry Pi data delivery from home network without exposed endpoints\r\n- **Data science:** Ad-hoc dataset sharing for collaborative analysis\r\n- All examples demonstrated with reproducible Python code (open-source client SDK)\r\n\r\n**8. Design Trade-offs & Lessons Learned (3 min)**\r\n- Lessons from building and dogfooding the protocol in beta\r\n- Why bytestreams over messages: native format support vs. framing overhead\r\n- Why ephemeral over persistent: privacy by design, no storage footprint\r\n- Why HTTP over custom protocol: infrastructure reuse, firewall-friendly\r\n- Stream limitations: seekability requirement, Unix pipe compatibility rule\r\n- Future directions: protocol evolution, additional language implementations\r\n\r\n**9. Q&A (6 min)**\r\n- Technical deep-dives and audience questions", "code": "HP7DLX", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/HP7DLX.png", "speaker_names": "Johannes Dr\u00f6ge", "speakers": "\n### Johannes Dr\u00f6ge\n\nJohannes holds a PhD in computer science, has developed open-source software, algorithms and statistic methods for genome data analysis, worked as a data scientist, and led a group of data engineers in a mid-size startup. He is currently bootstrapping SaaS infrastructure software projects with a focus on cross-organizational data sharing.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Rediscovering single-node processing: When does it make sense to move from Spark to Polars?", "abstract": "As data engineers, we are used to spinning up a Spark Cluster every time we want to do data processing and handle the overhead that comes with using such a mighty framework. But is this really necessary? In this talk I will argue that single-node processing with Polars is in many cases easier and cheaper. I will compare a typical ETL & Feature Engineering task in Spark and in Polars and offer a pragmatic opinion on when to use one or the other.", "full_description": "Apache Spark is the industry standard for big data processing, rightfully so. But for many data processing applications, a more light-weight solution will work just as well, avoiding Spark's compute and configuration overhead. Polars offers such a solution, with a fast single-node processing engine and a syntax that will pose no problems for experienced Spark developers.\r\nI will give a short comparison of Spark and Polars, where they have similarities and differences and show an implementation of a typical ETL and Feature Engineering task in both. I will compare the deployment, performance and cost of the two and, while giving my opinion on the topic, hope to enable you to also make an informed decision on when you want to use Polars and when to use Spark.", "code": "HQBC7R", "state": "confirmed", "created": "2025-12-02", "social_card_image": "/static/media/social/talks/HQBC7R.png", "speaker_names": "Jonas B\u00f6er", "speakers": "\n### Jonas B\u00f6er\n\nData Engineer at inovex since 2022, full-time software engineer since 2018, coder for as long as I can remember. With my experience working on data warehouses and machine learning applications from small-scale tests up to international deployments, I enjoy eliminating bugs and bottlenecks, getting cool systems online and writing beautiful code. Still proud of the time when a colleague complained that deploying to production has become too easy and is no longer a thrilling adventure because of me.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Data Handling & Data Engineering"}, {"title": "AI Is Changing the Game: Building Modular, AI-Ready Platforms on Top of Legacy Systems", "abstract": "AI is fundamentally changing how quickly business and domain teams can create new logic, validations, and insights. In regulated environments, this new speed collides head-on with legacy systems, monolithic architectures and IT landscapes that were never designed for continuous AI-driven change.\r\n\r\nThis talk presents an open, Python-based platform architecture that turns AI-driven pressure into an architectural advantage. Instead of embedding AI into existing monoliths, the platform introduces a central control layer that orchestrates independent, stateless apps\u2014ranging from classical algorithms to AI agents\u2014without binding them to specific infrastructure or legacy constraints.\r\n\r\nThe control layer, implemented using Python and optionally Django, provides workflow orchestration, security, tenant management, and self-service registration of new components. This allows domain teams to deploy AI agents\u2014such as anomaly detection for regulatory reporting\u2014within days, while IT retains governance, auditability, and operational stability.\r\n\r\nThe talk argues that AI will amplify architectural weaknesses\u2014and shows why modular orchestration layers will become essential for AI-ready systems far beyond finance.", "full_description": "# AI Is Changing the Game: Building Modular, AI-Ready Platforms on Top of Legacy Systems\r\n\r\nAI is no longer a future topic\u2014it is actively reshaping expectations inside organizations. Domain and business teams can now prototype new rules, validations, and analytical logic themselves, often within days. While this accelerates innovation, it puts enormous pressure on existing IT architectures, especially in environments dominated by legacy systems and monolithic platforms.\r\n\r\nThis talk explores how software architecture must evolve to absorb this pressure instead of breaking under it.\r\n\r\nRather than embedding AI capabilities directly into legacy systems, the presented approach introduces a modular, AI-ready platform built around independent, stateless apps orchestrated by a central control layer. These apps can represent classical reporting logic, risk calculations, or AI agents, all treated as first-class architectural components.\r\n\r\nThe talk is highly relevant for the **PyCon track \u201cProgramming, Software Engineering & Testing\u201d**, because it demonstrates how to design, orchestrate, and integrate AI-driven workflows in complex Python-based platforms. The central control layer, implemented using Python and optionally Django, provides workflow orchestration, security, tenant management, and self-service registration of new components. This allows domain teams to deploy AI agents or agents written with the help of AI within days, while IT retains governance, auditability, and operational stability.\r\n\r\nBy showing how AI-driven pressure can be turned into an architectural advantage, the talk provides patterns and practical lessons that apply far beyond finance, making it relevant for any domain dealing with legacy systems, modular design, and AI integration.\r\n\r\n\r\n## Architectural Concepts Covered\r\n\r\nThe talk introduces the key architectural principles behind the platform:\r\n\r\n- **Independent, stateless apps** that declare their data needs and outputs but remain unaware of infrastructure, environments, or other apps  \r\n- **Strict separation of concerns** between domain logic, orchestration, persistence, and presentation  \r\n- **Technology-indifferent design**, allowing apps to run on different databases, reporting tools, or compute backends  \r\n- **Parallel and distributed execution** as a default, not an optimization  \r\n\r\nThis architecture allows legacy systems to coexist with modern components instead of blocking innovation.\r\n\r\n## The Control Layer as an Enabler for AI\r\n\r\nA central part of the talk is the control layer that orchestrates all components. Implemented using Python and optionally Django, this layer is responsible for:\r\n\r\n- workflow orchestration and dependency management  \r\n- authentication, authorization, and tenant isolation  \r\n- self-service registration of apps and AI agents  \r\n- resource allocation, monitoring, and auditability  \r\n\r\nDjango is not used as a traditional CRUD backend, but as governance infrastructure: providing APIs, admin and self-service portals, and security mechanisms that allow fast innovation without losing control.\r\n\r\n## Example: Integrating an AI Agent into a Regulated Platform\r\n\r\nA concrete example demonstrates the architecture in action: integrating an AI agent for e.g. anomaly detection in regulatory reporting.\r\n\r\nThe example walks through:\r\n\r\n- developing the agent as an independent, containerized app  \r\n- registering it via standardized APIs  \r\n- declaring required data and produced results  \r\n- orchestrating it within existing workflows  \r\n- testing, monitoring, and scaling it without touching legacy systems  \r\n\r\nThis shows how new AI capabilities can be deployed within days while maintaining stability and compliance.\r\n\r\n## Why This Matters Beyond Finance\r\n\r\nWhile the example comes from regulatory reporting, the patterns discussed apply to many domains facing similar challenges: data-heavy systems, long-lived platforms, and increasing pressure to integrate AI safely.\r\n\r\nThe talk concludes with lessons learned and architectural patterns that help future-proof systems as AI continues to raise the bar for flexibility, speed, and modularity.", "code": "HRFYVS", "state": "confirmed", "created": "2025-12-23", "social_card_image": "/static/media/social/talks/HRFYVS.png", "speaker_names": "Werner Gothein", "speakers": "\n### Werner Gothein\n\nExperienced software architect and risk management expert with a focus on AI-ready, modular platform design. Over 25 years in developing and integrating financial systems, orchestrating complex workflows, and enabling rapid AI deployment while maintaining governance and stability.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Programming & Software Engineering & Testing"}, {"title": "Building Agentic Systems with Python, LangGraph, MCP, and A2A", "abstract": "Building an agentic system that collects and evaluates company information in real time\u2014without curated datasets\u2014requires solving difficult challenges in data acquisition, quality control, and agent orchestration.\r\n\r\nThis talk outlines the solution design for such a system, implemented with Python-based tooling including LangGraph, and emerging protocols such as A2A and MCP, within a multi-agent workflow. Because MCP and A2A are still new and lightly documented, we will share implementation lessons and a practical example of a hub-and-spoke architecture based on a recent real-world system.\r\n\r\nAttendees will learn architectural patterns for multi-agent systems, common pitfalls of using MCP/A2A in real-world scenarios, and strategies for maintaining data quality in agent-based workflows.", "full_description": "# What we are going to show\r\n\r\n- A live demo of a Python-based multi-agent system that retrieves, aggregates, and evaluates company information in real time.\r\n- The overall solution architecture: how LangGraph, MCP, A2A, and custom Python components fit together.\r\n- Key implementation lessons from building the system, covering both technical and business challenges.\r\n\r\n# What problem is our talk addressing\r\n\r\nAI analysis depends heavily on data. When systems cannot rely on pre-collected or curated datasets, developers must find, collect, and validate data of sufficient quality.\r\n\r\nAt the same time, emerging technologies such as MCP, A2A, and LangGraph are evolving quickly, with limited documentation, occasional breaking changes, and examples that rarely scale beyond minimal tutorials. Applying these tools to real-world Python applications introduces challenges in design, orchestration, versioning, and error handling that are not yet widely discussed.\r\n\r\n# Why is the problem relevant to the audience\r\n\r\nMany Python developers and data practitioners will soon need to build systems that combine LLMs, external data sources, and multi-agent logic, without relying on static datasets. This talk provides practical guidance for designing such systems using open-source Python tooling.\r\n\r\nThe presented solution is designed with a modular, scalable component approach. MCP and A2A protocols facilitate the connection between AI-related solutions, and this design demonstrates re-usable patterns for implementation.\r\n\r\nBy sharing our approach, design choices, and implementation pitfalls, the talk equips attendees to anticipate challenges early, evaluate whether MCP/A2A are appropriate for their own projects, and build more robust agentic systems.\r\n\r\n# What is our solution to the problem\r\n\r\nOur solution has split responsibilities in several blocks, though the overall idea is to present with code examples a Python system that combines LangGraph, A2A and MCP:\r\n\r\n- Data access via MCP servers\r\nMCP servers retrieve data from multiple sources (e.g., LinkedIn APIs, web scraping endpoints, Perplexity research). Using MCP makes it easy to plug in new data sources and manage them consistently. We demonstrate how to build and connect MCP servers in Python.\r\n- Data processing via LangGraph agents\r\nA set of agents implemented in LangGraph handle tasks such as coordinating the workflow, collecting company data, calculating evaluation scores, and validating results. These agents operate in a hub-and-spoke pattern centered around a coordinator agent. We show how this is implemented in Python using LangGraph.\r\n- Inter-agent communication via A2A\r\nAgents exposes capability \u201ccards,\u201d which the coordinator aggregates into a registry. An intent-detection step determines which agents should be invoked to answer a user's request. We demonstrate how A2A can be applied in Python to orchestrate agents effectively.\r\n- Data validation agent\r\nA dedicated validation agent checks retrieved data against defined rules to ensure quality. While no internet-sourced data is perfect, this approach significantly increases reliability. We show how validation logic is implemented within the LangGraph flow.\r\n- Scalability through configuration and deployment\r\nA centralized configuration file and simple Docker-based deployment make the system easy to scale and adapt. We explain how environment variables and shared configuration patterns can coordinate the various Python components.\r\n\r\n# What are the main takeaways from our talk\r\n\r\nAttendees will learn:\r\n\r\n- How to design and implement a practical multi-agent architecture using Python, LangGraph, MCP, and A2A.\r\n- How to acquire and validate external data dynamically without relying on curated datasets.\r\n- Common pitfalls and lessons from using MCP and A2A in larger-scale systems.\r\n- How to structure agent roles, orchestration flows, and validation strategies for scalable, extendable AI systems.", "code": "JBFGCA", "state": "confirmed", "created": "2025-11-26", "social_card_image": "/static/media/social/talks/JBFGCA.png", "speaker_names": "Holger N\u00f6sekabel", "speakers": "\n### Holger N\u00f6sekabel\n\nHolger N\u00f6sekabel has deep experience in data ecosystems, applied data science, and building production-grade systems with multidisciplinary teams. As CTO at TD Reply, he leads more than 20 engineers, data scientists, and visualization specialists in developing internal data products and delivering complex analytics projects for global Fortune 500 companies.\r\n\r\nBefore taking on the CTO role, Holger served as Director of Technical Consulting, supporting engineering teams and advising major brands on data-driven strategy. He is also a Certified ScrumMaster and an advocate for practical, team-focused agile practices.\r\n\r\nHolger enjoys working at the intersection of data, product development, and real-world impact - bringing technical insights to diverse audiences and helping teams turn ideas into reliable, scalable solutions.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Python Hates Being PID 1: Writing Container-Aware Code for Kubernetes", "abstract": "On Kubernetes, your Python app runs in a hostile environment, fighting for resources in a straitjacket, bombarded with signals, and being killed and ruthlessly dragged back to life time and again. This is in stark contrast to the wonderful weather of a Linux web server or the blissful utopia of localhost. If not hardened properly, your Python app will find the burden of being containerized too hard to bear. And the result? Zombies!\r\n\r\nWhether you are a Kubernetes expert, or you just deployed your first containerized Hello World, we will together explore how the Python Interpreter, the Linux Kernel and Kubernetes interact with each other.\r\n\r\nWe will uncover why Python struggles as an init process, how Kubernetes CPU-limits fight the Global Interpreter Lock (GIL) and why Python\u2019s Garbage Collector cannot save you from sudden OOM kills. Most importantly, we will see how to identify, debug, and avoid containerized Python pitfalls. The goal of this talk is to help you stop treating your container like a server and learn to write Cloud-Native Python that knows exactly where it lives.", "full_description": "**The Problem** : The large-scale adoption of Kubernetes means more Python developers are now writing code that runs as a containerized workload on Kubernetes. However, most of us still write applications with a standard Linux server in mind. In a containerized environment, these assumptions are either untrue or dangerous. Python apps not hardened for a containerized environment lead to production failures that are notoriously hard to debug:\r\n- Unexplained Latency: API requests that stall for hundreds of milliseconds due to Linux CFS Quota throttling, even when monitoring shows low CPU usage.\r\n- Silent OOM Kills: Containers that vanish instantly without a traceback because they hit a Cgroup limit that the Python Garbage Collector cannot see.\r\n- Zombie Processes: Subprocesses that were never truly killed and are now exhausting the process table because Python ignores its duties as PID 1.\r\n\r\n**The Solution** : This talk will briefly get you up to speed with containerization before taking a technical deep dive into the interactions between Kubernetes, the CPython interpreter and the Linux container runtime. We will move beyond basic Dockerfile best practices and focus on hardening the application code itself to survive in a hostile Kubernetes environment.\r\n\r\n**Pre-requisites** : This talk is aimed towards intermediate to senior Python Developers and Data Engineers having basic familiarity with Docker. No advanced Kubernetes or Linux Kernel knowledge required, we will run through the foundational topics in brief.\r\n\r\n**Outline (30 Minutes)**\r\n1. Who am I? (2 mins)\r\n2. The Lie of the Container (3 mins)\r\n    - Understanding how the container runtime isolates your process and the resources it needs.\r\n3. The PID 1 Problem (4 mins)\r\n    - How the Linux kernel treats PID 1 processes and why the standard Python interpreter fails these duties.\r\n    - Present well established solutions to the problem (init: true, tini, etc) and common pitfalls.\r\n4. The CPU Quota & Memory Limit (8 mins)\r\n    - How container CPU limits in Kubernetes translate to Linux CFS (Completely Fair Scheduler) quotas. \r\n    - Visualizing how the enforcement of CFS quotas interacts with the Python GIL to cause latency spikes.\r\n    - Python\u2019s memory management and the dreaded OOM kill.\r\n5. Hardening your Python Code (8 mins)\r\n    - How to use the Cgroup file system or psutil to achieve true resource awareness.\r\n    - Strategies for avoiding CPU throttling and tuning numeric libraries (Pandas/Numpy) from attempting to use too many cores.\r\n    - Why `gc.collect()` is often insufficient and how to release memory before the OOM killer strikes.\r\n6. Conclusion & Checklist (5 mins)\r\n    - A \"Production-Ready\" checklist for Python on K8s.\r\n    - Q&A.\r\n\r\n**After this talk you will** :\r\n- Understand the lifecycle of a containerized Python app and handle shutdowns gracefully.\r\n- Fine-tune a containerized Python app for stability and avoid CPU throttling and OOM kills.\r\n- Look beyond the standard system calls to write truly resource aware Python apps.", "code": "JJDCW3", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/JJDCW3.png", "speaker_names": "Kavish Nareshchandra Dahekar", "speakers": "\n### Kavish Nareshchandra Dahekar\n\nSenior Dev at SAP by day, dad by night. Married to another dev. No code-reviews at dinner table. Latest stable release : baby girl. Average guitar player, above average guitar teacher. All my neighbours know I sing. Based in Berlin, fluent in Python, German still in beta.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Heat: scaling the Python scientific stack to HPC systems", "abstract": "Python\u2019s scientific stack (NumPy/SciPy) is often confined to single-node execution. When datasets exceed local memory, researchers face a steep learning curve, typically choosing between complex manual distribution or the overhead of task-parallel frameworks.\r\n\r\nIn this talk, we introduce [Heat](https://github.com/helmholtz-analytics/heat), an open-source distributed tensor framework designed to bring high-performance computing (HPC) capabilities to the scientific Python ecosystem. Built on PyTorch and mpi4py, Heat implements a data-parallel model that allows users to process massive datasets across multi-node, multi-GPU clusters (including AMD GPUs) with minimal code changes.\r\n\r\nWe will discuss the design and architecture enabling \"transparent distribution\":\r\n\r\n- Heat\u2019s distributed n-dimensional array for data partitioning and communication under the hood;\r\n- The synergy of PyTorch as a high-performance compute engine and MPI for efficient, low-latency communication;\r\n- Scaling efficiency, encompassing both strong and weak scaling for memory-intensive operations;\r\n- Fundamental building blocks\u2014from linear algebra to machine learning\u2014re-implemented for distributed memory space.\r\n\r\nAttendees will learn how to leverage the cumulative RAM of supercomputers without leaving the familiar NumPy-like interface, effectively removing the \"memory wall\" for large-scale scientific analytics.", "full_description": "**Memory bottleneck in scientific computing (4 minutes)**\r\n - Limitations of single-node libraries\r\n - Complexity of existing workarounds: trade-offs between manual MPI programming (high developer effort) and task-parallel frameworks \r\n - The data-parallel alternative: performing uniform operations on distributed slices of a global tensor.\r\n\r\n**Architecture and implementation (8 minutes)**\r\n- The DNDarray structure: Technical breakdown of the distributed n-dimensional array, which provides a global logical view while managing local physical storage across MPI ranks.\r\n- The split axis concept: How data is partitioned along specific dimensions (e.g., rows or columns) to optimize communication for different mathematical operations.\r\n- Backend synergy: \r\n  - PyTorch as the compute engine for high-performance local tensor operations and GPU acceleration.\r\n  - mpi4py for communication in cluster environments.\r\n- Hardware interoperability: Transparent execution across CPUs and GPUs, including NVIDIA (CUDA) and AMD (ROCm) accelerators.\r\n\r\n**Algorithmic building blocks for distributed memory (8 minutes)**\r\n- Communication-aware linear algebra: Distributed matrix-matrix multiplication and its communication costs. Advanced matrix decomposition methods, such as hierarchical and randomized SVD (hSVD), for massive datasets.\r\n- Scalable machine learning and statistics: Example: clustering (K-Means) and Principal Component Analysis (PCA) on distributed arrays.\r\n- Temporal analysis using Dynamic Mode Decomposition (DMD) on large-scale scientific data like global wind speeds.\r\n\r\n**Performance and scaling efficiency (7 minutes)**\r\n- Scaling methodologies: strong scaling (speedup for a fixed problem size) and weak scaling (efficiency as both problem size and resources grow).\r\n- Memory wall removal: Utilizing the cumulative RAM of many cluster nodes to process datasets that are otherwise impossible to load.\r\n- Case studies: Reviewing performance results from large-scale runs\r\n\r\n**Summary and project roadmap (3 minutes)**\r\n- Key takeaways\r\n- Upcoming features\r\n- Open-source community", "code": "K9LCNQ", "state": "confirmed", "created": "2026-01-09", "social_card_image": "/static/media/social/talks/K9LCNQ.png", "speaker_names": "Claudia Comito, Thomas Saupe", "speakers": "\n### Claudia Comito\n\nI work in the Large-Scale Data Science division at the J\u00fclich Supercomputing Centre (JSC), and I lead the development of Heat, an open-source distributed tensor framework designed for high-performance data analytics. My work focuses on scaling scientific Python applications across multi-node, multi-GPU clusters.\r\n\r\nMy background is in astrophysics, I joined JSC in 2018 to co-design distributed analytics for scientific domains including aerospace and Earth system modeling. Since 2021, I have led the Heat project, focusing on technical user support, community growth, and project dissemination.\n\n### Thomas Saupe\n\n\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "Your First Open Source Contribution in Python: From Fork to Pull Request", "abstract": "Contributing to open source can feel intimidating, even for experienced Python developers. In this hands-on tutorial, participants will make their first real open source contribution to a Python project, learning the complete workflow from fork to pull request.\r\n\r\nUsing a real-world Python library, attendees will practice reading an unfamiliar codebase, making a small but meaningful change, running tests, and opening a pull request following community standards. The focus is on practical skills, tooling, and confidence \u2014 not theory.\r\n\r\nBy the end of the session, participants will understand how to start contributing to Python open source projects and feel prepared to continue contributing beyond the workshop.", "full_description": "Open source is a core pillar of the Python ecosystem, yet many developers struggle to make their first contribution. The barriers are often not technical ability, but uncertainty around workflows, expectations, and collaboration practices.\r\n\r\nThis 90-minute hands-on tutorial guides participants through their first real contribution to an open source Python project, focusing on clarity, safety, and reproducibility. Rather than working on toy examples, attendees will contribute to ScanAPI, an actively maintained open source Python library used for automated API integration testing and live documentation.\r\n\r\nThe tutorial is designed to demystify the contribution process while remaining technically grounded and respectful of real-world open source practices.\r\n\r\nWhat participants will learn:\r\n\r\n1. Understanding an Open Source Python Project\r\n- How to quickly navigate an unfamiliar Python repository\r\n- Reading project structure, tests, and documentation\r\n- Understanding contribution guidelines and expectations\r\n\r\n2. Open Source Workflow in Practice\r\n- Forking and cloning a repository\r\n- Creating a local development environment\r\n- Working with branches and commits\r\n\r\n3. Making a First Contribution\r\n- Working on a well-scoped, beginner-friendly issue\r\n- Writing or updating Python code, tests, or documentation\r\n- Running tests locally and validating changes\r\n\r\n4. Opening a Pull Request\r\n- Writing a clear and respectful pull request description\r\n- Understanding automated checks (CI)\r\n- Responding to maintainers\u2019 feedback\r\n\r\n5. Contributing Sustainably\r\n- How to continue contributing after the workshop\r\n- Common mistakes to avoid\r\n- How open source communities scale through good engineering and collaboration\r\n\r\nAll tutorial tasks are carefully scoped and prepared in advance to ensure a smooth experience within the 90-minute timeframe. Participants will leave with a forked repository, a commit, and a pull request opened or ready, as well as the confidence to contribute to other Python open source projects.\r\n\r\nWhy ScanAPI?\r\n\r\nScanAPI is a production-grade Python library distributed via PyPI and maintained in the open. It has been recognized by GitHub as part of initiatives focused on securing the open source supply chain, making it an excellent real-world example of sustainable Python open source development. The project is supported by the Cumbuca Dev open source community, which focuses on building inclusive, contributor-friendly environments through strong engineering practices.", "code": "KKCYJN", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/KKCYJN.png", "speaker_names": "Camila Maia", "speakers": "\n### Camila Maia\n\nBrazilian software engineer, open source maintainer, and co-founder of Cumbuca Dev, a community-driven initiative that supports underrepresented people entering and thriving in technology through real-world practice, open source collaboration, and education. With over a decade of professional experience, Camila focuses on backend engineering, developer experience, tooling and automation.\r\n\r\nShe is the creator and core maintainer of ScanAPI, a Python library for automated API integration testing and live documentation that has gathered widespread adoption and community contributions. ScanAPI has been recognized by GitHub as part of initiatives to strengthen the open source supply chain and is used by developers internationally. Camila\u2019s work spans not only code but also documentation, automation pipelines, and contributor experience practices that make open source projects more sustainable.\r\n\r\nCamila was the first Brazilian accepted into the GitHub Sponsors program, breaking new ground for maintainers in her country. She is also featured as one of ~50 global open source maintainers in the maintane.rs project, invited by the Open Source Initiative (OSI) to share her personal journey and perspectives on how open source can unlock opportunities in tech.\r\n\r\nHer engagement extends to speaking and mentoring at technical conferences around the world, including Pyjamas, EuroPython, Python Brasil, DjangoCon EU, and others, where she has presented both talks and hands-on workshops. \r\n\r\nThrough Cumbuca Dev, Camila advocates for practical learning and structured contributions as pathways to real experience, helping people from diverse backgrounds build skills, confidence, and visibility before their first job. She believes that open source is not just code \u2014 it is a vehicle for community, opportunity, and empowerment \u2014 and her work reflects a commitment to making technology spaces more accessible, collaborative, and humane.\r\n\r\nPeople > Tech \ud83d\udc9c\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Education, Career & Life"}, {"title": "Foundation Models in Forecasting: Are We There Yet? Lessons from the Trenches", "abstract": "The rise of time-series foundation models like Chronos-2 and TimesFM has sparked a debate: can a single pre-trained model replace the specialized \"local\" models we have tuned for years? We moved beyond the hype to test these models in production-like environments, from high-level market trends to granular article-level demand. In this talk, we share a transparent look at our journey: the zero-shot capabilities of these models, the reality of fine-tuning with exogenous business drivers, and a comparison between generative models and state-of-the-art classical methods. We categorize what is currently possible, what remains a challenge, and provide a roadmap for teams looking to integrate foundation models into their forecasting stack without sacrificing reliability.", "full_description": "## I. The Promise vs. The Reality (10 min)\r\n\r\n- The hype cycle: A brief overview of the current time-series foundation model landscape.  \r\n- The \"zero-shot\" myth: Why \"out-of-the-box\" performance often hits a ceiling in e-commerce due to a lack of domain context (discounts, commercial activities).  \r\n- Between two extremes: We experimented with stable market-level KPIs and noisy article-level demand \u2014 to see where time-series foundation models succeed or fall short versus tuned classical models.\r\n\r\n## II. Lessons from the Trenches: What Works and What Doesn\u2019t (15 min)\r\n\r\n- The setup: We run daily and weekly forecasts across 20+ markets at two horizons (5 and 16 weeks), evaluating KPIs like GMV, items sold, cancellation rate, plus article-level forecasts for established best-sellers and newly launched items.  \r\n- The \"win\" column: Scenarios where foundation models reduced MLOps overhead and delivered surprisingly strong zero-shot baselines (lower WAPE (weighted absolute percentage error) across 52 rolling validation rounds).  \r\n- The \"fail\" column: Why foundation models still struggle with rigid non-linear business constraints and \"rare event\" sensitivity (e.g., cold-starts for new SKUs).  \r\n- The \u201cstability\u201d column: How foundation model forecasts evolve as the target date approaches: do predictions converge and uncertainty shrink smoothly, or do they jump around unpredictably?\r\n\r\n## III. Summary: The 2026 Roadmap (5 min)\r\n\r\n- Are we there yet? Our final conclusion on whether foundation models are ready to be the primary engine or remain a strong supporting model.  \r\n- Final advice: A checklist for teams deciding where to start their foundation model journey.", "code": "KQM8JJ", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/KQM8JJ.png", "speaker_names": "Dr. Irena Bojarovska", "speakers": "\n### Dr. Irena Bojarovska\n\nIrena Bojarovska is an Applied Scientist at Zalando SE, focusing on time\u2011series forecasting and demand prediction across 24+ markets.\r\n\r\nOriginally from Macedonia, she earned a BSc and an MSc in Applied Mathematics and Computer Science in Russia and a PhD in Applied Harmonic Analysis from TU Berlin. She began her industry career as an analyst at Air Berlin and, since 2017, has worked on causal inference for marketing, automation, demand forecasting, hierarchical reconciliation, and time\u2011series foundation models at Zalando. Outside work she leads a math circle for children at Lyzeum 2 and enjoys spending time with her family.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Generative AI & Synthetic Data"}, {"title": "The Multimodal Era of Machine Learning (and How Python Made It Possible)", "abstract": "Multimodal learning - systems that combine vision, language, audio, and other sensory inputs\u2014has moved from a niche research topic to a central paradigm in modern machine learning. Today\u2019s most influential models no longer operate on a single modality but instead learn rich representations by combining language with images, videos, sound. This shift has fundamentally changed how we build, train, and evaluate current machine learning systems. Python has played a decisive role in this transformation. Acting as a unifying layer across modalities, Python enabled researchers and practitioners to seamlessly combine computer vision, natural language processing, and speech within a single ecosystem. Python-based frameworks lowered the barriers between research communities, and accelerated the rise of large-scale, weakly supervised, and foundation models. However, this success has also introduced new challenges. The ease of experimentation masks growing issues around scalability, reproducibility, and evaluation. Multimodal systems increasingly depend on complex Python-based stacks whose abstractions can obscure underlying assumptions and costs.\r\n...", "full_description": "Multimodal learning\u2014systems that combine vision, language, audio, and other sensory inputs\u2014has moved from a niche research topic to a central paradigm in modern machine learning. Today\u2019s most influential models no longer operate on a single modality but instead learn rich representations by combining language with images, videos, sound. This shift has fundamentally changed how we build, train, and evaluate current machine learning systems. Python has played a decisive role in this transformation. Acting as a unifying layer across modalities, Python enabled researchers and practitioners to seamlessly combine computer vision, natural language processing, and speech within a single ecosystem. Python-based frameworks lowered the barriers between research communities, and accelerated the rise of large-scale, weakly supervised, and foundation models. However, this success has also introduced new challenges. The ease of experimentation masks growing issues around scalability, reproducibility, and evaluation. Multimodal systems increasingly depend on complex Python-based stacks whose abstractions can obscure underlying assumptions and costs.\r\nThis keynote will reflect on the current state of multimodal learning, examine how Python shaped its trajectory, and critically discuss the technical and conceptual challenges that lie ahead aiming to provide a perspective on where machine learning in general and multimodal learning in particular is succeeding, where it is struggling, and what role the Python community can play in shaping its next phase.", "code": "LPUC9T", "state": "confirmed", "created": "2026-01-25", "social_card_image": "/static/media/social/talks/LPUC9T.png", "speaker_names": "Hilde K\u00fchne", "speakers": "\n### Hilde K\u00fchne\n\nProf. Dr. Hilde Kuehne is a Professor of Multimodal Learning at the T\u00fcbingen AI Center and an affiliated professor at the MIT\u2013IBM Watson AI Lab. Previously, she was a Professor of Computer Vision and Multimodal Learning at the University of Bonn. She received her PhD from the cv:hci lab at the Karlsruhe Institute of Technology (KIT), where she was supervised by Rainer Stiefelhagen, and subsequently held postdoctoral positions at Fraunhofer FKIE and in the Computer Vision Group led by Prof. J\u00fcrgen Gall.\r\nHer research focuses on video understanding, with a particular emphasis on learning without labels and multimodal video understanding. She has created several highly cited datasets and foundational works for analyzing large collections of untrimmed video data, including HMDB51, which was awarded both the ICCV 2021 Helmholtz Prize and the PAMI Mark Everingham Prize.\n", "domain_expertise": "None", "python_skill": "None"}, {"title": "A View of Sovereignty from The Cloud", "abstract": "While The Cloud is just someone elses computer, those computers come together from many places and many, many someone elses. The constituent parts to connect, power, house, and ultimately operate those computers are from many more places and someones still! We explore what these infrastructure pieces of The Cloud are explicitly; and how the many definitions of digital sovereignty can be viewed from the viewpoint high up in The Cloud.", "full_description": "While The Cloud is just someone elses computer, those computers come together from many places and many, many someone elses. The constituent parts to connect, power, house, and ultimately operate those computers are from many more places and someones still! We explore what these infrastructure pieces of The Cloud are explicitly; and how the many definitions of digital sovereignty can be viewed from the viewpoint high up in The Cloud.", "code": "LSJ3CN", "state": "confirmed", "created": "2026-01-26", "social_card_image": "/static/media/social/talks/LSJ3CN.png", "speaker_names": "Aaron Glenn", "speakers": "\n### Aaron Glenn\n\n\n", "domain_expertise": "None", "python_skill": "None"}, {"title": "Using Sensor Fusion and ML to Navigate Underground When GPS Fails", "abstract": "In the twisting vaults of a subway, metro, or U-Bahn, there\u2019s often no reliable cell service, wifi, or GPS. Which means riders had no good way of keeping track of their stops or ETA when underground.\r\nAfter collecting extensive ground truth data, we trained a motion classifier using the phone's accelerometer to identify a moving train. This prediction is fed into a location model that combines it with the train schedule to estimate a location, even when GPS fails. We cover our unique data pipeline, feature engineering, and the optimization for high-scale, offline edge deployment to millions of users.", "full_description": "Attendees will gain from the lessons learned developing a sensor fusion ML system for offline use in smartphones\r\n\r\n##### Data Collection & Annotation\r\nStrategies for gathering high-quality, labeled \"ground truth\", especially in cases where the labels can't be inferred by human annotators after the fact\r\n\r\n##### The ML Pipeline\r\nHyperparameter tuning of a convolutional neural network (CNN)\r\nBuilding a multi-stage training regimen, to leverage different datasets\r\n\r\n##### UX\r\nPresenting predictions to users in a way that expresses uncertainty when necessary, and inspires confidence when justified. We want users to forget GPS doesn't work underground.", "code": "LVJXK3", "state": "confirmed", "created": "2025-12-03", "social_card_image": "/static/media/social/talks/LVJXK3.png", "speaker_names": "\u00c9tienne Tremblay", "speakers": "\n### \u00c9tienne Tremblay\n\nHi, I'm \u00c9tienne! I am responsible for developing machine learning solutions to rider problems, and turning them into features that help Transit users in their journeys. Before working at Transit, I was building databases and analysis tools for the aerospace industry. I was also been involved in the mobility professorship at Polytechnique Montr\u00e9al. I hold a B. Eng. in Aerospace from Polytechnique Montr\u00e9al.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "\"You are an intelligent business analyst\": how i learned to talk to business", "abstract": "Developers don\u2019t need to become business analysts, but they do need business skills. This talk shows how learning to communicate with stakeholders, uncover real business needs, and bridge gaps between tech and business can dramatically increase your impact. Learn practical techniques to become a trusted technical partner and deliver solutions that truly matter.", "full_description": "I never planned to become a business analyst. In fact, I avoided it. I imagined endless meetings, unclear requirements, and conversations that had nothing to do with \u201creal\u201d technical work. I wanted to stay hands-on as a developer and data scientist.\r\n\r\nBut reality proved something important: you can't escape the business side if you want to build meaningful solutions. And once I learned how to talk to business stakeholders, everything changed: my impact, my influence, and the outcomes of the projects I worked on.\r\n\r\nIn this talk, we\u2019ll explore the practical business skills every developer needs but is rarely taught:\r\n\u2022 How to identify key stakeholders and understand what they really want\r\n\u2022 How to navigate communication in international, cross-functional teams\r\n\u2022 How to uncover business pain points before they become blockers\r\n\u2022 How to fix broken communication loops\r\n\u2022 How to become the go-to technical partner the business trusts\r\n\r\nBy the end, you won\u2019t just see yourself as a strong technical contributor: you\u2019ll see how to position yourself as an essential part of the broader business ecosystem, shaping better decisions and delivering solutions that truly matter.", "code": "LVRLSU", "state": "confirmed", "created": "2026-01-09", "social_card_image": "/static/media/social/talks/LVRLSU.png", "speaker_names": "Darya Petrashka", "speakers": "\n### Darya Petrashka\n\nDarya Petrashka is a Senior Data Scientist at SLB with 6 years of experience, focusing on NLP and GenAI. She is passionate about using data for problem-solving, with a strong interest in AWS services. An AWS Community Builder, Darya actively shares her expertise through public speaking at various industry events, including AWS Community Days, Summits, and PyCons. A dedicated learner, Darya continually hones her skills by participating in workshops, courses, and tech schools.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Education, Career & Life"}, {"title": "What Breaks When Automatic Speech Recognition Systems Go Multilingual", "abstract": "Building machine learning models for audio deepfake detection seems straightforward until datasets span multiple languages, such as Hindi, Korean, Mandarin, and German. In practice, multilingual Automatic Speech Recognition (ASR) systems often fail in production because language-specific acoustic variations and assumptions about the processing pipeline break down at scale.\r\n\r\nThis talk examines the engineering challenges of building a multilingual deepfake detection system using a Python-centric pipeline. It covers practical issues encountered during large-scale audio preprocessing, including memory-efficient data loading, resumable feature-extraction workflows, and validation strategies designed to prevent cross-lingual leakage. The session also shares lessons from deploying a multilingual ASR-based system, with a focus on pipeline structure, evaluation correctness, and operational robustness in real-world settings.", "full_description": "In a multilingual Automatic Speech Recognition (ASR) dataset containing over 440,000 audio samples, preprocessing methods that were effective for one language often failed silently for others. This resulted in shifts in acoustic features, misleading validation outcomes, and prolonged jobs that failed due to assumptions that held true only in monolingual contexts. This presentation examines the issues that arise when extending ASR systems to multilingual data, using a real-world deepfake detection system that includes Hindi, Korean, Mandarin, and German. It addresses the engineering challenges encountered while developing and operating a Python-based pipeline at scale.\r\n\r\nThe session will discuss practical issues in large-scale audio processing, including the creation of memory-efficient data loaders, the design of workflows that support resumable preprocessing and feature extraction, and strategies for managing long-running jobs to avoid redundant computations. Additionally, it will cover validation strategies for multilingual ASR systems, emphasizing that language imbalance and shared pipelines can lead to cross-lingual leakage, which skews evaluation results if not explicitly addressed.\r\n\r\nKey takeaways include:\r\n1. Multilingual ASR pipelines reveal language-specific issues that are not present in monolingual systems.\r\n2. Scalable audio processing requires memory-efficient and resumable Python workflows.\r\n3. Cross-lingual evaluation necessitates explicit control over language imbalance and leakage.", "code": "LYCBNT", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/LYCBNT.png", "speaker_names": "Rashmi Nagpal", "speakers": "\n### Rashmi Nagpal\n\nRashmi is a AI Research Scientist at Poseidon and a researcher at MIT CSAIL, working in the intersection of cybersecurity and artificial intelligence. She has six years of industrial experience, having brought ideas to life at pre-seed startups and contributed to impactful redesigns and features at established industry giants. Beyond coding, Rashmi finds inspiration in capturing the wonders of the cosmos through her telescope and engaging in board games with friends.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Building Non-Biased Synthetic Datasets: What Actually Works (and What Fails)", "abstract": "Synthetic data is often presented as an easy fix for missing or sensitive datasets, but in practice, it can silently introduce bias, leakage, and misleading evaluation results. This talk presents a practical, end-to-end pipeline for creating synthetic datasets that are reproducible, task-aligned, and bias-aware. We will walk through design decisions that matter: template-based generation vs. free-form generation, entity balancing, controlling distributional skew, filtering failure cases, and validating dataset quality before training any model. The session emphasizes what actually works in real pipelines, common failure modes that look fine at first glance, and concrete best practices for Python developers to apply when building synthetic datasets for machine learning, NLP, or evaluation.", "full_description": "This talk focuses on the engineering side of synthetic dataset creation, treating data as a first-class artifact rather than a byproduct of modeling. It presents a concrete, reusable pipeline for building synthetic datasets that are reproducible, bias-aware, and suitable for evaluation.\r\n\r\n1. Why Synthetic Data Is Not Automatically \u201cSafe\u201d\r\nWe begin by examining common assumptions about synthetic data. While synthetic datasets avoid privacy issues, they often introduce hidden bias, distribution collapse, or label leakage. This section highlights real-world failure modes and explains why many synthetic datasets perform well in benchmarks but fail in practice.\r\n\r\n2. What are the Main Properties of Synthetic Data\r\n\t\t1. Simulated Data\r\n\t\t2. Anonymized\r\n\t\t3. Not Copied\r\n\t\t4. Compliant\r\n\t\t5. It is based on statistical property of real data.\r\n\r\n3. Defining the Task Before Generating Any Data\r\nA dataset pipeline must start with a clear task definition. We discuss how ambiguous task definitions lead to incoherent data and misleading results, and how to formally specify label semantics, constraints, and negative space before generation begins.\r\n\r\n4. Template-Based vs. Free-Form Generation\r\nThis section compares controlled template-based generation with unconstrained LLM prompting. We show why decomposing generation into templates, placeholders, and curated value lists dramatically improves consistency, debuggability, and bias control.\r\n\r\n5. Bias Control by Construction\r\nRather than detecting bias after the fact, we show how to prevent it during generation. Topics include balanced entity lists, randomized substitution, avoiding demographic collapse, and preventing unintended correlations between labels and surface patterns.\r\n\r\n6. Pipeline Architecture and Tooling\r\nWe walk through a practical Python-based pipeline, covering modular generation stages, deterministic sampling, versioning, and reproducibility. Emphasis is placed on making dataset generation repeatable and auditable, just like code.\r\n\r\n7. Filtering, Validation, and Quality Gates\r\nSynthetic data must be filtered aggressively. This section covers structural validation, label consistency checks, distributional sanity checks, and lightweight heuristics that catch most generation errors before model training.\r\n\r\n8. Measuring Dataset Difficulty and Coverage\r\nWe discuss simple, task-agnostic ways to estimate dataset diversity and difficulty, ensuring that synthetic data does not collapse into trivially easy examples or overly clean language.\r\n\r\n9. What Did Not Work (and Why)\r\nThis section summarizes failed approaches, including direct JSON generation, inline annotation, and large one-shot prompts. Understanding these failures helps avoid repeating common mistakes.\r\n\r\n10. When Synthetic Data Is the Right Tool and When It Is Not\r\nWe close with guidance on appropriate use cases for synthetic datasets, their limitations, and how they should complement, not replace, real data and human evaluation.", "code": "M33SNJ", "state": "confirmed", "created": "2025-12-29", "social_card_image": "/static/media/social/talks/M33SNJ.png", "speaker_names": "Shiva Banasaz Nouri", "speakers": "\n### Shiva Banasaz Nouri\n\nShiva Banasaz Nouri is a Senior Data Scientist based in Berlin, Germany, working on applied machine learning with a focus on Python, NLP, computer vision, and generative AI. She builds production-grade AI systems across healthcare, legal, and enterprise domains using open-source technologies.\r\n\r\nShe is the Berlin Chapter Lead of Women in AI, where she actively fosters community building, knowledge sharing, and inclusive participation in the AI and Python ecosystems.\n", "domain_expertise": "None", "python_skill": "None", "track": "Generative AI & Synthetic Data"}, {"title": "Mastering the Hex: A Case Study in Reinforcement Learning for Strategy Games", "abstract": "How do you teach a computer to understand the nuance of a territorial strategy game? Over the last year, driven by private interest and a Master\u2019s degree in Computer Science, I set out to build an autonomous agent for Antiyoy\u2014a minimalist yet strategically deep hexagonal game. This talk takes you through the entire journey, from the first line of environment code to a trained model capable of complex strategic play.\r\nThe talk addresses a fundamental problem in modern AI: how do we translate complex, turn-based rules into a language a neural network can interpret? We will explore the architecture of a custom Reinforcement Learning environment, the mathematical elegance required to handle hexagonal coordinates, and the engineering behind a massive action space of over 17,000 possibilities. You will hear how we guided the agent through the \"sparse reward\" problem and what happened when a model was finally allowed to play against its own mistakes. Whether you are interested in the mechanics of PyTorch and PettingZoo or simply curious about the \"brain\" of a strategy bot, this session provides a practical roadmap for tackling high-dimensional problems in Python.", "full_description": "This work began as a personal challenge to bridge the gap between a childhood love for strategy games and a fascination with deep learning. Developed over the course of a year within the context of a Master\u2019s in Computer Science, the talk presents a methodological case study on creating a sophisticated game bot from scratch.\r\nStrategy games like Antiyoy present a unique hurdle for standard Reinforcement Learning: they aren't just about quick reflexes; they require long-term economic planning and spatial reasoning on a grid where every cell has six neighbors instead of four. The talk answers the question of how to move from a raw game engine to a standardized Python environment that stable learning algorithms can actually use.\r\nThe presentation is divided into three thematic explorations:\r\n**Part 1: How do we translate a world of hexagons into a language of tensors?**\r\nThe first challenge of any RL project is the interface. The talk describes the process of wrapping game logic into a Gymnasium-compatible environment. You will hear about the design decisions behind a 91-channel observation space\u2014essentially \"images\" that encode everything from unit positions to economic health. We will also address the \"spiral\" mathematics of hexagonal grids: how do we efficiently map 2D coordinates to a discrete action space that a model can output?\r\n**Part 2: How can a model learn to navigate 17,000 possible choices?**\r\nWith an action space significantly larger than that of Chess, a random agent will almost never find a winning move by accident. The talk explores how we use \"action masking\" to prevent the model from wasting time on illegal moves and how the architecture of the neural network\u2014splitting its focus between predicting the \"value\" of a board and the \"policy\" of a move\u2014allows it to develop a sense of strategy. We will discuss the trade-offs between lightweight models and deep residual networks when working with limited hardware.\r\n**Part 3: What does emergent strategy look like in a trained agent?**\r\nThe final part of the talk focuses on the training process itself. How do we provide feedback to an agent when the only thing that truly matters is winning or losing thirty minutes later? You will hear about \"hybrid reward shaping\"\u2014a way to give the agent small hints about territory and economy without distracting it from the ultimate goal. The talk concludes with an analysis of the results: identifying the moment the agent stopped making random moves and started demonstrating recognizable human-like tactics, as well as the limitations we discovered along the way.", "code": "MJTQEJ", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/MJTQEJ.png", "speaker_names": "Simon Hedrich", "speakers": "\n### Simon Hedrich\n\nSimon Hedrich is a computer scientist and AI enthusiast currently completing his Master\u2019s degree in Computer Science. His academic and professional journey is marked by a deep interest in bridging the gap between theoretical research and practical AI engineering.\r\n\r\nThrough his work at inovex GmbH, Simon has demonstrated expertise in specialized areas of Artificial Intelligence, including computer vision and the use of synthetic data to enhance small object detection. His technical writing highlights his ability to leverage generative AI models, such as Stable Diffusion, to solve complex real-world challenges like training data scarcity.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Why Did The Model Do That? Debugging the Ghost in the Machine", "abstract": "Why did the model say \"No\"? In an era where machine learning models increasingly influence high-stake decisions, \"trust me\" isn't a sufficient explanation. Yet, the logic behind many model decisions remains a black box, often hiding bias and making it difficult to establish trust.\r\n\r\nIn this talk, we move beyond the mystery of the \"ghost in the machine\" and into practical debugging using a structured *XAI Decision Tree*. Instead of guessing which method to use, we will walk through a logical framework that narrows down the field based on a few critical questions: the type of data you have, the level of model access available, and whether you need to explain a single prediction or the entire system.\r\n\r\nThe audience will leave with a clear path to choosing the right explainable AI (XAI) method - such as SHAP, LIME, or Integrated Gradients - and the corresponding Python framework for their specific use case.\r\n\r\nThis session will cover:\r\n\r\n- Importance of XAI: Understanding why XAI is crucial using a real-world example\r\n- XAI Landscape: An overview of existing XAI methods and how they are related\r\n- XAI Decision Tree: How to use the structured XAI decision tree to choose the right explanation method for your use case\r\n- Local vs global: A common understanding of local vs global explainability\r\n- XAI in Practice: A demo showcasing XAI in practice as well as corresponding Python frameworks to use", "full_description": "My planned outline for the talk is as follows:\r\n\r\n- **Intro and opening hook** (4 mins): A look at a clearly biased model and why \"black box\" decisions fail to establish trust\r\n- **The XAI Decision Tree** (17 mins):\r\n\r\n    * A practical overview of the landscape and walking through the tree: Selecting the right method based on your model and data\r\n     * Mapping these methods to specific Python libraries and frameworks (e.g., `shap`, `lime`, `captum`, `transformers-interpret`, `alibi`, `dalex`, ...)\r\n\r\n- **Closing and Take-away** (4 mins)\r\n- **Q&A and Buffer** (5 mins)", "code": "MLUK9M", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/MLUK9M.png", "speaker_names": "Cosima Meyer", "speakers": "\n### Cosima Meyer\n\nCosima Meyer is a data scientist with a strong focus on making machine learning models explainable and accessible. Passionate about trustworthy AI, she is committed to building systems that are not only technically robust but also transparent and ethical. As a Google's Women Techmakers Ambassador and an active member of PyLadies, Cosima is dedicated to fostering inclusive and collaborative communities, working to bridge the two groups and create spaces for knowledge-sharing and growth.\r\n\r\nDuring her PhD studies at the University of Mannheim, Cosima discovered her enthusiasm for sharing knowledge through technical blog posts and developing open-source software. Her work reflects a blend of technical expertise and a passion for community building, inspiring others to explore, learn, and contribute to the fields of AI and data science.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Ethics & Privacy"}, {"title": "AI Memory: From Stateless RAG to Persistent Knowledge Graphs in 6 Lines of Python", "abstract": "RAG-based AI agents fail in production because retrieval without memory is like a conversation with someone who forgets everything you've said. This talk introduces a memory architecture that transforms how you build AI applications with a Python SDK.\r\n\r\nUsing an open-source Python SDK, I'll demonstrate how to replace fragile RAG pipelines with a unified memory layer combining knowledge graphs and vector search. You'll see live code showing how 6 lines of Python can give your agents persistent, queryable memory that survives restarts learns and improves with interactions.\r\n\r\nWe'll build a working agent memory system using cognee, Kuzu, LanceDB, and your choice of LLM, all running optionally locally. No cloud dependencies required. By the end, you'll understand why the future of AI agents isn't better RAG but better memory.", "full_description": "RAG systems treat knowledge as disconnected chunks and rely purely on vector similarity to find relevant context. This works for simple lookups but breaks down when agents need to reason across multiple pieces of information or remember previous interactions. The core issue is architectural: RAG retrofits context onto stateless systems rather than building with memory as a foundation.\r\n\r\nThis talk demonstrates an alternative approach using cognee, an open-source Python library that combines knowledge graphs with vector search to create persistent memory. I'll start by showing the basic API, which reduces the typical RAG boilerplate to a few lines of async Python. From there, we'll look at what happens under the hood: how documents get transformed into graph structures, how the ECL pipeline (Extract, Cognify, Load) processes different data types, and how queries traverse both graph relationships and vector similarity.\r\n\r\nThe live coding portion will build a working memory system using Kuzu for the graph layer and LanceDB for vectors, with Ollama providing local LLM inference. No API keys or cloud services required. I'll walk through adding unstructured data and executing searches that combine graph traversal with semantic matching.\r\n\r\nWe'll also cover practical considerations: when graph-based retrieval outperforms pure vector search, how to define custom ontologies for domain-specific applications, and the tradeoffs between different strategies. The talk concludes with a brief look at feedback mechanisms that allow the memory layer to improve over time based on user corrections.\r\n\r\nAttendees will leave with working code they can run locally and a clear understanding of how memory-first architecture makes sense against RAG approaches.", "code": "MMQ8SX", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/MMQ8SX.png", "speaker_names": "Hande Kafkas", "speakers": "\n### Hande Kafkas\n\nI joined cognee early to help build that engine, and I've been growing with it since. My corner: growth and developer ecosystem, integrations, technical content, partnerships, community. I like the work that sits between building something and getting it into people's hands - understanding the need, driving adoption, and making complex infrastructure accessible. Before cognee, I was an AI engineer consultant and worked in advanced analytics in an enterprise. I took lots of lessons in how enterprise teams actually adopt new tech and that still shapes how I think about developer experience today.\r\n\r\nTechnical University of Munich (M.Sc.) and Bo\u011fazi\u00e7i (B.Sc.) alumni, member of 2hearts community. Based in Munich.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "AI Evals Done Right: From Vibes to Confident Decisions", "abstract": "Testing traditional software is \"simple\"... same input, same output. LLMs? Not so much. Same prompt, different result every time. So how do you actually know if your AI product is good?\r\n\r\nMost teams struggle with this. Generic metrics like \"Helpfulness: 4.2\" sound scientific but don't drive real decisions. And when a new model releases, it's weeks of debates instead of data.\r\n\r\nThis talk introduces Error Analysis: a methodology to discover the concrete failure modes of your AI product and turn them into measurable evals. You'll learn how to build a failure taxonomy that enables real prioritization. Which issues are critical? Which are frequent? What should developers fix next, and how do you measure success?\r\n\r\nThe payoff: A real quality number for stakeholders. Concrete improvement tasks for developers. And when a new model drops, a ship-or-skip decision within 24 hours based on actual data.\r\n\r\nExpect a meme-powered walkthrough, real-world examples from production, and a clear path to implement this yourself starting with just 20 traces.", "full_description": "Testing traditional software is \"simple\"... same input, same output. LLMs? Not so much. Same prompt, different result every time. So how do you actually know if your AI product is good?\r\n\r\nSpoiler: Most teams don't. They ship on vibes and hope for the best.\r\n\r\nThis talk takes you through our real journey at Blue Yonder, where we built an LLM-powered analytics system and needed a way to actually measure its quality. You'll see how we went from \"feels okay-ish\" to concrete numbers that let us make real decisions - with actual examples from production along the way.\r\n\r\nThe methodology is called Error Analysis: collect traces, annotate them from the user's perspective, group similar issues into failure modes, and turn those into automated evals. Along the way, we'll share practical best practices like why binary Pass/Fail beats rating scales, and why 100% pass rate means your evals are broken.\r\n\r\nThe payoff? When a new model drops, we run our pipeline and know within hours - not weeks - whether it's better or worse for our specific use case. Real percentages. Real trade-offs. Real decisions.\r\n\r\nExpect a meme-powered walkthrough and a clear path to implement this yourself starting with just 20 traces.\r\n\r\nOutline:\r\n- Introduction: The challenge of testing stochastic systems, why we needed a better approach\r\n- Collecting and Annotating Traces: Every trace is a user experiencing your product, Open Coding from the user perspective, real examples of failure modes we discovered\r\n- Building the Failure Taxonomy: Grouping observations into categories, Axial Coding, turning scattered comments into actionable failure modes\r\n- Writing Evals That Work: LLM-as-judge setup, binary scores vs rating scales, validating against human judgment\r\n- From Vibes to Decisions: Prioritizing what to fix, measuring improvement, 24-hour model benchmarking\r\n- Wrap-up: Your action plan, start with 20 traces", "code": "MQJVFU", "state": "confirmed", "created": "2025-12-03", "social_card_image": "/static/media/social/talks/MQJVFU.png", "speaker_names": "Martin Seeler", "speakers": "\n### Martin Seeler\n\nMartin Seeler supercharges global supply chains with GenAI as Sr Staff AI Engineer at Blue Yonder. He ships AI that survives angry customers, skeptical executives, and Black Friday traffic. Speaks globally about the messy reality of production AI. Measures success in customer value delivered.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Generative AI & Synthetic Data"}, {"title": "Escape the Hype: Teaching LLM Concepts Through an Interactive AI Factory Game", "abstract": "Everyone talks about LLMs, RAG, and AI agents - but who truly understands them? Marketing promises magic while documentation assumes expertise. Recent research from Gartner reveals the consequences: only 8% of HR leaders believe their managers possess adequate AI competency, while companies that restructure work around AI achieve revenue goals twice as often as those who merely train employees. The problem isn't lack of information; it's the lack of genuine understanding through experience.\r\n\r\nWe took a different approach. Instead of slides or tutorials, we built \"AI Factory\" - a non-profit educational platform in the form of escape room game where players learn by doing. Craft prompts under budget pressure. Watch guardrails fail in real-time. Break their own RAG pipeline. Each mistake teaches more than any documentation ever could. \r\n\r\nIn this talk, we'll share what we discovered while building and testing this game with real users: why failure-driven learning outperforms tutorials, how game mechanics create memorable \"aha moments,\" and the surprising concepts that clicked only through play.", "full_description": "The AI Literacy Crisis: Why This Matters Now\r\n\r\nThe gap between AI adoption and AI understanding has never been wider. Companies rush to integrate LLMs into products while developers copy-paste prompts without understanding why they work. Marketing materials promise magic; reality delivers confusion. And with the European AI Act now requiring organizations to ensure \"a sufficient level of AI literacy among their staff,\" this isn't just an educational problem - it's becoming a compliance imperative.\r\n\r\nTo address this gap, we developed \"AI Factory,\" an interactive game-based platform for teaching LLM concepts through hands-on experience. We built it primarily as an internal tool to teach our own colleagues and to explore what makes technical education actually work. But first, let's examine why traditional approaches are failing:\r\n\r\nTraditional learning approaches are failing to close this gap:\r\n\u2022\tDocumentation assumes ML backgrounds and drowns readers in technical jargon\r\n\u2022\tTutorials oversimplify trivial examples that don't transfer to real problems\r\n\u2022\tConference talks inspire but rarely build the intuition needed for day-to-day work\r\n\u2022\tVendor marketing actively obscures how systems actually work\r\n\r\nWhat's missing is embodied learning - the kind where you touch the parameters, break the system, and feel the consequences. This is exactly what the developed educational platform provides: a safe space to fail, experiment, and build genuine understanding through direct experience.\r\n\r\nWhy Games Work for Technical Education\r\n\r\nGames aren't just \"fun ways to learn\" - they're pedagogically powerful for specific, well-understood reasons:\r\n\u2022\tIntrinsic motivation replaces external pressure. When learners want to succeed in a game, they engage with material more deeply than when studying for an exam. The challenge becomes personally meaningful\r\n\u2022\tImmediate feedback accelerates learning cycles. In traditional education, you might wait days for assignment feedback. In a game, you see results in seconds. This tight loop between action and consequence is how humans naturally learn complex systems\r\n\r\n\u2022\tFailure becomes safe and instructive. Games normalize failure as part of the process. When a player's prompt gets injected or their RAG pipeline returns nonsense, they're motivated to understand why - not ashamed of not knowing. More importantly, experiencing these failures in a low-stakes environment builds pattern recognition: players who've watched an LLM leak \"confidential\" potion recipes in the game are far more vigilant about data exposure in production. They've seen how easily guardrails can be bypassed, how confidently LLMs hallucinate, how subtly prompt injections can hide in user input. Better to learn these lessons with fictional potion formulas than with actual customer data or proprietary information\r\n\r\n\u2022\tConcrete experience precedes abstract theory. Rather than explaining what temperature=1.0 means theoretically, players first experience its effects. The theory makes sense because they've already seen it in action\r\n\r\n\u2022\tConstraints create creativity. Budget limits, time pressure, and quality requirements force players to think strategically about tradeoffs - exactly the skill needed in production AI systems.\r\n\r\nThese aren't speculative claims. Research in educational psychology consistently shows that game-based learning improves retention, transfer, and engagement compared to passive instruction. We've applied these principles specifically to LLM education.\r\n\r\n\r\nAI Factory: Our Approach\r\n\r\n\"AI Factory\" is an educational platform we built to teach LLM concepts, not a commercial product, but a teaching tool we developed for our own training needs and client workshops. Built with Python and Streamlit, it takes the form of an interactive game where players learn through hands-on challenges. Set in a magical potion factory (a metaphor for an AI-powered company), players take on the role of an apprentice who must master increasingly complex AI systems to succeed.\r\n\r\nWhat makes it different from typical AI tutorials:\r\n\r\n\u2022\tReal API calls, not simulations. The game connects to actual LLM APIs. When players adjust temperature from 0.3 to 0.9, they see real output changes. When they misconfigure guardrails, real prompt injections succeed. This authenticity ensures that skills transfer to production environments. The challenge - and the opportunity - of working with real LLMs is their inherent randomness. Each time a player tackles a level, the output can differ, offering new insights and teaching fresh strategies for effective model interaction.\r\n\r\n\u2022\tBudget-driven decision making. Every API call costs in-game currency. Players must balance quality, cost, and speed - exactly the tradeoffs faced in real AI deployments. This transforms abstract concepts like \"token efficiency\" into visceral resource management.\r\n\r\n\u2022\tProgressive complexity with no prerequisites. The game assumes zero AI background but the platform isn't just for beginners. While newcomers start with prompting basics, experienced practitioners consistently discover blind spots\u2014guardrail tradeoffs they hadn't considered, RAG subtleties they'd glossed over, agent coordination patterns that surprised them. Each level includes optional technical deep-dives for those wanting more depth. Levels take 15-25 minutes each, allowing self-paced progression. An unexpected benefit: players with different backgrounds approach challenges differently, sparking rich discussions when played in teams\r\n\r\n\u2022\tFailure as the primary teacher. The game is designed so that players will fail - and learn from it. When a prompt injection succeeds, players don't just see \"wrong answer.\" They see exactly how their system was compromised and must figure out how to defend against it.\r\n\r\n\r\nDesign Decisions That Worked (And Didn't)\r\n\r\nBuilding this game taught us as much about AI education as playing it teaches about AI. In this talk, we'll share specific design decisions and their outcomes:\r\n\r\n\u2022\tThe prompt injection plot twist. In one of the levels players configure an AI classifier. Unknown to them, one of the \"batch reports\" contains a hidden prompt injection. When their classifier gets tricked, the game reveals what happened. This \"aha moment\" consistently produces better security awareness than any lecture about injection risks.\r\n\r\n\u2022\tBudget constraints transform engagement. Early versions had unlimited API calls. Players clicked randomly until something worked. Adding budget constraints forced strategic thinking: \"Is this prompt worth 5 coins? Should I iterate locally first?\" Scarcity created engagement.\r\n\r\n\u2022\tWrong answers need explanations. Simply showing \"incorrect\" frustrated players. Now, wrong answers include diagnostic information: \"Your prompt produced 'PASS' but the batch contained contamination. Here's why the AI missed it...\" Failure becomes education.\r\n\r\n\u2022\tWhat didn't work: Quiz-style challenges. Early prototypes included multiple-choice questions. Players gamed them by elimination rather than understanding. We removed all quizzes in favor of open-ended challenges with AI-evaluated responses.\r\n\r\n\u2022\tWhat didn't work: Too much text. Initial level introductions were walls of documentation. Players skipped them. We replaced text with brief narrative setup and let players learn by doing and AI generated short videos.\r\n\r\n\u2022\tWhat we struggled with: LLM non-determinism. Building an educational game on top of inherently unpredictable systems created unexpected challenges. The same prompt could produce different outputs, breaking our evaluation logic. We had to develop workarounds: structured output formats, temperature=0 for critical evaluations, multiple validation passes, LLM-as-a-judge, and designing challenges where variation was acceptable or even desirable. This struggle itself became a teaching moment - players learn that \"same input, same output\" isn't guaranteed with LLMs.\r\n\r\nWhat You'll Learn From This Talk\r\n\r\nThis presentation goes beyond \"look at our cool game.\" You'll leave with:\r\n\r\n\u2022\tA framework for gamifying technical education. The principles we applied - intrinsic motivation, tight feedback loops, progressive disclosure, authentic consequences - work for any complex technical domain, not just AI.\r\n\u2022\tUnderstanding of which AI concepts are hardest to teach. Based on player testing, we've identified where misconceptions cluster (RAG chunking, agent coordination, guardrail tradeoffs) and how to address them.\r\n\u2022\tPractical design patterns. Specific techniques: how to use budget constraints for engagement, how to make failure instructive, how to sequence concepts for progressive learning.\r\n\r\nThe technical stack is based on Python 3.11+, uses Streamlit for the frontend, Azure OpenAI (GPT-5, GPT-4o-mini) and the OpenAI Agents SDK for AI capabilities, PostgreSQL for persistence, and Docker for deployment.\r\n\r\nWho Should Attend\r\n\r\nThis talk is designed for multiple audiences:\r\n\u2022\tEducators and trainers looking for new approaches to teaching AI concepts\r\n\u2022\tTeam leads who need to upskill their teams on LLM fundamentals\r\n\u2022\tDevelopers who want deeper intuition for AI systems (the game design reveals how concepts connect)\r\n\u2022\tAnyone interested in gamification as an approach to technical education\r\nNo AI/ML background is required. Basic Python familiarity is helpful but not essential - the talk focuses on educational methodology, not implementation details.", "code": "MS7AWK", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/MS7AWK.png", "speaker_names": "Vadim Vlasov, Eric Glaser, Lisa Amrhein", "speakers": "\n### Vadim Vlasov\n\nI'm a Data Scientist based in Munich who believes AI should be understood, not feared. After earning my Master's at LMU Munich, I've spent the past four years turning complex ML challenges\u2014from computer vision to agentic systems\u2014into working solutions. But what really excites me is making AI click for others: whether through hands-on workshops or building interactive experiences that turn abstract concepts into \"aha!\" moments. When I'm not wrangling models, you'll find me exploring ways to gamify learning and bridge the gap between cutting-edge AI and everyday understanding\n\n### Eric Glaser\n\nI am a junior data scientist at Steadforce, building LLM and agent workflows with Python from cloud to edge. My current focus is AI literacy: helping teams understand what LLMs, RAG, and agents actually do beyond the hype. I co-designed \u201cAI Factory,\u201d a game where players break and fix AI systems to build real intuition.\n\n### Lisa Amrhein\n\n\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Education, Career & Life"}, {"title": "Accelerate FastAPI Development with OpenAPI Generator", "abstract": "Develop FastAPI applications faster with the contract-first approach using the OpenAPI Generator, no GenAI required. \r\n\r\nMachine learning models are often deployed as APIs, but the \"agreement\" between the consumer and the service is often fragile. How does the consuming app know if a parameter is optional or required? When the code diverges from the documentation, integration breaks.\r\nIn this tutorial you will learn to define an API contract using OpenAPI specification. We will use the OpenAPI Generator to automatically generate API endpoints and strictly typed Pydantic data models. Following this approach for all applications supports standardization, consistency, and maintainability across all projects.\r\n\r\nThe session will cover three key areas:\r\n**Design**: We will define an OpenAPI specification as our single source of truth for the API and end consumer.\r\n**Generate**: We will use the OpenAPI Generator to create a FastAPI skeleton and show possibilities for customization to fit specific project needs.\r\n**Implement**: We will connect our generated app to a ML model where we will create Mystic Creatures for Real Life Problems", "full_description": "Machine learning models are often deployed as APIs, where we have an endpoint that generates predictions given some input. For example, we can send a POST request specifying a color, a length, and a number of legs, and the endpoint predicts the best fitting animal. The description of the endpoint, the schema of the request, and the response acts as a form agreement between the consumer and the service. In practice, the restrictions on the API are not well defined. How does the consuming app know if a parameter is optional or required? \r\nIn this tutorial you will learn to define an API contract as an OpenAPI specification (OAS). OAS is a standardized description of the API endpoints and data models. We will demonstrate how to use the OpenAPI Generator to automatically generate the API endpoints and strictly typed Pydantic data models, by only designing the OAS in YAML format, without GenAI. OpenAPI Generator utilizes mustache templates to translate the specification into actual code. We will demonstrate use cases for customizing the template for specific needs of the resulting API stubs.  \r\nBy generating code from the contract, you ensure that the deployed application always reflects the agreed-upon specification. It automates the writing of repetitive code, such as Pydantic models and endpoint definitions, allowing developers to focus on the implementation logic. It enforces standard patterns and structures, ensuring consistency and maintainability across different projects.\r\n\r\nExpect fun mystic creatures after deploying the resulting API in your local environment.\r\n\r\n#### Target Audience\r\nEngineers and data scientists looking to standardize their FastAPI development workflow. We expect you to have basic knowledge in Python, virtualenv, Pydantic data models and FastAPI.\r\n#### Technical Setup\r\n* _Operating system:_ We recommend using Unix OS (Mac or Linux)\r\n* _Python:_ Version 3.10+\r\n* _OpenAPI Generator:_ Version 7.15.0\r\n   * Installation Guide: https://openapi-generator.tech/docs/installation/\r\n\r\n#### Outline\r\n1. Introduction (10 min)\r\n* The philosophy of Contract-First development\r\n* Overview of the OpenAPI specification and Pydantic data models\r\n* Introduction to the OpenAPI generator tool\r\n2. Design (20 min)\r\n* Introduction to the unicorn service logic (Input: Real Life Problems,  Output: Mystic Creatures)\r\n* Definition of the openapi specification, focusing on the Request and Response schemas\r\n3. Generate (30 min)\r\n* Running the standard vanilla OpenAPI generator\r\n* Introduction to mustache templates\r\n* Customization of the default mustache to inject our specific dependencies\r\n4. Implementing (15 min)\r\n* We will connect the generated API stubs to a predict() function that calls our unicorn generation service.\r\n5. Demo & QA (15m)\r\n* Running the server via u**v**icorn and testing our u**n**icorn service endpoint using the Swagger UI.", "code": "N8QVT8", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/N8QVT8.png", "speaker_names": "Dr. Evelyne Groen, Kateryna Budzyak", "speakers": "\n### Dr. Evelyne Groen\n\nI am a senior MLOps engineer at Malt. A long time ago I studied physics in Amsterdam, after which I moved to Berlin to discover the world of data science. Currently I'm working at Malt exploring the boundaries between devops and data.\n\n### Kateryna Budzyak\n\nKat is a Senior Machine Learning Engineer at Malt, the freelancer marketplace, where she works in the relevancy and matching team. She has a background in bioinformatics and passionate about beautiful code.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Practical Refactoring with Syntax Trees", "abstract": "The Python Abstract Syntax Tree powers tools like pytest, linters, and automatic refactoring. \r\nIn this talk, we'll approach syntax trees from first principles and see how Python code can be treated as structured data.\r\n\r\nWe'll then explore how syntax trees can be used to automate refactoring across large codebases. \r\nUsing a real-world example and the libCST library, we'll build a small refactoring tool and share practical advice for writing and applying automated refactorings.\r\n\r\nYou'll leave with a clear mental model of syntax trees and a solid starting point for writing your own refactoring tools.", "full_description": "Modern Python tooling relies heavily on syntax trees. In this talk, we take a practical look at Python's Abstract Syntax Tree (AST) and how Python code can be treated as structured data rather than plain text.\r\n\r\nWe'll start from first principles: how Python source code is parsed, what an AST represents, and how to reason about code as a tree. This builds a clear mental model that makes syntax-tree-based tooling easier to understand and work with.\r\n\r\nFrom there, we'll explore how syntax trees enable automated refactoring across large codebases using scripts to rewrite code (sometimes called codemods). \r\nUsing a realistic refactoring scenario, we'll implement a small refactoring tool using libCST.\r\n\r\nThe talk also shares practical tips from writing codemods. This includes how to use test-driven development when writing refactoring tools, where AI can help in refactoring tasks, and strategies for dealing with formatting.\r\n\r\nAttendees will leave with a solid understanding of how syntax trees work in Python and a concrete starting point for writing their own automated refactoring tools.\r\n\r\n\r\nOutline:\r\n\r\nMinutes 0-5: Primer on Python syntax trees and the AST mental model\r\nMinutes 5-12: From syntax trees to codemods and automated refactoring\r\nMinutes 12-22: Implementing a refactoring codemod with libCST\r\nMinutes 22-27: Test-driven codemods, formatting strategies, and AI assistance\r\nMinutes 27-30: Conclusion", "code": "N98BQT", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/N98BQT.png", "speaker_names": "Laurent Direr", "speakers": "\n### Laurent Direr\n\nI'm a freelance web developer helping small teams ship reliable software. I've been working with Python for 10+ years and enjoy automating work for other developers.\r\n\r\nThese days I'm very interested in local-first software technologies.\r\nI attended the Recurse Center (a programming retreat) in 2018. \r\n\r\n[GitHub profile](https://github.com/ldirer)\r\n[Blog](https://ldirer.com/blog)\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Restaurants around train stations are bad and I can prove it", "abstract": "Have you ever asked yourself: Why is there no good food option close to this main station? This talk tries to find out if this is a systematic problem - using publicly available data and Google APIs.\r\n\r\nAfter this talk, you will know about the best- and worst-rated restaurants close to main stations in Germany, if kebabs or pizza places are systematically a better choice, and which station is the worst to eat in all of Germany.", "full_description": "Does the quality of restaurants degrade with your proximity to a train station? And which German town is worst for the hungry traveller? In this culinary data exploration, we used publicly accessible data to assess whether busy train stations correlate with lower restaurant ratings - and which towns are actually the worst. Using the Google Maps API and the hottest framework for data manipulation, polars, we give an overview over publicly available data resources and show how far you can get with them.\r\n\r\nOf course, this talk will also deliver all the cold hard food facts: Analyzing the data of over 10,000 restaurants in Germany and worldwide, we will present the best and worst dining options available at train stations. We compare urban and rural environments, examine the impact of chain stores, and provide practical advice for you, the hungry traveler.", "code": "NAHX3L", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/NAHX3L.png", "speaker_names": "Dennis Schulz", "speakers": "\n### Dennis Schulz\n\nDennis Schulz is a Senior Consultant at TNG Technology Consulting. He holds a PhD in low temperature physics from the University of Heidelberg. Besides being a programmer, he organized and hosted the TV show Quasi Klar for RNF, published a book that was translated to Korean and Russian, and won Science Slam competitions all over Germany. As a part of the Innovation Hacking team at TNG, he worked on different AI showcases, fine-tuning embeddings, and data mining.\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Free T(h)r(e)ading: A Trading Systems Journey Beyond the GIL", "abstract": "Python 3.13's free-threaded mode opens new territory for Python concurrency. We embarked on an experiment: could a trading algorithm benefit from true parallelism, and what would it take to get there? This talk documents our research journey from async/await to free threading\u2014the hypotheses we tested, the benchmarks we designed, the unexpected behaviours we discovered, and the systematic approach we took to validating whether GIL-free Python could handle real-time market data. You'll see our experimental methodology, the data we collected, surprising findings about thread scheduling and memory patterns, and what our results suggest about Python's concurrent future.", "full_description": "The release of Python 3.13 with experimental free-threaded mode (PEP 703) represents a fundamental shift in Python's concurrency model. For decades, the Global Interpreter Lock has dictated how we write concurrent Python code, pushing developers toward async/await patterns for I/O-bound workloads and multiprocessing for CPU-bound tasks. But what happens when we remove that constraint?\r\n\r\nWe designed a research experiment to answer this question empirically: take a production trading algorithm built on asyncio, migrate it to free threading, and measure everything. Trading systems make ideal subjects for this research\u2014they're latency-sensitive, handle multiple concurrent data streams, perform both I/O and CPU-bound operations, and have clear, quantifiable performance metrics.\r\n\r\nThis talk presents our complete research journey, from initial hypothesis to validated conclusions, sharing both our methodology and findings.\r\n\r\nDetailed Outline:\r\n1. Research Question & Motivation (3 minutes)\r\n\r\nThe research question: can a trading algorithm benefit from true parallelism?\r\nWhy trading systems make ideal experimental subjects\r\nInitial hypotheses about performance characteristics\r\nBaseline system: async architecture and performance profile\r\n\r\n2. Experimental Design (4 minutes)\r\n\r\nMigration approach\r\nBenchmarking framework\r\nWorkload simulation\r\nControl variables and isolation of I/O vs. CPU-bound operations\r\n\r\n3. Migration Journey (5 minutes)\r\n\r\nArchitectural transformation\r\nKey refactoring patterns and synchronization strategies\r\nThread safety challenges\r\nLibrary ecosystem compatibility findings\r\n\r\n4. Results & Discoveries (8 minutes)\r\n\r\nPerformance data: latency, throughput, and resource utilization\r\nWorkload analysis: where free threading won, where async remained competitive\r\nVisual data presentation: charts and comparative analysis\r\n\r\n5. Practical Implications (4 minutes)\r\n\r\nDecision framework: when to choose free threading over async\r\nMigration best practices and lessons learned\r\nProduction readiness assessment\r\nWhat this means for Python's concurrent future\r\n\r\n6. Q&A (5 minutes)\r\n\r\nPrerequisites - This talk assumes attendees have:\r\n\r\n- Strong understanding of Python's concurrency models (asyncio, threading, multiprocessing)\r\n- Familiarity with the GIL and its implications\r\n- Basic understanding of systems programming concepts (thread safety, synchronization)", "code": "NDZSSB", "state": "confirmed", "created": "2026-01-09", "social_card_image": "/static/media/social/talks/NDZSSB.png", "speaker_names": "Tim Kreitner", "speakers": "\n### Tim Kreitner\n\nTim Kreitner is a Senior Software Engineer at Vattenfall Energy Trading GmbH in Hamburg, Germany. With a background in Mechanical and Computational Engineering, Tim transitioned into the field of finance, leveraging various programming languages.\r\n\r\nAt Vattenfall, Tim develops Algorithmic Trading Infrastructure applications. Including Order Routers, Market Data Servers, Exchange connections.\n", "domain_expertise": "None", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Programming Quantum Networks in Python", "abstract": "Quantum networks connect quantum devices including quantum computers, enabling applications not realizable in classical networks, such as secure quantum computing in the cloud and quantum key distribution. These networks are now moving from theory to reality, and as part of the Quantum Internet Alliance, we are actively building a prototype quantum network in Europe, driven by applications developed in Python.  \r\n\r\nIn this talk, we will introduce quantum networking and demonstrate how to program quantum network applications in Python by walking through the quantum teleportation protocol. We'll conclude by sharing resources so that you can begin experimenting with quantum network programming yourself. No prior quantum experience required.", "full_description": "Quantum networks connect quantum devices including quantum computers, enabling applications not possible in classical networks, such as secure quantum computing in the cloud and quantum key distribution. These networks are now moving from theory to reality, and as part of the Quantum Internet Alliance, we are actively building a prototype quantum network in Europe, driven by applications developed in Python.  \r\n\r\nEven though quantum systems are governed by the rules of quantum mechanics, you don't need to be an expert in quantum physics to start programming them!  \r\n\r\nDeveloping applications for quantum networks reveals new challenges. For example, unlike in classical networks where data is copied and retransmitted, quantum information cannot be copied. Once lost, it is irretrievable. This motivates a new networking primitive for transferring data, the quantum teleportation protocol.  \r\n\r\nIn this talk, we will walk through the quantum teleportation protocol step-by-step using the NetQASM SDK and the SquidASM simulator, Python tools developed by our research group for quantum network programming and simulation. We'll conclude by sharing resources so that you can begin experimenting with quantum network programming yourself. No prior quantum experience required.", "code": "NF7MKB", "state": "confirmed", "created": "2025-12-09", "social_card_image": "/static/media/social/talks/NF7MKB.png", "speaker_names": "Samuel Oslovich", "speakers": "\n### Samuel Oslovich\n\n\n", "domain_expertise": "None", "python_skill": "Novice"}, {"title": "Post-Processing and Visualization of Astrophysical Data with PyPLUTO", "abstract": "Modern scientific workflows increasingly rely on interactive analysis, reproducibility, and high-quality visualisation. **PyPLUTO** is a Python package designed to explore, analyse, and visualise numerical simulations produced by the **PLUTO** code for computational astrophysics. This talk shows how *PyPLUTO* leverages the Python ecosystem to transform raw simulation outputs into clear, flexible analysis and visualization workflows.\r\n\r\nThe session demonstrates how domain-specific simulation data can be integrated with tools such as `NumPy` and `Matplotlib` to support efficient post-processing, rapid exploration, and production of publication-quality figures. Attendees will see how structured Python workflows can replace fragmented, ad-hoc scripts, how visualisation accelerates scientific insight, and how Python lowers the barrier between simulation output and interpretation.\r\n\r\nAlthough examples are drawn from computational astrophysics, the approach is broadly applicable to any field working with structured simulation data. The talk highlights how lightweight, Python-based post-processing tools can improve clarity, reproducibility, and productivity without imposing heavy frameworks or tightly coupled visualisation pipelines.", "full_description": "Numerical simulations often generate vast amounts of structured data; yet, extracting insights from these outputs remains a major challenge. Analysis is frequently performed through fragmented, ad-hoc scripts that are difficult to maintain, reuse, or reproduce. **PyPLUTO** is a Python package designed to address this gap by providing a clear and flexible interface for post-processing, analyzing, and visualizing simulation data produced by the **PLUTO** code for computational astrophysics.\r\n\r\nThis talk presents **PyPLUTO** as a case study in building lightweight, domain-specific scientific tools on top of the Python scientific ecosystem. The emphasis is on offline analysis and visualisation workflows that operate on completed simulation outputs, enabling efficient exploration, comparison, and communication of results. Rather than coupling visualisation to simulation runtime, **PyPLUTO** focuses on clarity, composability, and integration with established PyData libraries.\r\n\r\nThrough concrete examples, the session demonstrates how structured simulation data can be processed and visualised using tools such as NumPy and Matplotlib. Attendees will learn how Python-based workflows can replace scattered analysis scripts, how visualization supports rapid scientific insight, and how a clean separation between simulation and analysis enhances reproducibility and productivity.\r\n\r\n## Outline\r\n\r\n#### 1. **From Simulation Output to Insight**\r\n- Common challenges in post-processing large numerical simulations\r\n- The gap between raw data and scientific interpretation\r\n- Why offline analysis and visualisation remain essential\r\n\r\n#### 2. **PyPLUTO: Scope and Design**\r\n- What PyPLUTO does and the problems it targets\r\n- Design goals: simplicity, flexibility, and interoperability\r\n- Clear separation between simulation execution and analysis\r\n\r\n#### 3. **Working with Simulation Data**\r\n- Loading and organising structured simulation outputs\r\n- Handling scalar and vector fields across space and time\r\n- Typical post-processing tasks and analysis patterns\r\n\r\n#### 4. **Visualisation Workflows**\r\n- Exploratory plots and diagnostic views\r\n- Time evolution and comparison between simulations\r\n- Producing publication-quality figures with Matplotlib\r\n\r\n#### 5. **Interactive GUI for Post-Processing**\r\n- Lightweight graphical interfaces for exploring simulation data\r\n- Interactive selection of fields, slices, and time steps\r\n- GUI as a complement to scripting, not a replacement\r\n\r\n#### 6. **Integration with the Python Ecosystem**\r\n- Efficient data handling with NumPy\r\n- Interoperability with existing scientific Python tools\r\n- Benefits of building on established libraries\r\n\r\n#### 7. **Software Design Lessons**\r\n- Building user-friendly scientific APIs\r\n- Balancing usability, transparency, and performance\r\n\r\n#### 8. **Broader Applicability and Outlook**\r\n- Relevance to other simulation-heavy fields\r\n- Reusable patterns for Python-based post-processing\r\n- Future directions and potential extensions\r\n\r\nThe talk is aimed at scientists, data practitioners, and Python developers interested in scientific visualisation and simulation data analysis. No background in astrophysics or PLUTO is required; the focus is on workflows, tools, and design principles applicable across the PyData community.", "code": "NN7CVP", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/NN7CVP.png", "speaker_names": "Giancarlo Mattia", "speakers": "\n### Giancarlo Mattia\n\nI am a postdoctoral researcher at the Max Planck Institute for Astronomy in Heidelberg. I am currently investigating the impact of non-ideal processes within protostellar and protoplanetary disks on their formation, evolution, and production of winds and collimated outflows.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Visualisation & Notebooks"}, {"title": "Tracking Knowledge Diversity in LLM-Generated Responses.", "abstract": "As large language models (LLMs)-powered \u201cAI highlights\u201d become the first information people see on the Web, a key question arises: how much variety and perspective do these systems actually deliver for information-seeking queries? Do LLMs offer broader viewpoints than traditional search or Wikipedia pages? Do larger models really produce more diverse answers\u2014or are they all converging on the same language, and framing, raising concerns about \u201cknowledge collapse\u201d?\r\n\r\nDrawing insights from experiments across LLM families, real-world topics, and hundreds of user-style prompts, this talk introduces an open-source framework for benchmarking and tracking epistemic diversity in LLMs. We focus on practical lessons for data scientists building and evaluating LLM-powered search, summaries, and knowledge systems\u2014where diversity of information actually matters.", "full_description": "This talk summarizes our research on how LLMs generate narratives and recurring tropes in real-world information-seeking setups via prompting.\r\n\r\n**Talk outline:**\r\n* Knowledge collapse and epistemic diversity: What they mean and why they matter for real-world information access (5 mins).\r\n* Framework overview: How we measure epistemic diversity across LLM outputs (5 mins).\r\n* Experimental design, results: Curating dataset for comparisons across model families, search results, and Wikipedia pages (7 mins).\r\n* Implications for designing LLM-powered systems that preserve information diversity (10 mins)\r\n\r\n**Key takeaways for AI practitioners:**\r\n* When can retrieval-augmented generation (RAG) increase diversity?\r\n* Can expanding Wikipedia via translation improve epistemic diversity or reinforce existing tropes?\r\n* What are some open challenges in measuring cultural and contextual diversity in LLM outputs?\r\n* Where are we headed in terms of model sizes, fluency, and breadth of knowledge?\r\n\r\n**Useful links:**\r\n* [Our open source framework](https://github.com/dwright37/llm-knowledge)\r\n* [Reproducible Data Hugging Face](https://huggingface.co/datasets/dwright37/llm-knowledge-collapse)\r\n* [Our Research paper](https://arxiv.org/abs/2510.04226)", "code": "P7NYXB", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/P7NYXB.png", "speaker_names": "Sarah Masud", "speakers": "\n### Sarah Masud\n\nSarah Masud is currently a postdoc at the University of Copenhagen, exploring stereotypes and narratives. During her PhD from Indraprastha Institute of Information Technology, New Delhi, she explored the role of different context cues in improving computational hate speech-related tasks\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "The Day the Agent Started Lying (Politely)", "abstract": "You deploy an agent to automatically route incoming customer support tickets. At first, it is a clear win: response times improve, customers are happier, and support teams finally get some rest.\r\n\r\nThen time passes.\r\n\r\nNothing crashes. Dashboards stay green. No alerts fire. Yet the agent\u2019s decisions slowly degrade first slightly, then inconsistently, and eventually becoming confidently wrong.\r\n\r\nThis is data drift.\r\n\r\nLLM-based agents in production operate in constantly changing environments. Products launch, outages happen, terminology evolves, and priorities shift. Unlike traditional ML models, LLMs can produce plausible, well-phrased outputs even when they are incorrect, making these failures difficult to detect.\r\n\r\nIn this talk, we focus on practical techniques for continuously evaluating and monitoring LLM-based agents after deployment. Using a support-ticket routing agent as an example, we examine drift signals such as increasing classification uncertainty, spikes in fallback categories, shifts in embedding distributions, and growing disagreement with historical or human decisions.\r\n\r\nThe emphasis is not on training or prompt tuning, but on operating agents safely over time: detecting silent failures early and knowing when intervention, retraining, or retirement is required before users notice.", "full_description": "In this talk, we will walk through a concrete production-style example of an LLM-based agent that automatically classifies and routes incoming customer support tickets. The agent takes raw ticket text as input, predicts a priority label, and routes the ticket to the appropriate support queue. A human override is possible but expected to be rare.\r\n\r\nAt deployment time, the system performs well. Classification confidence is high, fallback usage is low, and manual corrections are infrequent. Over time, however, the environment changes: new products are launched, outages introduce new failure modes, terminology evolves, and internal definitions of ticket priorities shift. Nothing crashes, latency remains stable, and traditional service-level metrics stay green; yet the agent\u2019s decisions slowly degrade.\r\n\r\nThis talk focuses on how to observe, measure, and act on that degradation.\r\n\r\nUsing recorded ticket data and a demo, I will show how to instrument an LLM-based agent with continuous evaluation signals, including:\r\n\r\n- Tracking class-probability entropy over time to detect increasing uncertainty\r\n- Monitoring the rate of \u201cunknown\u201d or fallback predictions as an early warning signal\r\n- Measuring embedding distribution drift between historical and recent tickets\r\n- Quantifying disagreement between current agent decisions and historical routing outcomes or human corrections\r\n\r\nI will demonstrate how these signals can be computed in rolling time windows, visualised on simple dashboards, and connected to alert thresholds. Rather than relying on a single accuracy number, the talk shows how multiple weak signals together reveal silent failure modes that would otherwise go unnoticed.\r\n\r\nThe focus is deliberately not on training new models or tuning prompts. Instead, we concentrate on operating LLM-based agents safely after deployment. You will see how to build a continuous evaluation pipeline, how to distinguish normal variation from meaningful drift, and how to decide when intervention is required whether that means retraining, prompt changes, label redefinition, or temporary rollback to human routing.\r\n\r\nBy the end of the talk, attendees will have a clear, practical blueprint for monitoring LLM-based agents in production and for detecting quiet, confident failure modes before they affect users or business operations.", "code": "P8Y9TD", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/P8Y9TD.png", "speaker_names": "Asya Melnik", "speakers": "\n### Asya Melnik\n\nI started as a data scientist, building ML microservices and deploying models into production. I later moved into a consulting role, where I helped adapt ML models to real customer needs, translate business problems into measurable objectives, interpret results, and monitor model performance over time.\r\n\r\nOver the years, my work gradually shifted towards GenAI. I now design and build AI agents from scratch for internal process optimisation, support colleagues in adopting GenAI and agentic AI responsibly, and promote security-aware practices in solution development. A large part of my work focuses on evaluating and monitoring agent behaviour in real environments to ensure these systems remain useful, safe, and trustworthy after deployment.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "MLOps & DevOps"}, {"title": "From Struggling to Mastery: A Practical Guide to Data Pipeline Operations", "abstract": "How mature are your data pipeline operations? A Roadmap to Operational Excellence.\r\n\r\nData teams often struggle to scale their pipeline operations, trapped in a cycle of manual fixes and reactive fire-fighting. But what does \"good\" actually look like? In this talk, we introduce a standardized 5-level maturity model for Data Operations, focusing on three critical pillars: Orchestration, Data Quality, and Data SLOs.\r\n\r\nWe will deconstruct the journey from \"Struggling\" (manual scripts, no guarantees) to \"Mastery\" (automated, resilient, and measured). Attendees will leave with a concrete framework to assess their team\u2019s current standing and a clear, step-by-step roadmap to raise the bar toward operational excellence.", "full_description": "The Problem: The \"it works on my machine\" trap. As data teams grow, ad-hoc processes that worked for a single engineer crumble under the weight of production requirements. Teams often know they need to improve, but they lack a unified definition of success. Without clear standards, it is impossible to measure progress.\r\n\r\nThis talk presents a comprehensive Operational Excellence Maturity Pyramid, designed to guide data teams from chaos to stability. We will explore a 5-level classification system (Struggling, Basic, Decent, Strong, and Mastery) applied across three foundational pillars of data engineering.\r\n\r\n1. Orchestration Maturity We will move beyond simple cron jobs and local scripts.\r\n\r\n- Struggling: Manual scheduling, no dependency management, lack of idempotency.\r\n\r\n- Mastery: Dynamic DAGs, event-driven triggers, automated backfills, modular infrastructure-as-code, and self-healing pipelines and more.\r\n\r\n\r\n2. Data Quality Maturity Data trust is hard to gain and easy to lose. We will define how to shift from reactive to proactive quality management.\r\n\r\n- Struggling: No testing program; quality issues are discovered by stakeholders downstream.\r\n\r\n- Mastery: Comprehensive coverage (Write-Audit-Publish patterns), automated anomaly detection, and \"circuit breakers\" that stop bad data before it hits the warehouse.\r\n\r\n3. Data SLOs (Service Level Objectives) Maturity You cannot improve what you do not measure.\r\n\r\nStruggling: Undefined targets; \"best effort\" delivery.\r\n\r\nMastery: Fully measurable SLIs (Service Level Indicators), defined Error Budgets, and automated alerting on burn rates.\r\n\r\n-- What You Will Learn: This session is not just theoretical; it is a practical guide for data engineers, platform leads, and managers. By the end of this talk, you will be able to:\r\n\r\n- Audit your current stack: Use the provided scorecard to classify your team's maturity level in each pillar.\r\n\r\n- Identify gaps: Understand exactly why you are stuck at the \"Basic\" or \"Decent\" levels.\r\n\r\n- Plan your roadmap: Walk away with actionable steps to advance to the next level, turning your data operations into a competitive advantage rather than a maintenance burden.", "code": "PARU7X", "state": "confirmed", "created": "2025-12-02", "social_card_image": "/static/media/social/talks/PARU7X.png", "speaker_names": "Akif Cakir", "speakers": "\n### Akif Cakir\n\nI am a Data and AI enthusiast with over 14 years of experience across the full data lifecycle \u2014 from ingestion and transformation to analytics and machine learning operations.\r\n\r\nMy expertise spans modern data architecture, ETL/ELT pipelines, Big Data technologies, and cloud-native solutions. I have deep hands-on experience designing and implementing end-to-end data and ML pipelines that are reliable, scalable, and cost-efficient, driving value through automation and operational excellence.\r\n\r\nI\u2019m passionate about leveraging data and AI to create impactful, efficient, and intelligent systems that empower both business and technology teams.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Data Handling & Data Engineering"}, {"title": "Fight your garbage data: implementation of a pythonic data quality monitoring framework in PySpark", "abstract": "The timeless phrase \u201cgarbage in, garbage out\u201d is even more important today with the growing usage of non-deterministic generative neuronal networks, which amplifies the effect of bad data quality. This presentation describes Data Quality Monitor \u2014 a tool to bring transparency into data quality and help drive real improvements. \r\n\r\nIn the talk, we'll cover what defines a successful data quality monitoring solution and share findings from our initial evaluation of available open-source frameworks. Next, we'll showcase our implementation based on DQX. DQX is a lightweight, open-source framework for performing row-level data quality checks programmatically, with business rules organized in manageable YAML files. DQX, originally developed by Databricks Labs, integrates seamlessly with PySpark, making it easy and affordable to run data quality checks within our IoT data lake. Finally, we will discuss the organizational processes and structures required to effectively respond to data quality issues.", "full_description": "In the talk we share our expirience from the project implemented in Q3 2025. We start with the motivation for the project, involved stakeholders and their needs. We will then define the criteria for a successful data quality monitoring solution and share findings from our evaluation of existing frameworks. We will also discuss why popular frameworks like Great Expectations or SODA did not meet our requirements. \r\n\r\nNext, we will demonstrate our implementation based on DQX\u2014a lightweight, open-source Python library designed for traceable, row-level data quality checks before and after data is persisted. DQX, developed and maintained by Databricks labs, allows developers to concentrate on the core implementation while providing business users YAML files for maintenance of business rules. Furthermore, DQX\u2019s seamless integration with PySpark enables efficient and cost-effective quality monitoring within our IoT data lake. \r\n\r\nFinally, we move beyond the code to the organisational reality. We will discuss how we embedded Data Quality Monitor into the organisation and share our opinion on the hard questions: who is responsible for maintaining rules? who monitors the results? \r\n\r\n**Talk outline** \r\n\r\n* Motivation for the project \r\n\r\n  * Initial situation and objectives   \r\n\r\n* Framework evaluation \r\n\r\n  * Evaluation criteria for a successful data quality monitoring \r\n\r\n  * Comparison of available frameworks \r\n\r\n* Our implementation with DQX \r\n\r\n  * How to use built-in data quality checks \r\n\r\n  * How to add custom data quality checks \r\n\r\n  * Automated rule generation with DQX Profiler \r\n\r\n  * Output and visualisation options \r\n\r\n  * Python project structure \r\n\r\n* Embedding in organisation \r\n\r\n  * Rule maintenance \r\n\r\n  * How to communicate data quality issues \r\n\r\n* Summary  \r\n\r\n \r\n\r\n**Key takeaways** \r\n\r\n* Understanding of most important criteria when choosing the framework for data quality monitoring from perspective of a data engineer and an architect \r\n\r\n* Understanding of DQX framework \r\n\r\n* Ideas how to integrate data quality monitoring into organisations", "code": "PFXR9G", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/PFXR9G.png", "speaker_names": "Joshua Finger, Rostislaw Krassow", "speakers": "\n### Joshua Finger\n\nI'm a data engineer working at inovex GmbH with a full-stack software engineering background and a master degree in computer science.\n\n### Rostislaw Krassow\n\nRostislaw, a data architect at RATIONAL AG, specializes in distributed databases, the Apache Hadoop ecosystem and Azure cloud. He leverages his expertise to maintain the enterprise Data & Analytics platform for IoT data, where his daily work involves reconciling diverse stakeholder perspectives to deliver sustainable solutions.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Come for the Code, Stay for the People.", "abstract": "\"Come for the language, stay for the community.\" If you've been around Python long enough, you've heard this before. I don't know when I first heard it, but I know exactly when I understood it.\r\n\r\nThis talk is a personal reflection on thirteen years within the Python community\u2014from my first tentative steps as a volunteer to organising conferences myself. It's a story about discovering that Python was always about more than code. It's about the people, the values, and the unexpected ways a community can shape a career and a life.\r\n\r\nThis isn't just my story. It's a story I've seen repeated in countless faces at registration desks, in hallway conversations, in first-time speakers finding their voice. I want to talk about what I've learned about kindness, mentorship, and the quiet power of feeling like you belong somewhere.\r\n\r\nI'll end with an open question: as the ways we connect continue to evolve, how do we preserve what matters while welcoming a new generation?\r\n\r\nIf you're new to this community and wondering what all the fuss is about, this talk is especially for you.", "full_description": "I have a confession to make: after thirteen years in the Python community and countless technical talks attended and organized, this is the first time I'm putting myself out there to talk about community itself. It feels vulnerable. It feels necessary.\r\n\r\n**How it started**\r\n\r\nMy journey began the way many do\u2014volunteering. Stuffing badge holders, directing people to rooms, answering the same question about Wi-Fi passwords a hundred times. It wasn't glamorous, but it was transformative. Volunteering was my first taste of what it means to contribute to something larger than myself, and it opened doors I didn't even know existed.\r\n\r\nBack then, I came for the code. I had no idea I'd stay for the people.\r\n\r\n**More than code**\r\n\r\nThrough thirteen years, the Python community taught me things I carry with me everywhere. Things that have nothing to do with syntax or libraries.\r\n\r\nThe value of patience and kindness when someone asks a \"basic\" question\u2014because we were all beginners once. The importance of explicit inclusion, because \"everyone is welcome\" means nothing without deliberate action. The power of mentorship, both giving and receiving. The understanding that community health requires active maintenance, not passive hope.\r\n\r\nThis is what \"stay for the people\" actually means.\r\n\r\n**Who this talk is for**\r\n\r\nHaving attended and organized Python conferences for years, I've noticed something consistent: there are always newcomers. People experiencing their first Python event, unsure of what to expect, wondering if they belong. This talk is for them.\r\n\r\nBut it's also for anyone thinking about community engagement\u2014whether in Developer Relations, open source maintainership, or simply as someone who cares about the spaces they inhabit.\r\n\r\n**Looking forward**\r\n\r\nI don't have all the answers. I want to end with questions rather than conclusions. How do we engage with a generation that communicates differently? How do we preserve depth in an age of fragmented attention? What can newcomers teach us about building community in ways we haven't imagined?\r\n\r\nMy hope is that this talk sparks conversations that continue long after I leave the stage. And honestly? I hope to revisit this topic in ten years and see how wrong\u2014or right\u2014we were.", "code": "PK8XNB", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/PK8XNB.png", "speaker_names": "Valerio Maggio", "speakers": "\n### Valerio Maggio\n\nValerio Maggio has been wandering around the Python community for thirteen years. He started as a volunteer, somehow ended up organising conferences like PyCon Italy, PyData, EuroPython, and EuroSciPy, and has given more talks than he can remember. He's a researcher and open-source contributor who cares about open science and good software practices. Also an unapologetic nerd\u2014the kind who plays D&D and still believes Magic: The Gathering was better when cards had proper frames and the stack was a new thing (_if you're a player too, you know what I mean_). He drinks unreasonable amounts of tea and coffee.\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Community & Diversity"}, {"title": "Production ML across 2015-2035: A Journey to the Past and the Future", "abstract": "This talk is an exciting journey that revisits the past decade of Production Machine Learning from 2015 until now, and provides a pragmatic outlook of the next decade towards 2035. We\u2019ll revisit some of the cornerstone python projects that served as the foundation of the \"messy innovation\" boom (feature stores, orchestration, model serving, monitoring), as well as how it transitioned towards the LLMOps era shifting the stack from training-centric to inference-centric. We will also provide a pragmatic set of predictions for the next decade of MLOps, including some of the trends in ML monitoring, agentic systems and beyond - this will provide actionable guidance to all practitioners to ensure we stay ahead of the curve on the expected skills and domains required to thrive in the near future to come.", "full_description": "# Outline\r\n\r\n1) Motivations;\r\n2) MLOps Foundations;\r\n3.1) The Past - 2015 - Genesis;\r\n3.2) The Past - 2018 - Messy Innovation;\r\n3.3) The Past - 2023 - LLMOps;\r\n4) The Future - 2025-2035 Outlook;\r\n5) Reflections.\r\n\r\n# Description\r\n\r\nThe lifecycle of a machine learning model only begins once it\u2019s in production. In this talk we take a practical journey through the last decade of production ML, tracing back the early beginnings of MLOps to the respective research and projects that helped drive the movement forward. We cover how the ecosystem went through explosive growth through COVID with a broad range of tools and vendors tacking similar problems in very different ways. We then talk about the most recent trends in LLMOps which has shifted the stack from training-centric to inference-centric as pre-trained models have become broadly available. Namely on how the locus of engineering moves to the application layer (ie inference time), introducing new artifacts such as prompts, vector databases, and tool metadata, and accelerating another wave of ecosystem heterogeneity. \r\n\r\nWith those lessons in place, we look forward to 2035 through a set of pragmatic milestones for consolidation and standardization: how monitoring and observability become more ubiquitous, how MLOps and LLMOps stacks align, how time-to-production compresses, and how operations gradually evolves toward more autonomous patterns (progressive rollouts, agent-assisted RCA, and early self-healing behaviors). \r\n\r\nFinally, we close with actionable guidance grounded in production reality: how to right-size platform complexity to organizational scale, where to invest early to reduce future operational debt, and how to increase the scale of ML delivery while actively reducing system complexity. Attendees should leave with a coherent mental model of the MLOps landscape, a sharper understanding of why production ML remains hard, and a concrete set of engineering priorities for building reliable ML systems through the next decade.", "code": "PMMEAG", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/PMMEAG.png", "speaker_names": "Alejandro Saucedo", "speakers": "\n### Alejandro Saucedo\n\nAlejandro is the Director of the Markets AI, Data & Platform at Zalando SE, where he is responsible for petabyte-scale AI & Data platforms that power the Pricing, Traffic and Trading technology across the group. He is also Scientific Advisor at the Institute for Ethical AI, where he has led contributions to EU policy, including the AI Act, the Data Act and the Digital Services Act, among others. Alejandro is currently appointed as AI Expert at the United Nations and the European Commission, and serves as Board Member at the ACM's Board of Directors.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "MLOps & DevOps"}, {"title": "Causal Inference through the lens of probabilistic programming", "abstract": "Causal inference asks the hardest question in data science: \"What would have happened if things were different?\" While traditional methods often rely on rigid rules, statistical tests or \"black box\" adjustments, Probabilistic Programming Languages (PPLs) like PyMC and NumPyro offer a transparent, flexible, and powerful lens to view these problems.\r\n\r\nIn this talk, we move beyond the standard \"correlation is not causation\" disclaimer. We will build a unified workflow that starts with robust A/B testing, moves to bias adjustment in observational data using multilevel models, and culminates with advanced Deep Causal Latent Variable Models (CEVAE).", "full_description": "Why should you use a Probabilistic Programming Language (PPL) for Causal Inference? Because causal problems are inherently about uncertainty and structure\u2014two things PPLs handle natively.\r\n\r\nIn this session, we will demonstrate how to translate causal diagrams (DAGs) directly into code, using PyMC and NumPyro to estimate causal effects with rigorous uncertainty quantification. We will cover three distinct levels of complexity, drawing on real-world examples and recent research:\r\n\r\n1. The \"Simple\" Case: Enhancing A/B Tests Even in randomized experiments, PPLs provide massive value. We will show how to:\r\n\r\n    - Use Prior Predictive Checks to prevent \"silly\" estimates (Twyman's Law) by incorporating domain knowledge into priors (e.g., preventing the model from predicting a 1000% lift). We also describe how to perform a *power* analysis in a Bayesian framework.\r\n\r\n    - Implement Bayesian CUPED to reduce variance and increase statistical power without collecting more data. We can combine these variance-reduction methods with smarter priors as described above.\r\n\r\n2. The Observational Challenge: Confounding & Structure When we can't randomize, we must adjust. We will explore (through concrete examples):\r\n\r\n    - Backdoor Adjustment: Show how PPLs implement the \"do-operator\" to estimate Average Treatment Effects (ATE) in the presence of observed confounders.\r\n\r\n    - Multilevel Causal Models: Demonstrate how to use multilevel models to account for time-invariant unobserved confounders. We discuss the pros and cons compared with similar methods, such as fixed effects. \r\n\r\n3. The Frontier: Deep Latent Variable Models: What if confounders are unobserved? We will introduce advanced methods combining Deep Learning with Probabilistic Programming:\r\n\r\n    - An introduction to the Causal Effect Variational Autoencoder (CEVAE).\r\n\r\nBy the end of this talk, you will understand how to view causal inference not as a collection of isolated statistical tricks, but as a coherent modeling process powered by probabilistic programming.\r\n\r\n### References\r\n\r\n* **A/B Testing & Priors:** [Prior Predictive Checks for Metric Lift](https://juanitorduz.github.io/prior_predictive_ab_testing/) & [Power Analysis](https://juanitorduz.github.io/power_sample_size_exclude_null/)\r\n* **Variance Reduction:** [Bayesian CUPED](https://juanitorduz.github.io/bayesian_cuped/)\r\n* **Observational Data:** [Introduction to Causal Inference with PyMC](https://juanitorduz.github.io/intro_causal_inference_ppl_pymc/)\r\n* **Hierarchical Models:** [Multilevel Causal Inference](https://juanitorduz.github.io/ci_multilevel/)\r\n* **CEVAE Paper:** Louizos, C., Shalit, U., Mooij, J., Sontag, D., Zemel, R., & Welling, M. (2017). [Causal Effect Inference with Deep Latent-Variable Models](https://arxiv.org/abs/1705.08821).\r\n* **Code Reference:** Adapting concepts from *CausalML* (Robert Osazuwa Ness), specifically [Chapter 11: Bayesian Causal Graphical Inference](https://github.com/altdeep/causalML/blob/master/book/chapter%2011/Chapter_11_Bayesian_Causal_Graphical_Inference.ipynb).", "code": "Q9DU8N", "state": "confirmed", "created": "2025-11-30", "social_card_image": "/static/media/social/talks/Q9DU8N.png", "speaker_names": "Dr. Juan Orduz", "speakers": "\n### Dr. Juan Orduz\n\nMathematician (Ph.D., Humboldt Universit\u00e4t zu Berlin) and data scientist. I am interested in interdisciplinary applications of mathematical methods, particularly time series analysis, Bayesian methods, and causal inference. Active open source developer (PyMC, PyMC-Marketing, and NumPyro, among others). For more info, please visit my personal website https://juanitorduz.github.io\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Schema-Driven Lambdaliths in Python with AWS Lambda Powertools and Pydantic", "abstract": "Modern web frameworks such as Hono have renewed interest in schema-driven development and the \u201cLambdalith\u201d architecture, where an application is delivered as a single AWS Lambda function. While this model provides a predictable developer experience, Python-based serverless systems often struggle to achieve the same consistency, validation, and maintainability in production.\r\n\r\nDeploying Python web frameworks to AWS Lambda frequently requires additional execution layers\u2014such as ASGI adapters or container-based runtimes\u2014which add complexity and blur data boundaries. For teams that prefer clear, minimal Lambda handlers, these abstractions can hinder both development and operations.\r\n\r\nThis session shares production-proven patterns for building schema-driven Lambdalith applications in Python using AWS Lambda Powertools and Pydantic, without relying on heavy framework abstractions. Through real-world examples, we show how these tools simplify handler logic, standardize request and response validation, and improve observability and error handling.\r\n\r\nAttendees will leave with practical techniques for building reliable and maintainable Python Lambdalith systems, and insights they can immediately apply to modernizing existing serverless codebases or delivering new production services with confidence.", "full_description": "The rise of modern web frameworks such as Hono has brought increased attention to schema-driven development and the \u201cLambdalith\u201d architecture, where an application is delivered through a single Lambda function. These approaches offer a highly streamlined developer experience, but many existing Python-based systems struggle to achieve the same level of consistency, validation, and maintainability.\r\n\r\nIn Python, closing this gap often means introducing additional execution layers outside the language itself. When frameworks designed around web servers and request lifecycles are deployed on AWS Lambda, they typically require ASGI adapters, web adapters, or container-based runtimes. While powerful, these layers can make it harder to focus on what many teams actually want: writing clear, minimal Python handlers with explicit data boundaries.\r\n\r\nThis talk explores how combining AWS Lambda Powertools and Pydantic can close that gap and enable a modern, predictable development workflow\u2014even in established Python ecosystems. Drawing from real-world product use cases, we will examine how these tools can simplify handler-level logic, standardize request and response validation, and improve observability and error handling.\r\n\r\nLambda Powertools provides far more than logging and metrics: it includes utilities for structured tracing, data parsing, idempotency, typed configuration, and other features that bring Python serverless development closer to the ergonomics of newer frameworks. When paired with Pydantic, developers can enforce clear data contracts, reduce boilerplate, and achieve stronger guarantees around application behavior.\r\n\r\nAttendees will learn practical patterns for improving quality and productivity in Lambda-based applications, including how to:\r\n\r\n- Validate event payloads and responses using Pydantic models\r\n- Implement consistent error handling strategies\r\n- Structure a Lambdalith-style architecture in Python\r\n- Leverage Powertools utilities to enhance reliability and developer experience\r\n\r\nThis session will be valuable for Python developers who want to apply schema-driven design principles, modernize existing serverless codebases, or build more maintainable Lambda applications with confidence.", "code": "Q9HMT3", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/Q9HMT3.png", "speaker_names": "moriha, tanio", "speakers": "\n### moriha\n\nIT engineer at DAIKIN INDUSTRIES, LTD. (Japan), working across the full stack\u2014infrastructure, frontend, and backend development. Primarily writes Python and TypeScript.\r\nCurrently building energy optimization tools that analyze HVAC system data to generate operational improvement proposals. \r\nWorks in a Scrum-based team environment and has experience contributing to open source projects. \r\nIn personal development, enjoys experimenting with Cloudflare Workers for serverless applications.\n\n### tanio\n\nIT engineer at DAIKIN INDUSTRIES, LTD. (Japan), working across the full stack\u2014infrastructure, frontend, and backend development. Primarily writes Python and TypeScript.\r\nCurrently building energy optimization tools that analyze HVAC system data to generate operational improvement proposals. \r\nWorks in a Scrum-based team environment and has experience contributing to open source projects.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Django & Web"}, {"title": "Ty after Alpha: The New Generation of Python Type Checking", "abstract": "Python\u2019s static typing ecosystem has long been shaped by mypy, but a new contender has entered the space: ty, a high-performance type checker from Astral that has recently exited alpha. With a focus on speed, modern ergonomics, and tight tooling integration, Ty represents a new direction for Python type checking.\r\n\r\nIn this talk, we\u2019ll explore what ty looks like in practice. We\u2019ll cover its core features, how it behaves on real-world codebases, and what changes when type checking becomes fast enough to run constantly. We\u2019ll also compare ty directly with mypy, highlighting strengths, limitations, and trade-offs teams should understand before adopting it.\r\n\r\nThis session will help Python developers evaluate whether ty is ready for production use today\u2014and what it suggests about the future of Python typing tools.", "full_description": "Static typing in Python has matured significantly over the past decade, with mypy becoming the de facto standard for many teams. At the same time, developers continue to struggle with slow feedback loops, noisy errors, and friction in CI and local workflows. ty, a new type checker from Astral.sh, aims to address these issues with a fundamentally different set of design priorities\u2014and it has now reached a post-alpha, production-ready stage.\r\n\r\nThis talk takes a practical, experience-based look at ty from the perspective of a Python developer using it on real code. We\u2019ll start by briefly reviewing the current state of Python type checking and the problems that motivated ty\u2019s design. From there, we\u2019ll dive into ty\u2019s feature set, performance characteristics, and developer experience, focusing on what actually changes when type checking becomes fast and ergonomic enough to feel \u201calways on.\u201d\r\n\r\nA central part of the talk will be a direct comparison with mypy: where ty already excels, where it behaves differently, and where mypy remains the better choice today. Rather than framing this as a replacement story, we\u2019ll explore the trade-offs between the two tools and what kinds of teams benefit most from each.\r\n\r\nBy the end of the session, attendees will have a clear mental model of how ty works, how mature it is today, and whether it\u2019s a good fit for their own projects. More broadly, we\u2019ll look at what ty signals about the future direction of Python\u2019s typing ecosystem.", "code": "QB7VLW", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/QB7VLW.png", "speaker_names": "Stefan Kraus", "speakers": "\n### Stefan Kraus\n\nSenior Software Engineer at UL Solutions, Software Intensive System. Using Python to make the world a safer place.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Python Language & Ecosystem"}, {"title": "Is digital sovereignty a new buzzword in AI development?", "abstract": "AI development usually focuses on feasibility and implementation, but a new buzzword is now being used: 'sovereignty'. While customers are excited about it, what does it mean for them and for AI developers? In this presentation, we analyse different aspects of sovereignty and explore how it can be used to build trustworthy AI solutions. \r\nWe will also discuss current examples from politics and development to identify the best practices for secure data processing.", "full_description": "AI development typically prioritises feasibility and implementation. While solutions should be efficient, high-performing and scalable, sovereignty and data security are often overlooked. These issues tend to be overlooked when solutions are being found, even though we don't use AI as an end in itself, but rather to benefit or support our customers. Customers operate within a regulatory framework and rely on responsible technology. \r\nRather than seeing regulation as a hindrance, we should view it as an opportunity to drive innovation and create sustainable, trustworthy solutions. However, this is only possible if we understand the full meaning of sovereignty. \r\nThis presentation will explore the various aspects of the term 'sovereignty' and its potential impact on AI projects. We will discuss current examples from politics and development to identify best practices for secure data processing.", "code": "QMBEZX", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/QMBEZX.png", "speaker_names": "Dr. Maria B\u00f6rner", "speakers": "\n### Dr. Maria B\u00f6rner\n\nDr. Maria B\u00f6rner holds a Ph.D. in physics from CERN and DESY and is an expert in the field of AI. She is the head of the AI Competence Center at Westernacher Solutions. In this position, she is responsible for developing AI tools for government, church, and justice organizations. She strengthens the company's internal AI comptences and promotes them externally. She is also the deputy chairwoman of the Legal Tech working group at Bitkom and the German ambassador of the Women in AI network. Maria has worked in the field of AI for over eight years, focusing on responsible and ethical AI.\n", "domain_expertise": "None", "python_skill": "None", "track": "Ethics & Privacy"}, {"title": "Do you know how well your model is doing? Evaluate your LLMs", "abstract": "Large Language Models (LLMs) are becoming central to modern applications, yet effectively evaluating their performance remains a significant challenge. How do you objectively compare different models, benchmark the impact of fine-tuning, or ensure your LLM responses adhere to safety guidelines (guard-railing)? This hands-on workshop addresses these critical questions.", "full_description": "We will begin with an essential revision of the Hugging Face Transformers library, covering basic LLM inference and fine-tuning. The core of the workshop will introduce and provide deep practice with Lighteval, an efficient and powerful LLM evaluation framework. Participants will learn how to leverage Lighteval to compare various LLMs available on the Hugging Face Hub using a range of pre-built tasks and metrics.\r\n\r\nFinally, we will delve into advanced evaluation techniques, focusing on creating custom tasks and metrics tailored to unique, real-world application requirements. Participants will learn how to prepare custom datasets on the Hugging Face Hub and integrate them into Lighteval for precise, domain-specific evaluation. By the end of this workshop, you will possess the practical skills to rigorously evaluate, benchmark, and fine-tune your LLMs with confidence.\r\n\r\nPrerequisites:\r\n\r\n    - Have experience coding in Python (with Python installed in the local machine)\r\n    - Basic understand of machine learning and LLMs\r\n    - Experience with Hugging Face Transformers preferred but not necessary\r\n    - A Hugging Face Hub account (sign up for free)\r\n    - A modern computer that can fine-turn small LLMs locally", "code": "QX8DDJ", "state": "confirmed", "created": "2025-12-05", "social_card_image": "/static/media/social/talks/QX8DDJ.png", "speaker_names": "Cheuk Ting Ho", "speakers": "\n### Cheuk Ting Ho\n\nAfter having a career as a Data Scientist and Developer Advocate, Cheuk dedicated her work to the open-source community. Currently, she is working as a developer advocate for JetBrains. She has co-founded Humble Data, a beginner Python workshop that has been happening around the world. Cheuk also started and hosted a Python podcast, PyPodCats, which highlights the achievements of underrepresented members in the community. She has served the EuroPython Society board for two years and is now a fellow and director of the Python Software Foundation.\n", "domain_expertise": "Advanced", "python_skill": "Advanced", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Simplifying RAG Document Pipelines with Multimodal Embeddings", "abstract": "In RAG-based systems, the main challenge is often not tuning the LLM itself, but making documents available in a form that can be retrieved reliably. In enterprise settings, the dominant input format is still PDF, ranging from text-heavy reports to slide decks, scanned documents, and visually dense presentations. \r\n\r\nTraditional document processing pipelines rely on OCR and layout analysis to extract text, followed by chunking and embedding. While this works well for text-heavy documents, much of the original structure is often lost\u2014especially for presentations, multi-column layouts, and visually driven content. Images, charts, and diagrams typically require separate processing, increasing pipeline complexity and fragility.\r\n\r\nRecent multi-modal embedding models enable a different approach: embedding entire PDF pages directly as images. This preserves layout, visual hierarchy, and embedded graphics in a single representation and significantly simplifies document ingestion. \r\n\r\nThis talk compares classical OCR-based document processing pipelines with multi-modal page embeddings, drawing on benchmarks conducted on real-world enterprise documents across different models. It highlights where this approach performs well, where its limitations lie, and how to design practical, cost-aware retrieval systems in Python.", "full_description": "This talk provides an overview of how document processing for RAG systems can be simplified using multimodal embeddings, grounded in benchmarks on real-world enterprise documents.\r\n\r\nWhat the talk covers\r\n\r\n1. **Motivation: Why RAG Is Still Hard**  \r\n   Why PDFs remain challenging in enterprise RAG systems, and where current document processing approaches break down\u2014especially for presentations and visually structured documents.\r\n\r\n2. **The Classical Approach: PDF \u2192 Text \u2192 Chunks**  \r\n   An overview of traditional OCR- and layout-based pipelines, including their strengths, typical failure modes, and why they tend to grow into complex and fragile systems over time.\r\n\r\n3. **A New Paradigm: Multimodal Page Embeddings**  \r\n   How embedding entire PDF pages as images changes the ingestion model, what information is preserved compared to text-only approaches, and what this means for retrieval quality and system simplicity.\r\n\r\n4. **Benchmark Setup**  \r\n   How the benchmark comparing classical pipelines and multimodal page embeddings was designed, using anonymized, real-world enterprise documents across multiple document types. Different models and vendors are referenced only as examples, not as the focus.\r\n\r\n5. **Results and Key Findings**  \r\n   Where multimodal page embeddings outperform text-based pipelines, where they do not, and how hybrid approaches can emerge as a practical solution.\r\n\r\n6. **Production Best Practices**  \r\n   Practical guidance for deploying these approaches in real systems, including index design, quality monitoring, cost control, and how to integrate multimodal retrieval cleanly into Python-based RAG architectures.\r\n\r\nAttendees will leave with a clear understanding of when multimodal embeddings are a strong replacement for classical PDF pipelines, and how to reason about the trade-offs involved.", "code": "R7TT3E", "state": "confirmed", "created": "2025-12-23", "social_card_image": "/static/media/social/talks/R7TT3E.png", "speaker_names": "Arne Grobr\u00fcgge", "speakers": "\n### Arne Grobr\u00fcgge\n\nWorked on multi-modal retrieval-augmented generation (RAG) and agentic LLM systems. Designed ingestion and retrieval pipelines across text, video, and structured data to integrate common knowledge platforms such as Microsoft SharePoint. Focused on scalable Azure-based infrastructure, multilingual and multimodal document processing, and continuous evaluation for reliability. Gathered experience in building browser-driven agents using modern orchestration frameworks and MCP integration.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Natural Language Processing & Audio (incl. Generative AI NLP)"}, {"title": "Mental Model-Driven Programming Education", "abstract": "First steps in programming can be notoriously difficult, and famously require students to learn much more than their first programming language - they need to adapt to an entirely different approach to problem-solving. To some students this comes naturally, while others struggle to develop a \"programmatic mindset\", sometimes to the point of giving up. Why does this happen, and what can we do about it?\r\n\r\nIn this talk I'll give my answer to these questions, centered around the idea of a **mental model of programming** - the ability to simulate, in your head, how code is executed. I'll explain why this is a critical component of learning to program, and offer pedagogical methods that nurture the creation of a mental model.", "full_description": "Why do some students *simply not \"get\"* programming?\r\n\r\nI've been struggling with this question for years, as I've accompanied thousands of students' first steps with programming. From teenagers to career-changers, from one-on-one sessions to creating learning materials, for a long time I felt I'm missing something. In the last few years an answer has been crystalizing: the key is to intentionally teach with the aim of developing a **mental model of programming**.\r\n\r\nIn the talk I'll explain:\r\n1. Exactly what I mean by \"mental model of programming\", and why it's so important\r\n2. Why, when you don't intentionally nurture such a mental model, some students won't acquire it naturally\r\n3. How to nurture the development of a mental model of programming\r\n\r\nI'll demonstrate pedagogic methods with examples that I use in curriculums, and which I've found to improve student performance.", "code": "RATA7T", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/RATA7T.png", "speaker_names": "Daniel Anderson", "speakers": "\n### Daniel Anderson\n\nHi! I'm Daniel, a machine learning research engineer from Israel.\r\n\r\nI love data and optimization, and had the opportunity to work on a variety of interesting problems, from analyzing brain signals to video-based 3D reconstruction.\r\n\r\nI'm also fascinated with learning, and love teaching and creating interactive learning experiences.\r\n\r\nI hold a B.Sc. in computer science from the Open University of Israel, and an M.Sc. in machine learning and data science from Reichman University, Israel.\r\n\r\nI love music, the outdoors, and playing with my cat.\r\n\r\nYou can [check out my blog](https://andersource.dev/) to see what I'm passionate about.\n", "domain_expertise": "None", "python_skill": "Novice", "track": "Education, Career & Life"}, {"title": "PyTorch and CPU-GPU Synchronizations", "abstract": "CPU\u2013GPU synchronizations are a subtle performance killer in PyTorch: they block the host, prevent the CPU from running ahead, and create GPU idle gaps. This talk explains what host-device synchronization is, how it\u2019s triggered by subtle code patterns (dynamic-shapes), and how to diagnose it with NVIDIA Nsight Systems by correlating utilization gaps with long CUDA API calls. We\u2019ll end with practical mitigation patterns, including unit testing for syncs via `torch.cuda.set_sync_debug_mode()` and when a small Triton kernel can help avoid syncs and fuse ops.", "full_description": "PyTorch gets its speed from asynchronous execution: the CPU launches operations quickly while the GPU executes them later. CPU\u2013GPU (host-device) synchronizations break this pipeline by blocking the host until the GPU reaches a specific point. The result is often counterintuitive: even if kernels are fast, the GPU develops idle gaps, throughput drops, and latency rises because the CPU can no longer run ahead and keep the GPU fed with work.\r\n\r\nThis talk builds intuition with a minimal loop that alternates a slow GPU operation with a quick \u201cbookkeeping\u201d operation, a pattern that resembles many inference and training pipelines. By adding a seemingly harmless action\u2014such as printing a CUDA tensor\u2014we\u2019ll see how easily a synchronization can be introduced and why the slowdown can be disproportionate to what the code appears to do.\r\n\r\nWe\u2019ll then walk through a practical profiling workflow in NVIDIA Nsight Systems. The key technique is to correlate GPU utilization gaps with long CPU-side CUDA API calls (for example cudaStreamSynchronize) that indicate the host thread is waiting. Comparing a healthy trace to a sync-heavy trace makes it clear where the pipeline stalls and which code region triggers it.\r\n\r\nBeyond the usual suspects (.item(), printing device tensors, explicit device transfers), the talk highlights dynamic shapes as a common synchronization trigger. Patterns like boolean indexing with a GPU mask or slicing with a GPU-resident index can force PyTorch to fetch information back to the CPU to determine output sizes and allocations. We\u2019ll discuss how to recognize these cases and how to restructure code toward shape-stable alternatives when possible.\r\n\r\nFinally, we\u2019ll cover how to prevent regressions. Instead of relying on profiling alone, we\u2019ll use PyTorch\u2019s experimental API `torch.cuda.set_sync_debug_mode()` in unit tests to surface synchronizations early, while keeping production code unchanged. We\u2019ll close with guidance on when a small Triton kernel is worth considering to avoid sync-inducing patterns and to fuse multiple small ops into a single, fully asynchronous kernel.", "code": "RNT9FV", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/RNT9FV.png", "speaker_names": "Tomas Ruiz", "speakers": "\n### Tomas Ruiz\n\nI am a research assistant at the Ludwig-Maximilian-University of Munich within Prof. Schwemmer\u2019s Computational Social Science Lab. My research area is the intersection of Machine Learning and Social Media, particularly on multi-modal understanding. In previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering.\n", "domain_expertise": "Advanced", "python_skill": "Intermediate", "track": "PyData & Scientific Libraries Stack"}, {"title": "How We Built an Inclusive Data Organization: Careers, Community & 50% Women", "abstract": "Building inclusive data teams and sustainable career paths is a challenge many organizations struggle with\u2014especially in fast-growing, highly technical environments. Data careers are often portrayed as linear, while diversity initiatives remain abstract or ineffective in practice.\r\n\r\nThis talk shares concrete, experience-based lessons from building an inclusive data organization that supports career growth, fosters an internal data science community, and achieved more than 50% women representation in data roles. Rather than focusing on theory, the session highlights practical decisions, structural changes, and leadership behaviors that made inclusion measurable and sustainable.\r\n\r\nAttendees will gain actionable insights into designing career paths that support non-linear journeys, creating internal data communities that encourage learning and collaboration, and implementing diversity practices that strengthen\u2014rather than dilute\u2014technical excellence. The talk is relevant for data scientists, engineers, team leads, and managers who want to build better teams and healthier data cultures.", "full_description": "Many organizations aim to grow strong data teams, yet struggle with three connected challenges: unclear career paths, weak internal data communities, and a lack of diversity\u2014especially in senior and technical roles. These challenges are often treated separately, even though they strongly influence one another.\r\n\r\nThis talk presents a holistic approach to building an inclusive data organization by aligning career development, community building, and diversity goals. The focus is on practical actions and structural choices that can be applied in real-world settings, regardless of company size or industry.\r\n\r\nTalk Outline\r\n1. The problem: why data organizations struggle\r\n- Common myths about data careers (linear paths, constant availability, narrow profiles)\r\n- Why diversity efforts often fail in technical teams\r\n- The cost of ignoring community and inclusion: attrition, silos, burnout, and missed talent\r\n\r\n2. Career growth beyond linear paths\r\n- Designing career paths that support different life phases and backgrounds\r\n- Recognizing and valuing transferable skills in data roles\r\n- Making progression criteria transparent and fair\r\n- Supporting growth from individual contributor to leadership without forcing a single model\r\n\r\n3. Building an internal data science community\r\n- Why internal communities matter for learning, retention, and impact\r\n- Creating spaces for knowledge sharing without gatekeeping\r\n- Encouraging collaboration across roles (data science, engineering, analytics)\r\n- Aligning community activities with business value and technical standards\r\n\r\n4. Achieving diversity with intention\r\n- What \u201c50% women in data\u201d actually requires in practice\r\n- Hiring processes that reduce bias while maintaining technical excellence\r\n- Inclusive team structures and ways of working\r\n- Leadership behaviors that support inclusion without tokenism\r\n\r\n5. What worked\u2014and what didn\u2019t\r\n- Trade-offs, challenges, and lessons learned\r\n- Why inclusion is a continuous process, not a one-time initiative\r\n\r\n6. Actionable takeaways\r\n- Practical steps attendees can apply in their own teams\r\n- Signals to look for when inclusion efforts are working\u2014or failing\r\n- How to start small and scale impact over time", "code": "RQTJFS", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/RQTJFS.png", "speaker_names": "Xia He-Bleinagel", "speakers": "\n### Xia He-Bleinagel\n\nHead of Data & Cloud, focused on inclusive career development, internal data science community, and creating diverse, high-performing data organization.\n", "domain_expertise": "None", "python_skill": "None", "track": "Community & Diversity"}, {"title": "From Prompt to Production: How to use AI Code Assistants for Python Data Systems", "abstract": "**Code-generating LLMs have matured** to the point where they can reliably scaffold **data pipelines and data agents**, when used in a **supervised, engineering-first workflow**. This tutorial demonstrates how to combine modern **AI coding assistants** with a **production-ready Python deployment platform (Tower.dev)** to build and operate **real data systems**.\r\n\r\nParticipants will learn how to structure **collaborative Human/AI Assistant development loops**, where engineers provide **architecture, domain knowledge, and review**, while AI accelerates implementation. We will build a **data pipeline** and a **lightweight data agent**, iterating with an AI assistant to **generate, test, and improve code**.  \r\n\r\nThe session also covers critical **operational concerns** such as:\r\n- **Security**\r\n- **Scaling**\r\n- **Observability**\r\n- **Debugging**\r\n\r\nYou will also see how **production feedback can be looped back into the assistant** to continuously improve generated code.\r\n\r\nThis is **not about \u201cvibe coding\u201d** a website. It is about **disciplined, review-driven AI collaboration** that meaningfully improves productivity for **data practitioners at all levels**.", "full_description": "This **90-minute hands-on tutorial** shows how to **design, build, and deploy Python data pipelines and data agents** using AI coding assistants in a **supervised engineering workflow**.\r\n\r\n### Outline\r\n\r\n- **The state of AI code generation** for data engineering  \r\n- Designing **collaborative Human/LLM development loops**  \r\n- Building a **data pipeline with structured AI assistance**  \r\n- Creating a **simple data agent**  \r\n- Deploying and operating **Python workloads in production** using **Tower.dev** \r\n- Using **logs, observability, and runtime feedback** to guide AI-driven refactoring  \r\n- **Best practices, risks, and guardrails**\r\n\r\nParticipants will leave with **practical patterns for integrating AI into real-world data engineering workflows**, from **prototype to production**.", "code": "RRLTBU", "state": "confirmed", "created": "2026-02-13", "social_card_image": "/static/media/social/talks/RRLTBU.png", "speaker_names": "Serhii Sokolenko", "speakers": "\n### Serhii Sokolenko\n\nSerhii Sokolenko is a co-founder of Tower, a Pythonic platform for data flows and agents running on top of open analytical storage. Prior to founding Tower, Serhii worked at Databricks, Snowflake and Google on data processing and databases.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Sponsor"}, {"title": "7 Anti-Lessons from Building a PydanticAI Agent: Mistakes We Made So You Don't Have To", "abstract": "Life sciences compliance isn't forgiving. When your software helps companies navigate FDA regulations, ISO 13485, and EU MDR, \"move fast and break things\" isn't an option. Audit trails matter. Documentation is mandatory. Getting it wrong means regulatory findings, delayed product launches, or worse \u2014 patient safety risks.\r\n\r\nDuring the development of our AI Assistant we made every mistake in the most unforgiving environment possible. After more than a year building with PydanticAI, pydantic-evals, and Claude \u2014 nearly 3,000 commits and 20+ contributors \u2014 here are 7 anti-lessons so you don't have to repeat them:\r\n\r\n1. **\"We need a multi-agent system\"** \u2014 We built one. Then deleted it.\r\n2. **\"Agents need sophisticated planning\"** \u2014 A todo list beat our workflow engine.\r\n3. **\"Give the agent lots of specific tools\"** \u2014 Two high-level tools replaced dozens.\r\n4. **\"Encode workflows in code\"** \u2014 Markdown files the agent reads at runtime won.\r\n5. **\"It works when I test it\"** \u2014 Simple tests \u2260 real user journeys. Realistic evals or you're blind.\r\n6. **\"Automate everything\"** \u2014 Human stays in the driver's seat, not the trunk.\r\n7. **\"Apply what made you successful before\"** \u2014 Your engineering instincts might hurt you here.\r\n\r\nReal code, real git commits, real mistakes from a domain where mistakes are expensive.\r\n\r\n**Come for the mistakes. Leave with shortcuts.**", "full_description": "## The Domain: Where Mistakes Are Expensive\r\n\r\n[Qualio](https://www.qualio.com/) builds quality management software for life sciences companies \u2014 the ones making medical devices, pharmaceuticals, and biotech products. Our customers navigate FDA 21 CFR Part 11, ISO 13485, EU MDR, and SOC 2. In this world, compliance isn't optional. Audit trails are mandatory. Documentation gaps mean warning letters, import bans, or product recalls.\r\n\r\nWhen we decided to build an AI agent to help users manage compliance gaps, create remediation plans, and handle documentation \u2014 we knew the stakes. An agent that hallucinates a regulatory requirement or skips an approval step isn't just annoying. It's a liability.\r\n\r\nSo we built carefully with PydanticAI and Claude. And we still made every mistake possible. Here are 7 anti-lessons from the trenches.\r\n\r\n---\r\n\r\n### Anti-Lesson 1: \"We need a multi-agent system\"\r\n\r\nIt seemed obvious: separate agents for documents, compliance, and events. Clean architecture. We built it, shipped it, and spent weeks debugging coordination failures and inconsistent responses. In a domain where consistency matters, multi-agent chaos was unacceptable. The fix? Delete it. One agent with dynamic capabilities. Simpler, faster, and \u2014 according to our evals \u2014 more accurate.\r\n\r\n### Anti-Lesson 2: \"Agents need sophisticated planning\"\r\n\r\nCompliance workflows are complex. Surely the agent needs workflow graphs, state machines, planning frameworks? We tried. The agent got confused, skipped steps, invented procedures. The fix? A todo list. Add a task, check it off, see what's next. In a regulated environment, simple and auditable beats clever and opaque.\r\n\r\n### Anti-Lesson 3: \"Give the agent lots of specific tools\"\r\n\r\nWe built dozens of tools using PydanticAI's tool registration: `create_document`, `update_control`, `get_gap_details`, `list_frameworks`, `submit_for_review`... The tool descriptions bloated the context. The agent picked wrong tools. The fix? Two high-level tools: `call API` (with OpenAPI specs for the details) and `read instruction` (load a markdown file). Fewer tools, better results, easier to audit.\r\n\r\n### Anti-Lesson 4: \"Encode workflows in code\"\r\n\r\nHow does the agent know how to remediate a compliance gap? How to create a controlled document? At first, it was buried in prompts and Python. The fix? Markdown files \u2014 like Claude's skills system. The agent reads them at runtime. Engineers can review them. Knowledge belongs in documents your compliance team can actually read.\r\n\r\n### Anti-Lesson 5: \"It works when I test it\"\r\n\r\nOur early tests passed. The agent handled every case we threw at it. Then real users arrived \u2014 and everything broke. The problem? Our test cases were simple, synthetic, and predictable. Real user journeys are messy, multi-step, and full of context we didn't anticipate. The fix? Realistic evaluation data. We capture actual user sessions, anonymize them, and run them through pydantic-evals with LLM-as-judge rubrics. Does the agent follow the procedure? Does it hallucinate requirements? Does it handle the weird tangents users take? A 95% pass threshold in CI means nothing if your test data doesn't reflect reality.\r\n\r\n### Anti-Lesson 6: \"Automate everything\"\r\n\r\nWe built a fully automated feedback loop: user feedback creates a Jira ticket, a dev triages it, a Claude instance picks it up, raises a PR, responds to review comments. The dream? The fix: keep the human in the driver's seat. PydanticAI's `DeferredToolRequests` pattern lets our agent propose actions and pause for approval \u2014 the same principle applies to our dev workflow. In compliance software, someone is always accountable. The automation handles grunt work. Humans make decisions. Assisted development, not autopilot.\r\n\r\n### Anti-Lesson 7: \"Apply what made you successful before\"\r\n\r\nThis is the meta anti-lesson. Good engineering habits \u2014 upfront design, comprehensive APIs, handling every edge case \u2014 _can_ slow you down with agents. The LLM will surprise you. Your assumptions will be wrong. The fix? Start scrappy, iterate fast, let evals tell you what's working. The hardest part isn't code. It's unlearning.\r\n\r\n---\r\n\r\n### Bonus: Scaling Agent Development with tmux\r\n\r\nHow to run multiple agent experiments in parallel. Low-tech, high-leverage.\r\n\r\n---\r\n\r\n## Who Should Attend\r\n\r\nDevelopers building AI agents, especially in domains where accuracy and auditability matter. Familiarity with PydanticAI is helpful but not required \u2014 you'll see enough code to get started.", "code": "RUSUYF", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/RUSUYF.png", "speaker_names": "Joshua G\u00f6rner", "speakers": "\n### Joshua G\u00f6rner\n\nPlatform Engineer by Day \u2699\ufe0f  \r\nProduct Engineer by Night \ud83c\udf19  \r\nEx-Data Scientist \ud83d\udcca  \r\nOnline Tutor \ud83d\udcfa   \r\nHusband to a gorgeous Wife \ud83d\udc8d  \r\nFather of 100<sub>2</sub> kids \ud83d\udc23\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "No, you can't 'eval' your way to fairness", "abstract": "Fairness is fundamentally not tractable to classic optimisation techniques. It's not a state of the world, it's an experience of it. No technology is fair in a vacuum - fairness can only be understood when a technical system collides with humans.\r\n\r\nWe're seeing a wave of off-the-shelf libraries measuring bad behaviours in LLM outputs, often simplifications of older fairness metrics. They can catch obvious failure modes like slurs. But this is one failure mode among many. Installing a library and calling the job done is fairness washing. The harder, more fruitful approach is to explore the space of failure modes, consider what an ideal world would look like, and design measures, mitigations, and feedback loops accordingly.\r\n\r\nThis is a talk for people who suspect we can't optimise our way to human dignity.", "full_description": "**Cold open**\r\nFairness is fundamentally not tractable to classic optimisation techniques.\r\n\r\n**The exposition**\r\nFairness is not a state of the world, it's an experience of it. No technology is fair in a vacuum. Fairness can only be understood when a technical system collides with humans in the world. It is felt as much as it is calculated. We can look at statistical results in aggregate to understand patterns, but these do not tell the story of the individual.\r\n\r\nFurther, attempting to optimise numerical fairness metrics is fundamentally coercive and technocratic: putting our thumb on the scale globally, injecting \"positive bias\" into single dimensions, framing fairness as a data problem rather than a problem of human dignity. It's a \"one metric to rule them all\" approach that fails to acknowledge differences in preference, culture, experience. To build systems that support human agency we must first abandon our idea of a single moral machine which consistently outputs correct answers from inputs and algorithms. Any system treating people as fungible or undifferentiated is structurally unfair.\r\n\r\nWhat might consent-based fairness look like instead? Asking \"Do you want extra help?\", making sure individual preferences and self-reported disadvantage can add a layer of human respect into the equation. But we rarely see even this. Instead we see universalist design that decides what's good for people without consulting them - the same pattern that Design Justice critiques as erasing those who experience intersectional disadvantage.\r\n\r\nWhat does this have to do with evals? We're seeing a wave of off-the-shelf libraries measuring bad behaviours in LLM outputs, often simplifications of older fairness metrics. And yes, they can catch obvious failure modes like slurs in outputs. But this is one failure mode among many. Installing a library and calling the job done is fairness washing. The harder, more fruitful approach is to explore the space of failure modes, consider what an ideal world would look like, and design measures, mitigations, and feedback loops accordingly. It also means grappling with the fact that we cannot avoid doing harm. What we can do is harm reduction, humility, and striving toward something better while acknowledging the impossibility of the task.\r\n\r\n**Third act**\r\nThis talk won't offer easy answers. Attend if you want to grapple with the gnarly problems of building systems for humans. We'll borrow ideas from Design Justice and the disability rights movements: nothing about us without us. Let's ask and answer better questions. You'll leave with sharper mental models and tools for the next tricky conversation at work.\r\n\r\n**Outline (30 minutes):**\r\nThe problem (10 min): \r\n- Fairness as experience, not state. \r\n- Why optimisation fails. \r\n- The individual vs the aggregate. \r\n- Why treating people as fungible is structurally unfair.\r\n\r\nThe critique (10 min): \r\n- Off-the-shelf fairness evals as fairness washing. \r\n- The temptation to install a library and call it done. \r\n- What these tools can and cannot catch without further analysis.\r\n\r\nThe alternative (10 min)\r\n- Borrowing from Design Justice and disability rights. \r\n- Exploring failure modes rather than optimising metrics. \r\n- Harm reduction over false perfection. \r\n- Transparency, explanation, empowerment.\r\n\r\n**What you'll take home**\r\nYou'll leave with sharper mental models for thinking about fairness in technical systems, frameworks borrowed from Design Justice and disability rights movements, and tools for the next tricky conversation at work about what fairness actually means. There are no easy answers here, but there are better questions.", "code": "S7KYEE", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/S7KYEE.png", "speaker_names": "Laura Summers", "speakers": "\n### Laura Summers\n\nLaura is a very technical designer\u2122\ufe0f, working at Pydantic as Lead Design Engineer. Her side projects include Sweet Summer Child Score (summerchild.dev) and Ethics Litmus Tests (ethical-litmus.site). Laura is passionate about feminism, digital rights and designing for privacy. She speaks, writes and runs workshops at the intersection of design and technology.\n", "domain_expertise": "None", "python_skill": "None", "track": "Ethics & Privacy"}, {"title": "Roll for Architecture: DungeonPy \u2013 A D&D Companion as Server + Thin Clients", "abstract": "### **DungeonPy** \u2013 an interactive Dungeon&Dragons app for remote campaigns\r\nAs a matter of fact, tabletop RPGs are secretly distributed systems: one canonical world state, many clients, lossy links (players), and strict access control (\u201cno peeking at the DM notes\u201d). This talk introduces **DungeonPy**, which evolves a Python D&D companion from two local app \u2013 a Pygame battle map and a PySimpleGUI initiative/condition tracker \u2013 connected by lightweight TCP messages, into an authoritative server with multiple role-aware clients. The result is a fully real-time interactive setup, where the DM controls the full state and can reveal information selectively \u2013 under the hood it\u2019s all about client intents, server validation, state updates, event broadcasting and periodic snapshots. We will cover protocol design (deltas vs snapshots, ordering/idempotency), server-side view projections (DM omniscience vs per-player truth and fog-of-war), UI-safe concurrency, and testing your homemade message bus without summoning race conditions. Expect patterns you can reuse in any stateful client/server app \u2013 just with more goblins.", "full_description": "## **Roll for Architecture: DungeonPy \u2013 A D&D Companion as Server + Thin Clients**\r\nMany tiny, personal projects reach a point where \u201cit works on my machine\u201d is no longer the interesting part, and it becomes more about making it *scale in structure*: clean boundaries, explicit state, testable behaviour and room for new features. This session is a case study of that journey using a D&D assistant for remote playing written in Python, that turned into something completely off-scale.\r\nThe starting point is a few desktop clients:\r\n- a **Pygame** (battle) *map* (grid, tokens, map objects, movement), and\r\n- a **PySimpleGUI** *tracker* for initiative, HP and conditions, with a clear \u201cactive combatant\u201d concept,\r\n\r\ninitially synchronized with lightweight TCP messages.\r\nThis already exposes real engineering questions: avoiding GUI thread violations, preventing feedback loops, and deciding what the \u201csource of truth\u201d is when both ends can initiate updates.\r\nThe evolved version introduces an **authoritative server**. Players connect as clients and can interact in real time \u2013 moving tokens and manipulating shared objects \u2013 while the DM client keeps full visibility and control. Clients do not share state with each other: they submit *intents* (move here, end turn, toggle condition), the server validates, updates state, and broadcasts events plus periodic snapshots. The key architectural move is *role-scoped state*: the server owns the full truth and projects different \u201cviews\u201d to each client (DM omniscience vs per-player information), so fog-of-war and hidden details are enforced by design. In other words:\r\n- **DM client**: full map + all combatants + hidden details.\r\n- **Player client**: a filtered view (only the player\u2019s character sheet details, their token, and whatever the DM has revealed).\r\n\r\nThe authoritative server runs on a small VPS with a public endpoint. Clients connect over secure WebSockets (wss://) on port 443, so players can join from anywhere without port forwarding. TLS is terminated by a standard reverse proxy, and the server speaks a small JSON message protocol (snapshots + events) over WebSocket frames.\r\n\r\n### **Open source software used**\r\n- `Pygame` (map rendering + input)\r\n- `PySimpleGUI` (initiative/conditions UI)\r\n- `asyncio` (multi-connection handling)\r\n- `websocket` (client/server transport)\r\n- (non-python) `NGINX` (reverse proxy, TLS)\r\n\r\n### **Detailed talk outline**\r\n1. - **Intro: D&D, remote play, and why am I doing this?**\r\n2. - **Setting up the table**\r\n        - PyGame\r\n        - PySimpleGUI\r\n3. - **State model and serialisation**\r\n        - Turning GUI objects into explicit data (combatants, map, doors, initiative order).\r\n        - JSON snapshots and versioning.\r\n4. - **Protocol design: events vs snapshots**\r\n        - Event messages for responsiveness (\u201ctoken moved\u201d, \u201ccondition added\u201d).\r\n        - Snapshot sync for recovery and late joiners.\r\n        - Idempotency and ordering: simple sequence numbers, replay safety and conflict avoidance.\r\n5. - **Role-based filtering (the privacy boundary)**\r\n        - A single canonical server state.\r\n        -  Server-side \u201cview projection\u201d: DM view vs per-player view.\r\n        - Practical examples: hidden enemies, secret doors, private notes, fog-of-war style reveals.\r\n6. - **Concurrency and UI integration**\r\n        - Socket threads feeding GUI event loops safely (posting events into the GUI thread rather than touching widgets directly).\r\n        - Keeping the map smooth under network jitter: optimistic UI vs confirmed updates (and when not to).\r\n7. - **Testing strategy for a networked hobby project**\r\n        - Unit tests for pure state transitions (\u201capply damage\u201d, \u201cadvance turn\u201d, \u201cillegal move rejected\u201d).\r\n        - Protocol tests with simulated clients.\r\n        - Logging that helps during live play without drowning you in noise.\r\n8. - **Extensibility hooks**\r\n        - Adding new client types (spectator screen, mobile character sheet).\r\n        - Plug-in style rules (different systems, homebrew conditions).\r\n        - Future improvements: authentication for remote play, persistence, and reconnection.\r\n\r\n### **Takeaways**\r\nAttendees will leave with a practical blueprint for:\r\n- Designing a tiny, testable message protocol in Python.\r\n- Updating GUIs safely from background network threads.\r\n- Enforcing \u201cwho can see what\u201d without duplicating logic everywhere.\r\n- More importantly, they\u2019ll be given an example on how python can be used for highly non-standard tasks (like allowing remote role-playing gaming).\r\n\r\n### **Intended audience**\r\nBasic and intermediate Python developers comfortable with basic classes and modules, and curious about architecture and networking. This talk will be as much about *Python* as it will be about *nerd culture*: the goal is learning something while keeping a light heart.", "code": "S9VSCV", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/S9VSCV.png", "speaker_names": "Francesco Conte", "speakers": "\n### Francesco Conte\n\n#### **Who am I?**\r\nI'm a scientific researcher with a PhD in astroparticle physics at the Heidelberg University with a strong will to get out of academia \u2013 but in my life I worked many jobs and each of them changed me somehow. I gave up trying to solve the puzzle.\r\n#### **What do I like?**\r\nI'm passionate about coding, gaming and cats \u2013 well, about many other things, actually. For a cat-loving nerd, I'm surprisingly at ease among people, and on a stage.\r\n#### **What makes me comfortable?**\r\nIf it's about working, kind of everything? Be it coding (mainly Python, Julia, C++, Fortran, IDL, SQL), developing hardware prototypes, scouting for a publishing house, or working in a bookshop. If it's about life \u2013 listening to people talking at ease, and entertain them.\r\n#### **What makes me uncomfortable?**\r\nPeople saying that the AI will bring the world to an end. I'm positive that if it happens, it's more likely it'll be because of them rather than the AI.\r\n#### **Anything else?**\r\nI'm Italian, but it's not my fault.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "How to compare apples with oranges: Proper evaluation of article-level demand forecasts", "abstract": "How do you evaluate performance when you predict more than 10 million time series each day? While a good plot can be worth more than a thousand metrics for a single time series, with large-scale machine learning models implemented with *LightGBM* and *PyTorch* we have to resort to meaningful aggregations. We will share insights and learnings from the past 2 years of deploying and operating our article-level demand forecasting models at the pricing department of Zalando.\r\nThis talk moves beyond basic metrics to showcase the pitfalls of aggregated error measures and the best practices we\u2019ve developed to keep our stakeholders informed and our models accurate.", "full_description": "At the pricing department in Zalando, we are predicting future demand  for millions of articles on a daily basis by large-scale machine-learning models. These forecasts are key for discount decisions taken downstream. As evaluating every forecast on its own becomes infeasible at this scale and frequency we created a set of aggregated metrics that help us make informed statements about the performance of our models. On the one hand these metrics are being used by us to further improve our forecasting models, on the other hand they are used by our stakeholders to make informed decisions.\r\n \r\nTo handle this volume, we use *PySpark* for data processing and scaling our evaluations across the entire assortment. Furthermore, evaluating forecast performance in this context is crucial in two different scenarios, namely when analysing past forecast performance and when creating and comparing alternative models. In both cases we look at different time ranges and possible different subsets of the forecasted articles and calculate aggregated performance measures to compare them. We want to answer questions like\r\n\r\n\r\n - \u201cIs this forecast performing better in low-discount periods than during sales events?\u201d\r\n - \u201cDid we make a higher error on highly discounted articles during last week?\u201d\r\n - \u201cIs this model well-suited to predict high (or low) selling articles?\u201d\r\n - \u201cDid our model perform well for sneakers during the last voucher event?\u201d\r\n\r\nEvaluating aggregated metrics like a relative mean squared error (MSE) or an mean absolute percentage error (MAPE) over different sets of articles has lots of pitfalls. Comparing different parts of the assortments leads to an \"Apples vs. Oranges\" problem that we want to elaborate on based on examples we experienced in our daily work.\r\n\r\nTo answer the questions above we developed a set of aggregated metrics that we monitor on a daily basis using *plotly* and *streamlit* for clear, interactive visualization. We want to present these metrics and explain how they are useful for the questions and tasks mentioned above. We will highlight the techniques and best practices to draw meaningful insights from evaluating forecast performance and how we are able to compare apples with oranges using meaningful lower bounds for our aggregated metrics.\r\n\r\nWe also want to share how observations from our monitoring influenced the evolution of our *LightGBM* and *PyTorch* models and how it shaped important parts like feature engineering, hyperparameter tuning and the choice of our loss functions. Lastly we will touch on how to communicate these sometimes very technical numbers with stakeholders so that they can make informed decisions without being overwhelmed by details.", "code": "TB9WYZ", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/TB9WYZ.png", "speaker_names": "Stefan Birr, Mones Raslan", "speakers": "\n### Stefan Birr\n\nSenior Applied Scientist at **Zalando**, working on developing large scale forecasting systems. Stefan holds a PhD in Mathematics from **Ruhr University Bochum** where his research focused on \"Analyzing dynamic dependencies in time series. Prior to his 3 years at Zalando he worked for 5 years at E.ON as a Data Scientist creating algorithms for smart meter analytics and forecasting.\n\n### Mones Raslan\n\n\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Machine Learning & Deep Learning & Statistics"}, {"title": "Zero-Copy or Zero-Speed? The hidden overhead of PySpark, Arrow & SynapseML for inference", "abstract": "\"Zero-copy\" data transfer promises free communication between Spark's JVM and Python workers, but at 6 billion rows daily, the reality is far more complex. This session explores the low-level mechanics of distributed inference, focusing on the serialization bottlenecks that plague large-scale Gradient Boosted Tree models.\r\n\r\nWe will conduct a forensic analysis of execution plans generated by `pandas_udf`, `mapInPandas`, and SynapseML. By profiling memory hierarchies and CPU cycles, we visualize the true cost of pickling, Arrow record batching, and JNI context switching. Join this deep dive to understand the physics of distributed inference and learn how to tune `spark.sql.execution.arrow.maxRecordsPerBatch` to prevent OOMs without starving the CPU.", "full_description": "This talk is a technical deep dive into the \"physics\" of distributed machine learning inference. While high-level APIs promise seamless integration between Spark (JVM) and Python, the underlying data transfer mechanisms often become the primary bottleneck for high-throughput systems. We start by reality-checking the \"Zero-Copy\" promise of Apache Arrow in a PySpark context, identifying exactly where the abstraction leaks and where \"Zero-Copy\" isn't actually free.\r\n\r\nWe will move beyond the code to look at the execution traces and memory profiles of a 6-billion-row inference job. Using Flame graphs, we will visualize the CPU time spent in serialization versus actual prediction across three scenarios: standard `pandas_udf`, optimized `mapInPandas`, and SynapseML. This forensic analysis reveals the hidden costs of pickling and the impact of JNI context switching when bypassing the Python Global Interpreter Lock (GIL).\r\n\r\nThe session concludes with a focus on tuning for throughput. We will explore the delicate balance of configuring `spark.sql.execution.arrow.maxRecordsPerBatch`, demonstrating how to find the \"Goldilocks\" zone that maximizes CPU saturation without causing JVM off-heap memory crashes. Attendees will gain a deep understanding of the memory hierarchy involved in distributed inference and practical strategies for profiling serialization overhead in production.\r\n\r\nKey Takeaways:\r\n\r\n* Internals Knowledge: Understand exactly how data moves from JVM heap to Python worker memory.\r\n\r\n* Tuning Skills: Learn how to configure Apache Arrow batch sizes to optimize CPU saturation.\r\n\r\n* Profiling Techniques: Discover methods for profiling distributed Spark jobs to visualize serialization overhead.", "code": "TPNBRN", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/TPNBRN.png", "speaker_names": "Petar Ilijevski", "speakers": "\n### Petar Ilijevski\n\nCurrently solving the MLOps puzzle at Zalando, ensuring our pricing recommendation algorithms are as streamlined as my swimming technique. I spend my days shaping ML standards at scale and my free time training in the real world.\r\n\r\nMy life in two modes:\r\n\r\n1. Running pipelines & Diving deep into infrastructure.\r\n\r\n2. Running trails & Diving into the ocean.\r\n\r\nAlways looking to optimize the former to make more time for the latter.\"\n", "domain_expertise": "Intermediate", "python_skill": "Advanced", "track": "Data Handling & Data Engineering"}, {"title": "Open Table Formats in the Wild\u2122 - Reloaded: Vortexing Ducks over Floating Icebergs", "abstract": "Open table formats have *almost* freed us from vendor lock-in. They form a critical building block of the modern, composable data stack. The most prominent open table format is Apache Iceberg - not only because of its storage layout, but also due to its REST catalog specification. Iceberg has gained significant traction through a recent stream of feature announcements from the community itself, major cloud providers like AWS, and data platform leaders such as Snowflake and Databricks.\r\n\r\nBut cutting through the hype: how does Iceberg actually perform in the real world if you are *not* Netflix or Apple which are capable of *Building Your Own Snowflake* (BYOS)? Can you realistically migrate from legacy solutions to Iceberg and enjoy all its promises without tradeoffs?\r\n\r\nThat, of course, is a rhetorical question. Some even argue that Iceberg got parts of the specification fundamentally wrong!?!\r\n\r\nCurious? Join me for another episode of Open Table Formats in the Wild\u2122. Expect a practical look at the current state of Apache Iceberg and Apache Parquet, alongside a gentle introduction to DuckLake and Vortex as promising contenders for table and file formats, respectively.", "full_description": "### Description\r\nThe core promise of open table formats is engine interoperability with ACID guarantees, mutability, and schema evolution for massive datasets stored on cheap, reliable cloud object storage. Modern data platforms demand far more than *just* interoperable, analytical batch processing. Engineers now require native support for CDC, incremental processing, streaming workloads, low-latency access, and point lookups - especially for AI-driven applications. Ideally, all of this would be covered by a single, unified solution.\r\n\r\nHowever, Parquet - the foundational format for physically storing much of today\u2019s data - predates both the AI boom and the era of unified batch and streaming systems. Likewise, Iceberg\u2019s original design DNA was firmly rooted in large-scale, batch-oriented analytical workloads. This raises an uncomfortable question: are Parquet and Iceberg truly up to the task?\r\n\r\nThis talk explores that question through real-world use cases and architectural constraints. While the focus is on conveying key ideas and practical insights, the session is aimed at an intermediate to advanced audience. If you are new to the topic, you may want to watch last year\u2019s [episode](https://youtu.be/YdFeHj5lRP4?si=NxO0Ot2-S_kYOokV) on Apache Parquet and Delta Lake, which provides a gentle introduction to the fundamentals of open table formats.\r\n\r\n### Takeaways\r\n\r\nAfter this talk, attendees will:\r\n- Understand why incremental processing is not a native concept in Apache Iceberg\r\n- Recognize how Iceberg\u2019s metadata model creates hard limits for low-latency streaming workloads\r\n- Learn why Parquet\u2019s physical layout becomes a bottleneck for point lookups and AI-driven access patterns\r\n- Get an early look at DuckLake and Vortex as emerging alternatives \r\n\r\n### Agenda\r\n\r\n**The Past (10 min)**\r\n- Rationale - **The Idealized Model**\r\n- Implications - **The Engineering Trade-offs**\r\n\r\n**The Present (15 min)**\r\n- Incremental Processing - **The Missing Primitive**\r\n- Streaming Workloads - **The Batch Inheritance**\r\n- AI Applications & Point Lookups - **The Access Wall**\r\n\r\n**The Future (15 min)**\r\n- DuckLake - **The Return of Relational Databases**\r\n- Vortex - **The Parquet of Tomorrow**", "code": "TRGQTL", "state": "confirmed", "created": "2025-12-15", "social_card_image": "/static/media/social/talks/TRGQTL.png", "speaker_names": "Franz W\u00f6llert", "speakers": "\n### Franz W\u00f6llert\n\nHi my name is Franz and I\u2019m an open source and python enthuisiast:\r\n\r\n- father of 3 girls\r\n- major in psychology\r\n- chess hobbiyst\r\n- former competitive ultimate frisbee player\r\n- likes cooking and baking sourdough bread\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Data Handling & Data Engineering"}, {"title": "Dynamic Knowledge Graphs", "abstract": "Traditional RAG systems struggle to understand holistic connections in distributed, constantly changing knowledge sources that characterize real-world organizations. While document-based approaches using vector embeddings provide basic retrieval, they fail to capture relationships and answer complex questions about interconnected information. Graph-based RAG offers a solution, but existing implementations like Microsoft's GraphRAG explicitly avoid dynamic operations due to complexity, requiring costly rebuilds when knowledge changes.\r\n\r\nThis talk introduces a production-ready dynamic knowledge graph system that supports real-time insertion, querying, and deletion of information. Through practical implementation details you will learn to build maintainable knowledge graphs that evolve with data, handle ambiguous entities and preserve information lineage.", "full_description": "Like many organizations, we at VisualVest face the challenge of distributed and constantly evolving knowledge sources. Documentation lives across repositories, internal wikis, JIRA tickets, and various file formats in cloud storage. With ~250 employees making daily changes, our source of truth is highly dynamic. While traditional document-based RAG using semantic embeddings solved some of these pain points, it couldn't answer holistic questions or understand relationships between sources, leading us to explore graph-based approaches.\r\n\r\nThe challenge? Real-world knowledge sources are inherently dynamic. When thinking about information management and retrieval, we cannot ignore this reality if we want to create powerful, machine-readable and actually useful products. Microsoft's popular [GraphRAG](https://microsoft.github.io/graphrag/) library [explicitly rejected dynamic features](https://github.com/microsoft/graphrag/issues/429) (like deletion) due to complexity concerns. However, we believe that constantly rebuilding entire graphs isn't feasible for production systems.\r\n\r\nThis talk presents our solution: a truly dynamic knowledge graph with full insertion, query and deletion capabilities. We are also working on reducing the high computational cost of building knowledge graphs. Through caching strategies and small language model fine-tuning, we are trying to minimized both computational effort and strengthen our independence from cloud providers.\r\n\r\nWhat you'll learn:\r\n- An industry perspective on the challenges of distributed knowledge sources\r\n- Formal definition and properties of dynamic knowledge graphs\r\n- Our transformation pipeline\r\n  - Experiments with fine-tuned small-language models\r\n- Implementation details:\r\n  - Inserting nodes and edges while preventing ambiguity through similarity matching\r\n  - Tracking information origin across sources\r\n  - Safely deleting documents from the graph without breaking relationships\r\n  - Graph inference strategies\r\n\r\nBy the end of this talk, you'll understand why real-world knowledge graphs should be dynamic, how to build one yourself as well as the limitations and future directions of our approach.", "code": "TST9LF", "state": "confirmed", "created": "2025-12-17", "social_card_image": "/static/media/social/talks/TST9LF.png", "speaker_names": "Jakob Leander M\u00fcller", "speakers": "\n### Jakob Leander M\u00fcller\n\nHey, Im Jakob. I have studied Data Science in my Bachelors and Masters and currently work at a fin-tech where Im involved in all kinds of projects. My main goal is creating things that are actually useful and not just full of buzz-words. Im a big fan of visualizing things and always make sure that anyone who is interested in the topics Im working on can follow the reasoning of the chosen approach.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Data Handling & Data Engineering"}, {"title": "5 Years of NiceGUI: What We Learned About Designing Pythonic UIs", "abstract": "NiceGUI has grown from a small experiment into a widely used framework for building modern web-based user interfaces entirely in Python. After five years of development, thousands of users, and countless design iterations, we have gathered a rich set of insights into what makes a UI framework feel truly \u201cPythonic\u201d while still leveraging the power of the web platform.\r\nThis talk presents the key lessons learned while evolving NiceGUI, with a focus on how Python\u2019s own language features can meaningfully improve the developer experience. We explore how context managers, method chaining, decorators, async/await, type hints, dataclasses, and even well-chosen default arguments contribute to a clean, expressive, and maintainable UI API. Attendees will walk away with a deeper understanding of how to design Python-first interfaces\u2014whether for web apps, dashboards, or internal tools\u2014without needing to write JavaScript, CSS, or frontend boilerplate.", "full_description": "Five years ago, the NiceGUI project set out to answer a simple question: Can we build modern, interactive web UIs entirely in Python without giving up power or flexibility? Since then, the framework has evolved into a production-ready, community-driven tool that builds on top of proven technologies such as HTML, CSS, JavaScript, Vue.js, Quasar, Tailwind, and FastAPI\u2014while exposing a Pythonic interface that feels natural to Python developers.\r\nThis talk traces that journey and distills the design principles that worked, those that didn\u2019t, and the patterns that ultimately enabled NiceGUI to provide a smooth developer experience.\r\nWe begin with a short demonstration of NiceGUI\u2019s \u201c3-line Hello World,\u201d highlighting how familiar Python code can generate dynamic web interfaces. From there, we examine the technical foundations that allow the framework to stand on the shoulders of major frontend and backend ecosystems.\r\nThe core of the talk focuses on Python language features and how they shape API design:\r\n\r\n* **Context managers** to express hierarchy and UI composition intuitively.\r\n* **Method chaining** inspired by the builder pattern for concise, readable configuration.\r\n* **Decorators** (such as @page and @refreshable) to define routing and reactive behaviour without ceremony.\r\n* **Async/await** for event handlers, background tasks, and page functions.\r\n* **Type hints** to support static analysis, IDE completion, and clearer API intent.\r\n* **Dataclasses** as bindable, structured state containers.\r\n* **Default arguments and sentinel patterns** to allow powerful yet discoverable APIs.\r\n\r\nAttendees will gain practical insights useful beyond NiceGUI itself: how to design Python APIs for GUI frameworks, dashboards, developer tools, or any domain where clarity, maintainability, and expressiveness matter. The talk is aimed at Python developers interested in web interfaces, framework design, or improving the ergonomics of their own libraries.", "code": "TZYGTL", "state": "confirmed", "created": "2025-12-10", "social_card_image": "/static/media/social/talks/TZYGTL.png", "speaker_names": "Falko Schindler", "speakers": "\n### Falko Schindler\n\nFalko Schindler is a software engineer at Zauberzeug and a creator of the open-source web UI framework, NiceGUI. He specializes in building the company\u2019s core software stack for robotics and automation projects.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Building Trust in Your Data Pipelines with Observability", "abstract": "In the daily work of a data engineer, building new data pipelines often takes priority, while maintaining them and ensuring their correctness becomes an afterthought. This focus can quickly turn into a pitfall: failures go undetected, incorrect data silently propagates, and complaints from stakeholders arrive before engineers notice any issues.\r\nIn practice, incorporating observability into every new data pipeline helps avoid these problems and enables teams to steadily increase system complexity while maintaining trust and peace of mind.\r\n\r\nIn this talk, I introduce observability in the context of data pipelines, covering its three core pillars: metrics, alarms, and logs. We will explore concepts such as white-box versus black-box monitoring and best practices like the four golden signals and how they apply to data pipelines. I will show easy to implement first steps and share real-world experiences where improved observability helped uncover previously unknown incorrect behavior, gradually improve data quality, and build trust in data systems.\r\n\r\nThis talk is well suited for data engineers that had little exposure to observability and want to learn about strategies how keep sane while managing a jungle of pipelines.", "full_description": "This talk explores how observability can be applied to data pipelines to improve reliability, data quality, and confidence in complex data systems.\r\n\r\nThe talk begins with an introduction to observability in the context of data engineering. It explains the three core pillars: metrics, alarms, and logs, and discusses why observability is particularly important for data pipelines, where failures are often silent and correctness issues may only surface through stakeholder complaints. Examples include detecting issues early, debugging problems that span multiple systems, and gaining a better understanding of how data pipelines behave under changing load and requirements.\r\n\r\nThe first section focuses on metrics. It demonstrates how straightforward it can be to instrument data pipelines with basic metrics using Python. The talk then discusses which metrics are worth monitoring, adapting established concepts such as the four golden signals to data engineering use cases. Topics include white-box versus black-box monitoring, monitoring latency, throughput, error rates, and data freshness, and using metrics to identify long-term trends such as performance regressions after code changes. A concrete example based on a near\u2013real-time event processing pipeline illustrates how fine-grained metrics can reveal systematic failures for specific event types.\r\n\r\nThe second section focuses on alerting. It addresses the challenge that engineers rarely have time to continuously inspect dashboards and therefore rely on alarms to surface important issues. The talk outlines what makes a good alarm, emphasizing that alarms should be actionable, reliable, and provide sufficient context for investigation. A scenario involving a complex system with excessive and noisy alarms is used to illustrate alarm fatigue and the normalization of deviance. The section shows how to reduce noise by identifying critical system components, removing low-value alarms, and gradually refining alerting based on a clear understanding of which failures are unacceptable.\r\n\r\nThe final technical section covers logging. It discusses why logs are often difficult to work with in data pipelines, as they may contain a mixture of critical errors, informational messages, and low-level framework output. The talk introduces structured logging as a way to add context and make logs easier to search, filter, and aggregate. Examples include monitoring the distribution of log levels to uncover hidden issues, tracing the processing of individual records or users through a pipeline, and using centralized logging to identify dependencies between systems that are otherwise hard to detect.\r\n\r\nThe talk concludes by emphasizing that observability is not a one-time effort but an evolving practice. Attendees are encouraged to start with small, high-impact improvements and adapt their observability setup as their data pipelines and organizational needs grow.", "code": "U9KQU9", "state": "confirmed", "created": "2025-12-20", "social_card_image": "/static/media/social/talks/U9KQU9.png", "speaker_names": "Stefan Dienst", "speakers": "\n### Stefan Dienst\n\nI am a data engineer with four years of experience working on a variety of data platforms, ranging from classic ETL pipelines and data warehousing to near\u2013real-time stream processing. Before moving into data engineering, I completed a PhD in physics, where I first started working extensively with Python.\n", "domain_expertise": "Intermediate", "python_skill": "None", "track": "Data Handling & Data Engineering"}, {"title": "Designing Maintainable Python APIs: Lessons from Real Backend Systems", "abstract": "Python makes it easy to build APIs quickly, but many APIs that start clean become fragile, difficult to change, and risky to maintain as users, features, and teams grow. This talk focuses on designing Python APIs that remain stable, understandable, and adaptable over time.\r\n\r\nDrawing from real backend systems built and maintained in production, I will share practical lessons on API design decisions that worked, those that failed, and the trade-offs behind them. We will cover topics such as boundary definition, validation strategies, versioning, authentication design, and how small early choices can either support or block long-term evolution.\r\n\r\nThis talk is aimed at developers who already build APIs with Python and want to move beyond \u201cit works\u201d toward systems that can survive real users, ongoing change, and long-term maintenance.", "full_description": "Python makes it easy to build APIs quickly, but many APIs that start clean become difficult to change as systems grow. New features, additional clients, and evolving requirements expose early design decisions that were never meant to last. This talk focuses on how to design Python APIs that remain understandable, adaptable, and safe to modify over time.\r\n\r\nThe session draws from real backend systems built and maintained in production. Rather than presenting framework tutorials, it focuses on design choices and trade-offs that repeatedly affect long-term maintainability.\r\n\r\nThe talk begins by clarifying what maintainability means for APIs in practice. Beyond readable code, maintainable APIs allow change without fear, make behavior easy to reason about, and reduce the risk of unintended breakage. Common warning signs of declining maintainability will be discussed to help teams recognize problems early.\r\n\r\nNext, the talk examines API boundaries and responsibility separation. It covers how unclear boundaries between routing, business logic, and data access lead to fragile systems, and how clearer structure supports growth without unnecessary complexity.\r\n\r\nValidation and contracts are then addressed as core design concerns. Treating requests and responses as explicit contracts helps prevent bugs and misunderstandings between systems. Practical validation approaches that worked in production will be shared, alongside examples of shortcuts that caused long-term issues.\r\n\r\nThe session also covers authentication and authorization design, with attention to where security logic belongs and how poor placement can limit future change. Patterns that allow permission rules to evolve safely will be discussed.\r\n\r\nFinally, the talk looks at API evolution and change management. It explores when versioning is necessary, when it signals deeper design problems, and how APIs can evolve without breaking clients. The session concludes with lessons from real failures and what could have been done differently.\r\n\r\nThis talk is intended for developers with experience building Python APIs who want to move beyond basic functionality and design systems that remain maintainable as they scale.", "code": "U9QZTW", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/U9QZTW.png", "speaker_names": "Motunrayo Koyejo", "speakers": "\n### Motunrayo Koyejo\n\nMotunrayo Koyejo is a software engineer with over six years of experience building secure and scalable financial applications. She is passionate about solving complex problems with practical, results-driven solutions. Beyond coding, she mentors early-career engineers through volunteer initiatives and enjoys sharing her knowledge at tech conferences and events, making technical concepts accessible and engaging.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "AsyncIO vs Threads: who survives in the No-GIL Era?", "abstract": "AsyncIO vs threads isn't about \"which is faster\" - it's about scheduling, memory, and the kind of load you run. We'll unpack what threads and asyncio do under the hood (OS scheduler vs event loop + epoll), run practical benchmarks, and show why many \"async\" libraries still rely on thread pools (aiofiles, Motor, Django bridges). Then we'll repeat the same tests on Python 3.14's free-threaded (no-GIL) build and discuss when an interpreter upgrade can beat an async rewrite.", "full_description": "Concurrency in Python is full of stereotypes: \"threads are useless because of the GIL\", \"async is always faster\", \"just make everything async\". This session replaces opinions with mechanics and measurements, and updates the story for Python 3.14's free-threaded (no-GIL) build.\r\n\r\nWhat we'll cover\r\n\r\n1) How things actually work under the hood\r\n- A Python thread is an OS thread (pthread_create/clone). The OS scheduler runs it like any other thread - the GIL only matters when Python bytecode executes.\r\n- asyncio is also scheduling: one OS thread, many Tasks, cooperative switching at await, and readiness notifications via epoll/select.\r\n\r\n2) Why IO-heavy workloads often look \"equally fast\" in threads and asyncio\r\n- both models hide IO latency by switching while waiting;\r\n- the real difference shows up in scalability and cost: per-thread memory/stack + OS limits vs lightweight Tasks.\r\n\r\n3) When \"async\" is secretly a thread pool\r\n- aiofiles delegates file operations to run_in_executor();\r\n- Motor (async MongoDB driver) runs the synchronous PyMongo core in a ThreadPoolExecutor;\r\n- frameworks like Django must bridge sync and async worlds (sync_to_async), adding overhead and sharp edges.\r\n\r\n4) Benchmarks that mirror real services\r\n- 100 / 1,000 / 10,000 concurrent IO waits: why \"10k threads\" fails but \"10k tasks\" is fine;\r\n- memory and CPU overhead comparison (what you pay for concurrency);\r\n- a microservice-style endpoint (FastAPI-like) in sync/threaded vs async mode.\r\n\r\n5) What changes with free-threading (no-GIL)\r\n- a high-level view of what CPython changes to make it possible;\r\n- rerunning the same benchmark with and without the GIL;\r\n- when an interpreter upgrade can deliver \"async-rewrite-level\" gains for mixed CPU+IO workloads.\r\n\r\nTakeaways\r\n- a practical checklist for choosing threading vs asyncio vs multiprocessing;\r\n- performance vs resource-usage intuition you can apply to real services;\r\n- guidance on how to read \"async\" claims in library docs.", "code": "UUHYUS", "state": "confirmed", "created": "2026-01-01", "social_card_image": "/static/media/social/talks/UUHYUS.png", "speaker_names": "Igor Anokhin", "speakers": "\n### Igor Anokhin\n\nI have been working with Python for over eight years, although I started programming back in school.\r\n\r\nI began with small personal projects, then worked with several startups, gaining hands-on experience with real-world systems.\r\n\r\nSince 2021, I have been part of the K2 Cloud development team, focusing on building and scaling production Python services in AWS-like cloud platform.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Python Language & Ecosystem"}, {"title": "Don\u2019t Let Imposter Syndrome Win: U Can Do Big Things from a Small Place, A 7-Year African AI Journey", "abstract": "Imposter syndrome affects engineers everywhere, but underrepresented professionals often face amplified self-doubt due to geography, limited access, and systemic biases. In this talk, I share my 7-year journey as an African AI engineer building global impact from outside major tech hubs. From founding DataFestAfrica and leading remote AI opportunities to getting the attention of organizations like Huawei, MongoDB, McKinsey, and AnyScale, I\u2019ll show how community, mentorship, open source, media presence, and strategic partnerships can create opportunities and influence. Attendees will gain practical strategies to overcome self-doubt, expand their reach, and make a meaningful difference in tech, no matter where they are.", "full_description": "Imposter syndrome affects engineers worldwide, but for underrepresented professionals, geographic location, limited access to resources, and systemic biases can amplify its impact. Many talented engineers outside major tech hubs face self-doubt, missed opportunities, and barriers to career growth, even when they have the skills and vision to make meaningful contributions globally. Recognizing and addressing these challenges is crucial for building inclusive and diverse tech ecosystems.\r\n\r\nThis topic is particularly relevant for early- to mid-career engineers, community builders, and professionals navigating global tech ecosystems. Many in these groups experience self-doubt, uncertainty about career paths, and difficulty gaining visibility and recognition. Understanding how to overcome these challenges can empower them to take bold steps, make an impact beyond their immediate environment, and thrive despite systemic limitations.\r\n\r\nIn this talk, I share actionable strategies that helped me overcome imposter syndrome and build influence from any location. Drawing on my 7-year journey as an African AI engineer, I highlight how leveraging community building, open source contributions, mentorship, media engagement, and strategic partnerships can create meaningful opportunities. I illustrate these strategies through real-world examples, including founding DataFestAfrica, growing AI and MLOps communities with limited funding in emerging regions, collaborating with global organizations, and gaining recognition such as the UK Global Exceptional Talent endorsement. Attendees will leave with practical insights to overcome self-doubt, expand their reach, and make a global impact from wherever they are.", "code": "V7LQGR", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/V7LQGR.png", "speaker_names": "Gift  Ojeabulu", "speakers": "\n### Gift  Ojeabulu\n\nGift Ojeabulu is a data scientist, AI/ML practitioner, and community builder with over six years of experience at the intersection of artificial intelligence, software engineering, and developer advocacy. He has led and scaled global AI communities, including growing Iterative.ai\u2019s community to over 30,000 data, ML, and AI professionals worldwide. Gift has curated hundreds of technical content pieces annually and has worked with AI startups such as Deci AI and DagsHub to shape developer relations and content strategies for highly technical audiences. He is a four-time AWS Community Builder in Machine Learning and AI, serving as a board advisor to DevNetwork (USA) in the areas of artificial intelligence and developer advocacy.\r\n\r\nAs the co-founder of Data Community Africa (DCA), the largest Black data and AI community on the continent, Gift has led initiatives that support education, open-source collaboration, and professional growth, including the African Data Community Newsletter, which reaches over 2,500 subscribers across 80 countries. He has contributed to major ecosystem efforts such as DatafestAfrica and leads the Lagos MLOps community, where he focuses on practical MLOps, large language models, and open-source AI development. Through his work, Gift actively advances Africa\u2019s data and AI ecosystem by connecting local talent to global opportunities and fostering sustainable innovation.\n", "domain_expertise": "Novice", "python_skill": "None", "track": "Community & Diversity"}, {"title": "Wetterdienst: Fast, Unified Access to Open Weather Data with Polars", "abstract": "Weather and environmental data power analytics, ML, and operations\u2014but APIs differ wildly and data prep is slow. Wetterdienst is a Python library that provides a unified, Polars\u2011first interface to multiple weather services (DWD, ECCC, EA, NOAA/NWS, Geosphere Austria, IMGW, Eaufrance, WSV, and more). It standardizes request patterns, returns tidy (long) data, converts to SI units, handles caching, timezones (UTC by default), and retries\u2014so teams can focus on analysis instead of plumbing. This talk introduces Wetterdienst\u2019s provider architecture, core request patterns, performance practices with Polars, and how to integrate via Python, CLI, or its REST API. We\u2019ll walk through real examples (station discovery, parameter selection, timeseries retrieval), exporting to databases, and patterns for robust pipelines in ETL and ML.", "full_description": "Problem: Accessing weather data involves inconsistent APIs, formats, and units\u2014slowing down data engineering and causing hard\u2011to\u2011reproduce pipelines.\r\n\r\nSolution: Wetterdienst unifies open weather data access behind a consistent Python API and CLI, returning Polars DataFrames by default for performance and ergonomics. It provides a declarative request pattern, robust caching, retries, and humanized parameter naming with unit conversion.\r\n\r\nCore concepts:\r\n- Polars\u2011first: All new data operations use Polars (v1.15+); pandas supported for some IO.\r\n- Unified request pattern: Build a request (provider/network/parameters, time window), filter stations, fetch values; get tidy/long data by default.\r\n- Sensible defaults: UTC timestamps, SI units, and humanized parameter names.\r\n- Reliability: diskcache caching, stamina\u2011based retries, timezone handling.\r\n- Provider architecture: Consistent interfaces across DWD, ECCC, EA, NOAA/NWS, Geosphere, IMGW, Eaufrance Hubeau, WSV, etc.\r\n- Multiple interfaces: Python API, CLI, and an optional REST API.\r\n\r\nExamples (live demo):\r\n- Station metadata: discovery and filtering by station id/region/parameter.\r\n- Timeseries retrieval: selecting daily/hourly parameters, time windows, and exporting.\r\n- Settings: toggling humanization and unit conversion; switching to wide vs. long shape when needed.\r\n- Integration: using CLI for quick fetches; REST API for cross\u2011language consumption.\r\n\r\nEcosystem & exports:\r\n- Extras for databases (DuckDB, PostgreSQL, MySQL, CrateDB), Pandas/Xarray/Zarr exports, plotting (matplotlib/plotly), and SQL querying.\r\n- Works well in ETL/ML: cache for speed, schema\u2011stable outputs, and consistent units.\r\n\r\nPerformance patterns:\r\n- Prefer Polars transformations; leverage caching; batch requests per provider.\r\n- Use parallel processing where beneficial; handle slow/remote datasets with retries.\r\n\r\nLimitations & trade\u2011offs:\r\n- Some providers have rate limits or latency; caching and incremental retrieval recommended.\r\n- Not all providers expose identical parameters\u2014Wetterdienst normalizes interfaces but respects data source constraints.\r\n\r\nProposed Outline (30 minutes)\r\n- 0:00\u20133:00  The problem: fragmented weather APIs and inconsistent data\r\n- 3:00\u20137:00  Wetterdienst in 5 minutes: concepts, request pattern, settings\r\n- 7:00\u201317:00 Live demo: stations \u2192 values (Polars), caching, units, CLI/REST\r\n- 17:00\u201323:00 Integrations and exports (DuckDB/DBs, Pandas/Xarray, plotting)\r\n- 23:00\u201327:00 Performance patterns, pitfalls, provider nuances\r\n- 27:00\u201330:00 Q&A\r\n\r\nTarget Audience\r\nData engineers, scientists, and platform teams who need reliable weather data for analytics, ML, and operations.\r\n\r\nPrerequisites\r\nBasic Python and DataFrame experience (Polars or pandas); familiarity with ETL/ML pipelines helpful.\r\n\r\nKey Takeaways\r\n- A unified, Polars\u2011first workflow to access and normalize open weather data.\r\n- Practical patterns for station discovery, timeseries retrieval, unit conversion, and caching.\r\n- How to integrate Wetterdienst via Python, CLI, and REST, and export to common formats and databases.\r\n\r\nLinks\r\n- Repo: https://github.com/earthobservations/wetterdienst\r\n- Docs: https://wetterdienst.readthedocs.io/\r\n- REST API: https://wetterdienst.eobs.org/\r\n- Examples: https://github.com/earthobservations/wetterdienst/tree/main/examples", "code": "V8DNCL", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/V8DNCL.png", "speaker_names": "Benjamin", "speakers": "\n### Benjamin\n\nBenjamin Gutzmann is a 32 year old Python/data engineer and maintainer of Wetterdienst, currently at Otto Group data.works (Data Engineer since 2023; previously Junior Data Engineer), working across Generative AI and data engineering on GCP with Python, SQL, Argo, and Terraform. He has built the Wetterdienst library at earth observations (hobby project, since 2018). Before his start into work life he has studied Hydrology (BSc, MSc) at TU Dresden.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}, {"title": "Designing and Scaling a Python Library in the Open: Architecture, Automation and Community", "abstract": "Designing a Python library that scales over time requires more than clean code. In this talk, we present ScanAPI, an open-source Python library for automated API integration testing and live documentation, as a case study in sustainable library design.\r\n\r\nWe explore how architectural decisions, Python features, and automation pipelines help reduce maintenance costs while improving developer experience. We also share how open collaboration and community practices turn a Python library into a long-term, scalable project.\r\n\r\nAttendees will leave with practical patterns to apply when building or evolving Python libraries in the open.", "full_description": "Building a Python library that remains reliable, maintainable, and welcoming to contributors is a challenge many projects face as they grow. This session presents ScanAPI as a real-world case study of how thoughtful engineering and automation can support both technical scalability and open source sustainability.\r\n\r\nScanAPI is an open-source Python library that enables automated API integration testing and live documentation using declarative specifications. Distributed via PyPI and actively maintained, the project has been adopted by developers across different contexts and was recognized by GitHub as part of initiatives focused on securing the open source supply chain.\r\n\r\nRather than focusing on abstract best practices, this talk dives into concrete engineering decisions made while designing and maintaining the library.\r\n\r\nWhat we will cover:\r\n\r\n1. Designing a Python Library for Growth\r\n- How the codebase is structured to separate configuration, execution, and reporting\r\n- Organizing modules and public APIs to remain stable over time\r\n- Packaging decisions and CLI design for ease of use\r\n\r\n2. Using Python Features Effectively\r\n- Configuration-driven workflows with YAML and JSON\r\n- Validation, error handling, and predictable failures\r\n- Type hints and interfaces to improve readability and contributor confidence\r\n\r\n3. Automation as a First-Class Concern\r\n- Continuous integration with GitHub Actions\r\n- Unit and integration testing strategies\r\n- Automated releases, versioning, and dependency management\r\n\r\n4. Developer Experience and Adoption\r\n- Documentation and live reports as part of the product, not an afterthought\r\n- Lowering the barrier for new users and contributors\r\n- Tooling choices that reduce cognitive load\r\n\r\n5. Community and Sustainability\r\n- Contribution guidelines and governance models\r\n- How open collaboration scales better than individual ownership\r\n- The role of the Cumbuca Dev open source community in sustaining the project\r\n\r\nBy the end of the talk, attendees will have a clear mental model for designing Python libraries that can scale technically and socially. The lessons shared are applicable to anyone maintaining or planning to publish Python libraries, whether in personal projects, companies, or community-driven initiatives.", "code": "VBPRQR", "state": "confirmed", "created": "2026-01-10", "social_card_image": "/static/media/social/talks/VBPRQR.png", "speaker_names": "Camila Maia", "speakers": "\n### Camila Maia\n\nBrazilian software engineer, open source maintainer, and co-founder of Cumbuca Dev, a community-driven initiative that supports underrepresented people entering and thriving in technology through real-world practice, open source collaboration, and education. With over a decade of professional experience, Camila focuses on backend engineering, developer experience, tooling and automation.\r\n\r\nShe is the creator and core maintainer of ScanAPI, a Python library for automated API integration testing and live documentation that has gathered widespread adoption and community contributions. ScanAPI has been recognized by GitHub as part of initiatives to strengthen the open source supply chain and is used by developers internationally. Camila\u2019s work spans not only code but also documentation, automation pipelines, and contributor experience practices that make open source projects more sustainable.\r\n\r\nCamila was the first Brazilian accepted into the GitHub Sponsors program, breaking new ground for maintainers in her country. She is also featured as one of ~50 global open source maintainers in the maintane.rs project, invited by the Open Source Initiative (OSI) to share her personal journey and perspectives on how open source can unlock opportunities in tech.\r\n\r\nHer engagement extends to speaking and mentoring at technical conferences around the world, including Pyjamas, EuroPython, Python Brasil, DjangoCon EU, and others, where she has presented both talks and hands-on workshops. \r\n\r\nThrough Cumbuca Dev, Camila advocates for practical learning and structured contributions as pathways to real experience, helping people from diverse backgrounds build skills, confidence, and visibility before their first job. She believes that open source is not just code \u2014 it is a vehicle for community, opportunity, and empowerment \u2014 and her work reflects a commitment to making technology spaces more accessible, collaborative, and humane.\r\n\r\nPeople > Tech \ud83d\udc9c\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "Your Data Is Leaking: A Hands-On Introduction to Differential Privacy with OpenDP", "abstract": "Data analysis and machine learning often involve sensitive information. But how can we ensure that our analyses and releases do not inadvertently reveal information about the individuals in our data? Traditional approaches such as anonymization or releasing only aggregate statistics have repeatedly proven insufficient.\r\n\r\nDifferential privacy is a mathematical framework that offers provable privacy guarantees while still enabling useful data analysis. In this tutorial, we provide a hands-on introduction to differential privacy, covering key concepts relevant to understanding and applying it in practice. The focus will be on practical implementation rather than underlying theory.\r\n\r\nUsing interactive examples in Python, we will explore the core ideas of differential privacy, highlight its attractive properties and limitations, and demonstrate how to build privacy-preserving analyses using OpenDP, an open-source Python library for differential privacy. Participants will leave equipped to continue exploring differential privacy on their own. Familiarity with the basics of Python programming is helpful, but no prior knowledge of differential privacy is required.", "full_description": "Aggregate statistics feel safe to release - just counts, means, and totals, no individual records. But a long history of privacy failures has shown otherwise. From the AOL search data leak to the Netflix Prize re-identification attack to LLM memorization, \"anonymized\" data has repeatedly revealed more than intended.\r\n\r\nDifferential privacy offers a different approach: a mathematical framework that quantifies and bounds the information any release reveals about any individual. It has moved from theory to practice in recent years, with deployments at the US Census, Wikimedia, Israel\u2019s national birth registry, Google, Apple, Linkedin and more.\r\n\r\nIn this tutorial, we provide a hands-on introduction to differential privacy. We'll start by making the problem concrete - executing an attack on aggregate statistics - and then explore how differential privacy addresses it. The focus will be on practical implementation rather than underlying theory.\r\n\r\n## What You'll Learn\r\n1. Why traditional anonymization and aggregation fail to protect privacy\r\n1. The core ideas of differential privacy: what it guarantees, what epsilon means, and when DP is a suitable solution\r\n1. How to use OpenDP's building blocks\r\n1. How to build differentially private data analyses using OpenDP's Polars integration\r\n1. Where to go next: resources for AI/ML with DP, synthetic data, and further learning\r\n\r\n## Tutorial Outline\r\n\r\n### Part 1 - The Privacy Problem (20 minutes)\r\n\r\n- Real-world privacy failures (such as AOL search data, Netflix Prize, LLM memorization)\r\n- Hands-on: execute a reconstruction attack on aggregate statistics\r\n- Discussion: why traditional approaches fail\r\n\r\n### Part 2 - Introduction to Differential Privacy (20 minutes)\r\n\r\n- Core ideas: masking the contribution of a single individual through calibrated noise; protection against membership inference attack\r\n- Learning by doing: exploring DP with OpenDP's building blocks\r\n- Tuning privacy protection with f-DP; the privacy-utility tradeoff\r\n- Real-world deployments (such as US Census, Israel birth registry, LinkedIn API)\r\n\r\n### Part 3 - Data Analysis with OpenDP (40 minutes)\r\n\r\n- OpenDP fundamentals: domains, transformations, measurements, chaining\r\n- Working with tabular data using OpenDP's Polars integration\r\n- Building a complete DP data analysis pipeline\r\n- Revisiting the attack: does it still work?\r\n\r\n### Part 4 - What's Next (10 minutes)\r\n\r\n- Beyond the basics: AI/ML with differential privacy, synthetic data generation\r\n- Resources and community\r\n- Q&A\r\n\r\n## Prerequisites\r\n- Python: Comfortable writing functions and working with notebooks\r\n- Statistics: Basic familiarity with mean, counts, histograms\r\n- Differential privacy: No prior knowledge required\r\n\r\n## Materials\r\nParticipants will have access to interactive Jupyter notebooks with all code and exercises. Materials will be publicly available after the tutorial.", "code": "VJPQCR", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/VJPQCR.png", "speaker_names": "Shlomi Hod, Marcel Neunhoeffer", "speakers": "\n### Shlomi Hod\n\nShlomi Hod is a researcher at the Weizenbaum Institute. His work focuses on creating tools for the real-world deployment of responsible computing systems, with particular emphasis on differential privacy. He has led workshops on operationalizing Responsible AI for policymakers, regulators, and diplomats across organizations worldwide, including the US Congress and the German Federal Foreign Office. Shlomi recently earned his Computer Science PhD from Boston University and completed an OpenDP fellowship at Harvard University and a one-year research visit at Columbia University during his doctoral studies.\n\n### Marcel Neunhoeffer\n\n\n", "domain_expertise": "None", "python_skill": "Intermediate", "track": "Ethics & Privacy"}, {"title": "Letting AI Move: Robotics Demos Powered by Python", "abstract": "AI is sometimes hard to explain, especially for people outside of tech. With robots, AI becomes visible and tangible. In this talk we want to show how we can use Python and the huggingface reachy mini as an example to make AI more concrete, interactive, and engaging for beginners and non-experts.", "full_description": "Artificial intelligence can be difficult to explain, especially to people outside of tech. We often rely on slides, diagrams, or on-screen demos, but the real impact does not always stick. For people encountering AI for the first time\u2014such as students or non-technical audiences\u2014AI terminology and concepts can remain abstract and disconnected from real-world experience.\r\n\r\nRobots can help change that. When AI controls a physical system, its behavior becomes visible, tangible, and easier to reason about. In this talk, we explore how Python and playful robotics experiments can be used to make AI more concrete, interactive, and engaging. Using the Hugging Face Reachy Mini robot as a case study, we show how physical interaction can turn abstract AI concepts into intuitive, memorable experiences.\r\n\r\nThe perspective of this talk is intentionally non-traditional: we started with no prior knowledge of robotics or mechanics and approached the problem purely from a Python developer\u2019s point of view. This journey strongly shapes the talk. Rather than focusing on advanced robotics engineering, the emphasis is on accessibility, experimentation, and learning by doing. The goal is to show that robotics can be an approachable medium for explaining AI, even for people without a hardware or engineering background.\r\n\r\nDuring the talk, we walk through basic building blocks such as movement, gestures, and simple interaction patterns, and show how AI-driven behavior can be layered on top of them using familiar Python tools. We share examples from real experiments and demos, including what worked well, what failed, and what we learned from unexpected behavior in live settings.\r\n\r\nImportantly, this is not a product demo or a hardware-specific tutorial. While Reachy Mini is used as a concrete example, the focus is on transferable ideas and design patterns:\r\n\r\nHow physical interaction changes the way people perceive AI\r\n\r\nHow Python lowers the barrier to experimenting with robotics\r\n\r\nHow to design demos that invite curiosity rather than intimidation\r\n\r\nHow to make AI systems easier to explain in educational and outreach contexts\r\n\r\nAttendees do not need access to a robot to benefit from this talk. The lessons and patterns discussed can be applied to a wide range of settings, including classrooms, workshops, meetups, and public demonstrations.\r\n\r\nThis talk is aimed at Python beginners and intermediate developers, especially educators and anyone who regularly needs to explain or demonstrate AI to others. Attendees will leave with new ideas, inspiration, and practical approaches for making AI more tangible, engaging, and human-centered.", "code": "VUHSG9", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/VUHSG9.png", "speaker_names": "Larissa Haas, Annika Herbert", "speakers": "\n### Larissa Haas\n\n\n\n### Annika Herbert\n\nAnnika Herbert is a Solution Architect in the AI & Data Unit at sovanta, working on data-driven and AI-powered solutions. With a background in Data Science, she enjoys making AI more approachable, tangible, and easy to explain, especially to non-technical audiences. Coming from a Python developer\u2019s perspective, she likes to explore new fields through hands-on experimentation and learning by doing. Outside of work, she enjoys dancing, concerts, and baking chocolate-filled treats.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Embedded Systems & Robotics"}, {"title": "Process, Analyze, and Transform Python Code with ASTs", "abstract": "You\u2019ve likely used a tool like black, flake8, or ruff to lint or format your code, or a tool like sphinx to document it, but you probably do not know how they accomplish their tasks. These tools and many more use **Abstract Syntax Trees (ASTs)** to analyze and extract information from Python code. An AST is a representation of your code's structure that enables you to access and manipulate its different components, which is what makes it possible to automate tasks like code migrations, linting, and docstring extraction.\r\n\r\nIn this workshop, you\u2019ll learn how to use the Python standard library\u2019s ast module to parse and analyze code. Using just the standard library, we will implement a couple of common checks from scratch, which will give you an idea of how these tools work and help you build the skills and confidence to use ASTs in your own projects.", "full_description": "This tutorial will be a roughly 50/50 split of lecture and exercises. Attendees will get hands-on experience working with ASTs in Python, using only the standard library. By recreating common code-quality checks from scratch, attendees will both learn how common tools work under the hood and how to work with the AST in an easy-to-understand fashion. If I\u2019m short on time, I will cut from the final section as that builds upon the previous ones and having a foundation from the other topics will be a more than sufficient base for attendees to continue their studies after the tutorial.\r\n\r\n1. Introduction to ASTs\r\n    1. Introduction to me and the plan for the session\r\n    2. Introduce the term and concept of Abstract Syntax Trees (ASTs)\r\n    3. Mention some of the ways ASTs are used by Python itself and by popular tools\r\n    4. Parsing code into an AST and printing it\r\n    5. Converting an AST into source code again\r\n    6. Exercise break\r\n2. AST traversal, part 1\r\n    1. Walking the tree\r\n    2. Overview of AST node types encountered during our walk (ast.Module, ast.ClassDef, ast.FunctionDef, etc.)\r\n    3. Exercise break\r\n3. AST traversal, part 2 (90 minutes)\r\n    1. Creating an ast.NodeVisitor for basic traversal covering the visit() and generic_visit() methods\r\n    2. Exercise break, preceded with an introduction to any new AST node types, where necessary\r\n    3. Visualization of how the traversal was performed and discuss the need to track node ancestry in some applications as a result\r\n    4. Tracking node ancestry during traversal with a stack\r\n    5. Exercise break, preceded with an introduction to any new AST node types, where necessary", "code": "VWCZXS", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/VWCZXS.png", "speaker_names": "Stefanie Molin", "speakers": "\n### Stefanie Molin\n\n[Stefanie Molin](https://stefaniemolin.com) is a software engineer at Bloomberg in New York City, where she tackles tough problems in information security, particularly those revolving around data wrangling/visualization, building tools for gathering data, and knowledge sharing. She is also a core developer of [numpydoc](https://github.com/numpy/numpydoc) and the author of \u201c[Hands-On Data Analysis with Pandas: A Python data science handbook for data collection, wrangling, analysis, and visualization](https://www.amazon.com/Hands-Data-Analysis-Pandas-visualization/dp/1800563450),\u201d which is currently in its second edition and has been translated into Korean and Chinese. She holds a bachelor\u2019s of science degree in operations research from Columbia University's Fu Foundation School of Engineering and Applied Science, as well as a master\u2019s degree in computer science, with a specialization in machine learning, from Georgia Tech. In her free time, she enjoys traveling the world, inventing new recipes, and learning new languages spoken among both people and computers.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Python Language & Ecosystem"}, {"title": "Simulating the World using SimPy: A practical Example", "abstract": "Modern systems are complex - and testing them in real environments is often expensive, risky, or simply not reproducible. Simulation is a practical way to explore behavior under controlled conditions: run scenarios, validate assumptions, inject failures on purpose, and repeat experiments without touching production.\r\n\r\nIn this talk, I build a concrete event-based simulation with `SimPy` to compare `load-balancing algorithms` under different conditions. I\u2019ll show how `SimPy`\u2019s processes and events fit together, how to structure the simulation cleanly, and how to move beyond a one-off demo by making runs reproducible and configurable - using `configuration files` and a simple `command-line interface`.", "full_description": "Real-world systems are often too complex to test reliably, in the same environment and under the same conditions. Changes are hard to measure, edge cases are difficult to reproduce, and external influences can hide the real behavior of a system. Simulation offers a way to abstract from reality while staying close enough to produce meaningful results. It allows full control over system components, timing, and disruptions, and makes it possible to test many scenarios in a repeatable way.\r\n\r\nThe practical example of this talk focuses on simulating `load-balancing algorithms`. Load-balancers are a good example of systems that are hard to evaluate in real environments. Some tested algorithms have no existing implementation, others differ across platforms, and cloud environments introduce many uncontrollable factors such as network latency, cloud noise, and reoccurring background workloads. These factors make fair and consistent testing almost impossible.\r\n\r\nThe problem is addressed by building an event-based simulation using `SimPy`. The session explains how `SimPy` works by using `Generators` to create the events, and how time and processes interact inside a simulation. An architecture for a practical example for a load-balancer simulation is presented, showing how different components interact and how algorithms can be swapped and compared.\r\n\r\nThe talk also covers improvements made to the simulation, including a `command-line interface` for easier execution and a `YAML` configuration file for flexible setup. It concludes with practical tips and lessons learned when working with `SimPy`, helping to avoid common pitfalls and improve simulation design.\r\n\r\nOverall, the session provides an introduction to simulation as a testing tool, a hands-on example using `SimPy`, and a realistic architecture for building and evolving simulations in Python.", "code": "WDHTQR", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/WDHTQR.png", "speaker_names": "Niklas", "speakers": "\n### Niklas\n\nCloud-Engineer at inovex helping to develop and provide a private cloud infrastructure with a focus on performance and optimization.\n", "domain_expertise": "None", "python_skill": "None", "track": "PyData & Scientific Libraries Stack"}, {"title": "Exploring Germany's Urban Geography with Census and OpenStreetMap Data", "abstract": "When conducting studies of the urban form, an important resource many researchers turn to is the massive OpenStreetMap dataset. But, as extensive as this dataset is, it lacks one very important aspect about the cities it covers: the people who live there. In this talk, I show you how to add this missing element to your research by bringing in German Census data to create rich analysis capable of answering some of the most pressing issues facing our cities today. I exemplify this by walking you through my own research in urban geography and sustainability with a study of how equitably distributed common amenities are in cities across Germany. Throughout, we look at how Python and PostgreSQL can be used as effective tools to enable this research and keep it organized.", "full_description": "By the end of this talk, audience members will be empowered with the tools they need to help identify and bring light to important problems affecting their cities. To achieve this, I show how to combine data on urban structure from OpenStreetMap and demographic data from the German Census in PostgreSQL. Once the data is gathered, I then show how to do the actual analysis and present the findings with Python.\r\n\r\nThe presentation will be broken up into the following sections:\r\n\r\n**Laying the foundation**\r\n\r\nThe first step is creating an organized database that will serve as the data source for the rest of the study. I show how to use \"PgOSM Flex\" for this plus a tool that I wrote in Python to make it easy to import German Census data into PostgreSQL.\r\n\r\n**Asking meaningful questions**\r\n\r\nWith all the data in place, it's time to formulate a research question to drive our analysis. Formulating a meaningful research question can keep our analysis on track and much better organized. To get there, we explore the data we have available and consider the types of questions we can actually answer.\r\n\r\n**Analyze and present**\r\n\r\nNow that we have a clear question in mind, we'll construct the queries we need to generate the data necessary for our analysis. Once exported from PostgreSQL, we perform the analysis and generate the final reports using popular scientific libraries in Python.\r\n\r\n**Final thoughts**\r\n\r\nTo conclude the talk, I share how this analysis could be extended by including even more datasets. I also discuss the limitations of these types of studies while offering practical advice on how you can make a positive impact with your research.", "code": "WQGXJ3", "state": "confirmed", "created": "2025-12-07", "social_card_image": "/static/media/social/talks/WQGXJ3.png", "speaker_names": "Travis Hathaway", "speakers": "\n### Travis Hathaway\n\nWearer of many hats, but some of my favorite are Python enthusiast, social science researcher and amateur musician. Currently based in Berlin, Germany where I work as a senior software engineer and am an active participant in the conda open source community. I'm also an organizer of the Python Users Berlin group. Feel free to reach out via LinkedIn!\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "Data Handling & Data Engineering"}, {"title": "Hype, Hope, or Headache? Making Sense of GenAI, LLMs, and AI Agents with Anecdotal Evidence", "abstract": "After nearly 20 years in data science, from MLPs, SVMs, and random forests to deep learning, I\u2019ve seen many \u201crevolutions\u201d come and go. The current tectonic shift around GenAI and LLMs feels different from previous hype cycles. Even with some understanding how these things work, I am still blown away by the stream of stunning new capabilities. But they also introduce new kinds of risks that go far beyond technical performance. This talk offers a pragmatic, experience-driven perspective on GenAI in industrial settings, including supply chains and the emerging wave of AI agents. We\u2019ll disentangle real opportunities from snake oil, especially where hype-driven promises meet senior management expectations. An anti-bullshit take on the possibilities ahead, with honesty, anecdotes, and (for those who know me, of course) a bit of humor.", "full_description": "After nearly 20 years in data science I\u2019ve seen many \u201crevolutions\u201d come and go: neural networks, SVMs, bayesian statistics, random forests, XGBoost and deep learning. Each came with bold promises, and each eventually settled into a realistic place in production systems (read: became boring). Generative AI, however, feels fundamentally different.\r\n\r\nIn this talk, I\u2019ll share my view *why* the current GenAI hype stands apart from previous cycles: technically, culturally, and organizationally. Even with some understanding how these things work, I am still blown away by the stream of stunning new capabilities. This is not a \u201cGenAI is bad\u201d rant. Instead, it\u2019s a critical attempt to understand the shift we\u2019re seeing, and the risks that come with it if we don\u2019t adjust our thinking.\r\n\r\nUsing industrial examples such as supply chains (just because I work in this field), but also personal experience, I\u2019ll show where LLM-based approaches still have serious limitations today, and where GenAI can realistically add value. We\u2019ll disentangle different categories of risk from technical fragility, evaluation problems and mere costs to organizational overconfidence and misuse.\r\n\r\nA big part of the talk dives into the rapidly emerging field of AI Agents. We\u2019ll explore what AI agents actually are, where they make sense today, and where the current hype is just snake oil, particularly to senior decision-makers who may underestimate complexity, costs, and failure modes.\r\n\r\nThe goal of this talk is not to slow innovation, but to enable better decisions. If we want GenAI to be a success in real-world systems, we need to understand both the change it represents and the limits it still has. \r\n\r\nAn anti-bullshit take on the possibilities ahead, with honesty, anecdotes, and (for those who know me, of course) a bit of humor.", "code": "WSNBD9", "state": "confirmed", "created": "2025-12-29", "social_card_image": "/static/media/social/talks/WSNBD9.png", "speaker_names": "Sebastian Neubauer", "speakers": "\n### Sebastian Neubauer\n\nData scientist forever; Worked everywhere in Blue Yonder, messed with data science, built platforms, now exploring GenAI & AI agents. Known to always ask the question nobody else dared.\n", "domain_expertise": "None", "python_skill": "None", "track": "Autonomous Systems & AI Agents"}, {"title": "Before You Ship Your Agent: An Agent Builder\u2019s Primer on Jailbreaking Attacks", "abstract": "Before you ship an AI agent to production, you need to understand how it can be broken. Jailbreaking and prompt injection attacks are not edge cases\u2014they are an inevitable consequence of deploying real-world, action-taking AI systems.\r\n\r\nThis talk is a practical primer on the most common ways agents fail under adversarial pressure. We\u2019ll break down how jailbreaking and prompt injection attacks actually work, including techniques such as excessive agency, prompt leakage, and weaknesses in vector search and embeddings. We\u2019ll examine why popular AI guardrails consistently fail in practice, and offer little more than a false sense of protection.\r\n\r\nWe\u2019ll also address a common misconception: the absence of major AI security incidents does not mean systems are safe. Instead, it reflects limited deployment, constrained agency, and cautious rollout. As organizations adopt browser agents, autonomous tools, and systems that can take real-world actions, these vulnerabilities quickly become critical attack surfaces.\r\n\r\nThis talk focuses on what organizations should do instead: applying proven security principles\u2014least privilege, isolation, monitoring, and abuse modeling\u2014adapted to the unique properties of AI systems. Attendees will leave with a clear understanding of the real risks, why they matter today, and the concrete steps to take before shipping an AI agent into production.", "full_description": "AI agents are rapidly moving from demos and copilots into production systems that browse the web, call APIs, execute workflows, and take real\u2011world actions. As this transition happens, a critical truth is becoming unavoidable: any agent with meaningful capability will be attacked\u2014and most are easy to break. Jailbreaking and prompt injection attacks are not theoretical research topics or rare edge cases; they are an inevitable outcome of deploying autonomous, instruction\u2011following systems in adversarial environments.\r\n\r\nThis talk is a practical, engineering\u2011focused primer on how AI agents fail under real\u2011world pressure, and what organizations must understand before shipping an agent into production. Rather than focusing on sensational examples or hypothetical risks, we will examine the concrete mechanisms that attackers use today, why they work, and why many popular defenses provide little real protection.\r\n\r\nWe begin with a clear, accessible overview of jailbreaking and prompt injection attacks. Attendees will learn how attackers manipulate model instructions, context windows, and tool\u2011calling behavior to override intended safeguards. We\u2019ll cover both direct prompt injection (explicitly malicious instructions) and indirect prompt injection, where hostile content is embedded in webpages, documents, emails, or user\u2011generated data that agents are designed to consume. These attacks are especially dangerous because they exploit normal, expected behavior rather than software bugs.\r\n\r\nFrom there, we\u2019ll explore several recurring failure modes that appear across nearly all production agent architectures:\r\n\r\nExcessive agency: Agents are often given broader permissions and autonomy than necessary, turning minor instruction hijacks into high\u2011impact incidents.\r\nPrompt leakage: System prompts, policies, secrets, and internal instructions are frequently exposed or inferable, providing attackers with a roadmap for further exploitation.\r\nVector and embedding weaknesses: Retrieval\u2011augmented generation systems can be poisoned or manipulated, allowing malicious content to outrank trusted sources and influence agent decisions.\r\nTool and browser abuse: Agents that browse the web or execute actions are uniquely vulnerable to hostile environments intentionally crafted to manipulate them.\r\nA key focus of the talk is why AI guardrails don\u2019t work the way many teams expect. We\u2019ll examine common approaches\u2014prompt\u2011based restrictions, content filters, and policy\u2011layer defenses\u2014and explain why they are brittle, bypassable, and often fail silently. Rather than stopping attacks, these mechanisms frequently create a false sense of security that masks deeper architectural risks.\r\n\r\nWe\u2019ll also address a common misconception in the industry: \u201cIf these vulnerabilities are so serious, why haven\u2019t we seen major AI security incidents yet?\u201d The answer is not that systems are safe, but that most deployments are still constrained\u2014limited autonomy, limited blast radius, and cautious rollout. As organizations move toward browser agents, long\u2011running autonomous workflows, and systems with real operational authority, the conditions that have so far prevented large\u2011scale incidents will disappear. When that happens, these attack classes will move from curiosity to crisis.\r\n\r\nThe final section of the talk focuses on what actually works. Instead of recommending yet another AI security product or guardrail framework, we will outline practical, proven steps organizations can take today, grounded in decades of security engineering experience:\r\n\r\nApplying least privilege and minimizing agent capabilities\r\nIsolating tools, credentials, and execution environments\r\nDesigning for failure and containment, not perfect prevention\r\nMonitoring agent behavior for abuse patterns rather than policy violations\r\nPerforming threat modeling that treats prompts and context as untrusted input\r\nAttendees will leave with a clear mental model of how AI agents are attacked, why these attacks succeed, and how to reduce risk without relying on ineffective silver bullets. This talk is intended for engineers, security practitioners, and technical leaders building or deploying AI agents who want to understand the real risks\u2014and take responsible action\u2014before putting these systems into production.", "code": "X3KQMQ", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/X3KQMQ.png", "speaker_names": "Simonas \u010cerniauskas", "speakers": "\n### Simonas \u010cerniauskas\n\nDr.-Ing. Simonas \u010cerniauskas is the founder and CTO of tisix.io, specializing in developing practical LLM solutions for media and publishers. With a doctorate from RWTH Aachen and experience as a principal researcher at Research Center J\u00fclich, he combines deep technical expertise with hands-on implementation experience. His work focuses on multi-modal content generation and media processing. Drawing from his background in mechanical engineering, quality assurance and machine learning engineering, Simonas develops scalable AI solutions while maintaining a strong focus on quality assurance and risk management. He regularly shares insights through speaking engagements and technical publications, helping organizations navigate the complexities of AI implementation with practical, business-focused approaches.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Tidy Finance in Practice: How Clean Data Structures Expose Bad Investment Strategies", "abstract": "Many investment strategies look convincing because they performed well in the past, but these results are often easy to misread and do not always say much about how the strategy would work in the future. In many cases, strong backtest results come not from real skill or insight, but from hidden rules, unclear data choices, or unrealistic assumptions. In this talk, I show how Tidy Finance principles help make these issues visible and easier to examine. Using clear examples from Tidy Finance with Python, I demonstrate that once assumptions are made explicit, many impressive results no longer hold up.", "full_description": "Many investment strategies look great because they performed well in the past. However, it is often unclear why they work or whether they would still work in the future. Strong backtest results are frequently driven by hidden assumptions, unclear data handling, or unrealistic rules rather than real skill or insight.\r\n\r\nIn this talk, I show how Tidy Finance principles help people better understand what is actually happening inside a financial backtest. Tidy Finance has become a popular open-source teaching and learning platform for empirical financial research. Its core idea is simple: financial analyses should be built from clear, well-structured data that makes assumptions easy to see and results easy to reproduce. \r\n\r\nUsing explicit examples from Tidy Finance with Python during the talk, I go through a real backtesting workflow and show how it changes when assumptions are written down clearly instead of being hidden inside the code. I demonstrate how small, often overlooked choices can have a large impact on results, and how these effects become visible when the analysis is structured cleanly. The focus is on learning how to read and question backtests, not on presenting new models or strategies.", "code": "XGL37G", "state": "confirmed", "created": "2025-12-21", "social_card_image": "/static/media/social/talks/XGL37G.png", "speaker_names": "Christoph Frey", "speakers": "\n### Christoph Frey\n\nChristoph Frey is a Quantitative Researcher and Portfolio Manager at a family office in Hamburg and Research Fellow at the Centre for Financial Econometrics, Asset Markets and Macroeconomic Policy at Lancaster University. Before this, he was the leading quantitative researcher for systematic multi-asset strategies at Berenberg Bank and worked as an Assistant Professor at the Erasmus Universiteit Rotterdam. Christoph published research on Bayesian Econometrics and specializes in financial econometrics and portfolio optimization problems.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Others"}, {"title": "Destructive Testing: 10 Practical Ways to Expose Hidden Application Risks", "abstract": "Modern applications rarely fail in obvious ways. Instead, they break at the edges: unexpected inputs, race conditions, misused APIs, and assumptions nobody realized they were making. This talk presents ten practical and repeatable ways to intentionally break an application, using a QA mindset with a strong Python focus.\r\n\r\nThe session is designed to help QAs sharpen their investigative approach and move beyond happy-path testing, while giving developers concrete insight into where real-world failures often originate. Each \u201cway to break an application\u201d highlights a common risk area such as data handling, state management, timing, configuration, or integration boundaries.\r\n\r\nAttendees will learn how to think more destructively (in a productive way), design better tests, and recognize fragile design decisions earlier. The goal is not to assign blame, but to improve collaboration and software quality by understanding how systems actually fail in practice.", "full_description": "Quality assurance is not about confirming that software works \u2014 it is about discovering how it fails. This talk explores ten concrete ways to break an application on purpose, based on real-world testing patterns and common failure modes seen in modern software systems.\r\n\r\nThe focus is on practical thinking, not theory. While Python is used as the primary example language for test automation and experimentation, the concepts apply to any technology stack. The session is relevant for QAs, test engineers, and developers who want to build more resilient systems and improve cross-discipline collaboration.\r\n\r\nGoals of the Talk\r\n* Improve destructive testing and exploratory thinking for QAs\r\n* Help developers understand common blind spots in application design\r\n* Demonstrate how Python can be used effectively to probe system weaknesses\r\n* Encourage a shared quality mindset across roles", "code": "YKQ33N", "state": "confirmed", "created": "2025-12-19", "social_card_image": "/static/media/social/talks/YKQ33N.png", "speaker_names": "Pascal Puchtler", "speakers": "\n### Pascal Puchtler\n\nAn experienced QA engineer and software developer with a strong focus on Python-based testing and test automation. Specialized in breaking applications through exploratory, risk-driven, and destructive testing approaches. With several years of experience working on complex software systems, the focus is on uncovering hidden failure modes, improving test strategies, and helping teams build more resilient applications. Passionate about bridging the gap between QA and development by sharing practical insights into how and why software fails in real-world scenarios.\n", "domain_expertise": "Intermediate", "python_skill": "Novice", "track": "Programming & Software Engineering & Testing"}, {"title": "Demystifying Agentic AI Using Small Language Models", "abstract": "The AI world is buzzing with claims about \u201cagentic intelligence\u201d and autonomous reasoning. Behind the hype, however, a quieter shift is taking place: Small Language Models (SLMs) are proving capable of many reasoning tasks once assumed to require massive LLMs. When paired with fresh business data from modern lakehouses and accessed through tool calling, these models can power surprisingly capable agents.\r\n\r\nIn this talk, we cut through the noise around \u201cagents\u201d and examine what actually works today. You\u2019ll see how compact models such as Phi-2 or xLAM-2 can reason and invoke tools effectively, and how to run them on development laptops or modest clusters for fast iteration. \r\nBy grounding agents in business facts stored in Iceberg tables, hallucinations are reduced, while Iceberg\u2019s read scalability enables thousands of agents to operate in parallel on a shared source of truth.\r\nAttendees will leave with a practical understanding of data agent architectures, SLM capabilities, Iceberg integration, and a realistic path to deploying useful data agents - without a GPU farm.", "full_description": "The Agentic Buzz  -  What\u2019s Real, What\u2019s Marketing\r\n\r\n- The explosion of \u201cagentic\u201d frameworks and the confusion it causes\r\n- What an agent really is at its core: planning, acting, and reasoning\r\n\r\nAnatomy of an Agent\r\n\r\n- The three basic functions: task decomposition, tool use, and code synthesis\r\n- How frameworks like LangChain and Python make it easy to chain these together\r\n\r\n\r\nWhy Small Models Are Catching Up\r\n- Review of research from NVIDIA and Georgia Tech\r\n- Benchmarks showing SLMs matching or exceeding performance of larger LLMs\r\n- Cost, latency, and deployability tradeoffs\r\n\r\nHands-On Demo: Building and Running an Agent on a Laptop\r\n\r\n- Using LangChain and Python to orchestrate reasoning, tool calls, and code execution\r\n- Example workflow: \u201cPlan a dataset cleanup pipeline\u201d using an SLM\r\n- Observing resource use, latency, and performance in real time\r\n\r\n\r\nKey Takeaways and Open Research Directions\r\n\r\n- Opportunities for local and edge deployments\r\n- The emerging role of SLMs in allowing everyone to experiment with agents\r\n- Future questions: scaling reasoning vs. scaling models", "code": "YZM8TA", "state": "confirmed", "created": "2026-01-09", "social_card_image": "/static/media/social/talks/YZM8TA.png", "speaker_names": "Serhii Sokolenko", "speakers": "\n### Serhii Sokolenko\n\nSerhii Sokolenko is a co-founder of Tower, a Pythonic platform for data flows and agents running on top of open analytical storage. Prior to founding Tower, Serhii worked at Databricks, Snowflake and Google on data processing and databases.\n", "domain_expertise": "Novice", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Learnings Building DevOps as a Software Engineer", "abstract": "When I joined my current company as a software engineer, I encountered a blank slate: no CI/CD pipelines, no deployment infrastructure, barely any monitoring\u2014in short, no software infrastructure at all. This talk shares the key learnings from building a DevOps environment from the ground up. I\u2019ll walk through the essentials: which foundations were laid first, what tools and practices made the difference, and how automation became a daily habit. Through real-world examples, I will demonstrate how pragmatic and incremental steps can jump-start productivity, reduce manual toil, and help teams avoid common pitfalls.", "full_description": "What do you do when you join a company as a software engineer, and there\u2019s zero DevOps in place\u2014but product delivery can\u2019t wait? In this talk, I\u2019ll share firsthand insights from building core DevOps infrastructure from the ground up, while simultaneously delivering the first software products under tight deadlines.\r\nI\u2019ll outline the key priorities and quick wins that enabled rapid, reliable releases\u2014such as setting up basic CI/CD pipelines, introducing automated tests, and using containerization for reproducible deployments. Rather than aiming for \u201cperfect\u201d infrastructure from day one, I\u2019ll show how to build DevOps foundations incrementally and pragmatically, integrating automation step by step as part of everyday development work.\r\nThrough practical examples, I\u2019ll discuss how to achieve reliability without losing agility, how to avoid common pitfalls in \u201cbuild as you go\u201d DevOps, and how to balance product delivery with infrastructure improvements. Attendees will leave with actionable tips on how to bootstrap DevOps quickly, so teams can ship software confidently\u2014even when starting from scratch.", "code": "ZESFRG", "state": "confirmed", "created": "2026-01-11", "social_card_image": "/static/media/social/talks/ZESFRG.png", "speaker_names": "Gaweng Tan", "speakers": "\n### Gaweng Tan\n\nI am a Software Architect at a manufacturing company, specializing in building reliable software products and establishing solid DevOps practices\u2014often from the ground up. My ongoing work with Python spans automation, scripting, and infrastructure, helping me to quickly deliver solutions even in \u201cgreenfield\u201d situations.\r\nCuriosity drives much of what I do\u2014I\u2019m always eager to understand how things work and love tackling technical challenges through hands-on experimentation. When I\u2019m not engineering or optimizing workflows, you\u2019ll find me exploring new recipes in the kitchen, running small coding side projects, or discovering the world in my own sometimes-cautious, adventure-seeking way.\r\nOutside of work, I enjoy deep conversations about technology and society, and occasionally share my thoughts and experiments on my personal blog. I like to think individuality and curiosity matter as much in tech as they do in everyday life.\n", "domain_expertise": "Novice", "python_skill": "Novice", "track": "MLOps & DevOps"}, {"title": "The Frugal AI Architect: Building Cost-Efficient Agentic Systems in Python", "abstract": "AI systems face six dimensions of scale: data, model, user, operational, infrastructure, and cost. While most talks focus on infrastructure and user scale, this session tackles the hardest dimension: Cost Scale, maintaining predictable and optimized compute costs as usage grows. Drawing from production experience, I'll demonstrate how I achieved 70% cost reduction through three architectural patterns: semantic caching that eliminates repeat LLM calls, model cascading using hybrid architecture (open-source SLMs + API-based LLMs), and optimized conversation state management. \r\nYou'll see real production metrics, honest failure post-mortems, and the critical trade-offs between batch vs. real-time inference, monolithic vs. microservices, and open-source vs. API models. Learn how architectural choices directly impact unit economics, and why scaling AI is fundamentally different from scaling traditional software. \r\nWalk away with reusable Python patterns (FastAPI, Pydantic, Redis, DynamoDB) and decision frameworks for building economically sustainable agentic AI systems.", "full_description": "The Cost Scale Problem\r\nAI systems must scale across six dimensions: data, model, user, operational, infrastructure, and cost. While traditional software scales by adding servers, AI systems have non-linear cost curves that can destroy unit economics overnight.\r\nThis talk focuses on Cost Scale, maintaining predictable compute costs as usage grows. \r\nWhat You'll Learn (40 min + Q&A)\r\nWhy AI Scaling Is Different (5 min)\r\n1. Traditional vs. AI systems: learning, degradation, token-based pricing\r\n2. The six dimensions of the scale framework\r\n3. Real economics: \u20ac50/month \u2192 \u20ac15,000/month \u2192 \u20ac4,500/month optimized\r\n\r\nArchitecture Foundation (4 min)\r\n1. Monolithic vs. microservices for AI\r\n2. Event-driven architecture choice\r\n3. Batch vs. real-time inference trade-offs\r\n\r\nPattern 1: Semantic Caching (12 min)\r\n1. Vector-based query matching with Redis\r\n2. Architecture walkthrough and code examples\r\n3. Production metrics: 72% hit rate, $0.001 vs. $0.015 per query\r\n4. Failures: threshold tuning (0.95 \u2192 0.85), semantic false positives\r\n\r\nPattern 2: Model Cascading - Hybrid Architecture (12 min)\r\n1. Open-source (Llama 3.2) + API (Claude/GPT-4) strategy\r\n2. Semantic router implementation with Pydantic\r\n3. Code examples: routing logic, confidence scoring, fallbacks\r\n4. Production metrics: 40% cost reduction through intelligent routing\r\n\r\nPattern 3: Conversation State (8 min)\r\n1. Multi-tier storage: Redis (hot) + DynamoDB (warm/cold)\r\n2. Context window management and summarisation\r\n3. Production metrics: storage vs. token cost trade-offs\r\n\r\nIntegration & Monitoring (3 min)\r\n1. Full system architecture\r\n2. FinOps dashboards and cost alerting\r\n3. Final results: 70% reduction, sub-2s latency, 0.3% error rate\r\n\r\nTakeaways (3 min)\r\n1. Decision frameworks for each pattern\r\n2. Open-source vs. API vs. hybrid criteria\r\n3. GitHub repo with reference implementations\r\n\r\nWho Should Attend\r\nEngineers moving AI to production, platform teams managing costs, managers evaluating AI feasibility. Basic Python and LLM knowledge helpful.\r\n\r\nTechnologies\r\nPython, FastAPI, Pydantic, Redis, DynamoDB, sentence-transformers, Llama 3.2, Claude/GPT-4.\r\n\r\nWhy It Matters\r\nProduction-tested patterns with real metrics, honest failures, and actionable frameworks for building economically sustainable AI systems.", "code": "ZFWASB", "state": "confirmed", "created": "2025-12-25", "social_card_image": "/static/media/social/talks/ZFWASB.png", "speaker_names": "Daniel Akhabue", "speakers": "\n### Daniel Akhabue\n\nDaniel is an AI/ML Engineer and Cloud Solutions Architect with hands-on experience in building AI-driven solutions across diverse domains, including data science, machine learning and generative AI. He has led and contributed to impactful projects across a range of industries such as EdTech, Assistive Technology, HealthTech, FinTech, and Supply Chain Technology, consistently delivering solutions that address real-world challenges.\r\n\r\nAs a community champion at Data Scientists Network (DSN), Daniel has won multiple Data Science and AI hackathons, and has led AI communities in Nigeria, driving innovation and knowledge sharing. He is also a technical writer, sharing insights and expertise through articles published on leading online platforms in the Data Science and AI space.\r\n\r\nIn his spare time, Daniel enjoys reading and reviewing research papers, as well as playing chess. He is driven by a deep passion to build solutions that create meaningful societal impact and foster socio-economic development\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Autonomous Systems & AI Agents"}, {"title": "Architecture Under Constraints: Designing Systems That Still Evolve", "abstract": "Most systems are built under constraints: legacy code, regulation, organizational boundaries, and long-term accountability. This talk explores how Staff+ engineers and tech leads can make sound architectural decisions when \u201cperfect\u201d isn\u2019t an option. Focusing on platforms and tooling, it presents practical ways to identify real constraints, preserve flexibility, avoid over-engineering, and communicate trade-offs that hold up over time - technically and organizationally.", "full_description": "Modern systems rarely exist in ideal conditions. They grow over years, integrate with legacy services, operate under regulatory or security constraints, and are shaped by organizational boundaries just as much as by code. Yet architectural guidance often assumes greenfield projects and unlimited freedom.\r\n\r\nThis talk focuses on architectural decision-making under real-world constraints, using  systems as the primary lens. Rather than discussing specific frameworks or patterns, it presents a practical way of thinking about architecture when trade-offs are unavoidable and decisions must hold up over time.\r\n\r\nDrawing from experience in regulated production environments, we will explore how to distinguish true constraints from accidental ones, how to think in terms of long-lived capabilities rather than short-lived components, and how to preserve optionality even when systems appear \u201clocked in.\u201d Examples will touch on Python-heavy platforms such as backend services, internal tools, data pipelines, and automation systems.\r\n\r\nThe session also addresses the human side of architecture: how Staff+ engineers and technical leaders communicate trade-offs, document decisions in a way that survives team changes, and align engineering, product, and compliance perspectives without over-engineering.\r\n\r\nThis talk is aimed at experienced engineers, tech leads, and engineering leaders who want to design systems that can evolve - even when constraints dominate the problem space.", "code": "ZLRFR9", "state": "confirmed", "created": "2025-12-30", "social_card_image": "/static/media/social/talks/ZLRFR9.png", "speaker_names": "Eduard Thamm", "speakers": "\n### Eduard Thamm\n\nEduard is a technical leader with a background in distributed systems, platform engineering, and security. He works as a Lead Engineer in regulated environments, designing and operating Kubernetes-based platforms where reliability, compliance, and developer experience must coexist. His work focuses on architecture under real-world constraints, supply-chain security, and building systems that remain adaptable over time. Eduard regularly advises engineering leaders on technical strategy and decision-making at scale, bridging hands-on experience with long-term architectural thinking.\n", "domain_expertise": "Advanced", "python_skill": "Intermediate", "track": "Programming & Software Engineering & Testing"}, {"title": "How to Search Through 800 Billion Records in Real Time", "abstract": "Large-scale distributed systems are inherently complex: hundreds of asynchronous services continuously emit updates, retries, corrections, and partial state. Turning that constant stream of noisy events into something that can be comprehensively searched through in real time, at the scale of hundreds of billions of records per day, can be harder that it looks.\r\n\r\nWhether you're building large-scale data systems, fighting real-time processing bottlenecks, or simply enjoy Kafka horror stories, you'll leave with practical ideas, a few scars, and hopefully fewer retries.", "full_description": "- intro (8 min)\r\n  - a very quick summary of what our company does in a few words, necessary to set up the background for the technical challenge this talk is solving (3 min)\r\n  - a very high-level overview of our distributed systems architecture (5 min)\r\n\r\n- reducing the number of processed messages (5 min)\r\n  - basic Kafka consumer loop (1 min)\r\n  - message deduplication by converting a list of keys into a set (1 min)\r\n  - skipping recently processed keys by keeping a TTL-based buffer (3 min)\r\n\r\n- avoiding data loss (8 min)\r\n  - process keys after evicting from buffer instead of when adding to it (3 min)\r\n  - when is it safe to acknowledge a message (5 min)\r\n\r\n- keeping ETL service healthy (5 min)\r\n  - avoiding Kafka timeouts\r\n\r\n- wrap up (4 min)\r\n  - final architecture (2 min)\r\n  - performance results (2 min)", "code": "ZYUJH3", "state": "confirmed", "created": "2025-12-18", "social_card_image": "/static/media/social/talks/ZYUJH3.png", "speaker_names": "Mirano Tuk, Filip Bacic", "speakers": "\n### Mirano Tuk\n\nPrincipal Software Engineer at ReversingLabs, working on large-scale distributed systems and data-intensive architectures.\r\n\r\nI design and operate high-throughput, real-time pipelines, with an emphasis on reliability, observability, and performance in real-world conditions, and a practical approach to engineering trade-offs and system failures.\n\n### Filip Bacic\n\nSoftware Development Manager at ReversingLabs, leading teams responsible for large-scale data processing, data quality, and technical writing. Specialized in turning complex systems into something that works, produces correct results, and is documented well enough that someone else can understand it, usually in that order.\n", "domain_expertise": "Intermediate", "python_skill": "Intermediate", "track": "Data Handling & Data Engineering"}]}