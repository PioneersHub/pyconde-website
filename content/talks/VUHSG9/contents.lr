title: Letting AI Move: Robotics Demos Powered by Python
---
created: 2025-12-19
---
code: VUHSG9
---
speaker_names: Larissa Haas, Annika Herbert
---
speakers:


### Larissa Haas



### Annika Herbert

Annika Herbert is a Solution Architect in the AI & Data Unit at sovanta, working on data-driven and AI-powered solutions. With a background in Data Science, she enjoys making AI more approachable, tangible, and easy to explain, especially to non-technical audiences. Coming from a Python developer’s perspective, she likes to explore new fields through hands-on experimentation and learning by doing. Outside of work, she enjoys dancing, concerts, and baking chocolate-filled treats.

---
abstract:

AI is sometimes hard to explain, especially for people outside of tech. With robots, AI becomes visible and tangible. In this talk we want to show how we can use Python and the huggingface reachy mini as an example to make AI more concrete, interactive, and engaging for beginners and non-experts.
---
full_description:

Artificial intelligence can be difficult to explain, especially to people outside of tech. We often rely on slides, diagrams, or on-screen demos, but the real impact does not always stick. For people encountering AI for the first time—such as students or non-technical audiences—AI terminology and concepts can remain abstract and disconnected from real-world experience.

Robots can help change that. When AI controls a physical system, its behavior becomes visible, tangible, and easier to reason about. In this talk, we explore how Python and playful robotics experiments can be used to make AI more concrete, interactive, and engaging. Using the Hugging Face Reachy Mini robot as a case study, we show how physical interaction can turn abstract AI concepts into intuitive, memorable experiences.

The perspective of this talk is intentionally non-traditional: we started with no prior knowledge of robotics or mechanics and approached the problem purely from a Python developer’s point of view. This journey strongly shapes the talk. Rather than focusing on advanced robotics engineering, the emphasis is on accessibility, experimentation, and learning by doing. The goal is to show that robotics can be an approachable medium for explaining AI, even for people without a hardware or engineering background.

During the talk, we walk through basic building blocks such as movement, gestures, and simple interaction patterns, and show how AI-driven behavior can be layered on top of them using familiar Python tools. We share examples from real experiments and demos, including what worked well, what failed, and what we learned from unexpected behavior in live settings.

Importantly, this is not a product demo or a hardware-specific tutorial. While Reachy Mini is used as a concrete example, the focus is on transferable ideas and design patterns:

How physical interaction changes the way people perceive AI

How Python lowers the barrier to experimenting with robotics

How to design demos that invite curiosity rather than intimidation

How to make AI systems easier to explain in educational and outreach contexts

Attendees do not need access to a robot to benefit from this talk. The lessons and patterns discussed can be applied to a wide range of settings, including classrooms, workshops, meetups, and public demonstrations.

This talk is aimed at Python beginners and intermediate developers, especially educators and anyone who regularly needs to explain or demonstrate AI to others. Attendees will leave with new ideas, inspiration, and practical approaches for making AI more tangible, engaging, and human-centered.
---
room: 
---
day: 
---
start_time: 
---
track: Embedded Systems & Robotics
---
python_skill: Novice
---
domain_expertise: Novice
---
social_card_image: /static/media/social/talks/VUHSG9.png

