title: OpenAI o1/o3: the New Scaling Laws for LLMs that can reason
---
created: 2024-12-22
---
code: F77EP9
---
speaker_names: Luca Baggi
---
abstract: The renowned Chinchilla paper changed the way we train Large Language Models (LLMs). The authors - including the current Mistral CEO - derived the so-called **scaling laws** to maximise your model performance under a compute budget, balancing the number of parameters and data *during training*.

However, with o1, OpenAI ushered a new era of LLMs: reasoning capabilities. This new breed of models broadened the concept of scaling laws, shifting focus from **train-time** to **test-time** (or inference-time) compute. How do these models work? What do we think their architectures look like, and what data do we use to train them? And finally - and perhaps more importantly: how expensive can they get, and what can we use them for?
