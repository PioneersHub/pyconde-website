title: Guardians of the Code: Safeguarding Machine Learning Models in a Climate Tech World
---
created: 2024-12-03
---
code: CVMPVG
---
speaker_names: Doreen Sacker
---
speakers:


### Doreen Sacker

I'm an MLOps Engineer from Berlin working at the start-up 1KOMMA5°, and I'm part of the women's tech podcast Unmute IT. I aim to empower underrepresented groups to have a say in shaping the algorithms that impact our world today. Also, I’m always on the lookout for the best coffee shop in town ☕️

---
abstract:

LLMs, Machine learning and AI are everywhere, yet their security is often overlooked, leaving your systems vulnerable to serious attacks. What happens when someone tampers with your model’s input, poisons your training data, or steals your model?

In this talk, I’ll explore these risks through the lens of the OWASP Machine Learning Security Top 10 using relatable, real-world examples from the climate tech world. I’ll explain how these attacks happen, their impact, and why they matter to you as a Python developer, data scientist, or data engineer.

You’ll learn practical ways to defend your models and pipelines, ensuring they’re robust against adversarial forces. Bridging theory and practice, you'll leave equipped with insights and strategies to secure your machine learning systems, whether you’re training models or deploying them in production. By the end, you’ll have a solid understanding of the risks, a toolkit of best practices, and maybe even a new perspective on how important security is everywhere.
---
full_description:

Machine learning is applied to a variety of challenges in climate tech, from optimising renewable energy to forecasting energy demands or predicting solar production. We rely more on these models, but we often forget a critical piece: their security. What happens if someone tampers with your model’s inputs, poisons your training data, or sneaks malicious code into an open-source package you’re using? These attacks can throw off predictions and disrupt energy systems or even the grid itself.

In this talk, I’ll walk you through the OWASP Machine Learning Security Top 10, using real-world examples from climate tech to show how these attacks can happen. I'll show you cases like manipulating energy consumption forecasts, poisoning datasets, or sneaking malware into open-source libraries used for climate modelling. It’s not just a hypothetical threat, these risks are real and the consequences can be serious.

I’ll also share practical solutions you can use as a Python developer, data scientist, or data engineer to protect your models and systems. I’ll talk about securing your ML supply chain, validating data, and monitoring your pipelines for suspicious activity. You'll leave with strategies to defend your work so you can build systems that are not only smart but also safe and reliable.

Why does this matter? Because in climate tech, the stakes are incredibly high. The predictions we make and the systems we build influence the grid, energy policies, resource allocation, and consumers trust.

During the talk, we'll cover:

- How attacks on machine learning models can disrupt climate tech applications.
- Examples of adversarial attacks, poisoned datasets, and supply chain vulnerabilities in renewable energy systems.
- Practical steps to protect your machine learning pipelines.
- Why security should be at the core of any ML project, especially in mission-critical fields like climate tech.

Outline of the Talk:

1. Why Security in Climate Tech Machine Learning Matters
    - How machine learning is powering renewable energy and climate solutions.
    - What can go wrong when systems are vulnerable.
2. Breaking Down the OWASP ML Security Top 10
    - Input manipulation: How attackers trick models with tampered data.
    - Data poisoning: Real-life example of skewing optimization models with bad data.
    - Supply chain attacks: How a hacked library could disrupt energy demand predictions.
3. Real-World Impact of Attacks
    - Manipulated energy consumption forecasts causing grid instability.
    - Corrupted solar panel efficiency datasets leading to poor resource allocation.
4. How to Protect Your Models
    - How to spot tampered inputs.
    - Data validation, cleaning and checking datasets.
    - Best practices for safe use of open-source libraries.
    - Monitoring and auditing: Setting up checks for unusual activity in your pipelines.

Key Takeaways

- Recap of risks and defences.
- Practical steps you can take today to secure your ML systems.
- A call to prioritize security as a core part of building trustworthy ML.

Climate tech is one of the most exciting and meaningful areas to work in. The systems we’re building have the potential to shape a more sustainable future. But if we don’t make security a priority, we risk undermining the customer's trust. This talk will give you the tools and confidence to keep your machine learning models safe and ensure they’re as reliable and impactful as they need to be.
#ThereIsNoPlanetB
---
room: Hassium
---
day: Friday
---
start_time: 11:35
---
track: MLOps & DevOps
---
python_skill: 
---
domain_expertise: 
---
social_card_image: /static/media/social/talks/CVMPVG.png

