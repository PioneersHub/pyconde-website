title: Bayesian Neural Networks for Beginners
---
created: 2024-12-18
---
code: JYQANY
---
speaker_names: Wenting Jiang
---
abstract:

Bayesian Neural Networks (BNNs) enhance traditional neural networks by providing uncertainty estimates in predictions, making them ideal for applications where confidence quantification is crucial. This talk offers a conceptual overview of BNNs, with minimal mathematical detail, and concludes with a practical demo using a simple dataset. This talk is beginner friendly.
---
full_description:

- Why Bayesian Neural Networks?
Bayesian Neural Networks (BNNs) has a unique advantage over traditional neural networks: they provide uncertainty estimates in their predictions. While other techniques can approximate uncertainty, the Bayesian framework naturally incorporates it, providing confidence interval in model outputs. This is important for applications where understanding model uncertainty is just as important as making accurate predictions.

- Conceptual Overview
In the Bayesian paradigm, the neural network's parameters (weights and biases) are treated as random variables, rather than fixed values. To generate predictive distributions, BNNs assume these parameters follow some probability distributions. Instead of sampling from the posterior distribution (which is computationally expensive), variational inference is used for scalability. Here, an approximate distribution (usually simpler) is optimized by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximation. At inference time, the BNN behaves as an ensemble of models, with weights and biases sampled from the learned distributions. This enables the generation of multiple outputs for the same input, from which confidence intervals can be computed.

- How It Works - An Example
We will walk through a short demo where a BNN is applied to a dataset.
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Machine Learning & Deep Learning & Statistics
