title: Feature or Preprocessing Step? How to Correctly Set a Baseline in NLP Classification Tasks
---
created: 2024-11-28
---
code: UVSVCY
---
speaker_names: Dr. Lisa Andreevna Chalaguine
---
abstract: In the field of Natural Language Processing (NLP), establishing a robust baseline is critical for the reliable evaluation and comparison of models. However, many NLP studies suffer from sloppy baseline models and improper preprocessing techniques, leading to flawed results. This presentation addresses key issues in setting effective baselines, focusing on the common pitfalls in research, such as the misuse of preprocessing steps and the confusion between preprocessing and feature engineering. The session will define the roles of preprocessing and features, provide a step-by-step guide for setting baselines with traditional machine learning models, and discuss the importance of hyperparameter tuning. We will also explore the fair comparison of traditional and advanced models, offering insights into appropriate preprocessing techniques for both. By the end, attendees will understand best practices for preprocessing, feature engineering, and baseline setting, enabling them to conduct more reliable and comparable NLP research. This talk is aimed at researchers, data scientists, and practitioners in NLP and machine learning looking to enhance their methodological rigor.
