title: Feature or Preprocessing Step? How to Correctly Set a Baseline in NLP Classification Tasks
---
created: 2024-11-28
---
code: UVSVCY
---
speaker_names: Dr. Lisa Andreevna Chalaguine
---
abstract:

In the field of Natural Language Processing (NLP), establishing a robust baseline is critical for the reliable evaluation and comparison of models. However, many NLP studies suffer from sloppy baseline models and improper preprocessing techniques, leading to flawed results. This presentation addresses key issues in setting effective baselines, focusing on the common pitfalls in research, such as the misuse of preprocessing steps and the confusion between preprocessing and feature engineering. The session will define the roles of preprocessing and features, provide a step-by-step guide for setting baselines with traditional machine learning models, and discuss the importance of hyperparameter tuning. We will also explore the fair comparison of traditional and advanced models, offering insights into appropriate preprocessing techniques for both. By the end, attendees will understand best practices for preprocessing, feature engineering, and baseline setting, enabling them to conduct more reliable and comparable NLP research. This talk is aimed at researchers, data scientists, and practitioners in NLP and machine learning looking to enhance their methodological rigor.
---
full_description:

OBJECTIVES:

- Identify Common Issues in Baseline Models: Highlight the prevalent issues found in numerous NLP papers, such as sloppy baseline models and the misuse of preprocessing techniques across different model types.
- Distinguish Between Preprocessing and Feature Engineering: Define what constitutes a preprocessing step versus a feature, and explain their roles in setting a baseline.
- Establishing a Robust Baseline: Provide a step-by-step guide on how to correctly set a baseline for traditional ML models and compare it to more sophisticated models.


OUTLINE:

1. Introduction
- Importance of a robust baseline in NLP classification tasks.
- Overview of common pitfalls in current research.

2. Identifying Sloppy Baselines
- Examples of inadequate preprocessing for traditional ML models.
- Consequences of improper preprocessing: deleting or generating features, inappropriate tokenisation, and not leveraging Scikit-learn capabilities.

3. Distinguishing Preprocessing Steps from Features
- Acceptable preprocessing steps for a baseline model: lower-casing, appropriate tokenisation, deleting single-appearance tokens.
- The necessity of experimenting with preprocessing techniques.

4. Setting a Baseline: A Detailed Approach
- Data splitting: training, validation, and test sets.
- Baseline with traditional ML models: using Naive Bayes with basic preprocessing.
- The importance of hyperparameter tuning for vectorisers.

5. Algorithm Selection and Optimisation
- Pipeline for algorithm selection and feature engineering.
- Examples of preprocessing and feature engineering techniques: normalisation, handling noise, replacing slang, stemming, etc.

6. Comparing Traditional and Advanced Models
 - Fair comparison to advanced models (neural networks, transformers).
 - Appropriate preprocessing for advanced models: minimal preprocessing (i.e. only lowercasing and deleting noise), avoiding lemmatization/stemming, appropriate tokenisation.

7. Tips and Best Practices for Advanced Models
- Handling unknown words and dummy tokens.
- Dealing with non-generalisable information (emails, paths, names).
- Enhancing embeddings with additional data.

8. Conclusion
- Recap of best practices for setting a robust baseline.
- Encouragement to adopt these practices for more reliable and comparable NLP research outcomes.


EXPECTED OUTCOMES:

Attendees will gain a comprehensive understanding of the importance of setting a robust baseline in NLP classification tasks. They will learn practical techniques to improve their preprocessing steps and distinguish between preprocessing and feature engineering. This knowledge will help them conduct more reliable experiments and produce more credible results in their research.


TARGET AUDIENCE:

Researchers, data scientists, and practitioners in the field of NLP and machine learning who are involved in model development and evaluation. This talk is especially relevant for those looking to improve their methodological rigor in establishing baselines and preprocessing practices.
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Natural Language Processing & Audio (incl. Generative AI NLP)
