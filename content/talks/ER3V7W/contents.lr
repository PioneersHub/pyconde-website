title: Bias Meets Bayes: A Bayesian Perspective on Improving Model Fairness
---
created: 2024-12-20
---
code: ER3V7W
---
speaker_names: Vince Nelidov
---
speakers:


### Vince Nelidov

Vince Nelidov is a Staff Data Science at Blue Yonder with diverse consulting experience in the data domain in a variety of industries from energy sector and banking to skincare and agriculture. Throughout his years in the data world, Vince has been combining advanced data science with business insights to make data work with an impact. He aspires to see far beyond what is on the surface and get to the essence of the problems, discovering robust and scalable long-term solutions rather than temporary fixes.

Vince is passionate about sharing his knowledge and insights, believing that Data literacy should not be a privilege of a few. And his goal is to be there to make this a reality. Making the intricacies of data science intelligible and uncovering the regularities hiding in the data is a major source of inspiration for Vince. With this goal in mind, he combines his years of experience in consulting with his background in statistics, research and teaching to make this knowledge accessible to businesses and individuals in need.

---
abstract:

Bias in machine learning models remains a pressing issue, often disproportionately affecting the most vulnerable groups in society. This talk introduces a Bayesian perspective to effectively tackle these challenges, focusing on improving fairness by modeling and addressing bias directly.
You will learn about the interplay between uncertainty, equity, and predictive accuracy, while gaining actionable insights to improve fairness in diverse applications. Using a practical example of a risk-scoring model trained on data with underrepresented minority groups, I will showcase how Bayesian methods compare to traditional techniques, demonstrating their unique potential to mitigate bias while maintaining performance.
---
full_description:

Machine learning models often perpetuate biases that exacerbate societal inequities, particularly for vulnerable groups. As machine learning increasingly shapes critical decisions, addressing these biases is more important than ever. In this talk, I will explain how Bayesian methods offer a principled and effective approach to improving fairness by directly addressing bias and incorporating uncertainty into machine learning models. 

The talk will cover:

1.	Theoretical Foundations: I will start by exploring the connection between Bayesian statistics, fairness, and accuracy, with a focus on why uncertainty is a crucial factor in fairness interventions.
2.	Practical Example: Using a risk-scoring model trained on a dataset with underrepresented minority groups, I will demonstrate how Bayesian methods compare to traditional fairness techniques. This example will illustrate their ability to not only mitigate bias but also adapt to complex, real-world data distributions while maintaining predictive accuracy.
3.	Key Insights and Applications: Finally, I will provide actionable takeaways on incorporating Bayesian thinking into existing workflows, enabling more equitable and robust outcomes across diverse applications. 

This talk is designed to be accessible to a broad audience. While minimal familiarity with machine learning concepts and fairness principles is recommended, no advanced knowledge of statistics is required. Attendees will leave with practical tools, code examples, and insights to address bias effectively in real-world scenarios, empowering them to promote fairness in their own projects and organizations.
---
room: Europium2
---
day: Thursday
---
start_time: 11:35
---
track: Machine Learning & Deep Learning & Statistics
---
python_skill: 
---
domain_expertise: 
---
social_card_image: /static/media/social/talks/ER3V7W.png

