title: Building Trustworthy Models: Practical Statistics Every Developer Should Know
---
created: 2025-12-04
---
code: 7D8VRW
---
speaker_names: Megan Robertson
---
speakers:


### Megan Robertson

Megan Robertson is the founder of MegRob Data Science and Analytics, a data science and machine learning consulting firm specializing in helping organizations translate complex technical capabilities into real business impact. Prior to starting her own venture, she spent six years at a Fortune 500 company working at the intersection of technical and non-technical teams, where she designed and delivered ML and analytics solutions to support strategic business goals. She is deeply committed to expanding access to the field and is passionate about helping individuals from underrepresented communities break into data science and machine learning.

When not running her business Megan enjoys skiing and climbing in her home state’s mountains, traveling to new countries, and photographing the places she explores.

---
abstract:

Are you looking to transition into a machine learning engineer role, or are you currently building systems to support machine learning models but don’t understand how they work? This talk covers the practical statistics that can get you a long way in machine learning. By walking through a real world use case it covers core concepts around model evaluation such as distributions, bias and variance, overfitting, evaluation metrics and more. 

Audience members can expect to learn statistical thinking through intuitive explanations and examples that will empower them to avoid common mistakes, diagnose model issues and build machine learning systems you can trust. They will walk away with a basic tool kit to understand and improve the models your stakeholders rely on.
---
full_description:

While the talk summarizes many concepts present in introductory level statistics courses, it is not meant to be a rigorous presentation of such concepts. The intent is to help developers understand some of the basic terms, metrics and tests required to select a machine learning model, understand its performance and put it into a production environment responsibly. They will walk away with a basic tool kit and understanding of these concepts that will serve as a foundation to further studies. 


I.	Use Case Introduction 
* Example based on speaker’s experience working  in industry
* Highlight the impact of this work and its effects on stakeholders/business 

II.	Model Evaluation 
* What is the bias/variance tradeoff and why do we care?
* Why can’t we always rely on a simple accuracy metric to evaluate performance?
* What other metrics can be used to evaluate performance? 
* What is the best metric for the presented use case? 

III.  Model Monitoring
* How can we use statistics to identify data issues? 
* How much model drift is too much? 

IV.	Further Resources and Recommendations
* Suggested blogs, books, sites, etc to study these concepts 
* Foundational models and statistical concepts needed to succeed in MLE
* Tips for learning statistics and machine learning concepts
---
room: 
---
day: 
---
start_time: 
---
track: Machine Learning & Deep Learning & Statistics
---
python_skill: Novice
---
domain_expertise: Novice
---
social_card_image: /static/media/social/talks/7D8VRW.png

