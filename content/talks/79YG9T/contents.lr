title: Detecting AI-Generated Text
---
created: 2025-01-05
---
code: 79YG9T
---
speaker_names: Nikita Kozodoi
---
abstract:

The rapid advancement of large language models (LLMs) has enabled the generation of highly coherent text, which is nearly indistinguishable from human-written content. While useful in many applications, LLMs can also be used for harmful purposes such as spreading disinformation, automating spam, and fostering plagiarism. This can negatively impact the reputation of companies and undermine the trust of users. To mitigate these risks, various detection methods have been proposed to determine whether a text is created by a machine or a human. 

In this talk, we will discuss differences between AI-generated and human-written texts. Next, we will overview state-of-the-art AI text detectors and discuss their differences, limitations, and empirical performance. We will analyze how detection performance changes depending on the text domain, information available to the detector, prompt complexity, and other factors. Finally, we will also touch on approaches to evade detectors, such as paraphrasing and spoofing.
---
full_description:

Recent advancements in natural language processing have led to the development of large language
models (LLMs) capable of generating highly coherent and contextually appropriate text. The rapid growth in the volume and quality of AI-generated content emphasizes the need for robust detection systems to prevent misuse of these technologies in dangerous applications such as misinformation propagation, phishing, and academic dishonesty. Accurately identifying AI-generated text is equally crucial in various business applications, including moderating online content, assessing the news trustworthiness and classifying product reviews.

The ability to accurately detect AI-generated text is especially critical for businesses that have a variety of customer-facing products where users can misuse LLMs. For instance, users of e-commerce businesses may suffer from product listings containing generic texts generated by a language model, e.g., «I cannot fulfill that request, it goes against our current policy». This makes listings look unprofessional and leads to poor user experience. Likewise, online product reviews have a high likelihood of being generated by an LLM, which undermines the trust for the platform and can be seen as irrelevant by customers. While some organizations can proactively ask users to disclose whether the AI-generated text was used in their content submission, many business use cases still lack trustworthiness and require additional moderation systems that could reliably identify such texts on a massive scale.

In this talk, we will focus on the topic of AI text detectors and discuss possible approaches and their performance in various settings. Our goal is to provide insights into the strengths and weaknesses of existing detectors and propose potential future directions to mitigate the misuse of LLMs in business applications. We will also talk about approaches to evade such detectors, such as paraphrasing and spoofing.
