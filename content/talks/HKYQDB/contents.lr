title: Langfuse, OpenLIT, and Phoenix: Observability for the GenAI Era
---
created: 2024-12-22
---
code: HKYQDB
---
speaker_names: Emanuele Fabbiani
---
abstract: Large Language Models (LLMs) are transforming digital products, but their non-deterministic behaviour challenges predictability and testing, making observability essential for quality and scalability.

This talk presents **observability for LLM-based applications**, spotlighting three tools: Langfuse, OpenLIT, and Phoenix. We'll share best practices about what and how to monitor LLM features and explore each tool's strengths and limitations. 

Langfuse excels in tracing and quality monitoring but lacks OpenTelemetry support and customization. OpenLIT, while less mature, integrates well with existing observability stacks using **OpenTelemetry**. Phoenix stands out in debugging and experimentation but struggles with real-time tracing.

The comparison will be enhanced by **live coding examples**.

Attendees will walk away with an improved understanding of observability for **GenAI applications** and will understand which tool to use for their use case.
