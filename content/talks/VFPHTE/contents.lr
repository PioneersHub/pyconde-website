title: Exploring LLM latency
---
created: 2024-12-22
---
code: VFPHTE
---
speaker_names: Pavel Kr√°l
---
abstract: Which LLM provider should you choose based on latency? 

As LLMs become integral to modern applications, speed can make or break your UX. This talk highlights the performance trade-offs of OpenAI, Anthropic, local models, and other providers, backed by real-world benchmarks that reveal how each stacks up in terms of response time and cost.
