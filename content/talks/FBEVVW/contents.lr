title: The Name’s Bot, Vulnerable Bot: AI Under Siege and why you should worry?
---
created: 2024-12-15
---
code: FBEVVW
---
speaker_names: Adarsh Kumar, Divyam Chandel
---
abstract: What if a well-crafted prompt could manipulate your AI system to act outside its intended boundaries? Or a subtle tweak could mislead it entirely? These aren’t just hypothetical scenarios; they are real threats called `Jailbreaking` or `Adversarial Attacks`, capable of compromising your projects, products, or systems.

In this talk, let’s dive deeper into these intriguing but concerning phenomena. You’ll learn:
How these attacks exploit vulnerabilities in LLMs.
Real-world examples of how they can bypass safeguards.
Some practical strategies to protect your AI systems from going rogue.

Whether you are a seasoned developer or just eager to understand the risks and resilience of Generative AI in a world that increasingly relies on it, this session is for you. 

**Because, let's be honest, no one wants their AI to go rogue, right?**
---
description: We will start with the basic introduction, while covering certain subtleties and practical examples of Adversities, we will move towards understanding Jailbreaking and what can be possibly done to safeguard against such attacks.

This talk will try to answer the following questions:

* What are Jailbreaking, Adversarial Prompting, and Prompt Injection in terms of LLMs?
* Why is it a concerning issue to you or anyone who works or uses LLMs?
      - Some example case studies and practical shortcomings of LLMs prone to JailBreaking 
* What are some types of Jailbreaking Techniques, with examples on Prompt Level and Token Level Jailbreaking
* What are some ways you can safeguard your systems against these attacks (with and without requiring retraining your models)?

By the end of this talk, listeners will have a good idea of risks involving LLMs and how adversities try to challenge the very purpose of LLM-based systems, compromising your projects or workflows, and also about approaches to safeguard against such attacks.
