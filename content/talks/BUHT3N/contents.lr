title: Strategies to Solve LLM Hallucinations
---
created: 2024-12-17
---
code: BUHT3N
---
speaker_names: Cheuk Ting Ho
---
abstract: Hallucinations have been a big pain in products that use LLM. Users expect high accuracy in answers and rely increasingly on LLM or AI assistants. Hallucinations would result from destroying the users' trust in real-life consequences. Solving it has been a high-priority task for AI researchers.
