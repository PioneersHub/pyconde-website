title: How to evaluate fairness and safety in LLM applications?
---
created: 2024-12-10
---
code: XEEBT9
---
speaker_names: Sebastian Krauss
---
abstract: Fairness and safety are fundamental criteria for building trustworthy and high-quality AI systems, whether they are credit scoring models, hiring assistants, or healthcare chatbots. But what does it truly mean for an AI system to be fair and safe? In this talk, I will explore the potential risks and challenges associated with these principles and introduce various approaches and techniques for evaluating AI systems. The discussion will center on applications powered by Large Language Models (LLMs), highlighting their unique considerations and implications.
---
description: Artificial intelligence holds immense promise to revolutionize industries and enhance lives, but its true potential hinges on trust. Fairness and safety are not just ethical ideals—they are critical pillars for the responsible development, deployment, and evaluation of AI systems. This is particularly important for applications powered by Large Language Models (LLMs), which are now shaping diverse fields like coaching, hiring, and financial services. However, ensuring these models meet rigorous fairness and safety standards presents complex challenges.

In this 30-minute talk, we will dive into five key areas:

1. Introduction to LLM-Based Applications:
We’ll begin with an accessible introduction to Large Language Models—avoiding technical deep dives - and showcase real-world examples of how LLMs are applied across industries.

2. Understanding Risks and Challenges:
LLMs, while powerful, come with vulnerabilities such as biases and the potential for harmful behaviors. Discriminatory or toxic outputs can infringe on human rights, damage reputations, and result in financial loss. Through real-world examples, I will highlight these risks and discuss why addressing them is crucial.

3. Defining Fairness and Safety in AI:
What does it mean for an AI system to be fair and safe? Here, I will outline the key criteria for trustworthy AI applications, offering a framework to evaluate their ethical and societal impact.

4. Evaluation Techniques and Tools:
From public benchmarks to innovative prompt designs and metrics, we’ll explore practical methods for assessing LLM trustworthiness. This section will empower participants to leverage the right tools to ensure their AI systems meet ethical and operational standards.

5. Building the Future: Strategies for Fair and Safe AI:
The final segment focuses on actionable strategies to design and deploy fair, safe, and compliant AI systems. I’ll also discuss the role of emerging regulations, such as the EU AI Act, in guiding responsible innovation.

This session offers valuable insights for researchers, developers, and decision-makers, equipping them to create AI systems that are not only high-performing but also aligned with societal values and evolving regulatory frameworks.
