title: Climate Data Standardization with Pymorize: A Case Study in Integrating Various PyData Libraries
---
created: 2024-12-06
---
code: EPDPCK
---
speaker_names: Paul Gierz
---
abstract:

Pymorize is designed for large-scale Earth System Modeling post-processing workflows, leveraging Prefect, Dask, and SLURM for orchestration. It handles big data with ease by paralleling tasks, caching results for fast retrieval, and monitoring SLURM jobs in real-time. Perfect for managing complex data operations such as multi-model output integration or regridding, Pymorize automates and streamlines the post-processing pipeline. It simplifies data handling and ensures efficient, scalable workflows for massive datasets, making your data flow seamlessly.
---
full_description:

# Climate Data Standardization with Pymorize: A Case Study in Integrating Various PyData Libraries
> Paul Gierz, Pavan Siligam, and Miguel Andres-Martinez
> Alfred Wegener Institute for Polar and Marine Research, Bremerhaven, Germany

The [PyData ecosystem](https://pydata.org) provides powerful tools for scientific research, yet combining these tools into workflows that handle massive datasets remains a formidable challenge. In climate and earth system modelling, where datasets often span terabytes, efficient data standardization is critical. This is where [`pymorize`](https://github.com/esm-tools/pymorize) comes in—a lightweight, open-source framework designed to streamline the standardization of scientific data into the widely-used [CMOR (Climate Model Output Rewriter) format](https://cmor.llnl.gov).

In this talk, we will showcase how `pymorize` bridges key PyData libraries, such as [`xarray`](https://xarray.dev), [`dask`](https://www.dask.org), along with several extensions such as as [`CF-Xarray`](https://cf-xarray.readthedocs.io/en/latest/), [`pint`](https://pint-xarray.readthedocs.io/en/stable/), and [`uxarray`](https://uxarray.readthedocs.io/en/latest/), together with the [Prefect workflow orchestrator](https://www.prefect.io) in order to simplify large-scale data processing pipelines. By leveraging SLURM for distributed task management and Dask for parallel computation, Pymorize ensures scalability on both local and high-performance computing environments. We will highlight how this modular system allows researchers to preprocess, validate, and transform data with ease, even under the constraints of limited memory or extreme data sizes.

Key takeaways will include:
* An overview of how Pymorize integrates tools like Pint and Xarray to handle multidimensional data and unit conversions.
* Practical approaches to parallelizing workflows for CMOR standardization using Dask and SLURM, orchestrated via Prefect.
* Real-world examples of efficiently handling datasets too large to fit in memory, while maintaining reproducibility and transparency.
* Lessons learned from developing scalable workflows in a research-driven environment.

This talk is aimed at anyone navigating the complexities of scientific data processing. Whether you’re a researcher, data engineer, or open-source enthusiast, join us to discover how Pymorize turns the PyData ecosystem into a robust solution for large-scale data standardization.

GitHub: https://github.com/esm-tools/pymorize
Documentation: https://pymorize.readthedocs.io
