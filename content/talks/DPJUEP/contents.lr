title: Learn Instructor for structured, robust and easy testable LLM interactions
---
created: 2024-12-22
---
code: DPJUEP
---
speaker_names: Felizia Quetscher
---
abstract:

How do you get structured, robust, easily testable outputs from an LLM? By using an (overly) complex prompt? Here is the better alternative: Instructor, a library for generating structured and robust outputs from your LLM.

In this workshop, you will explore the capabilities and advantages of the Instructor library based on a real-world example using the OpenAI API. This workshop aims to provide you with an overview of Instructor's useful tools to receive reliable, structured, robust, and easily testable LLM outputs. Through hands-on exercises, you will learn three tools of Instructor:

1. Function calling: Learn how OpenAI's function calling is simplified with instructor and how it helps creating a robust, structured LLM output. 

2. Validators: Learn to use validators for evaluating the output of an LLM and the advantage of automatically reprompting your model to provide the correct output.

3. Hooks: Explore Instructor's pre- and post-execution-hooks to evaluate the LLM's output.

After this workshop, you will have an overview of the possibilities provided by Instructor to generate structured, robust, and easily testable outputs from your LLM.
---
full_description:

Many LLM use cases in the "real world" require structured outputs like JSON, for example, extracting information from multiple sources. This is especially important for continuously testing and evaluating your LLM's output, before and during production. Unfortunately, the default output of LLMs is typically unstructured.

To receive structured responses, the library 'Instructor' offers tools to ensure structured outputs from your LLM and to evaluate them. In this workshop, you will learn about the Instructor library based on Pydantic and the OpenAI API for obtaining structured LLM outputs that are easily testable and improve the debugging handling of your LLM application. You will explore the advantages and capabilities of Instructor in a real scenario: using an OpenAI GPT model to extract and summarize data from a ticket system. Based on this use case, you will learn three main tools of Instructor through hands-on exercises:

1. Function calling: OpenAI's function calling allows LLMs to use user-defined functions to solve a task and thereby create a specific, structured output. In Instructor, OpenAI's function calling is combined with Pydantic. Instructor makes it easy to use function calling. Explore instructors features to simplify function calling, and the related advantages in creating robust, structured LLM outputs. 

2. Validators: Validators are provided by Instructor to evaluate the output of an LLM and reprompt it if the evaluation fails. You will learn how to create validators and examine specific use cases related to the ticket system. Furthermore, you will explore different types of validators and how they support you in creating a testable LLM system.

3. Hooks: Instructor provides pre- and post-execution hooks that are triggered at specific events. You will dive into the concept of pre- and post-execution hooks and explore their capabilities for evaluating the LLM's output. You will see how to implement these hooks for transparency of the LLM's output, ensuring easy logging and debugging.

After this workshop, you will have an overview of Instructor's tools, how they can help you let your LLM generate structured outputs, and how you can use Instructor for testing and evaluating your LLM's output.
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Generative AI
