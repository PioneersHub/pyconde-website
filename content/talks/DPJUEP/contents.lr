title: Learn Instructor for structured, robust and easy testable LLM interactions
---
created: 2024-12-22
---
code: DPJUEP
---
speaker_names: Felizia Quetscher
---
abstract: How do you get structured, robust, easily testable outputs from an LLM? By using an (overly) complex prompt? Here is the better alternative: Instructor, a library for generating structured and robust outputs from your LLM.

In this workshop, you will explore the capabilities and advantages of the Instructor library based on a real-world example using the OpenAI API. This workshop aims to provide you with an overview of Instructor's useful tools to receive reliable, structured, robust, and easily testable LLM outputs. Through hands-on exercises, you will learn three tools of Instructor:

1. Function calling: Learn how OpenAI's function calling is simplified with instructor and how it helps creating a robust, structured LLM output. 

2. Validators: Learn to use validators for evaluating the output of an LLM and the advantage of automatically reprompting your model to provide the correct output.

3. Hooks: Explore Instructor's pre- and post-execution-hooks to evaluate the LLM's output.

After this workshop, you will have an overview of the possibilities provided by Instructor to generate structured, robust, and easily testable outputs from your LLM.
