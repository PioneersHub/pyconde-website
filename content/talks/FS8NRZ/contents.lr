title: ML and LLM in production
---
created: 2024-12-22
---
code: FS8NRZ
---
speaker_names: Christian Barra
---
abstract: The complexity of bringing ML models into production went really out of the roof in the last few years. Now it’s not just deploying a model from your laptop to the cloud, but includes cars, IoT devices, mobile phones and custom hardware.

And LLMs require a completely new set of capabilities. To manage the cost of the inference, assess drifts and prevent your users from putting glue on Pizzas.
---
description: The job of data scientists and AI devs is pretty tough. It often starts from a paper with almost no runnable-code, or from an off-the-shelve model that has to be optimized or fine tuned.

But that’s not enough. Models might need to be trained or deployed on specific hardware (GPU, IoT and mobiles), and performance might drift over time.

During this talk I’ll share my experience of leading platform and AI service teams, how we tackled some of the major problems you might find along your AI journey like training, serving and monitoring ML models. 

We start from a data scientist’s laptop and talk on how to make her successful, covering both capabilities that you should provide and which tools might help.

The last part of the talk is dedicated to LLMs, how they are different from a serving and monitoring perspective compared to traditional ML models, and how to start using them in production.
