title: Inference with embeddings and convex optimization
---
created: 2024-12-22
---
code: PLYEUA
---
speaker_names: Ivan Herreros
---
abstract:

Since the ‘LLM Big Bang’ a couple of years ago, there has been a renewed interest in word embeddings, especially (though not exclusively) in retrieval-augmented generation tasks. Yet, most applications still rely on simple, pairwise embedding comparisons. In this talk, I will introduce a straightforward convex optimization approach that leverages the “meaning compositionality” property of embeddings to perform explainable inferences over sets of embeddings. This method goes beyond one-to-one similarity checks to determine, for example, whether one embedding is implied by a collection of others. Such inference capabilities can enhance re-ranking processes by removing redundancy, and they can also improve content-based recommendation systems. On a practical level, this technique is readily implementable using tools like the CVXPY library, which typically solves these problems with sub-second latency, even when dealing with hundreds of embeddings.
In sum, convex-optimization can provide NLP practitioners with an additional lightweight method to achieve explainable inferences in production systems.
---
full_description:

Using a furniture e-commerce site as a motivating example, this talk will illustrate why content-based recommendations sometimes require going beyond pairwise feature comparisons. I will demonstrate how mapping these flexible general feature comparisons to a convex optimization problem offers a powerful solution—one that can be readily implemented in Python using the well-known CVXPY library.

To help attendees fully grasp this approach, I will introduce the basics of embeddings and explain how meaning compositionality in embedding spaces allows for arithmetic reasoning. I will also provide a concise overview of convex optimization and how it can enhance the afore-mentioned reasoning.

Additionally, I will introduce a second use case of the same convex problem: finding non-redundant decompositions in retrieval-augmented generation (RAG) pipelines. By applying this method, we can ensure more diverse sets of retrieved information.

By the end of the talk, attendees will have a foundational understanding of how convex optimization can enhance NLP pipelines.
