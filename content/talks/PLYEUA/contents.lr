title: Inference with embeddings and convex optimization
---
created: 2024-12-22
---
code: PLYEUA
---
speaker_names: Ivan Herreros
---
abstract: Since the ‘LLM Big Bang’ a couple of years ago, there has been a renewed interest in word embeddings, especially (though not exclusively) in retrieval-augmented generation tasks. Yet, most applications still rely on simple, pairwise embedding comparisons. In this talk, I will introduce a straightforward convex optimization approach that leverages the “meaning compositionality” property of embeddings to perform explainable inferences over sets of embeddings. This method goes beyond one-to-one similarity checks to determine, for example, whether one embedding is implied by a collection of others. Such inference capabilities can enhance re-ranking processes by removing redundancy, and they can also improve content-based recommendation systems. On a practical level, this technique is readily implementable using tools like the CVXPY library, which typically solves these problems with sub-second latency, even when dealing with hundreds of embeddings.
In sum, convex-optimization can provide NLP practitioners with an additional lightweight method to achieve explainable inferences in production systems.
