title: Is Prompt Engineering Dead? How Auto-Optimization is Changing the Game
---
created: 2024-12-20
---
code: GURXPK
---
speaker_names: Iryna Kondrashchenko, Oleh Kostromin
---
speakers:


### Iryna Kondrashchenko

Iryna is a data scientist and co-founder of DataForce Solutions, a company specialized in delivering end-to-end data science and AI services. She contributes to several open-source libraries, and strongly believes that open-source products foster a more inclusive tech industry, equipping individuals and organizations with the necessary tools to innovate and compete.

### Oleh Kostromin

I am a Data Scientist primarily focused on Deep Learning and MLOps. In my spare time I contribute to several open-source python libraries.

---
abstract:

The rise of LLMs has elevated prompt engineering as a critical skill in the AI industry, but manual prompt tuning is often inefficient and model-specific. This talk explores various automatic prompt optimization approaches, ranging from simple ones like bootstrapped few-shot to more complex techniques such as MIPRO and TextGrad, and showcases their practical applications through frameworks like DSPy and AdalFlow. By exploring the benefits, challenges, and trade-offs of these approaches, the attendees will be able to answer the question: is prompt engineering dead, or has it just evolved?
---
full_description:

With the rise of LLMs, prompt engineering has become a highly impactful skill in the AI industry. However, manual prompt tuning is challenging, time-consuming, and not always generalizable across different models. This raises a reasonable question: can prompts be automatically learned from data? The answer is yes, and in this talk, we will explore how.

First, we will provide a high-level overview of various prompt optimization approaches, starting with a simple technique like bootstrapped few-shot, which automatically generates and selects an optimal set of demonstrations for each step in the LLM chain. Then, we will discuss more complex approaches, such as MIPRO and TextGrad, which directly optimize the instructions.

Afterwards, we will move on to a more practical part by showcasing how these techniques can be used via popular frameworks such as DSPy and AdalFlow.

Finally, we will discuss the benefits and trade-offs of these approaches and frameworks in terms of costs, complexity and performance, so the audience can decide whether prompt engineering is truly dead.

**Outline:**
* Introduction (2 min)
* Discussion of problems with manual prompt engineering (2 min)
* Overview of existing prompt optimization approaches (10 min):
    * Bootstrapped few-shot (3 min)
    * MIPRO (3 min)
    * TextGrad (4 min)
* Showcasing the prompt optimization frameworks (8 min):
    * DSPy (4 min)
    * AdalFlow (4 min)
* Comparison of methods and concluding remarks (3 min)
* Q&A (5 min)
---
room: Zeiss Plenary (Spectrum)
---
day: Wednesday
---
start_time: 17:50
---
track: Natural Language Processing & Audio (incl. Generative AI NLP)
---
python_skill: 
---
domain_expertise: 
---
social_card_image: /static/media/social/talks/GURXPK.png

