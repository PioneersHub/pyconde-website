title: Parallel and Distributed Data Processing with Dask
---
created: 2024-12-22
---
code: Z87QJX
---
speaker_names: Patrick Hoefler
---
abstract: Dask is a library for parallel and distributed computing with Python. It's just Python and thus doesn't need any heavy dependencies like Spark or similar tools do. Dask offers APIs for Arrays, DataFrames and general parallelism. We will explore how Dask DataFrame enables users to run distributed data processing queries with pandas syntax.
---
description: Dask is a library for parallel and distributed computing with Python. It's just Python and thus doesn't need any heavy dependencies like Spark or similar tools do. Dask offers APIs for Arrays, DataFrames and general parallelism. Dask integrates with most libraries in the PyData stack and enables us to scale these tools beyond a single machine.

We will focus on the DataFrame integration. Dask integrates seamlessly with pandas and started as a parallel pandas tool. It has since grown significantly beyond that. Recent developments in Dask that added a query optimizer to the interface allow us to process Terabytes of data right from our laptop. We will dive into the technical improvements and how those impact the user experience and performance of Dask DataFrames.

Dask is just Python. It supports arbitrary Python UDFs as first class citizens. We will illustrate how this feature makes Dask the perfect choice for dealing with messy data from the real world. Simple APIs give access to any data source that has a Python Connector.

We conclude with a brief comparison with Spark. Spark is the de factor standard for processing large scale tabular data. We will show how Dask fits into this environment and when Dask is a better and easier choice for users.
