title: 😲 Quantifying Surprise  -  A Data Scientist's Introduction To Information Theory
---
created: 2024-12-20
---
code: 7MRZ9K
---
speaker_names: Eyal Kazin
---
abstract: How do we measure information, and why does it matter for data scientists and machine learning practitioners?

This talk delves into the principles of Information Theory, a cornerstone of statistics and optimization, to show how quantifying information can enhance your statistical analyses and provide deeper insights into the mechanics of state-of-the-art machine learning algorithms.

Using intuitive examples—such as coin flips, dice rolls, and the famous Monty Hall problem—we’ll bridge foundational concepts with practical applications, including supervised classification and feature selection. I’ll also share a real-world case study from biotech 🧬 to demonstrate the practical impact of these tools.

Whether you're an experienced practitioner or just starting out, you’ll gain actionable knowledge, Python 🐍 code, and innovative ways to integrate Information Theory into your data science projects.

By the end of the talk you will gain an intuition for useful concept in Information Theory: 

* ***Shannon/Self-Information*** - the amount of information in an event
* ***Entropy*** - the average amount of information of a variable's outcome
* ***Cross-entropy*** - the misalignment between probability distributions (also described by its derivative KL-Divergence)
* ***Mutual Information*** - the co-dependency of two variables by their conditional probability distributions.
