title: ğŸ˜² Quantifying Surprise â€Š- â€ŠA Data Scientist's Introduction To Information Theory
---
created: 2024-12-20
---
code: 7MRZ9K
---
speaker_names: Eyal Kazin
---
abstract: How do we measure information, and why does it matter for data scientists and machine learning practitioners?

This talk delves into the principles of Information Theory, a cornerstone of statistics and optimization, to show how quantifying information can enhance your statistical analyses and provide deeper insights into the mechanics of state-of-the-art machine learning algorithms.

Using intuitive examplesâ€”such as coin flips, dice rolls, and the famous Monty Hall problemâ€”weâ€™ll bridge foundational concepts with practical applications, including supervised classification and feature selection. Iâ€™ll also share a real-world case study from biotech ğŸ§¬ to demonstrate the practical impact of these tools.

Whether you're an experienced practitioner or just starting out, youâ€™ll gain actionable knowledge, Python ğŸ code, and innovative ways to integrate Information Theory into your data science projects.

By the end of the talk you will gain an intuition for useful concept in Information Theory: 

* ***Shannon/Self-Information***â€Š-â€Šthe amount of information in an event
* ***Entropy***â€Š-â€Šthe average amount of information of a variable's outcome
* ***Cross-entropy*** -â€Šthe misalignment between probability distributions (also described by its derivative KL-Divergence)
* ***Mutual Information***â€Š-â€Šthe co-dependency of two variables by their conditional probability distributions.
---
description: In his 1948 seminal paper Claude Elwood Shannon addressed an age old question: how does one quantify information? His insights pioneered fields involving information: quantification, storage and communications. Insights made major contributions to fields such as Economics, Statistical Physics and Computer Science.

The focus of this talk is on the quantification of information as an essential tool for a data scientist's repertoire.  I will show its utility both for statistical analyses as well as understanding the inner-workings of many state of the art machine learning algorithms.

I demonstrate using common statistics such as coin and dice ğŸ² tosses as well as machine learning applications such as in supervised classification and feature selection. I will provide a case study in a biotech ğŸ§¬. For fun I also apply to the popular brain twister called the Monty Hall problem ğŸšªğŸšª ğŸ.

All examples will be shared in a Jupyter notebook with python code ğŸ.
