title: 😲 Quantifying Surprise  -  A Data Scientist's Introduction To Information Theory
---
created: 2024-12-20
---
code: 7MRZ9K
---
speaker_names: Eyal Kazin
---
abstract: How do we measure information, and why does it matter for data scientists and machine learning practitioners?

This talk delves into the principles of Information Theory, a cornerstone of statistics and optimization, to show how quantifying information can enhance your statistical analyses and provide deeper insights into the mechanics of state-of-the-art machine learning algorithms.

Using intuitive examples—such as coin flips, dice rolls, and the famous Monty Hall problem—we’ll bridge foundational concepts with practical applications, including supervised classification and feature selection. I’ll also share a real-world case study from biotech 🧬 to demonstrate the practical impact of these tools.

Whether you're an experienced practitioner or just starting out, you’ll gain actionable knowledge, Python 🐍 code, and innovative ways to integrate Information Theory into your data science projects.

By the end of the talk you will gain an intuition for useful concept in Information Theory: 

* ***Shannon/Self-Information*** - the amount of information in an event
* ***Entropy*** - the average amount of information of a variable's outcome
* ***Cross-entropy*** - the misalignment between probability distributions (also described by its derivative KL-Divergence)
* ***Mutual Information*** - the co-dependency of two variables by their conditional probability distributions.
---
description: In his 1948 seminal paper Claude Elwood Shannon addressed an age old question: how does one quantify information? His insights pioneered fields involving information: quantification, storage and communications. Insights made major contributions to fields such as Economics, Statistical Physics and Computer Science.

The focus of this talk is on the quantification of information as an essential tool for a data scientist's repertoire.  I will show its utility both for statistical analyses as well as understanding the inner-workings of many state of the art machine learning algorithms.

I demonstrate using common statistics such as coin and dice 🎲 tosses as well as machine learning applications such as in supervised classification and feature selection. I will provide a case study in a biotech 🧬. For fun I also apply to the popular brain twister called the Monty Hall problem 🚪🚪 🐐.

All examples will be shared in a Jupyter notebook with python code 🐍.
