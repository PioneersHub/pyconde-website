title: ğŸ˜² Quantifying Surprise â€Š- â€ŠA Data Scientist's Introduction To Information Theory
---
created: 2024-12-20
---
code: 7MRZ9K
---
speaker_names: Eyal Kazin
---
abstract: How do we measure information, and why does it matter for data scientists and machine learning practitioners?

This talk delves into the principles of Information Theory, a cornerstone of statistics and optimization, to show how quantifying information can enhance your statistical analyses and provide deeper insights into the mechanics of state-of-the-art machine learning algorithms.

Using intuitive examplesâ€”such as coin flips, dice rolls, and the famous Monty Hall problemâ€”weâ€™ll bridge foundational concepts with practical applications, including supervised classification and feature selection. Iâ€™ll also share a real-world case study from biotech ğŸ§¬ to demonstrate the practical impact of these tools.

Whether you're an experienced practitioner or just starting out, youâ€™ll gain actionable knowledge, Python ğŸ code, and innovative ways to integrate Information Theory into your data science projects.

By the end of the talk you will gain an intuition for useful concept in Information Theory: 

* ***Shannon/Self-Information***â€Š-â€Šthe amount of information in an event
* ***Entropy***â€Š-â€Šthe average amount of information of a variable's outcome
* ***Cross-entropy*** -â€Šthe misalignment between probability distributions (also described by its derivative KL-Divergence)
* ***Mutual Information***â€Š-â€Šthe co-dependency of two variables by their conditional probability distributions.
