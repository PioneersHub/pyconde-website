title: Structuring Unstructured Text using generative AI: The key to information extraction
---
created: 2024-12-03
---
code: 7WJEFT
---
speaker_names: Oren Matar
---
abstract: How can generative AI transform natural language into structured formats, like converting free-form queries into SQL, while maintaining accuracy and consistency? This session tackles the challenges of using transformer-based models for structured extraction and introduces best practices for building fine-tuning datasets and leveraging Constrained Generative AI to eliminate syntactic errors. We will also explore preprocessing, data generation, and augmentation techniques that achieved near-perfect accuracy in extracting dates and times from text. The framework presented is versatile and applicable to extracting any structured data from unstructured text, empowering attendees to create more robust NLP solutions.
---
description: Extracting structured information from natural language is a fundamental task in many NLP applications—whether it's converting natural queries to SQL, performing Named Entity Recognition (NER), extracting entity relationships for knowledge graphs, and more. While transformer-based generative AI models, such as GPT, show great promise in these areas, they often face challenges like format inconsistency and difficulty handling the diverse ways humans express themselves in language.
In this session, we’ll explore these challenges in depth and present actionable solutions to overcome them. We’ll introduce Constrained Generative AI, a powerful technique designed to minimize syntactic errors in structured outputs, ensuring that the generated content adheres to predefined formats. Additionally, we’ll cover key pre-processing steps and data generation/augmentation strategies that helped us achieve near-perfect accuracy in a real-world case of extracting dates and times from textual data—while still maintaining low latency with relatively small language models.
By the end of this session, attendees will have practical knowledge on how to leverage transformer models for structured output tasks, reduce errors, and enhance performance. You’ll walk away with proven best practices and techniques that can be applied to boost the reliability and efficiency of your NLP applications.
