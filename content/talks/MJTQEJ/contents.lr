title: Mastering the Hex: A Case Study in Reinforcement Learning for Strategy Games
---
created: 2025-12-21
---
code: MJTQEJ
---
speaker_names: Simon Hedrich
---
speakers:


### Simon Hedrich

Simon Hedrich is a computer scientist and AI enthusiast currently completing his Master’s degree in Computer Science. His academic and professional journey is marked by a deep interest in bridging the gap between theoretical research and practical AI engineering.

Through his work at inovex GmbH, Simon has demonstrated expertise in specialized areas of Artificial Intelligence, including computer vision and the use of synthetic data to enhance small object detection. His technical writing highlights his ability to leverage generative AI models, such as Stable Diffusion, to solve complex real-world challenges like training data scarcity.

---
abstract:

How do you teach a computer to understand the nuance of a territorial strategy game? Over the last year, driven by private interest and a Master’s degree in Computer Science, I set out to build an autonomous agent for Antiyoy—a minimalist yet strategically deep hexagonal game. This talk takes you through the entire journey, from the first line of environment code to a trained model capable of complex strategic play.
The talk addresses a fundamental problem in modern AI: how do we translate complex, turn-based rules into a language a neural network can interpret? We will explore the architecture of a custom Reinforcement Learning environment, the mathematical elegance required to handle hexagonal coordinates, and the engineering behind a massive action space of over 17,000 possibilities. You will hear how we guided the agent through the "sparse reward" problem and what happened when a model was finally allowed to play against its own mistakes. Whether you are interested in the mechanics of PyTorch and PettingZoo or simply curious about the "brain" of a strategy bot, this session provides a practical roadmap for tackling high-dimensional problems in Python.
---
full_description:

This work began as a personal challenge to bridge the gap between a childhood love for strategy games and a fascination with deep learning. Developed over the course of a year within the context of a Master’s in Computer Science, the talk presents a methodological case study on creating a sophisticated game bot from scratch.
Strategy games like Antiyoy present a unique hurdle for standard Reinforcement Learning: they aren't just about quick reflexes; they require long-term economic planning and spatial reasoning on a grid where every cell has six neighbors instead of four. The talk answers the question of how to move from a raw game engine to a standardized Python environment that stable learning algorithms can actually use.
The presentation is divided into three thematic explorations:
**Part 1: How do we translate a world of hexagons into a language of tensors?**
The first challenge of any RL project is the interface. The talk describes the process of wrapping game logic into a Gymnasium-compatible environment. You will hear about the design decisions behind a 91-channel observation space—essentially "images" that encode everything from unit positions to economic health. We will also address the "spiral" mathematics of hexagonal grids: how do we efficiently map 2D coordinates to a discrete action space that a model can output?
**Part 2: How can a model learn to navigate 17,000 possible choices?**
With an action space significantly larger than that of Chess, a random agent will almost never find a winning move by accident. The talk explores how we use "action masking" to prevent the model from wasting time on illegal moves and how the architecture of the neural network—splitting its focus between predicting the "value" of a board and the "policy" of a move—allows it to develop a sense of strategy. We will discuss the trade-offs between lightweight models and deep residual networks when working with limited hardware.
**Part 3: What does emergent strategy look like in a trained agent?**
The final part of the talk focuses on the training process itself. How do we provide feedback to an agent when the only thing that truly matters is winning or losing thirty minutes later? You will hear about "hybrid reward shaping"—a way to give the agent small hints about territory and economy without distracting it from the ultimate goal. The talk concludes with an analysis of the results: identifying the moment the agent stopped making random moves and started demonstrating recognizable human-like tactics, as well as the limitations we discovered along the way.
---
room: 
---
day: 
---
start_time: 
---
track: Machine Learning & Deep Learning & Statistics
---
python_skill: Intermediate
---
domain_expertise: Intermediate
---
social_card_image: /static/media/social/talks/MJTQEJ.png

