title: Efficient Embedding Generation using ONNX models in Rust
---
created: 2025-01-05
---
code: RJSV9J
---
speaker_names: Habeeb Shopeju, Diretnan Domnan
---
abstract:

Embeddings are now a popular representation for finding similar items from a collection, but how do you generate them using optimized ONNX models? 

This talk answers that question. Through it, you will learn how to access and load up ONNX models via the Hugging Face Hub. You will also see how to use them to convert multi-modal input (text, images) into embeddings using the ort crate.

In the end, you will walk away with knowledge that equips you with the ability to cut down the time it takes to go from input to embeddings for your self-hosted models. If you do not have Rust knowledge, the ideas are also easily transferrable to Python, as ONNX enables accessibility across multiple languages.
---
full_description:

Embeddings are now a popular representation for finding similar items from a collection, but how do you generate them using optimized ONNX models? 

This talk answers that question. Through it, you will learn how to access and load up ONNX models via the Hugging Face Hub. You will also see how to use them to convert multi-modal input (text, images) into embeddings using the ort crate.

A proposed structure for this talk is:

- Introducing the ONNX format: Here you will learn about what ONNX models are and why the format is widely accepted in the machine learning industry. You will also briefly see how to generate ONNX models from other models, say PyTorch or TensorFlow.

- Accessing model weights and model artifacts: Hugging Face has become an incredible enabler of machine learning work in industry and academia. This part of the talk goes into how to download model weights and other essential artifacts from the Hugging Face Hub using the hf_hub crate.

- Understanding the model artifacts: There are a couple of useful files needed for properly running inference with the downloaded model weights. You will learn about the contents of those files and understand how they help the model yield the best results.

- Using the ONNX Runtime: In this section, you will learn how to run inference via the ONNX Runtime using the ort crate. This section will cover some of the setup required to load ONNX models and then convert the preprocessed inputs into embeddings.

- Tying it all together through the lens of a real-world application: Ahnlich-ai is an open-source tool written in Rust and built to aid semantic search. It helps generate embeddings from text, images, and audio inputs using self-hosted models. At its foundation are the core ideas already discussed in the previous sections. Here you will learn how Ahnlich-ai puts together the various ideas to serve the purposes it does today.

In the end, you will walk away with knowledge that equips you with the ability to cut down the time it takes to go from input to embeddings for your self-hosted models. If you do not have Rust knowledge, the ideas are also easily transferrable to Python, as ONNX enables accessibility across multiple languages.
---
room: 
---
day: 
---
start_time: 
---
track: General: Rust
