title: Use Instructor for structured, robust and easy testable LLM interactions
---
created: 2024-12-22
---
code: VSC9NC
---
speaker_names: Felizia Quetscher
---
abstract:

How do you get structured, robust, easily testable outputs from an LLM? By using an (overly) complex prompt? Here is the better alternative: Instructor, a library for generating structured and robust outputs from your LLM.

In this talk, we will explore the capabilities and advantages of the Instructor library based on a real-world example using the OpenAI API. This talk aims to provide you with an overview of Instructor's useful tools to receive reliable, structured, robust, and easily testable LLM outputs. We will explore three tools of Instructor:
1. We will see how to use validators for evaluating the output of an LLM. With these validators, we can reprompt your model to encourage it to provide the correct output.
2. We will dive into the function calling capabilities that come with the Instructor library. With function calling, you ensure a structured LLM output using a Pydantic model.
3. We will explore Instructor's pre- and post-exectution hooks to evaluate the LLM's output.

After this talk, you will have an overview of the possibilities provided by Instructor to generate structured, robust, and easily testable outputs from your LLM.
---
full_description:

Many LLM use cases in the "real world" require structured outputs like JSON, for example, extracting information from multiple sources. This is especially important for continuously testing and evaluating your LLM's output, before and during production. Unfortunately, the default output of LLMs is typically unstructured.

To receive structured responses, the library 'Instructor' offers tools to ensure structured outputs from your LLM and to evaluate them. In this talk, we will explore the Instructor library based on Pydantic and the OpenAI API for obtaining structured LLM outputs that are easily testable and improve the debugging handling of our LLM application. We will explore the advantages and capabilities of Instructor in a real scenario: using an OpenAI GPT model to extract and summarize data from a ticket system. Based on this use case, we will learn about three main tools of Instructor: 

1. Function calling: OpenAI's function calling allows LLMs to use user-defined functions to solve a task and thereby create a specific, structured output. In instructor, OpenAI's function calling is combined with Pydantic. Instructor makes it easy to use function calling. Explore Instructors features to simplify function calling, and the related advantages in creating robust, structured LLM outputs. 

2. Validators: Validators are provided by Instructor to evaluate the output of an LLM and reprompt it if the evaluation fails. We will see how to create validators and examine specific use cases related to the ticket system. Furthermore, we will explore different types of validators and how they support us in creating a testable LLM system.

3. Hooks: Instructor provides pre- and post-execution hooks that are triggered at specific events. We will dive into the concept of pre- and post-execution hooks and explore their capabilities for evaluating the LLM's output. We will see how to implement these hooks for transparency of the LLM's output, ensuring easy logging and debugging.

After this talk, you will have an overview of Instructor's tools, how they can help you let your LLM generate structured outputs, and how you can use Instructor for testing and evaluating your LLM's output.
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Generative AI
