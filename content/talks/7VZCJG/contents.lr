title: Safeguarding ML: Secure usage of model files, pre-trained models and ML packages
---
created: 2024-12-20
---
code: 7VZCJG
---
speaker_names: Andreas Wieltsch
---
abstract: As Machine Learning (ML) and Artificial Intelligence (AI) models become increasingly critical to various industries and applications, the importance of securing the entire ML lifecycle is crucial. This talk focuses on the often overlooked aspects of ML security, specifically the vulnerabilities associated with loading and using model files, pre-trained models and ML Python packages. We discuss the risks and consequences of malicious attacks on these components, including model poisoning and package manipulation.
Our presentation covers a range of security measures to mitigate these threats, including:
1.	Secure model file formats and verification techniques to detect tampering and ensure authenticity, especially for securely loading and using pre-trained models.
2.	Simple package managers and dependency management strategies to prevent malicious package injection.
3.	Continuous scanning of model artefacts in the model registry to detect problematic pre-trained models.
We also share best practices for implementing these security measures and workflows when using popular ML frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn. 
By highlighting the importance of security in ML and providing actionable guidance, our goal is to enable ML practitioners in the industry to build secure and robust ML systems providing business value.
