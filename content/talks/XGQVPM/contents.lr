title: Rust powered Inference, Ingestion and Indexing
---
created: 2024-12-22
---
code: XGQVPM
---
speaker_names: Sonam Pankaj
---
abstract:

EmbedAnything is an opensource library that uses rust in the backend to process all your files, run any kind of embedding models, like Bert, Jina, ColPali or Sparse models. It also have a effective, memory efficient technique to index it to vector database, called vector streaming.
---
full_description:

Vector streaming, is a powerful method in which we break the main process like chunking and parsing is done on the other process and inference of the embedding model is run on the other, and they communicate through rust's powerful MPSC channel. EmbedAnything also supports inferencing on GPUs because rust's and candle hardware acceleration is extremely adaptable. 

Apart from that, running ColPali and sparse models is also easy with this library called EmbedAnything. 

EmbedAnything is a minimalist, highly performant, lightning-fast, lightweight, multisource, multimodal, and local embedding pipeline built in Rust. Whether you're working with text, images, audio, PDFs, websites, or other media, EmbedAnything streamlines the process of generating embeddings from various sources and seamlessly streaming (memory-efficient-indexing) them to a vector database. It supports dense, sparse, ONNX and late-interaction embeddings, offering flexibility for a wide range of use cases.
---
room: 
---
day: 
---
start_time: 
---
track: General: Rust
