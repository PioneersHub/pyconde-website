title: Parallelization: Blessing and Curse
---
created: 2024-12-22
---
code: DLQEXC
---
speaker_names: Carsten Frommhold
---
abstract:

Developing machine learning features on big data demands sophisticated parallelization strategies, fundamentally impacting how we approach both modeling and engineering. This talk explores the technical challenges that arise when feature engineering requires distributed computing frameworks, blurring traditional boundaries between ML modeling and engineering roles. Through a retail case study, we examine practical solutions in parallel data processing, including architectural decisions around class granularity and data structures. Drawing from project experiences, we discuss how parallelization requirements influence feature design, model development, and production deployment, demonstrating why success in big data ML projects requires deep collaboration between modeling and engineering teams.
---
full_description:

In this talk, we discuss the challenges of developing machine learning features on big data that require a parallelization and scaling framework. 

To begin with, we take a brief look at the roles involved in the development of a ML model through to production. In theory, a feature store is developed under the leadership of one or more data engineers. The data scientist takes care of the logic, creates feature ideas, tests and develops a model and then takes the next steps together with the ML engineer. But is this separation that simple to make?  

With large amounts of data, parallelization frameworks are generally used not only for preprocessing but also for feature engineering. During the training phase alone, it may not be sufficient or even possible to keep feature ideas and mathematical modeling within a lab environment. This is where the roles in a project meet and overlap. 

Next, we will take a brief look at the common parallelization and scaling tools, including the frameworks offered by the common cloud providers. We will learn how these can support us in efficient data processing. 

Then we get more specific. What can a Pure Python data model to be processed in parallel look like in practice? We will use an example of point of sale and article level time series in retail to illustrate this and learn about the advantages of nested structures. Further questions quickly arise: At what granularity should the classes be cut? What do we gain from it? What do we lose by doing so regarding the underlying processing cluster? Instead of shuffling within a cluster, so-called side inputs can help.  

The consideration of the later model application in production, i.e. the use of a feature store also for the creation of an inference data set, places further demand on the data models.  

I share project-experiences made on the arising trade-offs and challenges and how we can tackle those. We will conclude that parallelization is a way of thinking that needs to be internalized by all project members. The technical solution will be unique to each project. But there is one thing that AI projects on big data have in common: you can only be successful as a team.
