title: From Training to Deployment: A scalable Pipeline for ML Model Inference with Celery and HuggingFace
---
created: 2024-12-16
---
code: DFZQK7
---
speaker_names: Rahkakavee Baskaran, Jan Dix
---
abstract: You've trained your machine learning model - now it's time to deploy it and use it for inference at scale to process larger amounts of data.
This talk will introduce a scalable and modular pipeline designed to perform inference on millions of job postings integrating multiple Machine Learning Models. Built with Celery and Flower, it manages tasks such as document processing, inference, cloud storage, and database integration. The pipeline, built with open-source tools, supports both CPU and GPU inference leveraging tools such as Modal and Hugging Face. It is designed to be modular and customizable for different use cases. Learn how you can build such a pipeline to efficiently scale your Machine Learning Inference workflows and why we opted for Celery as a lightweight, flexible solution over other orchestration tools like SageMaker, or Airflow.
---
description: Extracting insights from a few thousand data points with a machine learning model is usually simple, but when scaling to millions of data points or daily extractions, the process becomes much more complex. It demands robust infrastructure, systematic monitoring, efficient workflows, and strategies to tackle long processing times.

In this talk, we present a flexible and scalable pipeline designed to perform inference on millions of job postings integrating multiple Machine Learning Models. The pipeline is built entirely with open-source tools, while being theoretically scaleable across different machines like cloud or on-premise. Built with Celery and Flower, the pipeline is capable of orchestrate the entire extraction workflow, including document processing, inference, cloud storage, result verification, and database integration. This approach is demonstrated with a practical use case: historical and daily data extractions of job postings.

The architecture supports both CPU and GPU inference, with GPU tasks handled via Modal, a serverless cloud solution. The Models are hosted on Hugging Face. Although we presenting this pipeline with these specific Tools, the pipeline has a modular structure and enables the integration and replacement of alternative hosting solutions or technologies. This makes it adaptable to other stacks and use cases.

We will demonstrate how to construct such a pipeline, share our best practices and experiences, and highlight why we chose Celery as our orchestration tool. We will discuss Celeryâ€™s scalability and flexibility, and how it addresses challenges such as handling sensitive data and optimizing resource use. Whether you're deploying ML models in production or scaling inference workflows, this talk provides actionable insights and practical strategies to elevate your machine learning projects.
