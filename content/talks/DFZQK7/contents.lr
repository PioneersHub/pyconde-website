title: From Training to Deployment: A scalable Pipeline for ML Model Inference with Celery and HuggingFace
---
created: 2024-12-16
---
code: DFZQK7
---
speaker_names: Rahkakavee Baskaran, Jan Dix
---
abstract: You've trained your machine learning model - now it's time to deploy it and use it for inference at scale to process larger amounts of data.
This talk will introduce a scalable and modular pipeline designed to perform inference on millions of job postings integrating multiple Machine Learning Models. Built with Celery and Flower, it manages tasks such as document processing, inference, cloud storage, and database integration. The pipeline, built with open-source tools, supports both CPU and GPU inference leveraging tools such as Modal and Hugging Face. It is designed to be modular and customizable for different use cases. Learn how you can build such a pipeline to efficiently scale your Machine Learning Inference workflows and why we opted for Celery as a lightweight, flexible solution over other orchestration tools like SageMaker, or Airflow.
