title: How to hack an agent - or not.
---
created: 2024-12-20
---
code: 9TLCBF
---
speaker_names: Thomas Fraunholz
---
abstract:

Large Language Models are not safe. And it's not because of their ability to hallucinate while working. LLMs can be manipulated. So far, the known security mechanisms have not really proven to be solid. For example, if you use jailbreaks correctly, you can get an LLM to do things it is not supposed to do. But hey, what's stopping us from giving an LLM a little email summary? Nothing. Unless you connect it to an agent that can also send emails. Which brings us to the issue: Can an AI agent be secured where even an LLM cannot be secured? We want to approach this question by looking at state-of-the-art security concepts, from catching LLM task drift with activations, to prompt shields, spotlighting, and instruction hierarchy. Is my AI agent safe? Let's find out!
---
full_description:

Large language models (LLMs) are not safe. And it's not just because they can “hallucinate” when processing tasks. LLMs can be manipulated. So far, the known security mechanisms have not proven to be solid. For example, if you use so-called “jailbreaks” correctly, you can get an LLM to do things it’s not supposed to do. But what’s to stop us from simply giving an LLM a short email summary? Nothing—unless you connect it to an agent that can also send emails. And therein lies the problem: can an AI agent be made secure when even an LLM is not secure?

In this talk, we will examine the current ecosystem surrounding the security of AI agents. How do you prevent an AI agent from performing a task it is not supposed to perform? As an example, we will consider the current “Capture the Flag” competition “Adaptive Prompt Injection: LLMail Inject,” which is part of the 3rd IEEE Conference on Secure and Trustworthy Machine Learning. This competition challenges participants to determine whether they can get an AI agent to send an email with certain content, even though the user is only asking for a summary.

Specifically, it addresses the vulnerabilities of the models Microsoft’s Phi3 and OpenAI’s GPT4-o-mini. Depending on the scenario, these models are protected by mechanisms such as a so-called LLM judge, task drift, prompt shields, spotlighting, or instruction hierarchy. All these procedures and models are also used in the Azure cloud and can be transferred to on-premises environments using open-source solutions. In 2025, this topic will become more urgent because, starting in August, the EU AI Act will require that general-purpose AI—including LLMs—be secured by state-of-the-art security concepts. This provides even more reason to examine the methods currently available and learn about their advantages and disadvantages.

Which leaves us with the question: Is my AI agent secure? Let’s find out!
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Natural Language Processing & Audio (incl. Generative AI NLP)
