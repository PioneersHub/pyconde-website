title: How to hack an agent - or not.
---
created: 2024-12-20
---
code: 9TLCBF
---
speaker_names: Thomas Fraunholz
---
abstract: Large Language Models are not safe. And it's not because of their ability to hallucinate while working. LLMs can be manipulated. So far, the known security mechanisms have not really proven to be solid. For example, if you use jailbreaks correctly, you can get an LLM to do things it is not supposed to do. But hey, what's stopping us from giving an LLM a little email summary? Nothing. Unless you connect it to an agent that can also send emails. Which brings us to the issue: Can an AI agent be secured where even an LLM cannot be secured? We want to approach this question by looking at state-of-the-art security concepts, from catching LLM task drift with activations, to prompt shields, spotlighting, and instruction hierarchy. Is my AI agent safe? Let's find out!
