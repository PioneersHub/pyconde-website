title: XAI in Cybersecurity: Understanding the "Why" behind the AI data-driven decisions
---
created: 2024-12-26
---
code: MVYJPG
---
speaker_names: Rashmi Nagpal
---
abstract:

As reported by Crowdstrike, cloud intrusions rose to 75% YoY. As GenAI tools are used for cyber activities, it’s hard to trust them. This session delves into the critical need for explainability in AI-driven security using real-world use cases to enable faster incident response and enhanced trust.
---
full_description:

84% of company codebases contained at least one vulnerability in the open-source software used, and 74% contained high-risk vulnerabilities, according to Synopsys’s ninth edition of the annual “Open Source Security and Risk Analysis” report. As organizations increasingly rely on AI models to identify and manage such risks in their security systems, a critical question arises: how can we be sure these systems are making the right decisions, especially when the stakes are high? Imagine your AI system flags a critical vulnerability in your code. But why? What specific patterns triggered the alert or ensured that other potentially more critical vulnerabilities haven’t been overlooked? Traditional AI models in security often operate as ‘black boxes,’ providing alerts without clear explanations. This lack of transparency hinders incident response, makes it difficult to fine-tune systems, and erodes trust in AI-driven security solutions. Without understanding the “why,” can we truly trust the system to remediate the issue? In this talk, using explainable AI techniques, let us comprehend the “why” of these “black box model decisions” via real-world use cases. Additionally, we will learn how to build transparent, trustworthy, and resilient AI-driven security systems.
---
room: 
---
day: 
---
start_time: 
---
track: General: Ethics & Privacy
