title: Harnessing LLMs to Craft a Curated Python Code Dataset for Robust Clone Detection
---
created: 2024-12-04
---
code: VTHJFB
---
speaker_names: Yenatfanta Shifferaw
---
abstract: In the dynamic world of software development, Python's rising popularity brings with it the challenge of code duplication, commonly known as code clones. This work addresses the urgent need for a robust dataset specifically crafted for Python code clone detection. Our method involves extracting Python files from GitHub and GitLab repositories under the categories of data science, web development, and system administration. We prioritize repositories with recent development, active contributions, and high star and fork counts to ensure a diverse and high-quality dataset.

Once categorized, we measure the cyclomatic complexity of all the code to understand their intricacies. We also assess the granularity level of each script. Large language models (LLMs) are then employed to generate type-1, 2, 3, and 4 code clones. These clones are validated to ensure they reflect real-world coding practices accurately.

Once the dataset is developed according to the above procedure, we attach metadata containing information on the code category, its complexity level, granularity level, and type of clone. This comprehensive dataset aims to enhance the performance of code clone detection tools and provides developers and researchers with valuable resources to tackle Python code duplication. Ultimately, our work strives to improve software quality and maintainability, fostering a deeper understanding of Python's coding landscape.
