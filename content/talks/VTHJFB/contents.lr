title: Harnessing LLMs to Craft a Curated Python Code Dataset for Robust Clone Detection
---
created: 2024-12-04
---
code: VTHJFB
---
speaker_names: Yenatfanta Shifferaw
---
abstract:

In the dynamic world of software development, Python's rising popularity brings with it the challenge of code duplication, commonly known as code clones. This work addresses the urgent need for a robust dataset specifically crafted for Python code clone detection. Our method involves extracting Python files from GitHub and GitLab repositories under the categories of data science, web development, and system administration. We prioritize repositories with recent development, active contributions, and high star and fork counts to ensure a diverse and high-quality dataset.

Once categorized, we measure the cyclomatic complexity of all the code to understand their intricacies. We also assess the granularity level of each script. Large language models (LLMs) are then employed to generate type-1, 2, 3, and 4 code clones. These clones are validated to ensure they reflect real-world coding practices accurately.

Once the dataset is developed according to the above procedure, we attach metadata containing information on the code category, its complexity level, granularity level, and type of clone. This comprehensive dataset aims to enhance the performance of code clone detection tools and provides developers and researchers with valuable resources to tackle Python code duplication. Ultimately, our work strives to improve software quality and maintainability, fostering a deeper understanding of Python's coding landscape.
---
full_description:

In the ever-evolving landscape of software development, Python stands out for its simplicity and versatility. However, its popularity brings the challenge of code duplication, known as code clones. This tutorial addresses the urgent need for a specialized dataset designed for Python code clone detection, essential for maintaining code quality and software maintainability.

We begin with a detailed methodology for extracting Python files from popular platforms like GitHub and GitLab. We focus on actively maintained repositories with recent activity, high star ratings, and significant fork counts to ensure a diverse and current dataset. This dataset reflects various domains, including data science, web development, and system administration.

Next, we use cyclomatic complexity to assess the structural intricacies of the scripts, providing crucial insights for clone detection. We also evaluate the granularity level of each script to determine the appropriate detection level, whether at the function, statement, or other relevant levels.

To further enrich the dataset, we employ large language models (LLMs) to generate type-1, type-2, type-3, and type-4 code clones. These synthetic clones are meticulously validated to mirror real-world coding practices, capturing both semantic and syntactic variations. Each Python script is accompanied by metadata detailing its category, complexity level, granularity level, and clone type, providing valuable context for training and evaluating clone detection tools.

The primary objective of this tutorial is to demonstrate effective techniques for enhancing code clone detection tools. By providing developers and researchers with a curated dataset that showcases a variety of coding styles and complexities, we aim to improve their ability to identify and address Python code duplication. This tutorial is designed to advance software engineering practices through practical, hands-on learning.

In summary, we introduce an innovative approach to building a robust dataset for Python code clone detection, highlighting the importance of diversity, complexity measurement, and leveraging LLMs. By filling the gaps in existing datasets and methodologies, this tutorial aspires to push the boundaries of code clone detection and elevate the standards of software development.
