title: Everything, all the time, right now! An introduction to stream processing
---
created: 2024-12-22
---
code: KJKFNB
---
speaker_names: Damion Werner
---
abstract:

Processing large volumes of data at low latency is key to enabling real-time and predictive products, but with millions of data points and complex processing requirements, traditional batched data pipelines quickly develop into hour-long dredges and when results are needed in a timely fashion using more resources and running these pipelines at a higher frequency is a pathway to exploding cost and diminishing returns.
Where traditional batch processing approaches just don't cut it anymore, stream processing is the answer. In this talk you'll get an overview of what stream processing is, when to use it, important techniques and frameworks, Python code examples, as well as some tips, caveats and best practices gathered from hard-earned real world experience.
By the end you'll be equipped with all the knowledge you need to start writing streaming pipelines of your own, enabling you to deliver data and insights at sub-second latency.
---
full_description:

The goal of this talk is giving you all the expertise you need to start writing streaming data pipelines and programs, both in terms of concepts and practical coding knowledge.

We will start with a quick introduction on what stream processing is, when it is useful, when it is not, and how it compares against traditional databases and batch processing paradigms.

Using code examples we will learn how to write some basic and then more complex streaming pipelines, introducing the most important stream processing techniques and concepts such as "stateful processing", "event time" and "out-of-orderness".

Next we will explore the limitations and complications of streaming pipelines when compared to more traditional processing models and how we can work around them, like dealing with incomplete data and non-replayable datasources

We will do a brief excursion into the niche (but immensely useful) topic of _distributed_ stream processing before getting to the hard-learned experience from several years of developing and operating business critical streaming pipelines at the worlds largest aviation company. This covers topics like debugging pipelines, making them reliable and how to reduce maintenance effort with fancy greek-letter architectures which make you sound smart at tech conferences.

Finally you will get an ecosystem overview with the most important stream processing frameworks (Python and others) as well as related technologies so that by the end of the talk you are equipped to architect and develop streaming pipelines, a surprisingly uncommon yet incredibly useful skill.
---
room: 
---
day: 
---
start_time: 
---
track: PyData: Data Handling & Engineering
