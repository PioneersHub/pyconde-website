title: Why Did The Model Do That? Debugging the Ghost in the Machine
---
created: 2025-12-17
---
code: MLUK9M
---
speaker_names: Cosima Meyer
---
speakers:


### Cosima Meyer

Cosima Meyer is a data scientist with a strong focus on making machine learning models explainable and accessible. Passionate about trustworthy AI, she is committed to building systems that are not only technically robust but also transparent and ethical. As a Google's Women Techmakers Ambassador and an active member of PyLadies, Cosima is dedicated to fostering inclusive and collaborative communities, working to bridge the two groups and create spaces for knowledge-sharing and growth.

During her PhD studies at the University of Mannheim, Cosima discovered her enthusiasm for sharing knowledge through technical blog posts and developing open-source software. Her work reflects a blend of technical expertise and a passion for community building, inspiring others to explore, learn, and contribute to the fields of AI and data science.

---
abstract:

Why did the model say "No"? In an era where machine learning models increasingly influence high-stake decisions, "trust me" isn't a sufficient explanation. Yet, the logic behind many model decisions remains a black box, often hiding bias and making it difficult to establish trust.

In this talk, we move beyond the mystery of the "ghost in the machine" and into practical debugging using a structured *XAI Decision Tree*. Instead of guessing which method to use, we will walk through a logical framework that narrows down the field based on a few critical questions: the type of data you have, the level of model access available, and whether you need to explain a single prediction or the entire system.

The audience will leave with a clear path to choosing the right explainable AI (XAI) method - such as SHAP, LIME, or Integrated Gradients - and the corresponding Python framework for their specific use case.

This session will cover:

- Importance of XAI: Understanding why XAI is crucial using a real-world example
- XAI Landscape: An overview of existing XAI methods and how they are related
- XAI Decision Tree: How to use the structured XAI decision tree to choose the right explanation method for your use case
- Local vs global: A common understanding of local vs global explainability
- XAI in Practice: A demo showcasing XAI in practice as well as corresponding Python frameworks to use
---
full_description:

My planned outline for the talk is as follows:

- **Intro and opening hook** (4 mins): A look at a clearly biased model and why "black box" decisions fail to establish trust
- **The XAI Decision Tree** (17 mins):

    * A practical overview of the landscape and walking through the tree: Selecting the right method based on your model and data
     * Mapping these methods to specific Python libraries and frameworks (e.g., `shap`, `lime`, `captum`, `transformers-interpret`, `alibi`, `dalex`, ...)

- **Closing and Take-away** (4 mins)
- **Q&A and Buffer** (5 mins)
---
room: 
---
day: 
---
start_time: 
---
track: Ethics & Privacy
---
python_skill: Intermediate
---
domain_expertise: Intermediate
---
social_card_image: /static/media/social/talks/MLUK9M.png

