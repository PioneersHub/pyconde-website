title: Foundational Models for Time Series Forecasting: are we there yet?
---
created: 2024-12-22
---
code: EGZZBA
---
speaker_names: Luca Baggi
---
abstract: Transformers have changed fields like NLP, Computer Vision, sound generation, and even protein-folding. But what about forecasting? This talk will explain how titans like Amazon and Google built a recipe for Foundational Models for Time Series Forecasting.

Most importantly, though, during the talk we will survey the state of the art of open datasets to train such models, and offer practical elements to assess the benchmarks that have been put out.
---
description: This is a pragmatic talk for Generative AI and ML practitioners alike, eager to understand how researchers managed to **adapt transformer-based architectures to forecasting problems**, as well as how to **evaluate new models for specific use-cases**.

The first part of the talk explains how researchers **discretised time series into a finite "dictionary"** and the **theoretical limitations** of this approach, such as how to deal with different sampling frequencies during training.

We will then make a parallel with Large Language Models (LLMs) scaling laws to evaluate the data strategies used to train such universal forecasting models. In other words, we will ask the question **whether we have enough publicly available time-series data to train foundational models**, and how this can **affect such models' evaluation**.

Finally, we will display how we attempted to **benchmark those models against a robust baseline of models**, and where our experiment is compared to the publicly available results.

No specific prior knowledge is required for attendance. Though forecasting practitioners stand to gain the most from this talk, every practitioner in machine learning or generative AI can follow along and draw the key conclusions.

**Outline**
Minutes 1-3. Problem statement: challenges in adapting transformers to the time-series domain.
Minutes 3-10. How Chronos and Moirai are implemented.
Minutes 10-15. Do we have enough data to train universal forecasters?
Minutes 15-25. Lessons learnt from evaluating transformer-based models for a specific use-case.
Minutes 25-30. Wrap up and Q&A.
