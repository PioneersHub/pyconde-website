title: Accelerating privacy-enhancing data processing
---
created: 2024-12-20
---
code: LDNYWY
---
speaker_names: Florian Stefan, Anna Schwarz
---
abstract:

Our mission is simple but profound: to improve and extend lives by learning from the experience of every person with cancer. Achieving this requires seamless feedback loops between scientists, engineers, and clinicians working with sensitive data across heterogeneous environments.

You might expect a story about how we tried and failed. And yes, we’ll share some failures and surprises along the way. But this is, at its core, a success story - because it works.

In this talk, we’ll dive into the data architecture and core technologies that enable us to learn from every patient’s journey with cancer. We’ll reveal how our cross-functional teams - scientists, engineers, and clinicians - collaborate to transform raw data into research-grade datasets. Along the way, we’ll share the challenges we faced, the lessons we learned, and the tools we developed to ensure the high data quality required for groundbreaking research.

We’ll also show how we leverage domain knowledge, data science, and established technologies to create tools that maintain this quality and accelerate feedback loops. By shifting insights generation left, we empower teams to iterate faster and drive more impactful outcomes.

This talk is ideal for anyone with data warehouse or lakehouse experience curious about harnessing the Pythonic data stack to iteratively generate insights from sensitive data in highly regulated environments.
---
full_description:

In this talk, we’ll begin by introducing the concept of real-world evidence datasets and their transformative impact on cancer research. We’ll explore the significant challenges of building high-quality real-world evidence datasets, including the fragmented healthcare data landscape, the complexity of source data, and stringent regulatory constraints.

To address these challenges, we’ll introduce our privacy-enhancing data architecture and foundational technology stack, which includes AWS, Snowflake, Python, DLT, DBT, Pandas, and DuckDB. We’ll explain how we leverage these tools to establish critical feedback loops and accelerate actionable insights. Key topics include:

- Integrating notebooks with production pipelines to bridge the gap between clinical domain expertise and production workflows.
- Employing a write-audit-publish pattern to ensure continuous data quality through unit tests, data tests, regression tests, and cohort-spanning statistical tests.
- Using DLT to move normalization steps closer to the data source, unlocking iterative data modeling and enabling more agile development cycles.

One of our most impactful innovations was shifting data investigations to the left. Previously, raw data inspections were delayed until pre-processed data was available in our data warehouse, causing prolonged feedback loops and inefficiencies. Disorganized file formats often required data scientists to manually inspect data, limiting their ability to draw meaningful, cohort-wide conclusions.

To overcome these challenges, we established local databases directly on compute instances where raw data is stored, leveraging the flexibility and transparency of this approach. Transformations developed within this setup can seamlessly transfer to the cloud data warehouse while remaining portable and adaptable to various environments. This enabled our data scientists to explore and understand the data earlier in the process, eliminating bottlenecks. With immediate access to organized datasets, our team could quickly assess incoming data, provide actionable feedback, and begin building analytical models.

This proactive, iterative approach not only streamlined workflows but also fostered deeper collaboration between data engineers and data scientists, ultimately accelerating the generation of high-quality insights for cancer research.
